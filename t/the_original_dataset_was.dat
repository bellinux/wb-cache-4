13|10000|Public
50|$|In March 2008, the Journal of Pediatrics {{released}} {{a study by}} Harvard University and the Boston Children's Hospital's Center on Media and Child Health showing that television viewing is, “neither beneficial nor deleterious to child cognitive and language abilities” for children under 2, in a study that examined all television rather than just education DVDs for babies. In January 2010, the founders requested that a judge order the University of Washington to release records for the 2007 study, saying, “Given that other research studies have not shown the same outcomes, we would like the raw data and analytical methods from the Washington studies so we can audit their methodology, and perhaps duplicate the studies". In 2013 <b>the</b> <b>original</b> <b>dataset</b> <b>was</b> reanalyzed by independent scholars who concluded that it was safest to suggest that baby videos had minimal impact on language development and that linking baby videos to decreased language development was not well supported by the data.|$|E
30|$|The ClueWeb dataset is a semi-synthetic dataset {{generated}} from the ClueWeb Category B dataset. <b>The</b> <b>original</b> <b>dataset</b> <b>was</b> very small. Thus, we randomly generated new pages each with 10 outgoing links, and added them to the original dataset graph structure to make it larger. The total size of the new dataset is 30 GB.|$|E
3000|$|... [...]. Findings, {{using the}} T-test {{statistics}} (t[*]=[*] 0.3915;[*]P(|T|[*]>[*]|t|)[*]=[*] 0.6955) {{showed that there}} was no significant difference between the original and imputed wind speed datasets. Thereby failing to reject the null hypothesis that presupposed mean wind speed of <b>the</b> <b>original</b> <b>dataset</b> <b>was</b> equal to the mean wind speed of the imputed wind speed dataset. These findings confirm the high level of reliability for the imputation method applied in this study.|$|E
5000|$|For comparison, the {{arithmetic}} mean of <b>the</b> <b>original</b> <b>dataset</b> <b>is</b> ...|$|R
30|$|<b>The</b> <b>original</b> <b>datasets</b> <b>were</b> {{published}} by <b>the</b> Guizhou Provincial Bureau of Statistics. These <b>datasets</b> <b>are</b> not applicable in a Web database, {{but they are}} available from the corresponding author on reasonable request.|$|R
30|$|<b>The</b> <b>original</b> <b>dataset</b> will <b>be</b> made {{available}} to reviewers upon request.|$|R
30|$|For this {{analysis}} <b>the</b> <b>original</b> <b>dataset</b> <b>was</b> imported into SPSS (Statistical Package for the Social Sciences) version 19.0. 0 {{and kept in}} a secure place to ensure confidentiality. Data cleaning was performed before commencing the analysis. Attained growth Z-scores < − 6 and > 6 as well as 3 -month velocity Z-scores in weight and length of < 10 and > − 10 were defined as implausible and set as missing. Of a total of 46694 measurements 69 values for WAZ, LAZ, WLZ and MUAZ and eight values for weight and length velocity Z-scores were set as missing.|$|E
40|$|International audienceIn {{order to}} {{evaluate}} the validity of prediction tools for two-phase flow pressure drops for conditions of high saturation temperatures, this paper focuses on the comparison between new experimental results and theoretical results predicted with the commonly used methods. <b>The</b> <b>original</b> <b>dataset</b> <b>was</b> obtained in a horizontal 3. 00 mm inner diameter during adiabatic flow with R- 245 fa as working fluid. The mass velocity ranges from 100 to 1500 kg/ m 2 s, the saturation temperature varies from 60 to 120 °C and the inlet vapor quality from 0 to 1. The database is composed of 249 data points covering four flow patterns: (i) intermittent flow, (ii) annular flow, (iii) dryout flow, and (iv) mist flow regimes. The dataset is compared against 23 well-known two-phase frictional pressure drop prediction methods. The effect of the saturation temperature and of the flow pattern {{on the ability of}} the methods to predict the frictional pressure drop was pointed out...|$|E
40|$|With the aim to {{evaluate}} the correctness of medical and surgical procedures, RAND Corporation and University of California Los Angeles (UCLA) developed the RAND/UCLA Appropriateness Method (RAM). In this study, the RAM was applied {{to evaluate}} the appropriateness of a dataset concerning kidney/pancreas transplantation in adult recipients for an information system funded by the Italian Ministry of Health. <b>The</b> <b>original</b> <b>dataset</b> <b>was</b> obtained using an interdisciplinary pool of experts (n= 60) involved in kidney/pancreas transplantation activity in the Liguria Region. This dataset held 291 items, stratified as pretransplantation items (n= 158), transplantation items (n= 49), and early posttransplantation and follow-up items (n= 84). In the second round, the dataset was subjected to an extraregional panel of independent experts (n= 9) to assess each item using a score ranging from 1 to 9 based on increasing appropriateness. The expert-opinion process returned a whole mean score of 8. 47 +/- 0. 43 (95 % confidence interval [CI] 8. 30 - 8. 63). Overall agreement, uncertainty, and disagreement between experts about item appropriateness were 98. 5 %, 1. 49 %, and 0 %, respectively. Agreement/uncertainty for pretransplantation, transplantation, and posttransplantation items were 99. 87 %/ 0. 12 %, 100 %/ 0 %, and 96. 37 %/ 3. 62 %, respectively. This study supported the utility of a structured expert-opinion process as an effective strategy {{to evaluate the}} appropriateness of large datasets for kidney/pancreas transplantation in adult recipients...|$|E
30|$|The BigCross <b>dataset</b> <b>is</b> a semi-synthetic <b>dataset</b> {{generated}} from the real-world BigCross dataset. Since the total size of <b>the</b> <b>original</b> <b>dataset</b> <b>is</b> 1.6 GB, we make it larger by repeating <b>the</b> <b>original</b> <b>dataset</b> nine times. <b>The</b> generated <b>dataset</b> <b>is</b> 16  GB. We randomly pick 60 points from the whole dataset and use them as initial centers.|$|R
5000|$|In {{this section}} {{we show that}} it is {{possible}} to release a <b>dataset</b> which <b>is</b> useful for concepts from a polynomial VC-Dimension class and at the same time adhere to -differential privacy as long as the size of <b>the</b> <b>original</b> <b>dataset</b> <b>is</b> at least polynomial on the VC-Dimension of the concept class. To state formally: ...|$|R
40|$|AbstractThe main {{scope of}} the paper is {{performing}} appropriate kriging interpolation of the diabetes prevalence data coming from the Pavia (Italy) Local Health Care Agency (ASL). <b>The</b> <b>original</b> <b>dataset</b> <b>is</b> analyzed, <b>the</b> Bayesian regularization is evaluated, which is applied by other authors and finally prevalence data are simulated by means of random fields, in order to tune and evaluate kriging interpolation...|$|R
40|$|The Delphi Method (DM) is {{the most}} {{frequently}} used technique to acquire structured expert-opinion elicitation (EOE). It has been increasingly applied to construct guidelines in medicine and to evaluate the appropriateness of clinical procedures. In this study, the RAND/UCLA appropriateness method was used as a structured EOE process to evaluate the appropriateness of a dataset concerning liver transplantation in adult and pediatric recipients for an information system funded by the Italian Ministry of Health. <b>The</b> <b>original</b> <b>dataset</b> <b>was</b> obtained using an interdisciplinary pool of regional experts (n = 60). This dataset held 280 items stratified into three groups: I. pretransplant items (n = 123); II. transplant items (n = 65); III. early posttransplant and follow-up items (n = 92). In the second DM round, the dataset was subjected to an extraregional panel of independent experts (n = 9) to assess a score ranging from 1 to 9 on each item based on increasing appropriateness, according to the RAND/UCLA Appropriateness Method. Overall agreement, uncertainty, and disagreement between experts was 95. 89 %, 3. 12 %, and 0. 99 %, respectively. For each group, agreement-uncertainty-disagreement were 99. 35 %/ 0. 65 %/ 0 % (group I), 91. 53 %/ 5. 30 %/ 3. 17 % (group II), and 96. 87 %/ 3. 13 %/ 0 % (group III), respectively. This study supported the use of a structured EOE process to evaluate the appropriateness of a large dataset for liver transplantation activity...|$|E
40|$|Abstract Background Genome-wide {{approaches}} to analyze {{single nucleotide polymorphism}} (SNP) data have proliferated due to the increased availability and affordability of markers, but in practice {{a small number of}} markers may be selected from sets that do not approach dense genome-wide coverage. This study focused on a genome-wide approach to identify markers useful to a breeding program using a Bayesian method to estimate effects for markers distributed across the genome at varied densities. A simulated dataset containing 4665 individual phenotypes for a quantitative trait and genotypes for 6000 SNPs spaced in 0. 1 cM increments across six chromosomes was analyzed using a Bayesian approach in which effects for all single markers are simultaneously estimated. The dataset was also analyzed with marker densities reduced to 0. 5, 1. 0, 2. 0 and 5. 0 cM. Type I errors were not a major concern but replications of each analysis were performed to determine acceptance of estimated marker effects. Results The Bayesian analysis of <b>the</b> <b>original</b> <b>dataset</b> <b>was</b> able to estimate genetic values for markers in a small number of regions while shrinking other marker effects to zero. Analysis of the reduced density datasets also showed clear signals in a small number of regions where some effects appeared to be distributed across multiple markers. Replicates of the analyses provided evidence for regions with moderate and large effects. Conclusion A Bayesian multiple marker approach appears to be suitable for predicting genetic values, even with reduced density datasets where large numbers of markers are not yet available for many species. These predicted genetic values can be implemented in marker assisted selection programs. </p...|$|E
40|$|In the mid- 1980 s, RAND Corporation and University of California Los Angeles (UCLA) {{developed}} the RAND/UCLA Appropriateness Method (RAM) {{to evaluate the}} correctness of medical and surgical procedures. In this study, the RAND/UCLA Appropriateness Method {{was used to evaluate}} the appropriateness of a dataset concerning kidney transplantation in adult and pediatric recipients for an information system funded by the Italian Ministry of Health. <b>The</b> <b>original</b> <b>dataset</b> <b>was</b> obtained using an interdisciplinary pool of regional experts (n= 60). This dataset held 514 items about kidney transplantation in adult (n= 268) and pediatric (n= 246) recipients. The items were stratified as 3 main groups: pretransplantation items (adult, n= 141; pediatric, n= 122), transplantation items (adult, n= 49; pediatric, n= 45), and early posttransplantation and follow-up items (adult, n= 78; pediatric, n= 79). In the second round, the dataset was subjected to an extraregional panel of independent experts (n= 9) to assess each item using a score ranging from 1 to 9 based on increasing appropriateness. The expert-opinion process returned for adult and pediatric kidney recipient items whole mean scores of 8. 52 +/- 0. 32 and 8. 65 +/- 0. 32, respectively. Overall agreement, uncertainty, and disagreement between experts about item appropriateness concerning adult kidney recipients were 94. 6 %, 5. 4 %, and 0 %, respectively. For pediatric kidney recipients, overall agreement, uncertainty, and disagreement between experts about item appropriateness were 96. 9 %, 2. 35 %, and 0. 07 %, respectively. This study supported the use of a structured expert-opinion process as an effective strategy to evaluate the appropriateness of large datasets for kidney transplantation in both adult and pediatric recipients...|$|E
25|$|Suppose <b>the</b> <b>original</b> <b>dataset</b> D {{contains}} <b>the</b> n spectra in rows. The {{signals of}} <b>the</b> <b>original</b> <b>dataset</b> <b>are</b> generally preprocessed. <b>The</b> <b>original</b> spectra are {{compared to a}} reference spectrum. By subtracting a reference spectrum, often the average spectrum of the dataset, so called dynamic spectra are calculated which form the corresponding dynamic dataset E. The presence and interpretation may be dependent on the choice of reference spectrum. The equations below are valid for equally spaced measurements of the perturbation.|$|R
40|$|Assimilation of {{remotely}} sensed ocean data (velocity, temperature, and salinity) into {{numerical model}} {{is of great}} importance in oceanic and climatic research. However, the data should be reconstructed (onto grids) before assimilation since <b>the</b> <b>original</b> <b>datasets</b> <b>are</b> usually noisy and sparse. This paper describes a recently developed optimal spectral decomposition (OSD) method for mapping and noise filtration with examples of reconstructing {{the data from the}} Argo profiling and trajectories, Ocean Surface Current Analyses – Real time (OSCAR), shore-based high...|$|R
5000|$|In the VBS {{structure}} {{used in a}} CDBMS, each unique {{value is}} stored once and given an abstract (numeric) identifier, regardless {{of the number of}} occurrences or locations in <b>the</b> <b>original</b> data set. <b>The</b> <b>original</b> <b>dataset</b> <b>is</b> then constructed by referencing those logical identifiers. The correlation index may resemble the storage below. Note that the value [...] "MN" [...] which occurs multiple times in the data above is only included once. As the amount of repeat data grows, this benefit multiplies.|$|R
30|$|In {{the first}} place, it is {{important}} to emphasize the reason why a single cardiac and respiratory PET frame, obtained by double gating <b>the</b> <b>original</b> <b>dataset,</b> <b>was</b> considered here for PVC instead of a motion-corrected PET dataset (using all available data). Several methods to obtain motion fields with which the PET dataset could be corrected for motion have been proposed in the past: using a dynamic CT acquisition [30], via estimation of the motion fields directly from the PET dataset [5, 6] or using motion fields derived from a truly simultaneous PET/MR acquisition [7]. In the case of a shorter scan, for example, using a single cardiac gate from a study with motion correction would increase the fraction of the counts that are used to create the motion-free image. However, the additional radiation burden associated with the CT acquisition, the possibly inaccurate and non-trivial motion estimation from the PET dataset and the non-availability of truly-simultaneous PET/MR devices in most of the PET centres, made us opt for simulating the worst-case scenario where a simple double-gating pass was applied to the PET dataset. In this way, all PET datasets are equally noisy and are equally unaffected by possibly inaccurate motion-correction issues, and a fair comparison of the MR-based and CT-based PVC could be performed. In addition, the results obtained in the simulated, worst-case scenario are also representative for the case where the cardiac uptake was limited, or if the PET scan time was further reduced to optimize the clinical workflow or to limit gross patient motion during the scan [31]. We believe that most of the findings of this work would also apply to reconstructed PET datasets with better statistics and/or accurately corrected for motion.|$|E
40|$|The current gold {{standard}} for diagnosis of attention deficit/hyperactivity disorder (ADHD) includes subjective measures, such as clinical interview, observation, and rating scales. The significant heterogeneity of ADHD symptoms represents a challenge for this assessment and could prevent an accurate diagnosis. The aim of this work {{was to investigate the}} ability of a multi-domain profile of measures, including blood fatty acid (FA) profiles, neuropsychological measures, and functional measures from near-infrared spectroscopy (fNIRS), to correctly recognize school-aged children with ADHD. To answer this question, we elaborated a supervised machine-learning method to accurately discriminate 22 children with ADHD from 22 children with typical development by means of the proposed profile of measures. To assess the performance of our classifier, we adopted a nested 10 -fold cross validation, where <b>the</b> <b>original</b> <b>dataset</b> <b>was</b> split into 10 subsets of equal size, which were used repeatedly for training and testing. Each subset was used once for performance validation. Our method reached a maximum diagnostic accuracy of 81 % through the combining of the predictive models trained on neuropsychological, FA profiles, and deoxygenated-hemoglobin features. With respect to the analysis of a single-domain dataset per time, the most discriminant neuropsychological features were measures of vigilance, focused and sustained attention, and cognitive flexibility; the most discriminating blood FAs were linoleic acid and the total amount of polyunsaturated fatty acids. Finally, with respect to the fNIRS data, we found a significant advantage of the deoxygenated-hemoglobin over the oxygenated-hemoglobin data in terms of predictive accuracy. These preliminary findings show the feasibility and applicability of our machine-learning method in correctly identifying children with ADHD based on multi-domain data. The present machine-learning classification approach might be helpful for supporting the clinical practice of diagnosing ADHD, even fostering a computer-aided diagnosis perspective...|$|E
40|$|The geometry-dependent {{functioning}} of the meniscus indicates that detailed knowledge on 3 D meniscus geometry and its inter-subject variation is essential to design well functioning anatomically shaped meniscus replacements. Therefore, {{the aim of this}} study was to quantify 3 D meniscus geometry and to determine whether variation in medial meniscus geometry is size- or shape-driven. Also we performed a cluster analysis to identify distinct morphological groups of medial menisci and assessed whether meniscal geometry is gender-dependent. A statistical shape model was created, containing the meniscus geometries of 35 subjects (20 females, 15 males) that were obtained from MR images. A principal component analysis was performed to determine the most important modes of geometry variation and the characteristic changes per principal component were evaluated. Each meniscus from <b>the</b> <b>original</b> <b>dataset</b> <b>was</b> then reconstructed as a linear combination of principal components. This allowed the comparison of male and female menisci, and a cluster analysis to determine distinct morphological meniscus groups. Of the variation in medial meniscus geometry, 53. 8 % was found to be due to primarily size-related differences and 29. 6 % due to shape differences. Shape changes were most prominent in the cross-sectional plane, rather than in the transverse plane. Significant differences between male and female menisci were only found for principal component 1, which predominantly reflected size differences. The cluster analysis resulted in four clusters, yet these clusters represented two statistically different meniscal shapes, as differences between cluster 1, 2 and 4 were only present for principal component 1. This study illustrates that differences in meniscal geometry cannot be explained by scaling only, but that different meniscal shapes can be distinguished. Functional analysis, e. g. through finite element modeling, is required to assess whether these distinct shapes actually influence the biomechanical performance of the meniscus...|$|E
40|$|Objective: To {{examine the}} sources of coding {{discrepancy}} for injury morbidity data and explore {{the implications of these}} sources for injury surveillance. [...] Method: An on-site medical record review and recoding study was conducted for 4373 injury-related hospital admissions across Australia. Codes from <b>the</b> <b>original</b> <b>dataset</b> <b>were</b> compared to <b>the</b> recoded data to explore the reliability of coded data aand sources of discrepancy. [...] Results: The most common reason for differences in coding overall was assigning the case to a different external cause category with 8. 5...|$|R
40|$|Abstract-The {{temperature}} and Pressure sensors {{play a vital}} role in Nuclear Power Plants (NPP). The Rosemount temperature sensor helps to produce the exact {{temperature and}} pressure measurement of the nuclear power plant. The sensors that supply real data must respond quickly to the safety systems of NPP. In this paper, first the Dimensionality of <b>the</b> <b>Original</b> <b>dataset</b> <b>is</b> reduced by using Principal Component Analysis (PCA), Independent Component Analysis (ICA) and Singular Value Decomposition (SVD). Finally the sensors Response Time is computed and compared with original response time...|$|R
40|$|International audienceThe {{characters}} used in taxonomy {{to describe}} new species cannot always {{be used to}} identify species in population surveys involving large samples. We used DNA barcoding to validate the taxonomic status of the morphospecies used in an ecological study involving 11 000 individual African drosophilids which had been determined without dissection. Some taxonomic information had been lost by not discriminating between rare species or by mistakenly splitting a morphologically variable species into two groups. However, <b>the</b> <b>original</b> ecological <b>dataset</b> provided a reliable picture of species diversity and the conclusions based on <b>the</b> <b>original</b> <b>dataset</b> <b>are</b> still supported by the molecular data...|$|R
40|$|Background: Mental {{well-being}} is an important, yet understudied, area of research, {{partly due}} to lack of appropriate population-based measures. The Warwick-Edinburgh Mental Well-being Scale (WEMWBS) was developed to meet the needs for such a measure. This article assesses the psychometric properties of the Norwegian version of the WEMWBS, and its short-version (SWEMWBS) among a sample of primary health care patients who participated in the evaluation of Prompt Mental Health Care (PMHC), a novel Norwegian mental health care program aimed to increase access to treatment for anxiety and depression. Methods: Forward and back-translations were conducted, and 1168 patients filled out an electronic survey including the WEMWBS, and other mental health scales. <b>The</b> <b>original</b> <b>dataset</b> <b>was</b> randomly divided into a training sample (≈ 70 %) and a validation sample (≈ 30 %). Parallel analysis and confirmatory factor analysis were carried out to assess construct validity and precision. The final models were cross-validated in the validation sample by specifying a model with fixed parameters based on the estimates from the trainings set. Criterion validity and measurement invariance of the (S) WEMWBS were examined as well. Results: Support was found for the single factor hypothesis in both scales, but similar to previous studies, only after a number of residuals were allowed to correlate (WEMWBS: CFI = 0. 99; RMSEA = 0. 06, SWEMWBS: CFI =. 99; RMSEA = 0. 06). Further analyses showed that the correlated residuals did not alter the meaning of the underlying construct and did not substantially affect the associations with other variables. Precision was high for both versions of the WEMWBS (>. 80), and scalar measurement invariance was obtained for gender and age group. The final measurement models displayed adequate fit statistics in the validation sample as well. Correlations with other mental health scales were largely in line with expectations. No statistically significant differences were found in mean latent (S) WEMWBS scores for age and gender. Conclusion: Both WEMWBS scales appear to be valid and precise instruments to measure mental well-being in primary health care patients. The results encourage the use of mental well-being as an outcome in future epidemiological, clinical, and evaluation studies, and may as such be valuable for both research and public health practice. </p...|$|E
2500|$|The {{solution}} of the system for a high resolution <b>dataset</b> would <b>be</b> computationally very expensive. In this case, <b>the</b> <b>original</b> <b>dataset</b> can <b>be</b> sub-sampled and ( [...] ) can be solved over a more limited set of data.|$|R
40|$|Abstract. We {{present a}} wikifier {{evaluation}} framework consisting of software support and two datasets (News and Tweets), which <b>were</b> derived from <b>datasets</b> previously published at WEKEX 2011 and MSM Challenge 2013. Entities recognized in <b>the</b> <b>original</b> <b>datasets</b> <b>were</b> enriched with new annotations – {{a link to}} Wikipedia and the most specific type from the DBpedia Ontology. The annotations were created by two annotators and a judge. The <b>datasets</b> <b>are</b> supplemented by plugins for their import to the GATE NLP framework and a DBpedia Ontology-aware plugin for aligning annotations created by a wikifier with the ground truth...|$|R
40|$|Proceedings on IEEE International Geoscience & Remote Sensing Symposium, DVD-ROM,Assimilation of {{remotely}} sensed ocean data (velocity, temperature, and salinity) into {{numerical model}} {{is of great}} importance in oceanic and climatic research. However, the data should be reconstructed (onto grids) before assimilation since <b>the</b> <b>original</b> <b>datasets</b> <b>are</b> usually noisy and sparse. This paper describes a recently developed optimal spectral decomposition (OSD) method for mapping and noise filtration with examples of reconstructing {{the data from the}} Argo profiling and trajectories, Ocean Surface Current Analyses – Real time (OSCAR), shore-based high-frequency (HF) Doppler radar (CODAR) and Global Temperature-Salinity Profile Program (GTSPP) ...|$|R
40|$|Abstract. In this {{document}} {{we describe the}} Amsterdam Museum Linked Open Data set. The <b>dataset</b> <b>is</b> a five-star Linked Data representation and comprises the entire collection of the Amsterdam Museum consisting of more than 70, 000 object descriptions. Furthermore, the institution’s thesaurus and person authority files used in the object metadata {{are included in the}} Linked Data set. The data is mapped to the Europeana Data Model, utilizing Dublin Core, SKOS, RDA-group 2 elements and the OAI-ORE model to represent the museum data. Vocabulary concepts are mapped to GeoNames and DBpedia. The two main contributions of this <b>dataset</b> <b>are</b> the inclusion of internal vocabularies {{and the fact that the}} complexity of <b>the</b> <b>original</b> <b>dataset</b> <b>is</b> retained...|$|R
40|$|In {{this paper}} we {{introduce}} a new cost function called Information Theoretic Mean Shift algorithm to capture the “predominant structure ” in the data. We formulate this problem with a cost function which minimizes the entropy of the data subject to the constraint that the Cauchy-Schwartz distance between the new and <b>the</b> <b>original</b> <b>dataset</b> <b>is</b> fixed to some constant value. We show that Gaussian Mean Shift and the Gaussian Blurring Mean Shift are special cases of this generalized algorithm giving a whole new perspective {{to the idea of}} mean shift. Further this algorithm {{can also be used to}} capture the principal curve of the data making it ubiquitous for manifold learning. 1...|$|R
40|$|Existing graph mining {{algorithms}} typically {{assume that}} the dataset can fit into main memory. As many large graph datasets cannot satisfy this condition, truly scalable graph mining remains a challenging computational problem. In this paper, we present a new horizontal data partitioning framework for graph mining. <b>The</b> <b>original</b> <b>dataset</b> <b>is</b> divided into fragments, then each fragment is mined individually {{and the results are}} combined together to generate a global result. One of the challenging problems in graph mining is about the completeness because the of complexity graph structures. We will prove the completeness of our algorithm in this paper. The experiments will be conducted to illustrate the efficiency of our data partitioning approach...|$|R
40|$|In {{the age of}} {{information}} technology, the amount of accumulated data is tremendous. Extracting the association rule from this data {{is one of the}} important tasks in data mining. Most of the existing association rules in algorithms typically assume that the data set can fit in the memory. In this paper, we propose a practical and effective scheme to mine association rules from frequent patterns, called Prefixfoldtree scheme (PFT scheme). <b>The</b> <b>original</b> <b>dataset</b> <b>is</b> divided into folds, and then from each fold the frequent patterns are mined by using the tree projection approach. These frequent patterns are combined into one set and finally interestingness constraints are used to extract the association rules. The experiments will be conducted to illustrate the efficiency of our scheme...|$|R
40|$|Abstract:- In this paper, {{we propose}} an {{efficient}} and effective clustering method that requires to scan a <b>dataset</b> only once. <b>The</b> <b>original</b> <b>dataset</b> <b>is</b> transformed first and merged into a hyper image of controllable size. Unlike traditional methods, the dissimilarity measurement between objects is calculated once for all objects by using various image processing methodologies, such as morphological operations. Image connect component extraction is thereby used to extract clusters from the hyper image. The proposed method is easy to use for clustering data in way of fuzzy and hierarchical fashion readily under a single <b>dataset</b> scan. It <b>is</b> also efficient for incremental and dynamic clustering without additional scan of <b>the</b> <b>original</b> <b>dataset.</b> Experimental {{results show that the}} proposed method is robust and stable under various parameter settings such that it is more effective and useful than traditional clustering methods, especially for very large datasets...|$|R
40|$|A new {{two-stage}} indexless search {{procedure is}} presented that {{makes use of}} the constrained edit distance in IDS misuse detection attack database search. The procedure consists of a pre-selection phase, in which <b>the</b> <b>original</b> <b>dataset</b> <b>is</b> reduced and <b>the</b> exhaustive search phase for the database records selected in the first phase. The maximum number of consecutive deletions represents the constraint. Besides {{eliminating the need for}} finer exhaustive search in the attack database records in which the detected subsequence is too distorted, the new search procedure also enables better control over the search process in the case of deliberate distortion of the attack strings. Experimental results obtained on the SNORT signature files show that the proposed method offers average search data set reduction in the typical cases of more than 70 % compared to the method that uses the unconstrained edit distance...|$|R
40|$|<b>The</b> <b>original</b> ImageNet <b>dataset</b> <b>is</b> {{a popular}} {{large-scale}} benchmark for training Deep Neural Networks. Since {{the cost of}} performing experiments (e. g, algorithm design, architecture search, and hyperparameter tuning) on <b>the</b> <b>original</b> <b>dataset</b> might <b>be</b> prohibitive, we propose to consider a downsampled version of ImageNet. In contrast to the CIFAR datasets and earlier downsampled versions of ImageNet, our proposed ImageNet 32 × 32 (and its variants ImageNet 64 × 64 and ImageNet 16 × 16) contains exactly {{the same number of}} classes and images as ImageNet, with the only difference that the images are downsampled to 32 × 32 pixels per image (64 × 64 and 16 × 16 pixels for the variants, respectively). Experiments on these downsampled variants are dramatically faster than on <b>the</b> <b>original</b> ImageNet and <b>the</b> characteristics of the downsampled datasets with respect to optimal hyperparameters appear to remain similar. The proposed datasets and scripts to reproduce our results are available at [URL] and [URL]...|$|R
40|$|Privacy-preservation {{is a step}} in {{data mining}} that tries to {{safeguard}} sensitive information from unsanctioned disclosure and hence protecting individual data records and their privacy. There are various privacy preservation techniques like k-anonymity, l-diversity and t-closeness and data perturbation. In this paper k-anonymity privacy protection technique is applied to high dimensional datasets like adult and census. since, both the data sets are high dimensional, feature subset selection method like Gain Ratio is applied and the attributes of the <b>datasets</b> <b>are</b> ranked and low ranking attributes are filtered to form new reduced data subsets. K-anonymization privacy preservation technique is then applied on reduced datasets. The accuracy of the privacy preserved reduced <b>datasets</b> and <b>the</b> <b>original</b> <b>datasets</b> <b>are</b> compared for their accuracy on the two functionalities of data mining namely classification and clustering using naïve Bayesian and k-means algorithm respectively. Experimental results show that classification and clustering accuracy are comparatively the same for reduced k-anonym zed <b>datasets</b> and <b>the</b> <b>original</b> data sets...|$|R
