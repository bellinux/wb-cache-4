17|5584|Public
40|$|Abstract. This paper {{presents}} a new constructive method and pruning approaches {{to control the}} design of Multi-Layer Perceptron (MLP) without loss in performance. The proposed methods use a multi-objective approach to guarantee generalization. The constructive approach searches for an optimal solution according to the pareto set shape with increasing number of hidden nodes. <b>The</b> <b>pruning</b> <b>methods</b> are able to simplify the network topology and to identify linear connections between the inputs and outputs of the neural model. Topology information and validation sets are used. 1...|$|E
40|$|One red and {{one white}} {{cultivar}} of winegrapes grown in Southern Arizona was pruned to four different methods. The red cultivar was 'Merlot' {{and the white}} was 'Sauvignon Blanc'. <b>The</b> <b>pruning</b> <b>methods</b> were 2 bud spur, 4 bud spur, cane and basal buds only. The basal bud treatment was eliminated for 'Sauvignon Blanc'. The 4 bud spur method resulted in significantly greater yield {{when compared to the}} other methods. Fruit produced from the basal bud only treatment resulted in fruit that was significantly greater in pH and acid content. The 'Sauvignon Blanc' cultivar had significantly higher yield with cane pruning with no difference in fruit quality...|$|E
40|$|This article {{presents}} a unifying framework {{according to which}} any pruning method {{can be defined as}} a four-tuple (Space, Operators, Evaluation function, Search strategy), and the pruning process can be cast as an optimization problem. Six well-known pruning methods are investigated by means of this framework and their common aspects, strengths and weaknesses are described. Furthermore, a new empirical analysis of the effect of post-pruning on both the predictive accuracy and the size of induced decision trees is reported. The experimental comparison of <b>the</b> <b>pruning</b> <b>methods</b> involves 14 datasets and is based on the cross-validation procedure. The results confirm most of the conclusions drawn in a previous comparison based on the holdout procedure...|$|E
40|$|The {{main goal}} {{of this paper is}} to {{describe}} a new <b>pruning</b> <b>method</b> for solving decision trees and game trees. <b>The</b> <b>pruning</b> <b>method</b> for decision trees suggests a slight variant of decision trees that we call scenario trees. In scenario trees, we do not need a conditional probability for each edge emanating from a chance node. Instead, we require a joint probability for each path from the root node to a leaf node. We compare <b>the</b> <b>pruning</b> <b>method</b> to <b>the</b> traditional rollback method for decision trees and game trees. For problems that require Bayesian revision of probabilities, a scenario tree representation with <b>the</b> <b>pruning</b> <b>method</b> is more efficient than a decision tree representation with the rollback method. For game trees, <b>the</b> <b>pruning</b> <b>method</b> is more efficient than the rollback method. ...|$|R
40|$|Abstract: <b>The</b> <b>methods</b> of <b>pruning</b> {{have great}} {{influence}} {{on the effect of}} the decision tree. By researching on <b>the</b> <b>pruning</b> <b>method</b> based on misclassification, introduced the conception of condition misclassification and improved <b>the</b> standard of <b>pruning.</b> Propose <b>the</b> conditional misclassification <b>pruning</b> <b>method</b> for decision tree optimization and apply it in C 4. 5 algorithm. The experiment result shows that <b>the</b> condition misclassification <b>pruning</b> can avoid over pruned problem and non-enough pruned problem to some extent and improve the accurate of classification...|$|R
5000|$|Pruning: <b>The</b> <b>pruning</b> <b>method</b> {{known as}} [...] "taille à queue du Mâconnais" [...] (lit. Maconnais tail pruning) {{is typical of}} the Maconnais {{district}} and is a variation on Guyot pruning. This technique of bending of the wood into an arc during pruning is designed to prevent acrotony, a characteristic defect of Chardonnay vines, and also serves to protect the plants against spring frosts.|$|R
40|$|Several {{methods have}} been {{proposed}} in the literature for decision tree (post) -pruning. This article presents a unifying framework according to which any pruning method {{can be defined as}} a four-tuple (Space, Operators, Evaluation function, Search strategy), and the pruning process can be cast as an optimization problem. Six well-known pruning methods are investigated by means of this framework and their common aspects, strengths and weaknesses are described. Furthermore, a new empirical analysis of the effect of post-pruning on both the predictive accuracy and the size of induced decision trees is reported. The experimental comparison of <b>the</b> <b>pruning</b> <b>methods</b> involves 14 datasets and is based on the cross-validation procedure. The results confirm most of the conclusions drawn in a previous comparison based on the holdout procedure...|$|E
40|$|In this paper, several {{techniques}} {{for reducing the}} search complexity of beam search for continuous speech recognition task are proposed. Six heuristic methods for pruning are described and {{the parameters of the}} pruning are adjusted to keep constant the word error rate while reducing the computational complexity and memory demand. The evaluation of the effect of each pruning method is performed in Mixture Stochastic Trajectory Model (MSTM). MSTM is a segment-based model using phonemes as the speech units. The set of tests in a speaker-dependent continuous speech recognition task shows that using <b>the</b> <b>pruning</b> <b>methods,</b> a substantial reduction of 67 # of search effort is obtained in term of number of hypothesised phonemes during the search. All proposed techniques are independent of the acoustic models and therefore are applicable to other acoustic modeling techniques...|$|E
40|$|Distance {{functions}} {{are the main}} tools to measure similarity of two sequences and to search the closest sequences to given query sequence. Several well known distance functions, however, have asymptotical time complexity of O(mn) which cannot be fully afforded by systems that deal with large volumes of data. These distance functions, including Edit distance on Real sequences (EDR) [5], have pruning methods to reduce execution time by dismissing false candidates as early as possible. In this paper, we propose the Histogram Distance on Fixed Reference (HDFR) ordering, with various reference histogram construction methods, to improve the filtering power of <b>the</b> <b>pruning</b> <b>methods</b> in EDR. Experiments show that a decrease in EDR execution time is observed after HDFR is applied. While we base our experiments on EDR, HDFR can also be applied to other distance functions with appropriate pruning methods. © 2006 IEEE...|$|E
40|$|Abstract — This paper {{introduces}} a new trellis <b>pruning</b> <b>method</b> which uses nonlinear convolutional coding for peak-to-average power ratio (PAPR) reduction of filtered QPSK and 16 -QAM modulations. The Nyquist filter {{is viewed as}} a convolutional encoder that controls the analog waveforms of the filter output directly. Pruning some edges of the encoder trellis can effectively reduce the PAPR. The only tradeoff is a slightly lower channel capacity and increased complexity. The paper presents simulation results of <b>the</b> <b>pruning</b> action and <b>the</b> resulting PAPR, and also discusses the decoding algorithm and the capacity of <b>the</b> filtered and <b>pruned</b> QPSK and 16 -QAM modulations on the AWGN channel. Simulation results show that <b>the</b> <b>pruning</b> <b>method</b> reduces <b>the</b> PAPR significantly without much damage to capacity. I...|$|R
40|$|This paper {{introduces}} a new trellis <b>pruning</b> <b>method</b> which uses nonlinear convolutional coding for peak-to-average power ratio (PAPR) reduction of filtered QPSK and 16 -QAM modulations. The Nyquist filter {{is viewed as}} a convolutional encoder that controls the analog waveforms of the filter output directly. Pruning some edges of the encoder trellis can effectively reduce the PAPR. The only tradeoff is a slightly lower channel capacity and increased complexity. The paper presents simulation results of <b>the</b> <b>pruning</b> action and <b>the</b> resulting PAPR, and also discusses the decoding algorithm and the capacity of <b>the</b> filtered and <b>pruned</b> QPSK and 16 -QAM modulations on the AWGN channel. Simulation results show that <b>the</b> <b>pruning</b> <b>method</b> reduces <b>the</b> PAPR significantly without much damage to capacity. Comment: 5 pages, 10 figures, 2005 IEEE International Symposium on Information Theor...|$|R
40|$|This paper {{presents}} {{two techniques}} for language model (LM) adaptation. The first aims {{to build a}} more general LM. We propose a distribution-based pruning of n-gram LMs, where we prune n-grams {{that are likely to}} be infrequent in a new document. Experimental results show that <b>the</b> distribution-based <b>pruning</b> <b>method</b> performed up to 9 % (word perplexity reduction) better than conventional cutoff <b>methods.</b> Moreover, <b>the</b> <b>pruning</b> <b>method</b> results in a more general ngram backoff model, in spite of the domain, style, or temporal bias in the training data. The second aims to build a more task-specific LM. We propose an n-gram distribution adaptation method for LM training. Given a large set of out-of-task training data, called training set, andasmall set of task-specific training data, called seed set, we adapt the LM towards the task by adjusting the n-gram distribution in the training set to that in the seed set. Experimental results show non-trivial improvements over conventional methods. 1...|$|R
40|$|Distributed {{data mining}} systems aim to {{discover}} and combine useful information that is distributed across multiple databases. One of the main challenges is the design of effective and efficient methods to combine multiple models computed over multiple distributed sources that scale well over many large distributed databases. We describe in detail several methods that evaluate, prune and combine large collections of imported models computed at remote sites into efficient and scalable meta-classifiers. We demonstrate and evaluate <b>the</b> <b>pruning</b> <b>methods</b> by detailing many experiments performed on actual credit card data sets supplied by collaborating financial institutions, where the target learning task is fraud detection. We show that pruned meta-classifiers can sustain or even improve predictive performance at a substantially higher throughput, compared to the unpruned meta-classifiers. Keywords: distributed data mining, meta-learning, classifier evaluation, pruning, ensembles of classifi [...] ...|$|E
40|$|With the {{overwhelming}} {{increase in the}} amount of data on the web and data bases, many text mining techniques have been proposed for mining useful patterns in text documents. Extracting closed sequential patterns using the Pattern Taxonomy Model (PTM) is one of <b>the</b> <b>pruning</b> <b>methods</b> to remove noisy, inconsistent, and redundant patterns. However, PTM model treats each extracted pattern as whole without considering included terms, which could affect the quality of extracted patterns. This paper propose an innovative and effective method that extends the random set to accurately weigh patterns based on their distribution in the documents and their terms distribution in patterns. Then, the proposed approach will find the specific closed sequential patterns (SCSP) based on the new calculated weight. The experimental results on Reuters Corpus Volume 1 (RCV 1) data collection and TREC topics show that the proposed method significantly outperforms other state-of-the-art methods in different popular measures...|$|E
40|$|Neural network pruning methods on {{the level}} of {{individual}} network parameters (e. g. connection weights) can improve generalization, as is shown in this empirical study. However, an open problem in <b>the</b> <b>pruning</b> <b>methods</b> known today (e. g. OBD, OBS, autoprune, epsiprune) is the selection of the number of parameters to be removed in each pruning step (pruning strength). This work presents a pruning method lprune that automatically adapts the pruning strength to the evolution of weights and loss of generalization during training. The method requires no algorithm parameter adjustment by the user. Results of statistical significance tests comparing autoprune, lprune, and static networks with early stopping are given, based on extensive experimentation with 14 different problems. The results indicate that training with pruning is often significantly better and rarely significantly worse than training with early stopping without pruning. Furthermore, lprune is often superior to autoprune (which is [...] ...|$|E
30|$|The size of {{a hidden}} layer {{is one of the}} most {{important}} considerations when dealing with real life tasks using FNN. However, <b>the</b> existing <b>pruning</b> <b>methods</b> may not <b>prune</b> <b>the</b> unnecessary weights efficiently, so how to efficiently simplify the network structure becomes our main task.|$|R
5000|$|Pruning {{generally}} {{takes place}} once the leaves {{have fallen in}} November, and continues throughout the winter months. Mechanized pre-pruning is carried out first, using a high-clearance tractor, and this cuts <b>the</b> time spent <b>pruning</b> manually by about a quarter. <b>The</b> following <b>pruning</b> <b>methods</b> are permitted: ...|$|R
40|$|This paper proposes and evaluates {{algorithms}} {{for fast}} music retrieval. The target {{of this paper}} is to retrieve music segments (query in retrieval) from music database. The algorithms retrieve music segments from music database by distance of spectrum and difference of power. For reduction of calculation, <b>the</b> <b>pruning</b> <b>method</b> is proposed. <b>The</b> experiment is retrieving ten seconds segment from 100 music database. The experiment results shows a detection rate is ¢¡¤£¥¢¦¨ § and retrieval processing time is©¡¤£¡¨ � seconds at SNR���� � £¥ �  dB. 1...|$|R
40|$|Join {{processing}} in the streaming {{environment has}} many practical {{applications such as}} data cleaning and outlier detection. Due to the inherent uncertainty in the real-world data, {{it has become an}} increasingly important problem to consider the join processing on uncertain data streams, where the incoming data at each timestamp are uncertain and imprecise. Different from the static databases, processing uncertain data streams has its own requirements such as the limited memory, small response time, and so on. To tackle the challenges with respect to efficiency and effectiveness, in this paper, we formalize the problem of join on uncertain data streams (USJ), which can guarantee the accuracy of USJ answers over uncertain data, and propose effective pruning methods to filter out false alarms. We integrate <b>the</b> <b>pruning</b> <b>methods</b> into an efficient query procedure for incrementally maintaining USJ answers. Extensive experiments have been conducted to demonstrate the efficiency and effectiveness of our approaches. Copyright 2009 ACM...|$|E
40|$|In this paper, {{we study}} the {{completeness}} of {{several types of}} pruning methods for the logical consequence finding procedure SOL-resolution. The consequence finding problem is a generalization of the refutation finding problem. The SOL-resolution is a Model-Elimination-like calculus with Skip operation, {{and is one of}} the most significant calculi for consequence finding. The concept "completeness" of consequence-finding calculi differs from the one of refutation finding calculi, hence <b>the</b> <b>pruning</b> <b>methods</b> that are complete for refutation finding tasks may not be complete for consequence finding problems. In this paper, we first reformulate SOL-resolution within connection tableaux and properly strengthen one of structural conditions, the regularity for skipped literals, which reduces a great amount of the redundancies in the search space. Next we investigate various types of pruning methods, such as order-preserving reduction, lemma matching, merge, unit subsumption, etc, and show the completeness theorems of each pruning method for consequence finding. Finally we show Skip operation itself has a significantly important future as a new local failure pruning method in consequence finding...|$|E
40|$|Recently, the {{management}} of transportation systems has become increasingly important in many real applications such as location-based services, supply chain management, traffic control, and so on. These applications usually involve queries over spatial road networks with dynamically changing and complicated traffic conditions. In this paper, we model such a network by a probabilistic time-dependent graph (PT-Graph), whose edges are associated with uncertain delay functions. We propose a useful query in the PT-Graph, namely a trip planner query (TPQ), which retrieves trip plans that traverse a set of query points in PT-Graph, having the minimum traveling time with high confidence. To tackle the efficiency issue, we present <b>the</b> <b>pruning</b> <b>methods</b> time interval pruning and probabilistic pruning to effectively rule out false alarms of trip plans. Furthermore, we design a pre-computation technique based on the cost model and construct an index structure over the pre-computed data to enable the pruning via the index. We integrate our proposed pruning methods into an efficient query procedure to answer TPQs. Through extensive experiments, we demonstrate the efficiency and effectiveness of our TPQ query answering approach...|$|E
2500|$|Pruning {{generally}} {{takes place}} once the leaves {{have fallen in}} November, and continues throughout the winter months. [...] Mechanized pre-pruning is carried out first, using a high-clearance tractor, and this cuts <b>the</b> time spent <b>pruning</b> manually by about a quarter. [...] <b>The</b> following <b>pruning</b> <b>methods</b> are permitted: ...|$|R
40|$|In this paper, {{the problem}} of rule pruning in {{associative}} text categorisation is investigated. We propose a new rule <b>pruning</b> <b>method</b> within an existing associative classification algorithm called MCAR. Experimental results against large text collection (Reuters- 21578) using <b>the</b> developed <b>pruning</b> <b>method</b> {{as well as other}} known existing methods (Database coverage, lazy <b>pruning)</b> are conducted. <b>The</b> bases of the experiments are the classification accuracy and the number of generated rules. The results derived show that <b>the</b> proposed rule <b>pruning</b> <b>method</b> derives higher quality and more scalable classifiers than those produced by lazy and database coverage pruning approaches. In addition, the number of rules generated by <b>the</b> developed <b>pruning</b> procedure is usually less than those of lazy pruning and database coverage heuristics...|$|R
40|$|In this paper, we {{introduce}} a new skeleton <b>pruning</b> <b>method</b> based on contour partitioning. Any contour partition can be used, but the partitions obtained by Discrete Curve Evolution (DCE) yield excellent results. The theoretical properties and the experiments presented demonstrate that obtained skeletons are in accord with human visual perception and stable, even {{in the presence of}} significant noise and shape variations, and have the same topology as the original skeletons. In particular, we have proven that the proposed approach never produces spurious branches, which are common when using <b>the</b> known skeleton <b>pruning</b> <b>methods.</b> Moreover, <b>the</b> proposed <b>pruning</b> <b>method</b> does not displace the skeleton points. Consequently, all skeleton points are centers of maximal disks. Again, many existing methods displace skeleton points in order to produces pruned skeletons...|$|R
40|$|Abstract|Neural network pruning methods on {{the level}} of {{individual}} network parameters (e. g. connection weights) can improve generalization, as is shown in this empirical study. However, an open problem in <b>the</b> <b>pruning</b> <b>methods</b> known today (OBD, OBS, autoprune, epsiprune) is the selection of the number of parameters to be removed in each pruning step (pruning strength). This work presents a pruning method lprune that automatically adapts the pruning strength to the evolution of weights and loss of generalization during training. The method requires no algorithm parameter adjustmentby the user. Results of statistical signi cance tests comparing autoprune, lprune, and static networks with early stopping are given, based on extensive experimentation with 14 di erent problems. The results indicate that training with pruning is often signi cantly better and rarely signi cantly worse than training with early stopping without pruning. Furthermore, lprune is often superior to autoprune (which is superior to OBD) on diagnosis tasks unless severe pruning early in the training process is required. 1 Pruning and Generalization The principal idea of pruning is {{to reduce the number of}} free parameters in the network by removin...|$|E
40|$|Ensemble {{learning}} is a method of combining learners to obtain more reliable and accurate predictions in supervised and unsupervised learning. However, the ensemble sizes are sometimes unnecessarily large which leads to additional memory usage, computational overhead and decreased effectiveness. To overcome such side effects, pruning algorithms have been developed; {{since this is a}} combinatorial problem, finding the exact subset of ensembles is computationally infeasible. Different types of heuristic algorithms have developed to obtain an approximate solution but they lack a theoretical guarantee. Error Correcting Output Code (ECOC) is one of the well-known ensemble techniques for multiclass classification which combines the outputs of binary base learners to predict the classes for multiclass data. In this paper, we propose a novel approach for pruning the ECOC matrix by utilizing accuracy and diversity information simultaneously. All existing pruning methods need the size of the ensemble as a parameter, so the performance of <b>the</b> <b>pruning</b> <b>methods</b> depends {{on the size of the}} ensemble. Our unparametrized pruning method is novel as being independent of the size of ensemble. Experimental results show that our pruning method is mostly better than other existing approaches...|$|E
40|$|Information {{extraction}} {{is initially}} applied for identification of desired information from natural language documents and {{conversion of the}} extracted text into a self-defined presentation. With the rapidly increasing amount of available information sources and electronic documents on the World Wide Web, information extraction is extended for identification from structured and semi-structured web pages. In the past years, a lot of solutions are described and various information extraction systems are implemented. In this thesis, we present a theoretical analysis and comparison of several information extraction systems in two sub-areas: on the one hand, the systems distinguish {{from each other in}} the features used for identification. Thereby, we compare several aspects of different information extraction systems, such as pre-processing for generation of features, various constraints for characterization of target information, and different representations of extraction patterns, namely, how constraints are utilized. On the other hand, in order to reduce human efforts and improve the portability of information extraction systems, diverse machine learning techniques are applied for building information extraction systems. In this thesis, we represent various types of training data and introduce the active learning technique, the boosting algorithm and different rule learning algorithms used in information extraction systems. Most of the information extraction systems mentioned in this thesis employ rule learning techniques. Thereby, the structure, the evaluation heuristics and <b>the</b> <b>pruning</b> <b>methods</b> are compared. In addition, bayesian learning applied in information extraction system is presented as well...|$|E
40|$|Pretropisms are {{candidates}} for the leading exponents of Puiseux series that represent solutions of polynomial systems. To find pretropisms, we propose an exact gift wrapping algorithm to <b>prune</b> <b>the</b> tree of edges of a tuple of Newton polytopes. We prefer exact arithmetic {{not only because of}} the exact input and the degrees of the output, but because of the often unpredictable growth of the coordinates in the face normals, even for polytopes in generic position. We provide experimental results with our preliminary implementation in Sage that compare favorably with <b>the</b> <b>pruning</b> <b>method</b> that relies only on cone intersections. Comment: exact, gift wrapping, Newton polytope, pretropism, tree pruning, accepted for presentation at Computer Algebra in Scientific Computing, CASC 201...|$|R
40|$|Abstract — In this paper, we {{introduce}} a new skeleton <b>pruning</b> <b>method</b> based on contour parti-tioning. Any contour partition can be used, but the partitions obtained by Discrete Curve Evolu-tion (DCE) yield excellent results. The theoretical properties and the experiments presented demonstrate that obtained skeletons are in accord with human visual perception and stable, even {{in the presence of}} significant noise and shape variations, and have the same topology as the original skeletons. In particular, we have proven that the proposed approach never produces spu-rious branches, which are common when using <b>the</b> known skeleton <b>pruning</b> <b>methods.</b> Moreover, <b>the</b> proposed <b>pruning</b> <b>method</b> does not displace the skeleton points. Consequently, all skeleton points are centers of maximal disks. Again many existing methods displace skeleton points in order to produces pruned skeletons...|$|R
40|$|Several {{algorithms}} {{for processing}} reverse skyline queries {{have been proposed}} in re-cent literature. However, these algorithms are based on pre-processing approaches, and hence involve complex procedures and waste storage space due to inefficient use of storage. In addition, they are not robust to frequently changing data as, they have to re-compute and update the pre-computed results. To overcome these issues, this paper proposes a novel algorithm to efficiently process reverse skyline queries using an ap-proach based on two <b>pruning</b> methods: <b>the</b> search-area <b>pruning</b> <b>method</b> and <b>the</b> candi-date-objects <b>pruning</b> <b>method.</b> Utilizing these <b>pruning</b> <b>methods,</b> <b>the</b> algorithm is able to process reverse skyline queries efficiently even in situations where data is changing fre-quently. The proposed algorithm also effectively reduces the inefficient use of storage under existing approaches for storing pre-computed results. We conducted extensive ex-periments {{to show that the}} proposed algorithm shows better performance compared to existing approaches regardless of the dimension, distribution, or size of the data...|$|R
40|$|Personal {{use of this}} {{material}} is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing {{this material}} for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. G. Martínez-Muñoz, D. Hernández-Lobato and A. Suárez, "An analysis of ensemble pruning techniques based on ordered aggregation", IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 2, pp. 245 - 249, February 2009 Several pruning strategies {{that can be used}} to reduce the size and increase the accuracy of bagging ensembles are analyzed. These heuristics select subsets of complementary classifiers that, when combined, can perform better than the whole ensemble. <b>The</b> <b>pruning</b> <b>methods</b> investigated are based on modifying the order of aggregation of classifiers in the ensemble. In the original bagging algorithm, the order of aggregation is left unspecified. When this order is random, the generalization error typically decreases as the number of classifiers in the ensemble increases. If an appropriate ordering for the aggregation process is devised, the generalization error reaches a minimum at intermediate numbers of classifiers. This minimum lies below the asymptotic error of bagging. Pruned ensembles are obtained by retaining a fraction of the classifiers in the ordered ensemble. The performance of these pruned ensembles is evaluated in several benchmark classification tasks under different training conditions. The results of this empirical investigation show that ordered aggregation can be used for the efficient generation of pruned ensembles that are competitive, in terms of performance and robustness of classification, with computationally more costly methods that directly select optimal or near-optimal subensembles. The authors acknowledge support form the Spanish Ministerio de Educación y Ciencia under Project TIN 2007 - 66862 -C 02 - 0...|$|E
40|$|In a deregulating power market, bidding {{decisions}} rely on good market {{clearing price}} predictions. One {{of the common}} forecasting methods is a Gaussian radial basis function (GRBF) network, which approximates input-output relationships by building localized Gaussian functions (clusters). The accuracy of network predictions relies {{on the degree of}} correctness of the input-output mapping built by the network. Currently, all input factors are used by each of the clusters. To some clusters, certain input factors may not be important and should be deleted because they mislead local learning and result in the misrepresentation of the underlying relationship in data. Existing pruning methods for neural networks, which examine the significance of connections between neurons, are not applicable to deleting center and standard deviation parameters since these parameters bear little sense of connection to <b>the</b> <b>pruning</b> <b>methods.</b> Based on the finding that the inverses of standard deviations can sever links between neurons, a new training method to identify and eliminate unimportant input factors is developed. ^ For a neural network, the misrepresentation of the underlying relationship in data cannot be completely solved by selecting input factors. In view of the commonly seen situation that there are insufficient data points available to represent all data features, a network in reality may misrepresent part of the input-output relationship that could have been more appropriately represented by different networks. For example, radial basis function networks are effective in exploiting local data characteristics, while multi-layer perceptron networks are good at capturing global data trends. The use of a “committee machine” composed of multiple networks can in principle alleviate the problem of misrepresenting the underlying relationship. Currently, combining the predictions from multiple networks is based on a straight average or the statistics of historical prediction errors though the performance of individual networks is varying and input-dependent. To solve this problem, our idea is to estimate the quality of a prediction, i. e., the prediction variance that is conditioned on the current input and the historical data. Under the Multiple Model framework, a new method that the prediction qualities of networks are utilized to determine better weighting coefficients is developed. ...|$|E
40|$|The {{implementation}} of the two-dimensional discrete cosine transform (2 D DCT) through the multiple one-dimensional (row-by-column approach) and the direct 2 D DCT is studied. It is observed that the execution times on different computer architectures using one-dimensional (1 D) algorithms vary significantly {{although some of the}} examined algorithms have the same computational complexity (additions and multiplications). The direct 2 D DCT outperforms all row-by-column approaches. In addition, <b>the</b> vector-radix <b>pruning</b> <b>method</b> is compared to <b>the</b> row-column <b>pruning</b> <b>method.</b> Properties and advantages of some of the algorithms are also discussed. We have been unable to find in the literature an evaluation of the execution time of the 2 D fast cosine transforms (FCT's) as a function of sequence length, when implemented on different computer architectures. This information is extremely useful for choosing an algorithm and a computer that will be used primarily t...|$|R
40|$|A {{complexity}} based pruning {{procedure for}} classification trees is described, and bounds on its finite sample performance are established. The procedure selects a subtree of a (possibly random) initial tree {{in order to}} minimize a complexity penalized measure of empirical risk. The complexity assigned to a subtree is proportional to the square root of its size. Two cases are considered. In the first <b>the</b> growing and <b>pruning</b> data sets are indentical, and in the second they are independent. Using the performance bound, the Bayes risk consistency of pruned trees obtained via the procedure is established when the sequence of initial trees satis es suitable geometric and structural constraints. <b>The</b> <b>pruning</b> <b>method</b> and its analysis are motivated by work on adaptive model selection using complexity regularization...|$|R
40|$|Resolution has {{not been}} an {{effective}} tool for deciding satisfiability of propositional CNF formulas, due to explosion of the search space, particularly when the formula is satisfiable. However, a new <b>pruning</b> <b>method,</b> which is designed to eliminate certain refutation attempts that cannot succeed, has been shown to eliminate much of the redundancy of propositional model elimination. <b>The</b> <b>pruning</b> <b>method</b> exploits <b>the</b> concept of "autarky", which was introduced by Monien and Speckenmeyer. Informally, an autarky is a "self-sufficient" model for some clauses, but which does not affect the remaining clauses of <b>the</b> formula. Autarky <b>pruning</b> permits <b>the</b> algorithm, called "Modoc", to be "two-sided" {{in the sense that it}} constructs a model if the formula is satisfiable and constructs a refutation proof if it is not. This talk describes new "lemma" and "cut" strategies that are efficient to apply in the setting of propositional resolution. It builds upon the C-literal strategy proposed by Shost [...] ...|$|R
40|$|Motivation: A {{promising}} sliding-window {{method for}} the detection of interspecific recombination in DNA sequence alignments is based on the monitoring of changes in the posterior distribution of tree topologies with a probabilistic divergence measure. However, as the number of taxa in the alignment increases or the sliding-window size decreases, the posterior distribution becomes increasingly diffuse. This diffusion blurs the probabilistic divergence signal and adversely affects the detection accuracy. The present study investigates how this shortcoming can be redeemed with a <b>pruning</b> <b>method</b> based on post-processing clustering, using the Robinson–Foulds distance as a metric in tree topology space. Results: An application of the proposed scheme to three synthetic and two real-world DNA sequence alignments illustrates the amount of improvement that can be obtained with <b>the</b> <b>pruning</b> <b>method.</b> <b>The</b> study also includes a comparison with two established recombination detection methods: Recpars and the DSS (difference of sum of squares) method...|$|R
