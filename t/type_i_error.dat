2680|10000|Public
25|$|A <b>Type</b> <b>I</b> <b>error</b> would falsely {{indicate}} that treatment A {{is more effective}} than the placebo, whereas a Type II error would be a failure to demonstrate that treatment A {{is more effective than}} placebo even though it actually is more effective.|$|E
25|$|To compare {{competing}} {{statistics for}} small samples under realistic data conditions. Although <b>type</b> <b>I</b> <b>error</b> and power properties of statistics {{can be calculated}} for data drawn from classical theoretical distributions (e.g., normal curve, Cauchy distribution) for asymptotic conditions (i. e, infinite sample size and infinitesimally small treatment effect), real data often do not have such distributions.|$|E
25|$|While in {{principle}} the acceptable level of statistical significance {{may be subject}} to debate, the p-value is the smallest significance level that allows the test to reject the null hypothesis. This is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the p-value, the lower the probability of committing <b>type</b> <b>I</b> <b>error.</b>|$|E
5000|$|... {{avoiding}} the <b>type</b> <b>I</b> <b>errors</b> (or false positives) that classify authorized users as imposters.|$|R
2500|$|<b>Type</b> <b>I</b> <b>errors</b> {{where the}} null {{hypothesis}} is falsely rejected giving a [...] "false positive".|$|R
5000|$|... using {{effect sizes}} {{obtained}} when {{the null hypothesis}} has been retained inflates <b>Type</b> <b>I</b> <b>errors</b> in meta-analysis; and ...|$|R
25|$|A typeI error (or {{error of}} the first kind) is the {{incorrect}} rejection of a true null hypothesis. Usually a <b>type</b> <b>I</b> <b>error</b> leads one to conclude that a supposed effect or relationship exists when in fact it doesn't. Examples of type I errors include a test that shows a patient to have a disease when in fact the patient {{does not have the}} disease, a fire alarm going on indicating a fire when in fact there is no fire, or an experiment indicating that a medical treatment should cure a disease when in fact it does not.|$|E
25|$|The {{standard}} {{approach is to}} test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of <b>type</b> <b>I</b> <b>error</b> is therefore {{the probability that the}} estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.|$|E
25|$|All {{statistical}} hypothesis tests have a probability of making type I and type II errors. For example, all blood tests for a disease will falsely detect {{the disease in}} some proportion {{of people who do}}n't have it, and will fail to detect the disease in some proportion of people who do have it. A test's probability of making a <b>type</b> <b>I</b> <b>error</b> is denoted by α. A test's probability of making a type II error is denoted by β. These error rates are traded off against each other: for any given sample set, the effort to reduce one type of error generally results in increasing the other type of error. For a given test, the only way to reduce both error rates is to increase the sample size, and this may not be feasible.|$|E
50|$|Empirical methods, which {{control the}} {{proportion}} of <b>Type</b> <b>I</b> <b>errors</b> adaptively, utilizing correlation and distribution characteristics of the observed data.|$|R
50|$|<b>Type</b> <b>I</b> <b>errors</b> which {{consist of}} rejecting a null {{hypothesis}} that is true; {{this amounts to}} a false positive result.|$|R
5000|$|... the {{probability}} of <b>type</b> <b>I</b> <b>errors</b> is called the [...] "false reject rate" [...] (FRR) or false non-match rate (FNMR), ...|$|R
25|$|A {{generalization}} of Student's t statistic, called Hotelling's t-squared statistic, {{allows for the}} testing of hypotheses on multiple (often correlated) measures within the same sample. For instance, a researcher might submit a number of subjects to a personality test consisting of multiple personality scales (e.g. the Minnesota Multiphasic Personality Inventory). Because measures of this type are usually positively correlated, it is not advisable to conduct separate univariate t-tests to test hypotheses, as these would neglect the covariance among measures and inflate the chance of falsely rejecting at least one hypothesis (<b>Type</b> <b>I</b> <b>error).</b> In this case a single multivariate test is preferable for hypothesis testing. Fisher's Method for combining multiple tests with alpha reduced for positive correlation among tests is one. Another is Hotelling's T2 statistic follows a T2 distribution. However, in practice the distribution is rarely used, since tabulated values for T2 are hard to find. Usually, T2 is converted instead to an F statistic.|$|E
2500|$|The fixed pre-defined [...] level can be {{interpreted}} as the rate of falsely rejecting the null hypothesis (or <b>type</b> <b>I</b> <b>error),</b> since ...|$|E
2500|$|In {{statistical}} hypothesis testing, a <b>type</b> <b>I</b> <b>error</b> is the incorrect rejection {{of a true}} null hypothesis (also known as a [...] "false positive" [...] finding), while a type II error is incorrectly retaining a false null hypothesis (also known as a [...] "false negative" [...] finding). More simply stated, a <b>type</b> <b>I</b> <b>error</b> is to falsely infer the existence of {{something that is not}} there, while a type II error is to falsely infer the absence of something that is.|$|E
5000|$|Thus, by assuring , the {{probability}} of making one or more <b>type</b> <b>I</b> <b>errors</b> in the family is controlled at level [...]|$|R
50|$|In statistics, family-wise {{error rate}} (FWER) is the {{probability}} of making one or more false discoveries, or <b>type</b> <b>I</b> <b>errors</b> when performing multiple hypotheses tests.|$|R
40|$|It {{is shown}} in Shaffer (1980) that {{step-down}} multiple tests designed to control <b>Type</b> <b>I</b> <b>errors</b> at rate ? actually control <b>Type</b> <b>I</b> and III <b>errors</b> jointly at rate ? under certain distributional assumptions. In this note it is shown that step-up multiple tests have a similar property under the same distributional assumptions...|$|R
2500|$|In {{terms of}} false {{positive}}s and false negatives, a positive result corresponds to rejecting the null hypothesis, while a negative result corresponds to failing {{to reject the}} null hypothesis; [...] "false" [...] means the conclusion drawn is incorrect. Thus a <b>type</b> <b>I</b> <b>error</b> is a false positive, and a type II error is a false negative.|$|E
2500|$|A {{statistical}} hypothesis test compares a test statistic (z or t for examples) to a threshold. The test statistic (the formula {{found in the}} table below) is based on optimality. For a fixed level of <b>Type</b> <b>I</b> <b>error</b> rate, use of these statistics minimizes Type II error rates (equivalent to maximizing power). The following terms describe tests in terms of such optimality: ...|$|E
2500|$|Before {{the test}} is {{actually}} performed, the maximum acceptable probability of a <b>Type</b> <b>I</b> <b>error</b> (α) is determined. Typically, values {{in the range of}} 1% to 5% are selected. (If the maximum acceptable error rate is zero, an infinite number of correct guesses is required.) Depending on this Type 1 error rate, the critical value c is calculated. For example, if we select an error rate of 1%, c is calculated thus: ...|$|E
50|$|The Byrd {{study had}} an {{inconsistent}} pattern of only six positive outcomes amongst 26 specific problem conditions. A systematic review suggested this indicates possible <b>Type</b> <b>I</b> <b>errors.</b>|$|R
3000|$|... {{some of the}} {{outliers}} in q^aux don’t have a correspondence in q, i.e., {{the fuzzy}} rules introduce additional outliers not supported by real data (<b>type</b> <b>I</b> <b>errors).</b> We will call such outliers false outliers.|$|R
30|$|A {{simulation}} {{study indicated}} that when the true model is specified, each of the proposed marginalized mixture models provides low biases, <b>Type</b> <b>I</b> <b>errors</b> and confidence interval coverages close to the nominal levels. As shown in additional simulation studies reported in Benecha et al. (2017), model mis-specification can result in undercoverage and inflated <b>Type</b> <b>I</b> <b>errors.</b> Use of empirical covariance estimation as proposed by Long et al. (2014) for MZIP models would likely improve coverage and <b>Type</b> <b>I</b> <b>errors</b> for large samples. In any case, assessment of model goodness-of-fit is highly recommended. Unfortunately, such assessment is often hampered by computational difficulties in fitting complex models such as MNB-Pois when the data at hand do not contain sufficient information to estimate all the model parameters. Reducing the number of covariate parameters often provides an expeditious remedy for this situation. Another advance would be to develop score tests for goodness of fit, as proposed by Ridout et al. (2001) in comparing ZIP and ZINB models, {{that do not require}} fitting the model under the alternative hypothesis.|$|R
2500|$|The p-value {{is widely}} used in {{statistical}} hypothesis testing, specifically in null hypothesis significance testing. In this method, as part of experimental design, before performing the experiment, one first chooses a model (the null hypothesis) and a threshold value for p, called the significance level of the test, traditionally 5% or 1% [...] and denoted as α. If the p-value {{is less than the}} chosen significance level (α), that suggests that the observed data is sufficiently inconsistent with the null hypothesis that the null hypothesis may be rejected. However, that does not prove that the tested hypothesis is true. When the p-value is calculated correctly, this test guarantees that the <b>Type</b> <b>I</b> <b>error</b> rate is at most α. For typical analysis, using the standard α=0.05 cutoff, the null hypothesis is rejected when p < [...]05 and not rejected when p > [...]05. The p-value does not, in itself, support reasoning about the probabilities of hypotheses but is only a tool for deciding whether to reject the null hypothesis.|$|E
2500|$|When {{the test}} subject {{correctly}} predicts all 25 cards, {{we will consider}} him clairvoyant, and reject the null hypothesis. Thus also with 24 or 23 hits. With only 5 or 6 hits, on the other hand, there is no cause to consider him so. But what about 12 hits, or 17 hits? What is the critical number, c, of hits, at which point we consider the subject to be clairvoyant? How do we determine the critical value c? It is obvious that with the choice c=25 (i.e. we only accept clairvoyance when all cards are predicted correctly) we're more critical than with c=10. In the first case almost no test subjects will be recognized to be clairvoyant, in the second case, a certain number will pass the test. In practice, one decides how critical one will be. That is, one decides how often one accepts an error of the first kind – a false positive, or <b>Type</b> <b>I</b> <b>error.</b> With c = 25 the probability of such an error is: ...|$|E
5000|$|Each {{hypothesis}} testing involves a set {{risk of a}} <b>type</b> <b>I</b> <b>error</b> (the alpha rate). If a researcher searches or [...] "fishes" [...] through their data, testing many different hypotheses to find a significant effect, they are inflating their <b>type</b> <b>I</b> <b>error</b> rate. The more the researcher repeatedly tests the data, the higher the chance of observing a <b>type</b> <b>I</b> <b>error</b> and making an incorrect inference {{about the existence of}} a relationship.|$|E
40|$|Three {{factors are}} related in {{analyses}} of performance curves such as learning curves: {{the amount of}} training, the learning algorithm, and performance. Often {{we want to know}} whether the algorithm affects performance and whether the effect of training on performance depends on the algorithm. Analysis of variance would be an ideal technique but for carryover effects, which violate the assumptions of parametric analysis of variance and can produce dramatic increases in <b>Type</b> <b>I</b> <b>errors.</b> We propose a novel, randomized version of the two-way analysis of variance which avoids this problem. In experiments we analyze <b>Type</b> <b>I</b> <b>errors</b> and the power of our technique, using common machine learning datasets. ...|$|R
30|$|For {{the present}} study, {{compulsory}} schooling regulation may present an econometric problem that prevents us from estimating a counterfactual prevalence of paid employment or a confounder {{that makes it}} difficult to use theory to identify <b>type</b> <b>I</b> <b>errors.</b>|$|R
30|$|As a {{convenient}} way, {{we can find}} <b>type</b> <b>I</b> <b>errors</b> and their counts in the unigram frequency list of the segmentation results, and find type II errors and their counts in the bigram frequency list of the segmentation results.|$|R
50|$|Although it is {{sometimes}} claimed that least squares (or classical statistical methods in general) are robust, they are only robust {{in the sense that}} the <b>type</b> <b>I</b> <b>error</b> rate does not increase under violations of the model. In fact, the <b>type</b> <b>I</b> <b>error</b> rate tends to be lower than the nominal level when outliers are present, and there is often a dramatic increase in the type II error rate. The reduction of the <b>type</b> <b>I</b> <b>error</b> rate has been labelled as the conservatism of classical methods.|$|E
50|$|A <b>type</b> <b>I</b> <b>error</b> {{occurs when}} the null {{hypothesis}} (H0) is true, but is rejected. It is asserting something that is absent, a false hit. A <b>type</b> <b>I</b> <b>error</b> may be likened to a so-called false positive (a result that indicates that a given condition is present when it actually is not present).|$|E
50|$|A {{test with}} a higher {{specificity}} has a lower <b>type</b> <b>I</b> <b>error</b> rate.|$|E
40|$|This article {{shows that}} any single-step or {{stepwise}} multiple testing procedure (asymptotically) controlling the family-wise error rate (FWER) can be augmented into procedures that (asymptotically) control tail probabilities {{for the number}} of false positives and the proportion of false positives among the rejected hypotheses. Specifically, given any procedure that (asymptotically) controls the FWER at level alpha, we propose simple augmentation procedures that provide (asymptotic) level-alpha control of: (i) the generalized family-wise <b>error</b> rate, <b>i.</b> e., the tail probability, gFWER(k), that the number of <b>Type</b> <b>I</b> <b>errors</b> exceeds a user-supplied integer k, and (ii) the tail probability, TPPFP(q), that the proportion of <b>Type</b> <b>I</b> <b>errors</b> among the rejected hypotheses exceeds a user-supplied value 0...|$|R
50|$|Duncan {{developed}} this test as {{a modification of}} the Newman-Keuls method that would have greater power. Duncan's MRT is especially protective against false negative (Type II) error {{at the expense of}} having a greater risk of making false positive (<b>Type</b> <b>I)</b> <b>errors.</b>|$|R
40|$|This {{technical}} report describes a simple method for using inertial data to facilitate locomotion in virtual environments. The means of locomotion for virtual environments contributes {{greatly to the}} users’ sense of presence in the environment. Although walking-in-place is not so realistic as real walking, {{it is a very}} cost-effective alternative. Using a simple version of this algorithm both <b>Type</b> <b>I</b> and <b>Type</b> II <b>errors</b> were decreased compared to a previous walking-in-place algorithm. <b>Type</b> <b>I</b> <b>errors</b> occur when the algorithm detects a step when none occurred. Type II errors occur when the algorithm does not detect a step when one did occur. <b>Type</b> <b>I</b> <b>errors</b> decreased from 3 % to 1 % and Type II errors decreased from 32 % to 11 %. The more complex version of this algorithm promises even better results...|$|R
