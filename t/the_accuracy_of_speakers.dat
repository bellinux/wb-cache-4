0|10000|Public
40|$|Nowadays, {{there exist}} many ways for speaker {{identification}} using different classifiers. But {{the problem is}} that all these methods use numerous and sometimes repetitive and unsuitable features that increase the modeling costs and decreases <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> identification. So, we seek a way which has the ability to select the best subset of the extracted features and to increase <b>the</b> <b>accuracy</b> <b>of</b> <b>the</b> classifier for <b>speaker</b> identification. In this paper, we have proposed a method based on synergy Genetic evolutionary algorithm with Gaussian Mixture Model classifier considering the selected suitable features of speech to improve the efficiency <b>of</b> text independent <b>speaker</b> identification systems. The algorithm was examined using a sample consisting of 40 people ranging from 30 to 50 years of age who had been selected randomly from Fars Data base. Experimental results showed that the proposed algorithm compared with the GMM classifier, increased <b>the</b> mean <b>accuracy</b> <b>of</b> <b>speaker</b> identification to 11. 37 % and decreased the mean number of selected features to 53. 84 %...|$|R
40|$|This book {{discusses}} speaker recognition {{methods to}} deal with realistic variable noisy environments. The text covers authentication systems for; robust noisy background environments, functions in real time and incorporated in mobile devices. The book focuses on different approaches to enhance <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> recognition in presence of varying background environments. The authors examine: (a) Feature compensation using multiple background models, (b) Feature mapping using data-driven stochastic models, (c) Design of super vector- based GMM-SVM framework for robust speaker recognition, (d) Total variability modeling (i-vectors) in a discriminative framework and (e) Boosting method to fuse evidences from multiple SVM models...|$|R
50|$|Mastering {{requires}} critical listening; however, {{software tools}} exist {{to facilitate the}} process. Mastering is a crucial gateway between production and consumption and, as such, it involves technical knowledge as well as specific aesthetics. Results still depend upon <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> monitors and <b>the</b> listening environment. Mastering engineers may also need to apply corrective equalization, dynamic compression, and stereo reconfiguration processes in order to optimise sound translation on all playback systems. It is standard practice to make {{a copy of a}} master recording, known as a safety copy, in case the master is lost, damaged or stolen.|$|R
40|$|Abstract: When {{parallel}} {{methods are}} used {{in order to raise}} the confidence degree, automatic speaker identification and impostors detection, requires a weighted inference of multiple tools. Special re-corded test data were mainly composed of Romanian vowels, command words, numbers, short sen-tences, pronounced in different moments, by different speakers- as gender and age. We propose a fuzzy emotional evaluation <b>of</b> the <b>speaker’s</b> voice in order to detect impostors. An aide-decision system with weighted certainty coefficients and a decision reinforcing schema was conceived to increase <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> identity detection. Further researches will be conducted on the opportunity of using weighted decision to conceive medical training systems dedicated to hearing impaired patients...|$|R
40|$|It {{has been}} shown in several recent {{publications}} that application of vocal tract normalization (VTN) is a successful method for improving <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> independent recognisers. We argue that VTN can be implemented in the filterbank domain and propose a model to achieve this. We show how the model can be implemented directly in the MFCC domain, where it may be viewed as a constrained version of maximum likelihood linear regression (MLLR). The parameter estimates produced by the model are in accord with our ideas about how it should operate to perform VTN. Recognition results on a phoneme recognition task are presented which show a small improvement in accuracy...|$|R
40|$|This paper {{presents}} an experimental {{study on the}} impact of telephone channels on <b>the</b> <b>accuracy</b> <b>of</b> automatic <b>speaker</b> recognition. Speaker models and the design of the recognizer used in this study are based on Hidden Markov models. In order to simulate telephonequality speech signals, several experimental conditions were introduced taking two control factors into consideration: the type of the applied codec and the probability of transmission errors. In addition, the impact of echo signals – that are often present in Internet telephony – on <b>the</b> <b>accuracy</b> <b>of</b> automatic <b>speaker</b> recognition systems is considered. Finally, the paper provides a brief overview of several methodologies for the adaptation of the recognizer to the expected environmental conditions that may enhance the robustness <b>of</b> the <b>speaker</b> recognizer...|$|R
40|$|Abstract [...] The {{significant}} progress in speaker recognition is by the use <b>of</b> pitch parameters. <b>The</b> <b>accuracy</b> <b>of</b> <b>speaker</b> recognition to a known degree of confidence for forensic tasks is given by the pitch. The experiments performed discriminates an individual and gives the model <b>of</b> a <b>speaker.</b> The analysis is performed on the TIMIT database {{and the results are}} 87 % effective and are verified by using PRAAT Tool. The ultimate aim of the present work is to develop the <b>speaker</b> model consisting <b>of</b> pitch and statistical measurement results to identify and verify the speaker. We evaluate these features for speaker identi£cation using TIMIT. Further the work is in progress to improve <b>the</b> speaker recognition <b>accuracy</b> by computing spectral moments for each spectrum...|$|R
40|$|This is the {{accepted}} {{version of a}} chapter published in Proceedings Interspeech 2012. Citation for the original published chapter: Edlund, J., Heldner, M., Gustafson, J. (2012) On {{the effect of the}} acoustic environment on <b>the</b> <b>accuracy</b> <b>of</b> perception <b>of</b> <b>speaker</b> orientation from auditory cues alone. In: (ed.), Proceedings Interspeech 2012 Portland, USA: ISCA N. B. When citing this work, cite the original published chapter. Permanent link to this version: [URL] On the effect of the acoustic environment on <b>the</b> <b>accuracy</b> <b>of</b> perception <b>of</b> <b>speaker</b> orientation from auditory cues alon...|$|R
40|$|The {{present study}} was {{conducted}} to evaluate <b>the</b> <b>accuracy</b> affecting factors <b>of</b> a Mel-Frequency Cepstral Coefficients (MFCC) and Vector Quantization (VQ) based speaker recognition system. This investigation analyses the factors that affecting recognition accuracy using speech signal from day to day life in surrounding environments. It was studied the mismatch affects of text-dependency, voice sample length, speaking language, speaking style, mimicry, the quality of microphone, utterance sample quality and surrounding noise. The corpuses of 10 people of 20 utterance subjects were collected which were indicate that any mismatch degrades recognition accuracy. It was found that most dominating factors that degrades <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> recognition systems were surrounding noise, quality of microphone by which voice sample were collected, disguise, and degrading of the sample rate and quality. Speech-related factors and sample length were less critical...|$|R
40|$|Abstract—Speaker diarization {{is defined}} as the task of {{determining}} “who spoke when ” given an audio track and no other prior knowledge of any kind. The following article shows how a state-of-the-art speaker diarization system can be improved by combining traditional short-term features (MFCCs) with prosodic and other long-term features. First, we present a framework to study the <b>speaker</b> discriminability <b>of</b> 70 different long-term features. Then, we show how the top-ranked long-term features can be combined with short-term features to increase <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> diarization. <b>The</b> results were measured on standardized datasets (NIST RT) and show a consistent improvement of about 30 % relative in diarization error rate compared to the best system presented at the NIST evaluation in 2007. Index Terms—Long-term features, prosody, speaker diarization. I...|$|R
40|$|Several {{features}} {{have been}} proposed for automatic speaker recognition. Despite their noise sensitivity, lowlevel spectral features {{are the most popular}} ones because of their easy computation. Although in principle different spectral representations carry similar information (spectral shape), in practice the different features differ in their performance. For instance, LPC-cepstrum picks more “details ” of the short-term spectrum than the FFTcepstrum with the same number of coefficients. In this work, we consider using multiple spectral presentations simultaneously for improving <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> recognition. We use the following feature sets: melfrequency cepstral coefficients (MFCC), LPC-cepstrum (LPCC), arcus sine reflection coefficients (ARCSIN), formant frequencies (FMT), and the corresponding deltaparameters of all feature sets. We study the two ways of combining the feature sets: feature-level fusion (feature vector concatenation), score-level fusion (soft combination of classifier outputs), and decision-level fusion (combination of classifier decision). ...|$|R
40|$|Abstract: <b>The</b> <b>accuracy</b> <b>of</b> {{present day}} <b>speaker</b> {{identification}} systems (SID) is degraded in the adverse acoustical environments i. e., by {{different kinds of}} interferences. The idea of usable speech is to identify and extract those portions of degraded speech which are considered useful for speaker identification. Recently, a usable speech extraction system was proposed to classify co-channel speech as usable speech and unusable speech for speaker identification. Speech segments can be declared usable based upon a Target-to-Interferer energy ratio (TIR). Studies indicate that {{a significant amount of}} the co-channel data is considered usable for speaker identification. By considering only usable speech for <b>speaker</b> identification instead <b>of</b> the corrupted co-channel speech, it is seen that there is an increase in <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> identification. A novel usable speech detection measure using the sinusoidal model of speech and ESPRIT (Estimation of Signal Parameters via Rotational Invariance Technique) Spectral Estimation is proposed and investigated, which resulted in 82 % correct detection of usable speech segments based on TIR. The usable speech frames extracted using ESPRIT when tested with the speaker identification system, resulted in 84 % accuracy in detecting speaker identity as compared to using entire co-channel speech which resulted in only 45 % accuracy. 1...|$|R
40|$|Similarity {{measurement}} is {{an important}} part <b>of</b> <b>speaker</b> Identification. This study has modified the similarity measurement technique performed in previous studies. Previous studies used the sum of the smallest distance between the input vectors and the codebook vectors <b>of</b> a particular <b>speaker.</b> In this study, the technique has been modified by selecting a particular speaker codebook which hes the highest frequency of vector pairs. Vector pair in this case is the smallest distance between the input vector and the vector in the codebook. This study used Mel Frequency Cepstral Coefficient (MFCC) es feature extraction, Self Organizing Map (SOM) as codebook maker and Euclidean as a measure of distance. The experimental results showed that the similarity measuring techniques proposed can improve <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> Identification. In <b>the</b> MFCC coefficients 13, 15 and 20 <b>the</b> average <b>accuracy</b> <b>of</b> identification respec;tively increased BS much BS 0. 61 %, 0. 98 % and 1. 27 %...|$|R
40|$|Today, {{more and}} more people have benefited from the <b>speaker</b> {{recognition}}. However, <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> recognition often drops off rapidly because of the low-quality speech and noise. This paper proposed a new speaker recognition model based on wavelet packet entropy (WPE), i-vector, and cosine distance scoring (CDS). In the proposed model, WPE transforms the speeches into short-term spectrum feature vectors (short vectors) and resists the noise. I-vector is generated from those short vectors and characterizes speech to improve <b>the</b> recognition <b>accuracy.</b> CDS fast compares with the difference between two i-vectors to give out the recognition result. The proposed model is evaluated by TIMIT speech database. The results of the experiments show that the proposed model can obtain good performance in clear and noisy environment and be insensitive to the low-quality speech, but the time cost of the model is high. To reduce the time cost, the parallel computation is used...|$|R
40|$|The {{following}} article {{shows how}} a state-of-the-art speaker diarization {{system can be}} improved by combining traditional short-term features (MFCCs) with prosodic and other longterm features. First, we present a framework to study the <b>speaker</b> discriminability <b>of</b> 70 different long-term features. Then, we show how the top-ranked long-term features can be combined with short-term features to increase <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> diarization. <b>The</b> results were measured on standardized data sets (NIST RT) and show a consistent improvement of about 30 % relative in diarization error rate compared to the best system presented at the NIST evaluation in 2007. This result was also verified on a wide set of meetings, which we call CombDev, that contains 21 meetings from previous evaluations. Since the prosodic and long-term features were selected using a diarization-independent speakerdiscriminability study, {{we are confident that}} the same features are able to improve other systems that perform similar task...|$|R
40|$|<b>The</b> <b>accuracy</b> <b>of</b> <b>speaker</b> diarisation in {{meetings}} {{relies heavily on}} determining the correct number <b>of</b> <b>speakers.</b> In this paper we present a novel algorithm based on time difference of arrival (TDOA) features that aims to find the correct number <b>of</b> active <b>speakers</b> in a meeting and thus aid the speaker segmentation and clustering process. With our proposed method the microphone array TDOA values and known geometry of the array are used to calculate a speaker matrix from which we determine the correct number <b>of</b> active <b>speakers</b> {{with the aid of}} the Bayesian information criterion (BIC). In addition, we analyse several well-known voice activity detection (VAD) algorithms and verified their fitness for meeting recordings. Experiments were performed using the NIST RT 06, RT 07 and RT 09 data sets, and resulted in reduced error rates compared with BIC-based approaches. Index Terms — Speaker diarisation {{in meetings}}, microphone array, time difference of arrival (TDOA), speech segmentation and clustering, BIC, voice activity detection (VAD) 1...|$|R
40|$|Abstract: <b>The</b> <b>accuracy</b> <b>of</b> <b>speaker</b> {{identification}} systems degrades when {{operating in}} an adverse acoustical environment {{due to a}} reduction in the quality of the speech being input into the system. Speech which is corrupted by stationary or non-stationary noise or interferer speech could be improved by taking only the portions which are minimally degraded but still useful for speaker identification systems. The extracted speech, which is also known as usable speech, could then be input into a speaker identification system to improve its accuracy. Several usable speech measures have been developed to identify usable segments from degraded speech. Unfortunately, the currently available measures only detect about 74 % of usable speech, with 26 % false alarms. To improve the classification rate, a decision level fusion technique, consensus theory, with an intelligent weighing scheme based on classifier’s performance has been used. The experiment shows an improvement of 8 % in correct detection and 22 % reduction in false alarms from the best performing usable speech measure...|$|R
40|$|In {{this work}} the {{objective}} is to increase <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> dependent phonetic transcription of spoken utterances using continuous density and semi-continuous HMMs. Experiments with LVQ based corrective tuning indicate that the average recognition error rate can be made to decrease about 5 % [...] 10 %. Experiments are also made to increase the efficiency of the Viterbi decoding by a discriminative approximation of the output probabilities of the states in the Markov models. Using only a few nearest components of the mixture density functions instead of every component decreases both the recognition error rate (5 % [...] 10 % for CDHMMs) and the execution time (about 50 % for SCHMMs). The lowest average error rates achieved were about 5. 6 %. 1 INTRODUCTION Several suggestions have been recently published, describing training methods for HMMs using the minimization of the number of misclassifications directly as a training criterium. A formal way to realize this criterium is to define a cont [...] ...|$|R
40|$|In this paper, {{we present}} a new {{longitudinal}} and bilingual broadcast database designed for speaker clustering and text- independent verification research. The broadcast data is ex- tracted from the archives of Omrop Fryslaˆn which is the re- gional broadcaster {{in the province of}} Fryslaˆn, located in the north of the Netherlands. Two speaker verification tasks are provided in a standard enrollment-test setting with language consistent trials. The first task contains target trials from all speakers available appearing in at least two different programs, while the second task contains target trials from a subgroup <b>of</b> <b>speakers</b> appearing in programs recorded in multiple years. The second task is designed to investigate the effects <b>of</b> ageing on <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> verification systems. This database also contains unlabeled spoken segments from different radio pro- grams for speaker clustering research. We provide the output <b>of</b> an existing <b>speaker</b> diarization system for baseline verification experiments. Finally, we present the baseline speaker verifi- cation results using the Kaldi GMM- and DNN-UBM speaker verification system. This database will be an extension to the recently presented open source Frisian data collection and it is publicly available for research purposes...|$|R
40|$|An {{important}} application <b>of</b> <b>speaker</b> {{recognition is}} forensics. However, <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> recognition in forensic cases often drops off rapidly {{because of the}} ill effect of ambient noise, variable channel, different duration of speech data, and so on. Therefore, finding a robust speaker recognition model {{is very important for}} forensics. This paper builds a new speaker recognition model based on wavelet cepstral coefficient (WCC), i-vector, and cosine distance scoring (CDS). This model firstly uses the WCC to transform the speech into spectral feature vecors and then uses those spectral feature vectors to train the i-vectors that represent the speeches having different durations. CDS is used to compare the i-vectors to give out the evidence. Moreover, linear discriminant analysis (LDA) and the within-class covariance normalization (WCNN) are added to the CDS algorithm to deal with the channel variability problem. Finally, the likelihood ratio estimates the strength of the evidence. We use the TIMIT database to evaluate the performance of the proposed model. The experimental results show that the proposed model can effectively solve the troubles of forensic scenario, but the time cost of the method is high...|$|R
40|$|Speaker {{recognition}} {{has been}} developed and evolved {{over the past few}} decades into a supposedly mature technique. Existing methods typically utilize robust features extracted from clean speech. In real-world applications, especially security and forensics related ones, reliability of recognition becomes crucial, meanwhile limited speech samples and adverse acoustic conditions, most notably noise and reverberation, impose further complications. This paper is presented from a study into the behavior <b>of</b> typical <b>speaker</b> recognition systems in adverse retrieval phases. Following a brief review, a speaker recognition system was implemented using the MSR Identity Toolbox by Microsoft. Validation tests were carried out with clean speech and the speech contaminated by noise and/or reverberation of varying degrees. The image source method was adopted to take into account real acoustic conditions in the spaces. Statistical relationships between recognition accuracy and signal to noise ratios or reverberation times have therefore been established. Results show noise and reverberation can, to different extents, degrade the performance of recognition. Both reverberation time and direct to reverberation ratio can affect recognition <b>accuracy.</b> <b>The</b> findings may be used to estimate <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> recognition and further determine the likelihood a particular speaker...|$|R
40|$|Speaker {{recognition}} {{can be used}} as {{a security}} means to authenticate the speaker or as a forensic tool to determine who is likely to be the talker. For such critical applications, robustness or reliability of the system is crucial. In spite of the development and advancement in the field <b>of</b> <b>speaker</b> recognition, there are still many limitations and challenges. Amongst these, environment factors, in particular background noise and reverberation, are known to be difficult to tackle. Environmental noises and reverberation compromise <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> recognition, especially when the signal to noise ratio (SNR) becomes low and reverberation time is long. Noises and reverberation mitigate reliability <b>of</b> <b>speaker</b> recognition systems via signal transmission channel mismatch. This paper is presented from attempts to improve system robustness by adding noises and convoluting room impulse responses in the training phase of typical Gaussian Mixture Model based speaker recognition systems. Validation tests were carried with emulated noisy and reverberant conditions with controlled signal to noise ratios and reverberation times. Two scenarios have been considered the first one used the clean speech samples in enrolment phase and the second included noisy or reverberant samples in enrolment phase, thus the potentials and limitations of including noisy and reverberant samples in the training phase to improve system robustness is identified...|$|R
40|$|The domain area of {{this topic}} is Bio-metric. Speaker Recognition is {{biometric}} system. This paper deals with speaker recognition by HMM (Hidden Markov Model) method. The recorded speech signal contains background noise. This noise badly affects <b>the</b> <b>accuracy</b> <b>of</b> <b>speaker</b> recognition. Discrete Wavelet Transforms (DWT) greatly reduces the noise present in input speech signal. DWT often outperforms {{as compared to}} Fourier Transform, due to its capability to represent the signal precisely, in both frequency & time domain. Wavelet thresholding is applied to separate the speech and noise, enhancing the speech consequently. The system is able to recognize the speaker by translating the speech waveform into a set of feature vectors using Mel Frequency Cepstral Coefficients (MFCC) technique. But, input speech signals at different time may contain variations. Same speaker may utter the same word at different speed which gives us variation in total number of MFCC coefficients. Vector Quantization (VQ) is used to make same number of MFCC coefficients. Hidden Markov Model (HMM) provides a highly reliable way for recognizing a speaker. Hidden Markov Models have been widely used, which are usually considered {{as a set of}} states with Markovian properties and observations generated independently by those states. With the help of Viterbi decoding most likely state sequence is obtained. This state sequence is used for speaker recognition. For a database of size 50 in normal environment, obtained result is 98 % which is better than previous methods used for speaker recognition...|$|R
3000|$|... [...]), we can {{notice that}} female {{speakers}} give better system overall accuracy. This difference {{is more than}} 2 % in case where nonnative speakers {{are involved in the}} training and the native ones in the test. On the other hand, the improvement <b>of</b> using female <b>speakers</b> is almost 5 % when native speakers are used in the training and nonnative speakers in the test. By incorporating a LM and by considering the word level (i.e., in experiments Exp. 6 (b) through Exp. 9 (b), we see that the argument is inverted. In other words, the LM improved <b>the</b> <b>accuracy</b> <b>of</b> male <b>speakers</b> in a much better way than in the case <b>of</b> female <b>speakers.</b>|$|R
40|$|This paper {{presents}} {{investigations into}} the performance <b>of</b> open-set, text-independent <b>speaker</b> identification (OSTI-SI) under mismatched data conditions. The scope of the study includes attempts to reduce the adverse effects of such conditions through {{the introduction of a}} modified parallel model combination (PMC) method together with condition-adjusted T-Norm (CT-Norm) into the OSTI-SI framework. The experiments are conducted using examples of real world noise. Based on the outcomes, it is demonstrated that the above approach can lead to considerable improvements in <b>the</b> <b>accuracy</b> <b>of</b> open-set <b>speaker</b> identification operating under severely mismatched data conditions. The paper details the realisation of the modified PMC method and CT-Norm in the context of OSTI-SI, presents the experimental investigations and provides an analysis of the results...|$|R
40|$|With {{the recent}} advance in {{microphone}} array speech processing, achieving robustness <b>of</b> <b>speaker</b> localization becomes most significant aspect. At {{the same time}} considerable research growth is performed in developing the multiple microphone sensors equipped rooms are developed also called as smart rooms for real time applications. <b>The</b> <b>accuracy</b> <b>of</b> <b>speaker</b> localization is down casted by acoustic noise and room reverberations. In distributed meeting environment speaker localization is performed by far field microphone arrays {{with the help of}} beamforming. But far field Microphone performance is degraded by room reverberations and acoustic noise. In this master thesis, speaker localization with two adaptive beamforming techniques in distributed meeting application in reverberated environment with the help of far filed microphone arrays is design and implemented. The two beamforming methods examined are multichannel wiener beamformer and multichannel sub band wiener beamformer. These methods use wiener filtering technique for their implementation and they are implemented to capture the human voice using widely separated microphone arrays even when irregular disturbances are present. A smart room is developed with Image source model for generating reverberation in which beamformers are implemented. In sub band beamformer WOLA filter bank is designed. The sub band beamforming is further extended to steered response power with phase transform for speaker localization is achieved with the cross correlation but speech is heavily degraded by the noise which can be further studied to eliminated it. Finally the quality of the speech is tested using SNR and PESQ (Perceptual Evaluation of Speech Quality) and also the performance of the system with respect to reverberation time is calculated. The results show that the two implementations are acceptable in terms of PESQ score. 073478450...|$|R
40|$|The {{diffusion}} of Device-to-Device (D 2 D) communications {{opens the door}} to exploit the contributions of multiple Mobile Devices (MDs) to accomplish collaborative tasks. In this paper a speaker recognition algorithm for MDs based on a multiple-observations approach is presented. We propose various fusion and clustering algorithms aimed at efficiently exploiting data coming from MDs. Numerical results show that in many cases our multiple-observation approach is able to significantly improve <b>the</b> <b>accuracy</b> <b>of</b> <b>the</b> considered <b>speaker</b> recognition algorithm...|$|R
40|$|Abstract—In this {{research}} paper, {{we have developed}} a system that identifies users by their voices and helped them to retrieve the information using their voice queries. The system takes into account speaker identification as well as speech recognition i. e. two pattern recognition techniques in speech domain. The conglomeration <b>of</b> <b>speaker</b> identification task and speech recognition task provides multitude of facilities in comparison to isolated approach. The speaker identification task is achieved by using SVM where as speech recognition is based on HMM. We have used {{two different types of}} corpora for training the system. Gamma tone cepstral coefficients and mel frequency cepstral coefficients are extracted for speaker identification and speech recognition respectively. <b>The</b> <b>accuracy</b> <b>of</b> <b>the</b> system is measured from two perspective i. e. <b>accuracy</b> <b>of</b> <b>speaker</b> identity and <b>accuracy</b> <b>of</b> speech recognition task. <b>The</b> <b>accuracy</b> <b>of</b> <b>the</b> <b>speaker</b> identification is enhanced by adopting the speech recognition at the initial stage <b>of</b> <b>speaker</b> identification. Index Terms—Speaker identification, speech recognition, mel-frequency cepstral coefficients, gammatone frequency cepstral coefficients, support vector machine. I...|$|R
40|$|Full text of {{this paper}} is not {{available}} in the UHRA. This paper presents investigations into the performance <b>of</b> open-set, text-independent <b>speaker</b> identification (OSTI-SI) under mismatched data conditions. The scope of the study includes attempts to reduce the adverse effects of such conditions through the introduction of a modified parallel model combination (PMC) method together with condition-adjusted T-Norm (CT-Norm) into the OSTI-SI framework. The experiments are conducted using examples of real world noise. Based on the outcomes, it is demonstrated that the above approach can lead to considerable improvements in <b>the</b> <b>accuracy</b> <b>of</b> open-set <b>speaker</b> identification operating under severely mismatched data conditions. The paper details the realisation of the modified PMC method and CT-Norm in the context of OSTI-SI, presents the experimental investigations and provides an analysis of the results...|$|R
40|$|<b>Accuracy</b> <b>of</b> an {{automatic}} <b>speaker</b> recognition system predominantly depends on speaker models and {{features that are}} used. An influence of the shape of auditory critical bands and a contribution of individual components of MFCC-based feature vectors are investigated in the paper and some experimental results are presented and showed their impact on <b>the</b> <b>accuracy</b> <b>of</b> automatic <b>speaker</b> recognition. <b>The</b> speaker-discrimination capability of the MFCCs was experimentally determined by comparing training and test models for the same speaker. The experiments are conducted with three speech databases and showed that 0 th and 19 th (the last one) MFCCs are non speaker discriminative. The values of MFCCs {{are determined by the}} type of applied auditory critical band. The exponential auditory critical bands based on the lower part of exponential function have outperformed <b>the</b> <b>speaker</b> recognition <b>accuracy</b> <b>of</b> other auditory critical bands such as rectangular or triangular shape...|$|R
40|$|We {{evaluate}} <b>the</b> <b>accuracy</b> <b>of</b> an MFCC-based <b>speaker</b> recognition method. We analyse {{the recognition}} results using speech signal from everyday life environments. We study the mismatch effects of text-dependency, sample length, language, style of speaking, cheating, microphone, sample quality, and noise. The experiments on a self-collected corpus of 30 subjects indicate that any mismatch degrades recognition <b>accuracy.</b> <b>The</b> most dominating factors are noise, microphone, disguise, and degrading {{of the sample}} rate and quality. Speech-related factors and sample length are less critical...|$|R
40|$|Speech Segmentation is {{the process}} change point {{detection}} for partitioning an input audio stream into regions each of which corresponds to only one audio source or one <b>speaker.</b> One application <b>of</b> this system is in Speaker Diarization systems. There are several methods for speaker segmentation; however, most <b>of</b> the <b>Speaker</b> Diarization Systems use BIC-based Segmentation methods. The main goal {{of this paper is}} to propose a new method for speaker segmentation with higher speed than the current methods - e. g. BIC - and acceptable accuracy. Our proposed method is based on the pitch frequency <b>of</b> <b>the</b> speech. <b>The</b> <b>accuracy</b> <b>of</b> this method is similar to <b>the</b> <b>accuracy</b> <b>of</b> common <b>speaker</b> segmentation methods. However, its computation cost is much less than theirs. We show that our method is about 2. 4 times faster than the BIC-based method, while <b>the</b> average <b>accuracy</b> <b>of</b> pitch-based method is slightly higher than that of the BIC-based method. Comment: 14 pages, 8 figure...|$|R
40|$|In this paper, a {{technique}} for audio indexing based on speaker identification is proposed. When speakers are known a priori, a speaker index {{can be created}} in real time using the Viterbi algorithm to segment the audio into intervals from a single talker. Segmentation is performed using a hidden Markov model network consisting <b>of</b> interconnected <b>speaker</b> sub-networks. Speaker training data is used to initialize sub-networks for each speaker. Sub-networks {{can also be used}} to model silence, or non-speech sounds such as a musical theme. When no prior knowledge <b>of</b> the <b>speakers</b> is available, unsupervised segmentation is performed using a nonreal time iterative algorithm. The speaker sub-networks are first initialized, and segmentation is performed by iteratively generating a segmentation using the Viterbi algorithm, and retraining the sub-networks based on the results of the segmentation. Since <b>the</b> <b>accuracy</b> <b>of</b> <b>the</b> <b>speaker</b> segmentation depends on how well the speaker sub-networks are initialize [...] ...|$|R
40|$|A {{common problem}} in speech {{recognition}} for foreign accented speech {{is that there}} is not enough training data for an accent-specific or a speaker-specific recognizer. Speaker adaptation can be used to improve <b>the</b> <b>accuracy</b> <b>of</b> a <b>speaker</b> independent recognizer, but a lot of adaptation data is needed for speakers with a strong foreign accent. In this paper we propose a rather simple and successful technique of stacked transformations where the baseline models trained for native speakers are first adapted by using accent-specific data and then by another transformation using speaker-specific data. Because the accent-specific data can be collected offline, the first transformation can be more detailed and comprehensive, and the second one less detailed and fast. Experimental results are provided for speaker adaptation in English spoken by Finnish speakers. The evaluation results confirm that the stacked transformations are very helpful for fast speaker adaptation. Peer reviewe...|$|R
40|$|This work aims to {{understand}} and model the inter-modal temporal relations between the audio and visual modalities of speech and validate whether the captured relations can improve the performance of audio-visual bimodal modeling for such applications as audio-visual speaker identification. We propose to extend our audio-visual correlative model (AVCM) with explicit durational modeling of the partial temporal synchrony between the two speech modalities, i. e. where the audio may lead, lag or remain synchronized with the video. We refer to the new extended model as Durational-AVCM. Experiments on the CMU database and a homegrown database demonstrate that Durational-AVCM can improve <b>the</b> <b>accuracies</b> <b>of</b> audio-visual <b>speaker</b> identification {{at all levels of}} acoustic signal-to-noise ratios (SNR) from 0 dB to 30 dB with varying acoustic conditions compared to original AVCM model. The results indicate the importance of incorporating the partial temporal synchrony between audio and visual modalities for audio-visual bimodal modeling. 1...|$|R
40|$|Linear {{regression}} based speaker adaptation approaches {{can improve}} Automatic Speech Recognition (ASR) accuracy significantly for a target speaker. However, when the available adaptation data {{is limited to}} a few seconds, <b>the</b> <b>accuracy</b> <b>of</b> <b>the</b> <b>speaker</b> adapted models is often worse compared with speaker independent models. In this paper, we propose an approach to select a set <b>of</b> reference <b>speakers</b> acoustically close to the target speaker whose data can be used to augment the adaptation data. To determine the acoustic similarity <b>of</b> two <b>speakers,</b> we propose a distance metric based on transforming sample points in the acoustic space with the regression matrices <b>of</b> the two <b>speakers.</b> We show the validity of this approach through a speaker identification task. ASR results on SCOTUS and AMI corpora with limited adaptation data of 10 to 15 seconds augmented by data from selected reference speakers show a significant improvement in Word Error Rate over speaker independent and speaker adapted models...|$|R
