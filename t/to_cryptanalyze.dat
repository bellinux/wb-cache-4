40|10|Public
50|$|Ken Follett - The Key to Rebecca (1980), World War II spy novel whose plot {{revolves}} around the heroes' efforts <b>to</b> <b>cryptanalyze</b> a book cipher with time running out.|$|E
5000|$|This adversary (call it A1) {{will attempt}} <b>to</b> <b>cryptanalyze</b> its input by brute force. It {{has its own}} DES implementation. It gives a single query to its oracle, asking for the 64-bit string of all zeroes to be encrypted. Call the {{resulting}} ciphertext E0. It then runs an exhaustive key search.The algorithm looks like this: ...|$|E
50|$|Serge Vaudenay {{suggested}} using MDS matrices in cryptographic primitives {{to produce}} {{what he called}} multipermutations, not-necessarily linear functions with this same property. These functions have what he called perfect diffusion: changing t of the inputs changes at least m-t+1 of the outputs. He showed how to exploit imperfect diffusion <b>to</b> <b>cryptanalyze</b> functions that are not multipermutations.|$|E
50|$|The {{first was}} an {{incident}} during the Siberian Intervention. The Japanese army came into possession of some Soviet diplomatic correspondence, but their cryptanalysts were unable decipher the messages. Someone suggested asking the Polish military <b>to</b> try <b>cryptanalyzing</b> them. It took the Poles {{less than a}} week to break the code and read the messages.|$|R
40|$|Abstract. In some {{applications}} for synchronous stream ciphers, {{the risk of}} loss of synchronization cannot be eliminated completely. In these cases frequent resynchronization or resynchronization upon request may be necessary. In the paper it is shown that {{this can lead to}} significant deterioration of the cryptographic security. A powerful general attack on nonlinearly filtered linear (over Z 2) systems is presented. This attack is further refined <b>to</b> efficiently <b>cryptanalyze</b> a linear system with a multiplexer as output function. ...|$|R
30|$|In {{the case}} of hash functions, a design may even {{trade-off}} advanced security properties (such as collision resistance) for simplicity in some scenarios. A clear case is the construction of short Message Authentication Codes (MAC) from non-collision resistant hash functions, such as in SipHash [111], or digital signatures from short-input hash functions [112]. In conventional applications, BLAKE 2 [113] is a stronger drop-in replacement <b>to</b> recently <b>cryptanalyzed</b> standards [114] and faster in software than the recently published SHA- 3 standard [115].|$|R
50|$|Likewise, if code {{groups were}} used to {{indicate}} a switch to a new chart, this could also have weakened the code unnecessarily. In fact, Yardley specifically mentions it as making the codes easier <b>to</b> <b>cryptanalyze.</b> Generally speaking, substitution systems switch alphabets as often as possibly because that provides the best security. Their strength lies in how many alphabets they use and how randomly they switch between them.|$|E
50|$|Second {{generation}} systems (1970s) {{were all}} electronic designs based on vacuum tubes and transformer logic. Algorithms {{appear to be}} based on linear feedback shift registers, perhaps with some non-linear elements thrown in to make them more difficult <b>to</b> <b>cryptanalyze.</b> Keys were loaded by placing a punched card in a locked reader on the front panel. The cryptoperiod was still usually one day. These systems were introduced in the late 1960s and stayed in use until the mid-1980s. They required a great deal of care and maintenance, but were not vulnerable to EMP. The discovery of the Walker spy ring provided an impetus for their retirement, along with remaining first generation systems.|$|E
50|$|German {{military}} cryptographers {{failed to}} realize that their Enigma, T52 and other systems were insecure. Although many {{attempts were made to}} try and validate the security of the Enigma, which the whole of the Wehrmacht cryptographic infrastructure regarding secure communication, rested on, they failed. The reason for this, was they were unable to conduct sufficiently deep security tests to determine how secure they were. They were also unable to put forth the costly practical effort required to solve them. Their security tests were theoretical only, and they were unable to imagine what a large concerted effort at traffic analysis could achieve. A security measure which would have proved productive, was the issue of new Enigma rotors. However, so many Enigma machines were out in the field, that it would prove impractical to update them. OKW/Chi also felt that even if a particular Enigma unit was captured, it would still be considered secure, since no process was known by OKW/Chi that could break it. They also had not advanced sufficiently in cryptology to realize what could be achieved by a large combined engineering team. The Allies had undertaken that effort and had been reward with huge successes Also Germany was unable <b>to</b> <b>cryptanalyze</b> British and American high-grade systems (Ultra) carrying critical Allied data. As a result, OKW/Chi had no hint that their own high-grade systems were insecure.|$|E
40|$|Abstract. This paper {{presents}} three {{methods for}} strengthening public key cryptosystems {{in such a}} way that they become secure against adaptively chosen ciphertext attacks. In an adaptively chosen ciphertext attack, an attacker can query the deciphering algorithm with any ciphertexts, except for the exact object ciphertext <b>to</b> be <b>cryptanalyzed.</b> The rst strengthening method is based on the use of one-way hash functions, the second on the use of universal hash functions and the third on the use of digital signature schemes. Each method is illustrated by an example ofapublickey cryptosystem based on the intractability ofcomputing discrete logarithms in nite elds. Two other issues, namely applications of the methods to public key cryptosystems based on other intractable problems and enhancement of information authentication capability to the cryptosystems, are also discussed. ...|$|R
40|$|Yuliang Zheng and Jennifer Seberry ? The Centre for Computer Security Research Department of Computer Science University of Wollongong Locked Bag 8844, Wollongong, NSW 2521 AUSTRALIA E-mail: fyuliang,jennieg@cs. uow. edu. au Abstract. This paper {{presents}} three {{methods for}} strengthening public key cryptosystems {{in such a}} way that they become secure against adaptively chosen ciphertext attacks. In an adaptively chosen ciphertext attack, an attacker can query the deciphering algorithm with any ciphertexts, except for the exact object ciphertext <b>to</b> be <b>cryptanalyzed.</b> The first strengthening method is based on the use of one-way hash functions, the second on the use of universal hash functions and the third on the use of digital signature schemes. Each method is illustrated by an example of a public key cryptosystem based on the intractability of computing discrete logarithms in finite fields. Two other issues, namely applications of the methods to public key cryptosystems based on other i [...] ...|$|R
40|$|Abstract-This paper {{presents}} three {{methods for}} strengthening public key cryptosystems {{in such a}} way that they become secure against adaptively chosen ciphertext attacks. In an adaptively chosen ciphertext attack, an attacker can query the decipher-ing algorithm with any ciphertexts, except for the exact object ciphertext <b>to</b> be <b>cryptanalyzed.</b> The first strengthening method is based on the use of one-way hash functions, the second on the use of universal hash functions, and the third on the use of digital signature schemes. Each method is illustrated by an example of a public key cryptosystem based on the intractability of computing discrete logarithms in finite fields. Seeurity of the three example cryptosystems is formally proved. Two other issues, namely, applications of the methods to public key cryptosystems based on other intractable problems and enhancement of information authentication capability to the cryptosystems, are also discussed. Yuliang Zheng and Jennifer Seberry I...|$|R
40|$|Recently, Wang {{proposed}} a new method <b>to</b> <b>cryptanalyze</b> SHA- 1 and found collisions of the 58 -round SHA- 1. The complexity of Wang's method <b>to</b> <b>cryptanalyze</b> the 58 -round SHA- 1 is 2 34 SHA- 1 computation. Moreover, Wang et al. gave the complexity evaluation against the full SHA- 1 which is {{claimed to be}} 2 62. The aim {{of this article is}} to sophisticate and improve Wang's attack by using Gröbner basis techniques and to reduce the complexity of the attack for SHA- 1. In this article, we apply Gröbner basis techniques to a cryptanalysis of SHA- 1. We introduce a new notion of "semi-neutral bit" and propose an improved message modification technique based on Gr&quot;obner basis technique. In the case of the 58 -round SHA- 1, the complexity of an attack based on our improved message modification is...|$|E
40|$|Abstract-We {{examine the}} problem of {{deciphering}} a file that has been Huffman coded, but not otherwise encrypted. We find that a Huffman code can be surprisingly difficult <b>to</b> <b>cryptanalyze.</b> We present {{a detailed analysis of}} the situation for a three-symbol source alphabet and present some results for general finite alphabets. Index Terms-Huffman codes, cryptography, encoding rules, ambiguity, independent sources, Markov sources. I...|$|E
30|$|The file sizes {{taken as}} 1024 bits where {{as the key}} sizes are {{dependent}} on the algorithms. The encryption and cryptanalysis are done on an AMD™ Phenom™ Octa-core Processor and times are normalized. The time required <b>to</b> <b>cryptanalyze</b> One Time Pad (OTP) (Highland 1992) is taken as infinity (∞) and all other parameters are normalized against the maximum values in their corresponding range. The numerical values are tabulated.|$|E
40|$|Algebraic {{immunity}} (AI) is {{a property}} of a Boolean function f that measures its sus-ceptibility to an algebraic attack. If f {{has a low}} algebraic immunity and f is used in an encryption protocol, then there are ways <b>to</b> successfully <b>cryptanalyze</b> the system. As a result, {{it is important to}} have an efficient means to compute the algebraic immunity of Boolean functions. Unfortunately, algebraic immunity is one of the most complex crypto-graphic properties to compute. For example, it is significantly more difficult to compute than nonlinearity [2]. Here, we show the advantage of a reconfigurable computer in comput-ing a function’s algebraic immunity. For example, we show that a reconfigurable computer is 4. 9 times faster than a conventional computer in this computation for 5 -variable functions. Indeed, we compute the distribution of functions to algebraic immunity for all 5 -variable functions, a computation that has not been previously accomplished. Interestingly, the problem we address is to design a logic circuit that computes a characteristic of a logic function. ...|$|R
40|$|AbstractThe term “historical cryptanalysis” is {{introduced}} and {{defined as the}} solving of cryptograms whose keys have been lost or misplaced. Numerous examples of such cryptograms, many undeciphered since their deposit in archives hundreds of years ago, still exist and may contain important historical and literary information. Credit is given to previous investigators in this field, who often performed individual cryptanalytic feats of great brilliance. A new approach of international coordinated research in historical cryptanalysis [...] bringing together the scholars who discover cryptographic materials and those who can help with their cryptanalysis [...] is discussed. A few examples of solved cryptograms are presented to demonstrate some of the methods used <b>to</b> encipher and <b>cryptanalyze</b> information. Included are a Papal cipher containing information about {{the election of the}} king of Poland in 1573, a German cryptogram written by a participant in a key event of the Counter-Reformation [...] the choice of the archbishop of Cologne in 1583, a Venetian cipher from 1630 whose key may have been eaten to preserve the cipher's security, a Russian cryptanalyst's worksheet from the time of Catherine the Great, and a military cipher from the American Civil War...|$|R
40|$|The Matsumoto-Imai {{public key}} scheme was {{developed}} to provide very fast signatures. It is based on substitution polynomials over GF (2 m). This paper shows in two ways that the Matsumoto-Imai public key scheme {{is very easy to}} break. In the faster of the two attacks the time <b>to</b> <b>cryptanalyze</b> the scheme is about proportional to the binary length of the public key. This shows that Matsumoto and Imai greatl...|$|E
40|$|We {{introduce}} a problem setting {{which we call}} ``the freedom fighters' problem''. It subtly differs from the prisoners' problem. We propose a steganographic method that allows Alice and Bob to fool Wendy the warden in this setting. Their messages are hidden in encryption keys. The recipient has no prior knowledge of these keys, and has <b>to</b> <b>cryptanalyze</b> ciphertexts in order to recover them. We show {{an example of the}} protocol and give a partial security analysis...|$|E
40|$|We {{present a}} formal view of {{cryptography}} that overcomes the usual assumptions of formal models for reasoning about security of computer systems, i. e. perfect cryptography and Dolev-Yao adversary model. In our framework, equivalence among formal cryptographic expressions is parameterized by a computational adversary that may exploit {{weaknesses of the}} cryptosystem <b>to</b> <b>cryptanalyze</b> ciphertext with a certain probability of success. To validate our approach, we show that in the restricted setting of ideal cryptosystems, for which the probability of guessing information that the Dolev-Yao adversary cannot derive is negligible, the computational adversary {{is limited to the}} allowed behaviors of the Dolev-Yao adversary. ...|$|E
40|$|In {{this paper}} we {{implemented}} {{new methods of}} public keys exchange in the existing mutual authentication and key agreement protocol in wireless communication. The existing mutual authentication and key agreement protocol in wireless communications has been studied and the break points have been observed. We used “CS attack ” <b>to</b> <b>cryptanalyze</b> the user’s public key and obtain the private key. We overcame this break point by implementing DES encrypting algorithm along with NTRU encryption algorithm to improve the security. We also have studied the cryptanalyzation of NTRU encryption algorithm with various parameters and calculated the average window size to send and receive the public key...|$|E
40|$|We devise {{the first}} closed formula {{for the number}} of rounds of a blockcipher with secret {{components}} so that these components can be revealed using multiset, algebraic-degree, or division-integral properties, which in this case are equivalent. Using the new result, we attack 7 (out of 9) rounds of Kuznyechik, the recent Russian blockcipher standard, thus halving its security margin. With the same technique we attack 6 (out of 8) rounds of Khazad, the legacy 64 -bit blockcipher. Finally, we show how <b>to</b> <b>cryptanalyze</b> and find a decomposition of generic SPN construction for which the inner-components are secret. All the attacks are the best to date...|$|E
40|$|The Data Encryption Standard (DES) is {{the most}} widely used {{cryptosystem}} developed by a team of cryptographers working at IBM. DES has been cryptanalyzed intensively by researchers, but no efficient attack has been found on DES so far. This is mainly {{due to the lack of}} an obvious algebraic relation in the structure of S-boxes, which makes it impossible to use known methods to attack DES. S-boxes are the nonlinear part of DES with strong properties. This paper presents a semilinear relation between input and output of S-boxes that could be used <b>to</b> <b>cryptanalyze</b> DES. This is based on Differential Cryptanalysis method proposed by Biham and Shamir...|$|E
40|$|Abstract. In {{this paper}} we {{describe}} {{improvements to the}} techniques used <b>to</b> <b>cryptanalyze</b> SHA- 0 and introduce the first results on SHA- 1. The results include a generic multi-block technique that uses nearcollisions {{in order to find}} collisions, and a four-block collision of SHA- 0 found using this technique with complexity 2 51. Then, extension of this and prior techniques are presented, that allow us to find collisions of reduced versions of SHA- 1. We give collisions of variants with up to 40 rounds, and show the complexities of longer variants. These techniques show that collisions up to about 53 – 58 rounds can still be found faster than by birthday attacks. ...|$|E
40|$|This paper {{describes}} a method {{about how to}} determine parameters of some double-scroll chaotic systems, including the Lorenz system and the Chua’s circuit, from one of its variables. The geometric properties of the system are exploited firstly to reduce the parameter search space. Then, a synchronization-based approach, {{with the help of}} the same geometric properties as coincidence criteria, is implemented to determine the parameter values with the wanted accuracy. The method is not affected by a moderate amount of noise in the waveform. As an example of its effectiveness, the method is applied <b>to</b> <b>cryptanalyze</b> two two-channel chaotic cryptosystems, figuring out how the secret keys can be directly derived from the driving signal z(t) ...|$|E
40|$|Many {{distance}} bounding protocols {{appropriate for}} RFID technology {{have been proposed}} recently. However, the design and the analysis of these protocols are not based on a formal perspective. Motivated by this need, a formal framework is presented that helps the future attempts <b>to</b> <b>cryptanalyze</b> and design new distance bounding protocols. We first formalize the adversary scenarios, the protocol means, and the adversary goals in general. Then, {{we focus on the}} formalism for RFID systems by describing and extending the adversary strategies and the prover model. Two recently published distance bounding protocols are cryptanalyzed using our formal framework to demonstrate its relevancy and efficiency. Our formalism thus allows to prove that the adversary success probabilities are higher than the originally claimed ones...|$|E
40|$|Homophonic {{substitution}} ciphers {{employ a}} one-to-many key to encrypt plaintext. This {{is in contrast}} to a simple substitution cipher where a one-to-one mapping is used. The advantage of a homophonic substitution cipher is that it makes frequency analysis more difficult, due to a more even distribution of plaintext statistics. Classic transposition ciphers apply diffusion to the ciphertext by swapping the order of letters. Combined transposition-substitution ciphers can be more challenging <b>to</b> <b>cryptanalyze</b> than either cipher type separately. In this research, we propose a technique to break a combined simple substitution- column transposition cipher. We also consider the related problem of breaking a combination homophonic substitution-column transposition cipher. These attacks extend previous work on substitution ciphers. We thoroughly analyze our attacks and we apply the homophonic substitution-columnar transposition attack to the unsolved Zodiac- 340 cipher...|$|E
40|$|AbstractThe {{shrinking}} generator is {{a simple}} keystream generator with applications in stream ciphers, which is still considered as a secure generator. This work shows that, in order <b>to</b> <b>cryptanalyze</b> it, fewer intercepted bits than indicated by the linear complexity are necessary. Indeed, whereas the linear complexity of shrunken sequences is between A⋅ 2 (S− 2) and A⋅ 2 (S− 1), we claim that the initial states of both component registers are easily computed with fewer than A⋅S shrunken bits located at particular positions. Such a result is proven thanks {{to the definition of}} shrunken sequences as interleaved sequences. Consequently, it is conjectured that this statement can be extended to all interleaved sequences. Furthermore, this paper confirms that certain bits of the interleaved sequences have a greater strategic importance than others, which must be considered as a proof of weakness of interleaved generators...|$|E
40|$|The ubiquity, dependability, and extensiveness of {{internet}} access {{has seen a}} migration of local services to cloud services where the advantages of scalability can be efficiently exploited. In doing so, the exposure of sensitive data to eavesdropping is a principal concern. Asymmetric cryptosystems attempt {{to solve this problem}} by basing access on the knowledge of a solution to mathematically difficult problems. Shor demonstrated that on a quantum computer, cryptosystems based on the difficulty of factoring integers or solving discrete logarithms were efficiently solvable. As the most ubiquitous asymmetric cryptosystems in modern use are based on these problems, new cryptosystems had to be considered for post-quantum cryptography. In 1978, McEliece proposed a cryptosystem based on the difficulty of decoding random linear codes but the key sizes were too large for practical consideration. These systems, though, do appear to resist Shor’s algorithm and other quantum attacks. More recently, Gabidulin proposed using codes in the rank metric to design secure cryptosystems because they could be designed with smaller parameters. In this direction, many proposals for cryptosystems based on rank metric codes were designed. Overbeck managed <b>to</b> <b>cryptanalyze</b> many of these systems, but there remain several which resist all known structural attacks. In this work, we investigate the use of rank metric codes for cryptographic purposes. Firstly, we investigate the construction of MRD codes and propose some new constructions based on combinatorial methods. We then generalize Overbeck’s attack and show how our generalized attack can be used <b>to</b> <b>cryptanalyze</b> some of the cryptosystems which were designed to resist the attack of Overbeck. Our attack is based on a new approach of exploiting the structure of low weight elements in the code. Our approach also allows us to extend a result of Gaborit to obtain a polynomial time decoding algorithm for codes with certain parameters. Lastly, we consider the use of codes in the subspace metric– which are based on rank metric codes–in order to create an alternative instance of Juels’ and Sudan’s fuzzy vault primitive...|$|E
30|$|Here, {{it should}} be {{mentioned}} that due to the acyclic behavior of the encryption module, the output keystream has all the merits of one-time pads, and thus {{it is very difficult}} <b>to</b> <b>cryptanalyze,</b> using statistical attacks. For this reason some tests have been performed to check the security of the encryption system. Towards this direction, let us assume that an unauthorized user knows the QSWTs, where the encrypted biometric signal of Figure 3 (c) is hidden and tries to decrypt it by, brute force attack. Let us also assume that he has also obtained a rearranged version of the image, where all pixels are on proper position. If the exact key is used, then the content can be decrypted. However, even if the key differs by just one bit, the content will not be decrypted as it can be seen in Figure 5 (c).|$|E
40|$|We {{describe}} an algorithm for inverting an iteration of the one-dimensional cellular automaton. The algorithm {{is based on}} the linear approximation of the updating function, and requires less than exponential time for particular classes of updating functions and seed values. For example, an n-cell cellular automaton based on the updating function CA 30 can be inverted in O#n# time for certain seed values, and at most 2 n= 2 trials are required for arbitrary seed values. The inversion algorithm requires at most 2 #q, 1 ## 1,##n trials for arbitrary nonlinear functions and seed values, where q is the number of variables of the updating function, and # is the probability of agreement between the function and its best affine approximation. The inversion algorithm coupled with the method of Meier and Staffelbach # 6 # becomes a powerful tool <b>to</b> <b>cryptanalyze</b> the random number generators based on one-dimensional cellular automata, showing that these random number generators provide less amount [...] ...|$|E
40|$|It is {{possible}} <b>to</b> <b>cryptanalyze</b> simple substitution ciphers (both monoand polyalphabetic) {{by using a}} fast algorithm based on a process where an initial key guess is refined {{through a number of}} iterations. In each step the plaintext corresponding to the current key is evaluated and the result used as a measure of how close we are in having discovered the correct key. It turns out that only knowledge of the digram distribution of the ciphertext and the expected digram distribution of the plaintext is necessary to solve the cipher. The algorithm needs to compute the distribution matrix only once and subsequent plaintext evaluation is done by manipulating this matrix only, and not by decrypting the ciphertext and reparsing the resulting plaintext in every iteration. The paper explains the algorithm and it shows some of the results obtained with an implementation in Pascal. A generalized version of the algorithm can be used for attacking other simple ciphers as well. Keywords: Cryptanalysis, [...] ...|$|E
40|$|Abstract. Invariant {{subspace}} {{attacks were}} introduced at CRYPTO 2011 <b>to</b> <b>cryptanalyze</b> PRINTcipher. The invariant subspaces for PRINTcipher {{were discovered in}} an ad hoc fashion, leaving a generic technique to discover invariant subspaces in other ciphers as an open problem. Here, based on a rather simple observation, we introduce a generic algorithm to detect invariant subspaces. We apply this algorithm to the CAESAR candidate iSCREAM, the closely related LS-design Robin, {{as well as the}} lightweight cipher Zorro. For all three candidates invariant subspaces were detected, and result in practical breaks of the ciphers. A closer analysis of independent interest reveals that these invariant subspaces are underpinned by a new type of self-similarity property. For all ciphers, our strongest attack shows the existence of a weak key set of density 2 − 32. These weak keys lead to a simple property on the plaintexts going through the whole encryption process with probability one. All our attacks have been practically verified on reference implementations of the ciphers...|$|E
40|$|Data-compression {{techniques}} such as Huffman coding are often {{used in conjunction with}} cryptographic schemes. By removing redundancy in the source document, they can significantly increase the difficulty of cryptanalysis. In this thesis we consider the question: "what is the difficulty of breaking a data-compression scheme by itself ?" We examine most closely the problem of deciphering a file that has been Huffman-coded, but for which the Huffman code used is unavailable. We find that a Huffman-code can be surprisingly difficult <b>to</b> <b>cryptanalyze.</b> We present a detailed analysis of the situation for a three-symbol source alphabet, and concisely derive the conditions when the source probabilities lead to true ambiguity. We also present some general results for the case of an arbitrary (finite) alphabet, and show that: 1. If the source probabilities are highly skewed, then there is no ambiguity. 2. If the source probabilities are equally likely {{and the size of the}} source alphabet is a pow [...] ...|$|E
40|$|In this thesis, {{the author}} hypothesizes {{that the use}} of {{computationally}} intensive mathematical operations in password authentication protocols can lead to security vulnerabilities in those protocols. In order to test this hypothesis: 1. A generalized algorithm for cryptanalysis was formulated to perform a clogging attack (a formof denial of service) on protocols that use computationally intensive modular exponentiation to guarantee security. 2. This technique was then applied <b>to</b> <b>cryptanalyze</b> four recent password authentication protocols, to determine their susceptibility to the clogging attack. The protocols analyzed in this thesis differ in their usage of factors (smart cards, memory drives, etc.) or their method of communication (encryption, nonces, timestamps, etc.). Their similarity lies in their use of computationally intensivemodular exponentiation as amediumof authentication. It is concluded that the strengths of all the protocols studied in this thesis can be combined tomake each of the protocols secure from the clogging attack. The conclusion is supported by designing countermeasures for each protocol against the clogging attack...|$|E
30|$|The {{system of}} lattice algebra plays a {{significant}} role in information theory [1], information retrieval [2], information access controls [3] and cryptanalysis [4]. In [1], Bell described the co-information lattice, used it to show how to express the probability density under a general hypergraphical model, and then used this to derive the lattice of dependent component analysis algorithms. In [2], Carpineto and Romano applied lattices to information retrieval. They introduced the bound facility and the integration of this and several other useful features, such as automatic indexing, fisheye view browser for lattice, and the use of thesaurus into a basic lattice framework. In [3], Sandhu showed that lattice-based mandatory access controls can be enforced by appropriate configuration of RBAC components. His constructions demonstrated that role hierarchies and constraints were required to effectively achieve this result. In [4], Durfee applied tools from the geometry of numbers to solve several problems in cryptanalysis. They used algebraic techniques <b>to</b> <b>cryptanalyze</b> several public key cryptosystems. They focused on RSA and RSA-like schemes and used tools from the theory of integer lattices to get some results.|$|E
