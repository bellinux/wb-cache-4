32|10000|Public
500|$|The {{game was}} {{developed}} and produced by Matt Thorson, who previously made Planet Punch and browser games. TowerFall was his first full commercial game. The idea came from a visit with Alec Holowka as they worked at a game jam, the 48-hour June 2012 Vancouver Full Indie Game Jam. The team iterated through a Legend of Zelda-inspired multiplayer mode that became a single-player platformer Flash game where the player was a [...] "skilled archer out of an ancient legend". They intended to add multiple weapons, but chose to keep their first one—the bow and arrow—due to its feel. The arrow was designed to fire without charging and <b>to</b> <b>bias</b> <b>towards</b> targets so as to give the player [...] "more leeway". Thorson also chose to limit the aim direction to the eight ordinal directions rather than affording complex 360 degree controls. They also added levels, items, a store, and a story based on ascending a tower. Along with progress, players would gaining new items and skills. Thorson originally intended to send the game to Adult Swim for [...] "easy money", but changed his mind upon developing a multiplayer version after the jam.|$|E
50|$|Recent {{analysis}} {{suggests that the}} sparse data (besides the absence of data from South West England for four years, only one station was used per region until the 1820s) from early years can lead <b>to</b> <b>bias</b> <b>towards</b> drier conditions since higher and wetter areas {{are not likely to}} be accounted for, though no effort has yet been made to examine the data. There has also been a suggestion that many of the very earliest values, before circa 1780 and for a few years near 1800 and between 1809 and 1813, are rather too low compared to other estimates from A.F. Jenkinson of the University of East Anglia.|$|E
5000|$|The {{game was}} {{developed}} and produced by Matt Thorson, who previously made Planet Punch and browser games. TowerFall was his first full commercial game. The idea came from a visit with Alec Holowka as they worked at a game jam, the 48-hour June 2012 Vancouver Full Indie Game Jam. The team iterated through a Legend of Zelda-inspired multiplayer mode that became a single-player platformer Flash game where the player was a [...] "skilled archer out of an ancient legend". They intended to add multiple weapons, but chose to keep their first one—the bow and arrow—due to its feel. The arrow was designed to fire without charging and <b>to</b> <b>bias</b> <b>towards</b> targets so as to give the player [...] "more leeway". Thorson also chose to limit the aim direction to the eight ordinal directions rather than affording complex 360 degree controls. They also added levels, items, a store, and a story based on ascending a tower. Along with progress, players would gaining new items and skills. Thorson originally intended to send the game to Adult Swim for [...] "easy money", but changed his mind upon developing a multiplayer version after the jam.|$|E
5000|$|The douk-douk is a {{very simple}} slipjoint knife, having no locking mechanism, but only a very strong backspring <b>to</b> <b>bias</b> it <b>towards</b> opening and closure. It {{consists}} of only six parts: ...|$|R
25|$|Transmissions {{used the}} 405 line {{black and white}} {{analogue}} system. Initially, content tended <b>to</b> be <b>biased</b> <b>towards</b> Plymouth, as the news and programme making studios were based at Derry's Cross, and Electronic News Gathering facilities were decades away.|$|R
50|$|At the {{postgraduate}} level, Unison tends <b>to</b> be <b>biased</b> <b>towards</b> scientific subjects, but it {{also has}} a number of strong humanities and social science schools. There are 6 subjects accredited in the level of academic excellence for CONACYT.|$|R
30|$|Fire scar {{detection}} utilised a semi-automated approach. Polygons were segmented {{from the}} raster data layer using Definiens® ECognition™ v 4.0. The automatic segmentation algorithm was selected <b>to</b> <b>bias</b> <b>towards</b> errors of commission, {{which were then}} removed by manual editing. All fire scars derived from MODIS were supplied by the Tropical Savannas Cooperative Research Centre.|$|E
40|$|Abstract — Peer-to-peer {{systems are}} {{becoming}} increasingly popular, with millions of simultaneous users and {{a wide range of}} applications. Understanding existing systems and devising new peer-to-peer techniques relies on access to representative models derived from empirical observations. Due to the large and dynamic nature of these systems, directly capturing global behavior is often impractical. Sampling is a natural approach for learning about these systems, and most previous studies rely on it to collect data. This paper addresses the common problem of selecting representative samples of peer properties such as peer degree, link bandwidth, or the number of files shared. A good sampling technique will select any of the peers present with equal probability. However, common sampling techniques introduce bias in two ways. First, the dynamic nature of peers can bias results towards short-lived peers, much as naively sampling flows in a router can lead <b>to</b> <b>bias</b> <b>towards</b> short-lived flows. Second, the heterogeneous overlay topology can lead <b>to</b> <b>bias</b> <b>towards</b> high-degree peers. We present preliminary evidence suggesting that applying a degreecorrection method to random walk-based peer selection leads to unbiased sampling, at the expense of a loss of efficiency. I...|$|E
40|$|We {{present a}} phrasal {{synchronous}} gram-mar model of translational equivalence. Unlike previous approaches, {{we do not}} resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior <b>to</b> <b>bias</b> <b>towards</b> compact grammars with small translation units. Inference is per-formed using a novel Gibbs sampler over synchronous derivations. This sam-pler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sam-pling iteration is highly efficient, allowing the model {{to be applied to}} larger transla-tion corpora than previous approaches. ...|$|E
50|$|Transmissions {{used the}} 405 line {{black and white}} {{analogue}} system. Initially, content tended <b>to</b> be <b>biased</b> <b>towards</b> Plymouth, as the news and programme making studios were based at Derry's Cross, and Electronic News Gathering facilities were decades away.|$|R
5000|$|Intermediate-term: Intensification rules {{intended}} <b>to</b> <b>bias</b> {{the search}} <b>towards</b> promising {{areas of the}} search space.|$|R
5000|$|... {{the punitive}} nature of Australian state {{defamation}} laws which, like their English models, tend <b>to</b> be heavily <b>biased</b> <b>towards</b> the plaintiff ...|$|R
40|$|Abstract—This paper {{addresses}} the difficult problem of selecting representative samples of peer properties (e. g., degree, link bandwidth, number of files shared) in unstructured peer-to-peer systems. Due {{to the large}} size and highly dynamic nature of these systems, measuring the quantities of interest on every peer is generally prohibitively expensive, while sampling provides a natural means for estimating system-wide behavior efficiently. However, commonly-used sampling techniques for measuring peer-to-peer systems tend to introduce considerable bias for two reasons. First, the dynamic nature of peers (i. e., churn) can bias results towards short-lived peers, much as naively sampling flows in a router can lead <b>to</b> <b>bias</b> <b>towards</b> short-lived flows. Second, the heterogeneous nature of the overlay topology can lead <b>to</b> <b>bias</b> <b>towards</b> highdegree peers. We present a detailed examination {{of the ways that}} the behavior of peer-to-peer systems can introduce bias and suggest the Metropolized Random Walk with Backtracking (MRWB) as a viable and promising technique for collecting nearly unbiased samples. We conduct an extensive simulation study to demonstrate that the proposed technique works well {{for a wide variety of}} common peer-to-peer network conditions. Using the Gnutella network, we empirically show that implementing the MRWB technique yields more accurate samples than relying on commonly-used sampling techniques, and we provide insights into the causes of the observed differences. The tool we have developed, ion-sampler, selects peer addresses uniformly at random using the MRWB technique. These addresses may then be used as input to another measurement tool to collect data on a particular property. ...|$|E
40|$|Although the {{multi-channel}} shopper {{has recently}} become a dominant consumer type, firms {{are still struggling}} with consciously designing their multi-channel service mix. In this paper, a design method based on QFD (Quality Function Deployment) is introduced and tested for defining e-services that have to function in a multi-channel context. Within a design research perspective, a structured field experiment was conducted, using control group testing. Two measurement instruments were used: questionnaires for business participants (n= 62) and a protocol for external observers (n= 56) to measure performance of design tasks throughout the process. We found that business teams tend <b>to</b> <b>bias</b> <b>towards</b> the supplier’s perspective {{at the expense of}} customers and channel partners. The new method scored significantly better than the control group method on a number of evaluation criteria: customer orientation, channel coherence and communication between different stakeholder perspectives...|$|E
40|$|Feature {{selection}} {{is an important}} component of text categorization. This technique can both increase a classifier’s computation speed, and reduce the overfitting problem. Several feature selection methods, such as information gain and mutual information, have been widely used. Although they greatly improve the classifier’s performance, they have a common drawback, which is that they do not consider the mutual relationships among the features. In this situation, where one feature’s predictive power is weakened by others, and where the selected features tend <b>to</b> <b>bias</b> <b>towards</b> major categories, such selection methods are not very effective. In this paper, we propose a novel feature selection method for text categorization called conditional mutual information maximin (CMIM). It can select a set of individually discriminating and weakly dependent features. The experimental results show that CMIM can perform much better than traditional feature selection methods...|$|E
50|$|Noted <b>to</b> be <b>biased</b> <b>towards</b> real robots, {{instead of}} super robots. The defense rating for super robots is {{rendered}} insignificant, in that these units {{will take the}} same amount of damage as real robots, eliminating their advantage of tanking by absorbing damage.|$|R
5000|$|... {{augmented}} frequency, <b>to</b> {{prevent a}} <b>bias</b> <b>towards</b> longer documents, e.g. raw frequency {{divided by the}} raw frequency of the most occurring term in the document: ...|$|R
25|$|Universities {{have been}} blamed for being sexist in their hiring decisions. In particular, men have been {{reported}} <b>to</b> be <b>biased</b> <b>towards</b> male applicants. However, recent data suggests that women have caught up, at least {{when it comes to}} the number of faculty positions offered to women (see Table).|$|R
40|$|For an ill-posed problem like {{boundary}} detection, human labeled datasets play {{a critical}} role. Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. The goal {{of this paper is}} to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. In the paper, we first introduce a psychophysical experiment to show that many of the “weak ” boundary labels are unreliable and may contaminate the benchmark. Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm <b>to</b> <b>bias</b> <b>towards</b> those problematic “weak ” boundary labels. With this evidence, we focus on a new problem of detecting strong boundaries as one alternative. Finally, we assess the performances of 9 major algorithms on different ways of utilizing the dataset, suggesting new directions for improvements. 1...|$|E
40|$|In {{order to}} {{quantify}} the trophic impact of gelatinous predators, digestion time estimates are commonly applied to counts of prey in the guts. Three primary approaches are used, the Manual-feeding, Natural-feeding and Steady-state methods; these differ in methodology and their underlying assumptions. The criteria used to define the end-point of digestion, and the resolution at which digestion progress is observed, also vary across studies. To understand the impact of such differences, we estimate digestion times of the scyphomedusa Aurelia aurita fed adult females of the copepod Acartia tonsa using these various approaches. We find ~fourfold differences which can be attributed <b>to</b> <b>bias</b> <b>towards</b> the slowest rates of digestion by some end-point criteria, and overestimation from low observation resolution. Artificial manipulation {{and the degree to}} which swimming and feeding behaviour are natural may also influence estimates. We provide recommendations for those quantifying digestion times of Aurelia aurita medusae and gelatinous predators...|$|E
40|$|Least squares fitting {{procedures}} and weighted averages imply {{the assignment of}} proper weighting factors to the stochastically distributed data involved. In {{the case of a}} Poisson distribution, the obvious choice of setting the weighting factor equal to the inverse of the measured value is prone <b>to</b> <b>bias</b> <b>towards</b> low values. Alternatively, using the inverse of the fitted value {{may turn out to be}} biased towards higher values, depending on the procedure followed. Additional problems arise when also the possibility of zero counts has to be taken into account. In this work, several least squares statistics are considered and their performance compared with maximum likelihood estimation. The particular case of an exponential time interval distribution of a stationary Poisson process is investigated. Formulas are presented to determine the event rate of the Poisson process from the central moments of the time interval distribution. These count rates are compared to estimated values from the fit procedures. JRC. D. 4 -Isotope measurement...|$|E
3000|$|... 5 In {{the attempt}} <b>to</b> reduce this <b>bias</b> in estimation, we ran our {{analysis}} also on a sub-sample of firms {{which do not}} report any subsidiary or branch in the database. The main conclusions hold in this case as well, although of course the sample is likely <b>to</b> be <b>biased</b> <b>towards</b> smaller firms (tables available on request).|$|R
30|$|Added for comparison, cMVGC {{detection}} proved <b>to</b> be <b>biased</b> <b>towards</b> {{a reduction}} of the FP rates in many cases. By contrast, examination of its behaviour for other K (available in more detail from our Web site) suggests that, for small K, it tends to miss existing connections more often than the other methods.|$|R
50|$|The Greek {{crowd at}} the final reacted <b>to</b> {{perceived}} <b>bias</b> <b>towards</b> Milan by referee Christos Michas by throwing missiles during the victors' lap of honour, but despite protests, the result was not overturned. UEFA later banned Michas for life due to match fixing, although his role in this match was not investigated.|$|R
40|$|Most of {{existing}} online social networks, such as Facebook and Twitter, are designed <b>to</b> <b>bias</b> <b>towards</b> information disclosure {{to a large}} audience. Google recently launched a new social network platform, Google+. By introducing the notion of ‘circles’, Google+ enables users to selectively share data with specific groups within their personal network, rather than sharing with all of their social connections at once. Although Google+ can help mitigate {{the gap between the}} individuals ’ expectations and their actual privacy set-tings, it still only allows a single user to restrict access to her/his data but cannot provide any mechanism to enforce privacy con-cerns over data associated with multiple users. In this paper, we propose an approach to facilitate collaborative privacy management of shared data in Google+. We formulate a circle-based multiparty access control model (CMAC) to capture the essence of collabora-tive authorization requirements in Google+, along with a multiparty policy specification scheme and a policy enforcement mechanism. We also discuss a proof-of-concept prototype of our approach and describe system evaluation and usability study of our prototype...|$|E
40|$|Background Recent {{developments}} in deep (next-generation) sequencing technologies are significantly impacting medical research. The global analysis of protein coding regions in genomes of interest by whole exome sequencing {{is a widely}} used application. Many technologies for exome capture are commercially available; here we compare the performance of four of them: NimbleGen’s SeqCap EZ v 3. 0, Agilent’s SureSelect v 4. 0, Illumina’s TruSeq Exome, and Illumina’s Nextera Exome, all applied to the same human tumor DNA sample. Results Each capture technology was evaluated for its coverage of different exome databases, target coverage efficiency, GC bias, sensitivity in single nucleotide variant detection, sensitivity in small indel detection, and technical reproducibility. In general, all technologies performed well; however, our data demonstrated small, but consistent differences between the four capture technologies. Illumina technologies cover more bases in coding and untranslated regions. Furthermore, whereas most of the technologies provide reduced coverage in regions with low or high GC content, the Nextera technology tends <b>to</b> <b>bias</b> <b>towards</b> target regions with high GC content. Conclusions We show key differences in performance between the four technologies. Our data should help researchers who are planning exome sequencing to select appropriate exome capture technology for their particular application...|$|E
40|$|Almost 7 million {{children}} under the age 5 die each year, and most of these deaths are attributable to vaccine-preventable infections. Young infants respond poorly to infections and vaccines. In particular, dendritic cells secrete less IL- 12 and IL- 18, CD 8 pos T cells and NK cells have defective cytolysis and cytokine production, and CD 4 pos T cell responses tend <b>to</b> <b>bias</b> <b>towards</b> a Th 2 phenotype and promotion of regulatory T cells (T regs). The basis for these differences is not well understood and may be in part explained by epigenetic differences, as well as immaturity of the infant's immune system. Here we present a third possibility, which involves active suppression by immune regulatory cells and place in context the immune suppressive pathways of mesenchymal stromal cells (MSC), myeloid-derived suppressor cells (MDSC), CD 5 pos B cells, and T regs. The immune pathways that these immune regulatory cells inhibit are similar to those that are defective in the infant. Therefore, the immune deficiencies seen in infants could be explained, in part, by active suppressive cells, indicating potential new avenues for intervention...|$|E
50|$|Oonopidae are {{frequently}} encountered as subfossils preserved in copals and as fossils preserved in amber. Oonopids even occur in more amber deposits {{than any other}} spider family, which may {{be accounted for by}} their widespread distribution, small size, and wandering behaviour, as amber appears <b>to</b> be <b>biased</b> <b>towards</b> trapping such spiders. In contrast, sedimentary fossils of Oonopidae are unknown.|$|R
40|$|Objectives: To {{investigate}} whether posttraumatic stress symptoms (PTSS) are related <b>to</b> attentional <b>bias</b> <b>towards</b> cancer-related stimuli among {{parents of children}} recently diagnosed with cancer. Methods: Sixty-two parents completed questionnaires measuring PTSS, depression, and anxiety and the emotional Stroop task via the Internet. The emotional Stroop task included cancer-related words, cardiovascular disease-related words, and neutral words. Results: Participants were split in two groups based on the median of PTSS: High-PTSS and Low-PTSS. There was a significant interaction between word-type and group and a planned contrast test of this interaction indicated that the High-PTSS group had longer response latencies on cancer-related words {{compared to the other}} word-type and group combinations. Conclusions: Findings suggest that PTSS are related <b>to</b> attentional <b>bias</b> <b>towards</b> cancer-related stimuli among parents of children recently diagnosed with cancer. Implications of this finding for the understanding of PTSS in this population, future research, and clinical practice are discussed...|$|R
50|$|The Supreme Prosecutors' Office of the Republic of Korea (SPO) was alleged <b>to</b> be <b>biased</b> <b>towards</b> the {{government}} and domestic right wing scenes. The future Prosecutor General of the SPO, Han Sang-dae, among others, was criticized for falsely registering his address. The SPO lost consecutive cases concerning Han Myeong-sook for bribery charges when the Seoul High Court found her innocent.|$|R
40|$|Mendelian {{randomization}} (MR) is {{a method}} for estimating the causal relationship between an exposure and an outcome using a genetic factor as an instrumental variable (IV) for the exposure. In the traditional MR setting, data on the IV, exposure, and outcome are available for all participants. However, obtaining complete exposure data may be difficult in some settings, due to high measurement costs or lack of appropriate biospecimens. We used simulated data sets to assess statistical power and bias for MR when exposure data are available for a subset (or an independent set) of participants. We show that obtaining exposure data for a subset of participants is a cost-efficient strategy, often having negligible effects on power in comparison with a traditional complete-data analysis. The size of the subset needed to achieve maximum power depends on IV strength, and maximum power is approximately equal {{to the power of}} traditional IV estimators. Weak IVs are shown to lead <b>to</b> <b>bias</b> <b>towards</b> the null when the subsample is small and towards the confounded association when the subset is relatively large. Various approaches for confidence interval calculation are considered. These results have important implications for reduc-ing the costs and increasing the feasibility of MR studies. epidemiologic methods; instrumental variable; Mendelian randomizatio...|$|E
40|$|In {{the field}} of bioinformatics, {{selection}} of genes in multiclass sample classification {{can be done by}} filtering methods using microarray data. Such approaches usually contribute <b>to</b> <b>bias</b> <b>towards</b> a few classes that are easily recognizable from other classes due to imbalances of strong features and sample sizes of distinct classes in a microarray data. Many methods have been used for the filter methods, as they are very commonly used in gene ranking from microarray data in multiclass problems. In this research, we discuss various methods to decompose multiclass ranking statistics into class specific statistics and then need of Pareto-front analysis for selection of genes. This mitigates the bias induced by class intrinsic characteristics of dominating classes. The need of Pareto-front analysis is to indicate on two filter criteria commonly used for gene selection: F-score and KW-score. A significant development in classification performance and reduction in redundancy among top ranked genes were achieved in experiments with both synthetic and real-benchmark data sets. The following work is analysis over the traditional and improved filter methods used for gene selection of various classes through various mechanisms available in the literature. Keywords Aggregation statistics, filter methods, gene selection, multiobjective evolutionary optimization, Pareto-front analysis. 1...|$|E
40|$|Scheduling {{policies}} that favor small jobs have received growing attention {{due to their}} superior performance with respect to mean delay, e. g., Shortest Remaining Processing Time (SRPT) and Preemptive Shortest Job First (PSJF). In this paper, we study the delay distribution of a generalization {{of the class of}} scheduling policies called SMART (because policies in it have “SMAll Response Times”), which includes SRPT, PSJF, and a range of practical variants, in a discrete-time queueing system under the many sources large deviations regime. Our analysis of SMART in this regime (large number of flows and large capacity) hinges on a novel two-dimensional (2 -D) queueing framework that employs virtual queues and total ordering of jobs. We prove that all SMART policies have the same asymptotic delay distribution as SRPT, i. e., the delay distribution has the same decay rate. In addition, we illustrate the improvements SMART policies make over First Come First Serve (FCFS) and Processor Sharing (PS). Our 2 -D queueing technique is generalizable to other policies as well. As an example, we show how the Foreground-Background (FB) policy can be analyzed using a 2 -D queueing framework. FB is a policy, not contained in SMART, which manages <b>to</b> <b>bias</b> <b>towards</b> small jobs without knowing which jobs are small in advance...|$|E
50|$|However, though HDI is {{thus more}} universally applicable, its {{relative}} sparsity of indicators {{also makes it}} more susceptible <b>to</b> <b>bias.</b> Indeed, some studies have found it <b>to</b> be somewhat <b>biased</b> <b>towards</b> GDP per capita, as demonstrated by a high correlation between HDI and the log of GDPpc. Hence, HDI {{has been criticized for}} ignoring other development parameters.|$|R
40|$|Multilevel {{clustering}} {{has proven}} to be an effective technique for a number of problems. In this paper, we study a clusters-first approach to circuit placement, compare it to more traditional methods, and study the relationship between cluster sizes and placement quality. We also reveal shortcomings in recently developed benchmarks, that can serve <b>to</b> <b>bias</b> experiments <b>towards</b> a clusters-first methodology...|$|R
30|$|Only {{patients}} on the SPRINT TGC protocol were considered {{for this analysis}} as they had sufficient data density to identify SI hourly. Patients were put on the SPRINT protocol because they were hyperglycemic and thus were likely <b>to</b> be <b>biased</b> <b>towards</b> lower insulin sensitivity compared with other ICU patients. However, {{in the context of}} investigating the implications of SI variability on TGC, this cohort is appropriate.|$|R
