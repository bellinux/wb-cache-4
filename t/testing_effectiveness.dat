74|6549|Public
5000|$|... #Subtitle level 3: Intervention: <b>Testing</b> <b>effectiveness</b> of {{interventions}} ...|$|E
50|$|A target ship is {{a vessel}} — {{typically}} an obsolete or captured warship — {{used as a}} seaborne target for naval gunnery practice or for weapons testing. Targets may be used {{with the intention of}} <b>testing</b> <b>effectiveness</b> of specific types of ammunition; or the target ship may be used {{for an extended period of}} routine target practice with specialized non-explosive ammunition. The potential consequences of a drifting wreck require careful preparation of the target ship to prevent pollution, or a floating or submerged collision risk for maritime navigation.|$|E
40|$|AbstractSocial {{networks}} have small-world property, hierarchical community structure, {{and some other}} properties. This paper proposes models of networks with these properties and algorithm for community structure recognition. The models are useable for <b>testing</b> <b>effectiveness</b> and efficiency of different algorithms for social network analysis...|$|E
40|$|Approaches for {{measuring}} the <b>effectiveness</b> of <b>tests</b> and test programs are considered, taking into account system <b>test</b> <b>effectiveness,</b> thermal-vacuum <b>test</b> <b>effectiveness,</b> and vibration <b>test</b> <b>effectiveness.</b> Methods for predicting space performance {{on the basis of}} system <b>test</b> <b>effectiveness</b> are discussed along with aspects of defect detection in thermal-vacuum tests. Questions regarding the thermal-vacuum test costs in relation to the probability of early space failures are also explored...|$|R
40|$|Abstract – Testing {{plays an}} {{important}} role in any project irrespective of the domain. The ability to test right before it reaches the intended customer matters. Test effectiveness is an important metric that tracks the ability of the testing team. The efficiency of a tester to cover all aspects of testing and ensuring 100 % coverage determines the quality of the product. There could be number of factors that influence the <b>test</b> <b>effectiveness.</b> Organizations rely on the quality assurance team to strategize and plan the testing phase. Past experience in handling similar testing projects matter. Shifting left, the opportunity to be in pro-active mode helps to improve the efficiency. Predictive process performance models can be built for <b>test</b> <b>effectiveness.</b> This paper illustrates the process performance model to predict <b>test</b> <b>effectiveness</b> based on data from life science project in an organization. [Venkatesh, Priyesh Cherurveettil, Thenmozhi. S, Balasubramanie. P. Predicting <b>test</b> <b>effectiveness</b> using performance models in Life Science IT projects. Life Sci J 2012; 9 (4) : 96 - 100] (ISSN: 1097 - 8135) ...|$|R
50|$|Once {{the best}} testing option is chosen from the {{different}} <b>test</b> <b>effectiveness</b> values, the organisation can finish the planning of the test.|$|R
40|$|A {{new type}} of {{discriminant}} space for functional data is presented, com-bining the advantages of a functional discriminant coordinate space and a functional principal component space. In order to provide a comprehen-sive comparison, we conducted a set of experiments, <b>testing</b> <b>effectiveness</b> on 35 functional data sets (time series). Experiments show that constructed combined space provides a higher quality of classification of LDA method compared with component spaces...|$|E
30|$|Omar and Ghosh [18] {{presented}} four {{approaches to}} generate higher order mutants for AspectJ programs. The approaches were evaluated {{in terms of}} the ability to create mutants of higher order resulting in higher efficacy and less effort when compared with first order mutants. All approaches proposed can produce higher order mutants {{that can be used to}} increase <b>testing</b> <b>effectiveness</b> and reduce testing effort and reduce the amount of equivalent mutants. Differently from Omar and Ghosh’s work, our work only considers first order mutations.|$|E
40|$|Software {{testing is}} a key {{procedure}} to ensure high quality and reliability of software programs. The key issue in soft-ware testing is the selection and evaluation of different test cases. Code coverage has been proposed to be an estima-tor for <b>testing</b> <b>effectiveness,</b> but it remains a controversial topic which lacks of support from empirical data. In this study, we hypothesize that the estimation of code coverage on <b>testing</b> <b>effectiveness</b> varies under different testing profiles. To evaluate the performance of code coverage, we employ coverage testing and mutation testing in our experiment to investigate the relationship between code coverage and fault detection capability under different testing profiles. From our experimental data, code coverage is simply a moderate indicator for the capability of fault detection on the whole test set. However, it is clearly a good estimator for the fault detection of exceptional test cases, but a poor one for test cases in normal operations. For other testing profiles, such as functional testing and random testing, the correla-tion between code coverage and fault coverage is higher in functional test than in random testing, although these differ-ent testing profiles are complementary in the whole test set. The effects of different coverage metrics are also addressed in our experiment. Categories and Subject Descriptor...|$|E
50|$|The first {{letters of}} each {{potential}} defect forms the acronym PCOLA-SOQ. PCOLA-SOQ represents {{a more comprehensive}} indicator of a PCBA manufacturing <b>test</b> <b>effectiveness</b> than the historical test coverage formulas.|$|R
40|$|Code {{coverage}} visualizations using block coverage neither guided developers toward productive testing strategies, nor {{did these}} visualizations motivate developers to write more tests or {{help them find}} more faults than the control group. Nevertheless, code coverage visualizations did influence developers in a few important ways. Code coverage visualizations led developers to overestimate their <b>test</b> <b>effectiveness</b> more than the control group. Yet, these same visualizations reduced the variability {{in the number of}} test cases developers wrote by changing the standard developers used to evaluate their <b>test</b> <b>effectiveness.</b> Thus, the true power of testing visualizations lies not only with the faults that visualizations can highlight; it also lies in how visualizations can change how developers think about testing. Testing visualizations guide developers toward a particular standard of effectiveness, so if we want developers to test software adequately, we must ensure that the coverage criteria we choose to visualize leads developers toward a good standard of <b>test</b> <b>effectiveness.</b> " [...] Conclusio...|$|R
40|$|We {{study the}} {{simultaneous}} <b>test</b> <b>effectiveness</b> and efficiency improvement achievable by Strongly Subsuming Higher Or-der Mutants (SSHOMs), constructed from 15, 792 first order mutants in four Java programs. Using SSHOMs {{in place of}} the first order mutants they subsume yielded a 35 %- 45 % {{reduction in the number of}} mutants required, while simulta-neously improving test efficiency by 15 % and effectiveness by between 5. 6 % and 12 %. Trivial first order faults often com-bine to form exceptionally non-trivial higher order faults; apparently innocuous angels can combine to breed monsters. Nevertheless, these same monsters can be recruited to im-prove automated <b>test</b> <b>effectiveness</b> and efficiency. 1...|$|R
40|$|This chapter {{addresses}} {{the problem of}} reliability growth analysis; it shows how reliability trend analyses can help the project manager control {{the progress of the}} development activities and appreciate the efficiency of the test programs. Reliability trend changes occur for various reasons. They may be desirable and expected (such as reliability growth due to fault removal) or undesirable (slowing down of <b>testing</b> <b>effectiveness,</b> for example). Identification at time of the latter allows the project manager to make the appropriate decisions in order to avoid problems which may surface later...|$|E
40|$|Abstract- Software {{testing is}} the process of {{validation}} and verification of the software product which in turn deliver the reliable and quality oriented software product to users with lower maintenance cost, and more accurate and reliable results. Software <b>testing</b> <b>effectiveness</b> always depends on issues like generated test cases, prioritization of test cases etc. These issues demands on effort, time and cost of the testing. Many academicians and researchers are using soft computing based approached for better accuracy in testing. The aim of this research paper is to evaluate and compare soft computing approaches to do software testing and determine their usability and effectiveness...|$|E
40|$|Abstract — Pair-wise and, more generally, t-wise testing are {{the most}} common and {{powerful}} combinatorial testing approaches. This paper investigates the effectiveness of the t-wise approach for testing logical expressions in software in terms of its fault-detecting capabilities. Effectiveness is evaluated experimentally using special software tools for generating logical expressions and t-wise test cases, simulating faults in expressions, testing faulty expressions, and evaluating effectiveness of the testing. T-wise <b>testing</b> <b>effectiveness</b> is measured in its totality and for specific types of faults; it is then compared with random testing. A detailed analysis of the experimental results is also provided. Keywords-testing; pair-wise; t-wise; effectiveness; logical expressions; experimental evaluation I...|$|E
40|$|System {{testing is}} the last phase before the product is {{delivered}} for customer use and thus represents the last opportunity for verifying that the system functions correctly and as desired by customers. System test is time consuming in that it involves configuring and testing multiple complete, integrated systems (including hardware, operating system, and cooperating and co-existing applications) that are representative of a subset of customer environments. As a result, prioritizing the execution order of system test cases to maximize system <b>test</b> <b>effectiveness</b> would be beneficial. We are developing a statistical test case prioritization model that uses static metrics and system failure data {{with the goal of}} improving system <b>test</b> <b>effectiveness.</b> 1...|$|R
40|$|Progesterone {{receptor}} {{does not}} improve {{the performance and}} <b>test</b> <b>effectiveness</b> of the conventional 3 -marker panel, consisting of estrogen receptor, vimentin and carcinoembryonic antigen in distinguishing between primary endocervical and endometrial adenocarcinomas in a tissue microarray extension stud...|$|R
40|$|Serious adverse drug {{reactions}} {{are an important}} cause of hospitalization and can result in the withdrawal of licensed drugs. Genetic variation {{has been shown to}} influence adverse drug reaction susceptibility, and predictive genetic tests have been developed for a limited number of adverse drug reactions. The identification of patients with adverse drug reactions, obtaining samples for genetic analysis and rigorous evaluation of clinical <b>test</b> <b>effectiveness</b> represent significant challenges to predictive genetic test development. Using the example of serious drug-induced liver injury, we illustrate how a database of routinely collected electronic health records (EHRs) could be used to overcome these barriers by facilitating rapid recruitment to genome-wide association studies and supporting efficient randomized controlled trials of predictive genetic <b>test</b> <b>effectiveness...</b>|$|R
40|$|We {{propose the}} need for a {{specific}} educational scholarship when using e-learning in medical education. Effective e-learning has additional factors that require specific critical attention, including the design and delivery of e-learning. An important aspect is the recognition that e-learning is a complex intervention, with several interconnecting components that have to be aligned. This alignment requires an essential iterative development process with usability <b>testing.</b> <b>Effectiveness</b> of e-learning in one context may not be fully realized in another context unless there is further consideration of applicability and scalability. We recommend a participatory approach for an educational scholarship for using e-learning in medical education, such as by action research or design-based research...|$|E
40|$|This paper {{describes}} {{the potential of}} community intervention perspectives to increase the relevance and public health impact {{of mental health services}} research. A model for integrating community and health services intervention approaches is presented that combines community empowerment, partnership and education with healthcare quality improvement goals and evaluates their joint impacts on individuals, communities, and markets. A staged approach to research development is proposed: 1) <b>testing</b> <b>effectiveness</b> of community intervention strategies; 2) reducing social stigma through education and local campaigns in areas that have such programs in place; and 3) broader community empowerment and intervention dissemination. Challenges in developing this approach, as well as possible solutions, are described. ...|$|E
40|$|Failure {{patterns}} describe typical ways {{in which}} inputs revealing program failure are distributed across the input domain in many cases, clustered together in contiguous regions. Based on these observations several debug testing methods have been developed. We examine the upper bound of debug <b>testing</b> <b>effectiveness</b> improvements possible through making assumptions about the shape, size and orientation of failure patterns. We consider the bounds for testing strategies with respect to minimizing the F-measure, maximizing the P-measure, and maximizing the E-measure. Surprisingly, {{we find that the}} empirically measured effectiveness of some existing methods that are not based on these assumptions is close to the theoretical upper bound of these strategies. The assumptions made to obtain the upper bound, and its further implications, are also examined...|$|E
40|$|In {{order to}} deliver high quality {{software}} projects, a developing team probably needs a well-developed test suite. There are several methods that aim to evaluate test suites in some way, such as Code coverage and Mutation testing. Code coverage describes {{the degree of}} source code that a program executes when running a test suite. Mutation testing measures the <b>test</b> suite <b>effectiveness.</b> More development teams use code coverage {{to a greater extent}} than mutation testing. With code coverage being monitored throughout a project, could the development team risk drop of the <b>test</b> suite <b>effectiveness</b> as the codebase getting bigger with each version? In this thesis, a mutation testing tool called PIT is used during progress of four well known open source projects. The reason for this is to show that mutation testing is an important technique to ensure continuously high <b>test</b> suite <b>effectiveness,</b> and does not only rely on code coverage measurements. In general, all projects perform well in both code coverage and <b>test</b> suite <b>effectiveness,</b> with the exception of one project inwhich the <b>test</b> suite <b>effectiveness</b> drops drastically. This drop shows that all projects are at risk of low <b>test</b> suite <b>effectiveness,</b> by not using mutation testing techniques...|$|R
40|$|Phospholipids in {{specimens}} of amniotic fluid from 346 pa-tients were quantified {{and the results}} evaluated {{in light of the}} clinical outcome. Fifty-eight neonates had respiratory dis-tress syndrome. We used this data base to compare different statistical methods for evaluating <b>test</b> <b>effectiveness</b> and diagnostic discrimination. Dichotomizing quantitative tests into binary tests with arbitrary cutoff values was inadequate for comparing <b>test</b> <b>effectiveness.</b> Subgrouping the data into deciles and calculating the incidence of respiratory distress syndrome for each decile avoided the problems of the preceding approach and was easy to calculate and compre-hend; however, this method lacked statistical power. Relative operating characteristic curves yielded more statistical pow-er, but results were more difficult to calculate and were not intuitively obvious to most workers in the laboratory. ...|$|R
30|$|To <b>test</b> <b>effectiveness</b> of the {{proposed}} framework, we analyzed simple recordings, e.g., hand-clap recordings and relatively complex recordings, e.g., speech recordings made in a diverse set of recording environments including small offices, a large office, hallway, staircase, restroom, atrium, and outdoor environments. These recordings were made using three microphones.|$|R
40|$|The paper {{presents}} the results of the <b>testing</b> <b>effectiveness</b> of the integrated model in the short-term forecasting of demand for telephone services in 24 -hour cycles. The linear regression model with dichotomous (binary) independent variables was integrated with the feed forward neural network. The regression model was used as a filter of modelled variability of the demand. The neural network was used to model residual variability. The research shows that the integrated model has a higher possibility of approximation and prediction in comparison to the non-integrated linear regression model. The research study was based on data provided by the selected telecommunications network operator. The range of empirical material consisted of hourly counted seconds of outgoing calls and generated by network subscribers in various analytical sections...|$|E
40|$|Software {{testing is}} {{extensively}} used for uncovering bugs in large, complex software. Testing relies on well designed regression test suites that anticipate all reasonable software usage scenarios. Unfortunately, testers today {{have no way}} of knowing how much of real-world software usage was untested by their regression suite. Recent advances in low-overhead path profiling provide the opportunity to rectify this deficiency and perform residual path profiling on deployed software. Residual path profiling identifies all paths executed by deployed software that were untested during software development. We extend prior research to perform low-overhead interprocedural path profiling. We demonstrate experimentally that low-overhead path profiling, both intraprocedural and interprocedural, provides valuable quantitative information on <b>testing</b> <b>effectiveness.</b> We also show that residual edge profiling is inadequate as a significant number of untested paths include no new untested edges...|$|E
40|$|Abstract—Automated random {{testing is}} {{effective}} at detecting faults {{but it is}} certainly not an optimal testing strategy for every given program. For example, an automated random testing tool ignores that some routines have stronger preconditions, they use certain literal values, or they are more error-prone. Taking into account such characteristics may increase <b>testing</b> <b>effectiveness.</b> In this article, we present Evotec, an enhancement of random testing which relies on genetic algorithms to evolve a best testing strategy for contract-equipped programs. The resulting strategy is optimized for detecting more faults, satisfying more routine preconditions and establishing more object states on a given set of classes to test. Our experiment tested 92 classes over 1710 hours. It shows that Evotec detected 29 % more faults than random+ and 18 % more faults than the precondition-satisfaction strategy...|$|E
40|$|Mutants are automatically-generated, {{possibly}} faulty {{variants of}} programs. The mutation adequacy ratio {{of a test}} suite is the ratio of non-equivalent mutants {{it is able to}} identify to the total number of non-equivalent mutants. This ratio {{can be used as a}} measure of <b>test</b> <b>effectiveness.</b> However, it can be expensive to calculate, due to the large number of different mutation operators that have been proposed for generating the mutants. In this paper, we address the problem of finding a small set of mutation operators which is still sufficient for measuring <b>test</b> <b>effectiveness.</b> We do this by defining a statistical analysis procedure that allows us to identify such a set, together with an associated linear model that predicts mutation adequacy with high accuracy. We confirm the validity of our procedure through cross-validation and the application of other, alternative statistical analyses...|$|R
40|$|The {{correlation}} between test coverage and <b>test</b> <b>effectiveness</b> {{is important to}} justify the use of coverage in practice. Existing results on imperative programs mostly show that <b>test</b> coverage predicates <b>effectiveness.</b> However, since functional programs are usually structurally different from imperative ones, {{it is unclear whether}} the same result may be derived and coverage {{can be used as a}} prediction of effectiveness on functional programs. In this paper we report the first empirical study on the {{correlation between}} test coverage and <b>test</b> <b>effectiveness</b> on functional programs. We consider four types of coverage: as input coverages, statement coverage and expression coverage, and as oracle coverages, count of assertions and checked coverage. We also consider two types of effectiveness: raw effectiveness and normalized effectiveness. Our results are twofold. (1) In general the findings on imperative programs still hold on functional programs, warranting the use of coverage in practice. (2) On specific coverage criteria, the results may be unexpected or different from the imperative ones, calling for further studies on functional programs...|$|R
2500|$|RCTs can be {{classified}} as [...] "explanatory" [...] or [...] "pragmatic." [...] Explanatory RCTs test efficacy in a research setting with highly selected participants and under highly controlled conditions. In contrast, pragmatic RCTs (pRCTs) <b>test</b> <b>effectiveness</b> in everyday practice with relatively unselected participants and under flexible conditions; in this way, pragmatic RCTs can [...] "inform decisions about practice." ...|$|R
40|$|The {{effectiveness}} of testing {{is a major}} determinant of software quality. It is believed that indi-vidual testers vary in their effectiveness, but so far the factors contributing to this variation have not been well studied. In this study, we examined whether personality traits, as described by the five-factor model, affect performance on a software testing task. ICT students were given a small software testing task at which their effectiveness was assessed using several different criteria, in-cluding bug location rate, weighted fault density, and bug report quality. Their personality was assessed using the NEO PI- 3 personality questionnaire. We then compared testing performance according to individual and aggregate measures against different five-factor personality traits. Several weak correlations between two of these personality traits, extraversion and conscientious-ness, and <b>testing</b> <b>effectiveness</b> were found. 1...|$|E
40|$|Software Testing {{is one of}} {{the most}} {{important}} parts of the software development lifecycle. Functional and structural testing are the most widely used testing methods to test softwares. <b>Testing</b> <b>effectiveness</b> can be achieved by the State Transition Testing (STT) which is commonly used for carrying out functional testing of software systems. The tester is required to test all the possible transitions in the system under built. Structural testing relies on identifying effective paths of the code. Aim of the current paper is to present a strategy by applying ant colony optimization technique, for the generation of test sequences for state transitions of the system as well as path generation for the Control Flow Graph of the software code using the basic property and behavior of the ants. This Proposed strategy gives maximum software coverage with minimal redundancy...|$|E
40|$|Improving model-based {{component}} testability {{can further}} model-based approaches to software component testing (SCT) for desirable <b>testing</b> <b>effectiveness.</b> Component contracts are useful testing-support artefacts to improve component testability. This paper presents a new contract-based SCT technique, Test by Contract (TbC), which extends the Design by Contract concept to the SCT domain, and leverages UML-based testing with the contract mechanism to design model-based test contracts for UML-based SCT. We {{introduce a new}} concept of Contract for Testability as the principal goal of the TbC technique, and develop a set of important contract-oriented concepts (test contract, effectual contract scope, and internal/external test contract) and useful test criteria to improve model-based testability. We develop a useful stepwise TbC working process, and use a case study to demonstrate how to put the TbC technique into practice to undertake contract-based SCT with UML models. 1...|$|E
5000|$|To {{determine}} the best testing option, the <b>test</b> <b>Effectiveness</b> ratio (e)is calculated for one or more assumptions based on the estimated Costs (C), Time spend on testing (T) and the estimated reduction (R) between the NPV values of the assumptions (P {{as a percentage of}} the NPV range). Using these parameters, the effectiveness ratio (e) is calculated: ...|$|R
5000|$|An {{experiment}} to <b>test</b> the <b>effectiveness</b> of antibiotics on E. Coli in space ...|$|R
30|$|With {{respect to}} {{measures}} use, most measures (96.34 %) {{were used in}} practical initiatives. Only the measures <b>test</b> <b>effectiveness,</b> review preparation rate and review rate, cited in P 43, were not applied in a real situation reported in the selected publications. We did not eliminate these measures because the P 43 authors argued that they are suitable for SPC and we agree with them.|$|R
