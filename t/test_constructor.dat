11|24|Public
40|$|Recently, linear {{programming}} models for test construction were developed. These models {{were based on}} the information function from item response theory. In this paper another approach is followed. Two 0 - 1 {{linear programming}} models for the construction of tests using classical item and test parameters are given. These models are useful, for instance, when classical test theory has to serve as an interface between an IRT-based item banking system and a <b>test</b> <b>constructor</b> not familiar with the underlying theory...|$|E
40|$|A goal {{programming}} approach for selecting items from an item bank {{to construct a}} test, based on item response theory (IRT), was proposed. This approach can simultaneously handle multiple and conflicting goals, which is more realistic and practical than the linear {{programming approach}} that deals with only a single goal. An example was presented to show the procedures of applying the goal programming approach for constructing a test based on IRT one-parameter logistic model. The results provide the <b>test</b> <b>constructor</b> flexible ways to select items to meet different needs...|$|E
40|$|Despite the {{well-known}} theoretical advantages of item response theory (IRT) over clas-sical test theory (CTT), research examining their empirical properties {{has failed to}} reveal consistent, demonstrable differences. Using Monte Carlo techniques with simulated test data, {{this study examined the}} behavior of item and person statistics obtained from these two measurement frameworks. The findings suggest IRT- and CTT-based item difficulty and person ability estimates were highly comparable, invariant, and accurate in the test conditions simulated. However, whereas item discrimination estimates based on IRT were accurate across most of the experimental conditions, CTT-based item discrimina-tion estimates proved accurate under some conditions only. Implications of {{the results of this study}} for psychometric item analysis and item selection are discussed. The development of achievement, ability, aptitude, interest, and personal-ity tests is generally a multistep process that can follow one of two distinct measurement frameworks. These are usually called the classical test theory (CTT) and the item response theory (IRT) measurement strategies. De-pending on which method the <b>test</b> <b>constructor</b> chooses, different steps are taken in the statistical analysis of the initial pool of test items, possibly lead-ing to different selections of those items for the final test form. The question facing a <b>test</b> <b>constructor</b> is whether these differences will result in substan-tively different products. If so, is one product superior to the other in terms of the test’s overall psychometric properties? The purpose of this article is to re-port new empirical data bearing on these questions...|$|E
40|$|The {{purpose of}} this {{research}} is to investigate if <b>test</b> <b>constructors,</b> language examiner and teachers take Swedish L 2 students in consideration when creating tests in history. The purpose is also to discover what types of assignments that best evaluate Swedish L 2 students’ knowledge and abilities, and examine the teachers view of the Swedish national assessment in the subject history. The first research question is: How do <b>test</b> <b>constructors,</b> language examiner and teachers take Swedish L 2 students’ in consideration when creating tests in history? And which are the challenges? The second research question is: What kinds of assignments best evaluate Swedish L 2 students’ knowledges and abilities in the subject history according to the classified statistics, <b>test</b> <b>constructors,</b> language examiner and teachers? The third research question is: What are the teachers' views on the Swedish national assessment in grade nine? This research is composed of qualitative interviews with three <b>test</b> <b>constructors</b> who create the national assessment in history and seven teachers from three schools. One of the interviewed teachers is also interviewed for her role as a language examiner. The research is also based on a study of confidential statistics from the SAT history test. The main theoretical concepts of the study address to guidelines in Vygotsky’s theory of proximal development and Drie and Boxtels theoretical framework for historical reasoning. As well as research of the difficulties Swedish L 2 students’ meet in the Swedish language. Other central concepts of the study are validity and reliability. The empirical findings suggest that <b>test</b> <b>constructors,</b> language examiner and teachers meet many challenges but they do their best to take all students in consideration and follow criterias when constructing assessment for Swedish L 2 learners. The <b>test</b> <b>constructors</b> use a language examiner who scan and rewrite grammatical query constructions, and the teachers use different methods to take all students in consideration. Many teachers are in an agreement that the national assessment in history is well constructed, useful and inspiring. There is a belief among the interviewed participants that L 2 students find it easier to respond to open questions. However, this is not verified by the analysis of statistics. The <b>test</b> <b>constructors</b> and the language examiner also consider closed questions to be more secure to measure. The research is concluded by a discussion that all students might improve their skills if teachers were adapting the grammatical rewrites as the <b>test</b> <b>constructors</b> and language examiner do. Another conclusion is also that there are both advantages and disadvantages with open and closed questions. Open questions are preferable for Swedish L 2 students, however closed questions are more safe to measure. Finally, many teachers are pleased with the national assessment in history. Although it is discussed if L 2 students would benefit from doing the test orally instead of writing...|$|R
40|$|This paper {{discusses}} {{methods for}} obtaining verbal report data on second-language test-taking strategies; reports on types of findings obtained {{in a number}} of studies conducted by university students on how learners take reading tests, in particular cloze and multiple-choice tests; and finally con-siders the implications of the findings for prospective test takers and <b>test</b> <b>constructors.</b> The main conclusion is that a closer fit should be obtained between how <b>test</b> <b>constructors</b> intend for their tests to be taken and how respondents actually take them. There is a small but growing literature on how students go through the process of taking language tests- i. e. the steps that they take to arrive at answers to questions. Such research has generally focused on the testing of native language skills, but it has come to encompass second-language testing as well. The purpose of such research has been to explore the closeness-of-fit between the tester’s presumptions about what is being tested and the actual processes that the test take...|$|R
40|$|The term ‘validity’ {{is one of}} {{the most}} {{important}} and one of the most debated concepts in educational measurement. In this paper, I argue that various different approaches can all be viewed from an associational perspective. I also argue that our understanding will be enhanced by adopting some basic ideas of scientific reasoning to the process of establishing validity and making fully transparent the assumptions and procedures used by both <b>test</b> <b>constructors</b> and <b>test</b> users...|$|R
40|$|This paper {{deals with}} the problem of {{selecting}} tests from item banks. In an item bank, items are stored with their characteristics, such as item format, content de-scnptors, and psychometric parameters (e. g., item difficulty). The test construction problem is to select a test from an item bank that fits all requirements stated by the <b>test</b> <b>constructor.</b> Researchers have approached this problem from a linear programming point of view. The topic {{of this paper is to}} give an introduction to this approach. Keywords: test construction, linear programming, psychometrics, educational measurement, computerized testing, test theory. The large-scale introduction of computers in education and devel-opments in modern test theory (item response theory) have paved the way for computerized test item banking. Once banks of items with known properties have been stored, test construction takes the form of selecting tests from the banks that should meet the speci-fications formulated by the <b>test</b> <b>constructor.</b> For the item selection process this practice implies that several constraints with respect to the content, format, and measurement qualities of the items should be imposed, while at the same time the test should have optimal qualities with respect to some of its parameters. Since the mid [...] ig 8 os research has been conducted to solve this test construction problem by applying linear programming (LP) techniques, and at present several LP models are available that al-low the construction of tests with maximum information, maxi-mum reliability, minimum length, or other optimal properties at the same time guaranteeing full control over the other test param-eters and the composition of the test. Such models can be solved through the application of general LP software packages or the ap...|$|E
40|$|This work studies IRT-based Automated Test Assembly (ATA) of {{multiple}} test forms (tests) that meet an absolute target information function, i. e. selecting from an item bank only the tests that have information functions {{that are at}} a small distance away from the target. The authors introduce the quantities multiplicity of tests and probability of selecting a test with particular number of items N and distance E from the target. A Grand Canonical Monte Carlo test-assembly algorithm is proposed that selects tests according to this probability. The algorithm allows N to vary during the simulation. This work demonstrates {{that the number of}} tests that meet the target depends strongly on N. The algorithm is capable of finding tests with small values of E and various values of N depending on the need of the <b>test</b> <b>constructor.</b> Most importantly, it can determine the optimal N for which a maximal number of tests with certain specified small E exists...|$|E
40|$|A {{method is}} {{described}} for simultaneous test construction using the Operations Research technique zero-one programming. The model for zero-one programming {{consists of two}} parts. The first contains the objective function that describes the aspect to be optimized. The second part contains the constraints under which the objective function should be optimized. The selection of items is based on information from item response theory. Simultaneous test design is used when tests have to be constructed {{so that there is}} a certain relationship between them. Trio examples of simultaneous test construction are presented. The construction of two parallel tests is considered, and designs of three tests that should measure best at successive parts of the ability scale are described. Examples were carried out using an item bank of 10 items chosen at random. Tha sequential construction of the same series of tests was compared. The examples illustrate that the tests constructed using simultaneous techniques besc fit the intentions of the <b>test</b> <b>constructor.</b> Three tables give the data for th, 1 test construction methods. (Author/SLD) Reproductions supplied by EDRS are the best that can be made from the original document...|$|E
40|$|Abstract: A {{new type}} of approach, the {{analysis}} of the process and strategies of test taking, provides new insights into the factors that affect test performance and construct validity. One can find the fact that what <b>test</b> <b>constructors</b> think is being tested or will be tested is not always what the test actually will do when test-taking strategies are concerned about. This paper makes a brief review of the approach, mainly focusing on test-taking strategies and the most common way — verbal report approach — of inquiring those strategies. Key words: construct validity; test-taking strategies; verbal report approach 1...|$|R
2500|$|Most IQ {{tests are}} {{constructed}} {{so that there}} are no overall score differences between females and males. Popular IQ batteries such as the WAIS and the WISC-R are also constructed in order to eliminate sex differences. In a paper presented at the International Society for Intelligence Research in 2002, it was pointed out that because <b>test</b> <b>constructors</b> and the United States' Educational Testing Service (which developed the US SAT test) often eliminate items showing marked sex differences {{in order to reduce the}} perception of bias, the [...] "true sex" [...] difference is masked. Items like the MRT and RT tests, which show a male advantage in IQ, are often removed. Meta-analysis focusing on gender differences in math performance found nearly identical performance for boys and girls, and the subject of mathematical intelligence and gender has been controversial.|$|R
40|$|To {{efficiently}} assess multiple psychological constructs and {{to minimize}} the burden on respondents, psychologists increasingly use shortened versions of existing tests. However, compared to the longer test, a shorter test version may have a substantial impact on the reliability and {{the validity of the}} test scores in psychological research and individual decision making. In this study, we reviewed the psychological literature for recent trends in the use of short tests and examined in depth how and to what extent <b>test</b> <b>constructors</b> and <b>test</b> users addressed the impact on reliability and validity, other potential consequences of using short tests. The sample consisted of shortened tests found in six peer-reviewed psychological journals in the period 2005 – 2010. Based on our review, we provided recommendations for psychologists considering test shortening. Keywords: literature review, psychological tests, reliability of short tests, test shortening, validity of short test...|$|R
40|$|WebGL is a {{standard}} for drawing graphics in a web browser. Currently it isn’t widely understood how consistently WebGL performs across {{a majority of the}} de- vices that support it. Determining if an image looks correct to a human observer is an interesting problem. The solution for this is useful when developing WebGL applications, since a developer could make better informed decisions during de- velopment. The differences in capability between WebGL implementations are studied, and a few factors are selected that likely will contribute to variations in the rendered output. These factors are found by studying the WebGL specification documen- tation, and in the cases where it is ambiguous, further, authorative sources have contributed to the choice of factors studied. A prototype testing system is developed, including a tool for simulating imple- mentation differences. Two image processing algorithms are evaluated for their suitability in an automatic testing system. For testing, four test cases are devel- oped. The testing system is run with the test cases on wide range of devices, both mobile and desktop. The results show that image processing is not suitable alone the source for deter- mining a test success or failure. However, some promise is shown in using image processing as one component in a fully automatic testing system. Furthermore, developing test cases that perform as the <b>test</b> <b>constructor</b> intends is proven to be a challenge in itself...|$|E
40|$|The {{approach}} to achievement testing {{used in the}} schools today is, with few exceptions, identical to that used fifty or sixty years ago—one test, given to all students, containing items taken from a bailiwick guarded carefully by the teacher or <b>test</b> <b>constructor</b> and producing scores interpretable only by him. Although this ap-proach may have been adequate in the past, such {{is not the case}} now. What we need today is not a new refinement on what we have been doing, but a new {{approach to}} assessing student achievement in an instructional program. Achievement testing is a well-worn topic in the literature, and it is understandable that the reader might wonder "What could possibly be new here? " What is new here is an explicit conceptual-ization of the relationship between achievement tests and an in-structional program. Everyone knows that an achievement test must be related to the instructional program, but the nature of this relationship has always been ambiguous or mysterious, and the resultant problems have become unbearable. So sad is the current state of affairs that we can barely distinguish between a "good " and a "bad " achievement test and must rely upon expert opinion—when the difference should be obvious to every student in the instructional program. What follows here and constitutes the substance of this article is The opinions expressed herein are those of the author and do not necessaril...|$|E
40|$|This is {{a conference}} paper. Over {{the past ten}} years the use of {{computer}} assisted assessment in Higher Education (HE) has grown. The majority of this expansion has been based around the application of multiple-choice items (Stephens and Mascia, 1997). However, concern has been expressed about the use of multiple choice items to test higher order skills. The Tripartite Interactive Assessment Development (TRIAD) system (Mackenzie, 1999) has been developed by the Centre for Interactive Assessment Development (CIAD) at the University of Derby. It is a delivery platform that allows the production of more complex items. We argue that the use of complex item formats such as those available in TRIADs could enhance validity and produce assessments with features not present in pencil and paper tests (cf. Huff and Sireci, 2001). CIAD was keen to evaluate tests produced in TRIADs and so sought the aid of the National Foundation for Educational Research (NFER). As part of an initial investigation a test was compiled for a year one Systems Analysis module. This test was produced by the tutor (in consultation with CIAD) and contained a number of item types; both multiple-choice items and complex TRIADs items. Data from the test were analysed using Classical Test Theory and Item Response Theory models. The results of the analysis led to a number of interesting observations. The multiple-choice items showed lower reliability. This was surprising since these items had been mainly obtained from published sources, with few written by the <b>test</b> <b>constructor.</b> The fact that the multiple-choice items showed lower reliability compared to more complex item types may flag two important points for the unwary test developer: the quality of published items may be insufficient to allow their inclusion in high-quality tests, and furthermore, the production of reliable multiple-choice items is a difficult skill to learn. In addition it may not be appropriate to attempt to stretch multiple-choice items by using options such as ‘all’ or ‘none of the above’. The evidence from this test seems to suggest that multiple-choice items may not be appropriate to test outcomes at undergraduate level...|$|E
5000|$|Most IQ {{tests are}} {{constructed}} {{so that there}} are no overall score differences between females and males. Popular IQ batteries such as the WAIS and the WISC-R are also constructed in order to eliminate sex differences. In a paper presented at the International Society for Intelligence Research in 2002, it was pointed out that because <b>test</b> <b>constructors</b> and the United States' Educational Testing Service (which developed the US SAT test) often eliminate items showing marked sex differences {{in order to reduce the}} perception of bias, the [...] "true sex" [...] difference is masked. Items like the MRT and RT tests, which show a male advantage in IQ, are often removed. Yet a meta-analysis focusing on gender differences in math performance found nearly identical performance for boys and girls, and the subject of mathematical intelligence and gender has been controversial.|$|R
40|$|In {{the present}} paper, linear {{programming}} {{was used to}} select items from item pools based on one-, two-, and three-parameter models so that a target test information function was reached. The primary interest was in the distributional characteristics of the items thus selected. The {{results suggest that the}} linear programming approach focuses on the "worst feature" of the target information function (i. e., the extremes of a uniform target and the maximum of a peaked target). The values of the parameters of the selected items tend to form clusters. For uniform targets, these clusters are associated with the extremes of the target range, whereas for peaked targets they are associated with the maximum of the target. Selecting items from an item pool by linear programming appears to be a useful addition to the <b>test</b> <b>constructor’s</b> repertoire. However, additional refinement may be needed to obtain a specific distribution of item parameters for a given test. Index terms: Item response theory, Item selection, Linear programming, Target information function...|$|R
40|$|In this study, {{the author}} {{examined}} the comparability of item statistics {{generated from the}} frameworks of classical test theory (CTT) and 2 -parameter model of item response theory (IRT). A 60 -item Physics Achievement Test was developed and administered to 724 senior secondary school two students (Age 16 - 18 years), who {{were randomly selected from}} 16 senior secondary schools in Ibadan Educational Zone I, Oyo State, Nigeria. Results showed that item statistics obtained from both frameworks were quite comparable. However, item statistics obtained from IRT 2 -parameter model appeared more stable than those from CTT. Moreover for item selection process, IRT 2 -parameter model led to deletion of fewer items than CTT model. This result implies that test developers and public examining bodies should integrate IRT model into their test development processes. Through IRT model, <b>test</b> <b>constructors</b> would be able to generate more reliable items than in the CTT model being currently used and ultimately the test scores of examinees will be more reliably estimated...|$|R
40|$|Henk Notté, {{geographer}} and <b>test</b> <b>constructor</b> at Cito Institute, {{has developed}} a test (the gea-test) {{in order to measure}} geographical awareness amongst scholars aged 10 to 18 years old. The test is expected to distinguish scholars with a strongly developed geographical awareness from those with a less developed geographical awareness and to translate these outcomes into a score. Being a part of its validity research this thesis revolves around the question: ‘on what type of geographical knowledge scholars base their answers, when answering questions of the gea-test and what does that imply for the validity of the test? ’. This research takes a closer look to the reasoning behind a good or wrong answered question. Why can a scholar answer the question correctly? Do certain misconceptions exist within geographical education? The research method used in this thesis are ‘thinking-aloud’-protocols, a research method often perceived as being extremely time-consuming and therefore rarely used. Because this method has never been applied on larger scales this research offers a unique insight in {{what goes on in the}} scholar’s mind when asked to answer a geographical question. In the first part of the research scholars were asked to make the gea-test consisting out of 35 randomly selected questions. Afterwards, scholars with either exceptionally high or low scores or scores equal to the class average were selected for the ‘thinking-aloud’-protocols, a total of 24 scholars have participated in the protocols. During the ‘thinking-aloud’-protocols scholars were asked to make each six preliminary selected exercises while thinking and reasoning aloud. This allows the researcher to clarify why a pupil has chosen a certain answer. The concept of geographical awareness comprises out of nine different skills and abilities: photo-interpretation, map and graph reading, scales, comparisons, factual knowledge, thematic mental map, topographical mental map, conceptual knowledge and principal knowledge. These skills were all evaluated and quantified during the ‘thinking-aloud-protocols’ and made a comparison possible between the outcomes of the gea-test and the ‘thinking-aloud’-protocols could have been made by computer program TiaPlus. Based on the results we can say that when answering questions of the gea-test scholars aged 11 to 12 mainly use the skills: scale, topographical mental map and conceptual knowledge while scholars aged 14 to 15 mostly have used the skills: thematic mental map, topographical mental map and conceptual knowledge and principal knowledge. These results have led to the conclusion that the gea-test does make a rightful distinction between scholars with a strongly and scholars with a lesser developed geographical awareness. This conclusion indicates that the gea-test is a valid method to evaluate the scholars capability regarding geographical skills such as the use of scales, topographical and thematic mental maps, conceptual knowledge and principal knowledge...|$|E
40|$|Testing and {{teaching}} are closely interrelated since testing is aimed {{to determine the}} achievement of the objective of education. It can be seen whether or not the teaching learning activities are successful. Teachers as a <b>test</b> <b>constructor</b> should be able to construct a good test so that the test is qualified enough to be given to the students. In fact the quality of the test made by the teacher is doubtful. It is still to be questioned whether the test is valid and reliable o not since the teachers rarely analyze and revise the test they made. Teachers use to prefer an unanalyzed and unrevised test items. It makes the validity and reliability of teacher-made test doubtful. It can be low or even unknown. Knowing this fact, the teachers should analyze their test so that they can know the quality of their test. By analyzing the test, the teachers will know which items can be used or which items should be revised. Based on the reality above, the teacher-made English lest items in UAS semester 2 2008 / 2009 of the first year students of SMA 2 Muhammadiyah Sidoaijo are analyzed, whether it is really constructed in a right way, following the right principles or not. The problem, there are four questions formulated, 1) how is the content validity of the teacher-made English test items in UAS semester 2 2008 / 2009 of the first year students of SMA 2 Muhammadiyah Sidoaijo? 2) how is the reliability of the teacher- made English test? 3) how is the item difficulty of the teacher-made English test? 4) how is the discrimination index of the teacher-made English test? This study is expected to be useful for teachers in constructing a good English test items, for students in showing their real achievement in their studying, and for those who are involved in the teaching learning process in determining the effectiveness of the teaching learning activities. It is chosen since many teachers rarely analyze their own test. It is still to be questioned whether the test is really constructed in a right way, following the right principles or not. UAS or final test concerns with the pupil’s achievement at the end of instruction. It is used to judge the final quality and quantity of student achievement and /or the success of the instructional program. Since SMA 2 Muhamadiyah Sidoaijo is one of the favorite senior high school in Sidoaijo, it is chosen as the school representing. There are thirteen classes can represent all the first year classes and to know whether the test is valid and reliable or not Those are classes are class XI and X 2. The source of the data is a teacher-made English test items in UAS semester 2 2008 / 2009 for the first year students of SMA 2 Muhammadiyah Sidoaijo. The data of this study are l) the problem of the teacher-made English test items in UAS semester 2 2008 / 2009 for the first year students of SMA 2 Muhammadiyah Sidoaijo and the key answers of the teacher-made English test items in UAS semester 2 2008 / 2009 for the first year students of SMA 2 Muhammadiyah Sidoaijo, 2) the students' answers of the teacher-made English test items in UAS semester 2 2008 / 2009 for the items in UAS semester 2 2008 / 2009 for the first year students of SMA 2 Muhammadiyah Sidoaijo, 2) the students' answers of the teacher-made English test items in UAS semester 2 2008 / 2009 for the first year students of SMA 2 Muhammadiyah Sidoaijo, and 3) the students' scores of the teacher- made English items in UAS semester 2 2008 / 2009 for the first year students of SMA 2 Muhammadiyah Sidoaijo. To collect the data, first the test by teacher- made, then it is given to the students in the UAS on Friday, 12 nd of June 2009. After the test has been given to the students and answered by the students, it is collected and scored. Finding the result, then the problems and the key answers of the test, the students’ answers of the test and the students’ scores are analyzed. To know the test is good, its validity, reliability, index of difficulty, and index of discrimination are analyzed. Then the result is checked with the criteria. The test is good if the test shows high validity, adequate reliability, acceptable index of difficulty and index of discrimination. The result of this study leads {{to the conclusion that the}} teacher-made English test items in UAS semester 2 2008 / 2009 for the first year students of SMA 2 Muhammadiyah Sidoaijo have high content validity and adequate reliability. Or the index of difficulty, the multiple choice test shows different result. One class shows acceptable and the other shows unacceptable. It is caused by difference ability of the students in each class in absorbing the lesson, the rank of each class where class X 2 has higher than XI, the way of teacher in teaching...|$|E
40|$|In the past, {{assessment}} was mainly used to streamline second language {{education in the}} Netherlands. Levels of proficiency were based upon empirical data, gathered through language tests. The problems with this practical approach were manifold. Since it was not based upon a view of language, levels could not be properly described. There was also a lack of pedagogical underpinning {{of the use of}} these instruments, which resulted in the use, or some might say the misuse, of the tests for accountability reasons only. Although the <b>test</b> <b>constructors</b> tried to emphasize the function of measurement for learning by providing self assessment instruments along with formal tests, this did not lead to the intended results. Self assessment appeared not to be used at all. Another problem was that the data was collected from a very heterogeneous, but literate population. As a result, {{it was hard to tell}} whether the established levels were based upon language progress or upon complexity of tasks. This made these instruments less suitable for low-educated students and of no use at all for illiterate students...|$|R
50|$|Unit testing {{refers to}} tests that verify the {{functionality}} {{of a specific}} section of code, usually at the function level. In an object-oriented environment, this is usually at the class level, and the minimal unit <b>tests</b> include the <b>constructors</b> and destructors.|$|R
40|$|Too often public {{examinations}} {{are left}} {{entirely in the}} hands of <b>test</b> <b>constructors</b> and statisticians. The wide ranging considerations related to examinations call for a change in this trend. Perhaps, those responsible for examining ought to regard themselves less as statisticians and <b>test</b> <b>constructors</b> and more as educationists. There is all the more reason for this in Fiji {{in light of the fact}} that it is a multi-cultural society. The thesis is essentially a theoretical exploration into the major functions of public examinations in Fiji. The whole exercise is based on the maxim that examinations form an integral part of the educative process and on the contention that the behaviour elicited before, during and after an examination from candidates is heavily influenced by their past experiences, nourished within the restraints and limits of their cultural milieu. By way of introduction, general problems in education in multi-cultural societies are traced and the language-problem dealt with in depth to highlight the complexities of such problems. After a brief look at the composition of the Fiji Society and its education system, the major public examinations are described. Then, the stated functions, purposes and effects of examinations are reviewed and some implications drawn. From the literature reviewed it is clear that examinations need to be validated against the declared and agreed upon educational aims. In the Fiji context, a search for some validating criteria is also discussed. In order to explore the interaction between the public examinations and aims of education attention is focussed upon the specific cultural values and educational aspirations of the three dominant cultural groups in Fiji, viz., Fijians, Indians and Europeans. Examination problems in Fiji, arising from an importation of foreign examinations and the multi-cultural set-up, become the theme for discussion in the final sections of the thesis. The relationship between the long-term effects of both, examinations and a number of socio-political ideologies – integration, assimilation, pluralism – is then outlined. Pluralism proves attractive as a base for decision-making regarding examinations in Fiji. It is likely that in order to solve tomorrow's problems here, allowances for existing differences in expressions and life-styles will help. It is suggested that examinations in Fiji can be assigned a re-vitalizing role in the educational system if they are, inter alia, multi-modal and accommodate 'originality' and diversity of values, expressions and the like. With almost a complete dearth of research information on various aspects of education in Fiji, this exploration ends with a note on the necessity for research in the area of examinations...|$|R
40|$|By definition, {{differential}} item functioning (DIF) {{refers to}} unequal probabilities of a correct {{response to a}} test item by examinees from two groups when controlled for their ability differences. Simulation results are presented for an attempt to purify a test by separating out multidimensional items {{under the assumption that}} the intent of the <b>test</b> <b>constructors</b> was to construct a unidimensional test for a given population. The procedure used to arrive at the purified and essentially unidimensional subtest used the multidimensional theory of DIF/bias proposed by Shealy and Stout (1993) and the statistical procedure DIMTEST for assessing essential unidimensionality. When applicable, the proposed methodology leads to a statistically validated construct valid subtest {{that can be used in}} the matching criterion for DIF/bias analysis. This methodology can be applied to an internal or external matching criterion. It is only applicable when the majority of test items are tapping the intended ability for a given population, while a few items are tapping other major abilities in addition to the intended ability. This method is not applicable when DIF/bias is pervasive. Ten tables summarize analysis results. (Contains 28 references.) (SLD) Reproductions supplied by EDRS are the best that can be made from the original document...|$|R
40|$|An {{analysis}} of theoretical trends {{in research on}} self-disclosure lends support to Benner's (1968) distinction between cognitive, behavioural and affective dimensions to the self-disclosure construct. However, since {{there is very little}} attempt in the research at coordinating theoretical and practical (measurement) aspects (the importance of which is stressed by Fiske & Pearson, 1970), problems have arisen, the most serious of which {{is the fact that the}} <b>test</b> <b>constructors</b> in this field always assume their instruments to measure the totality of the target concept. On the basis of the above three-dimensional view, a logical assumption was made that of the twenty-three different types of self-disclosure measures to date, three measures appear to be emphasizing one aspect of self-disclosure, i. e. the affective dimension, viz. : an Essay Topic procedure after Burhenne & Mirels (1969) stressing affect by virtue of the rating procedure; the Hurley Rating Scale (1968), stressing affect by means of its introductory and definitive paragraphs; and the Shapiro Disclosure Seale (1969) consisting of statements referring to behaviours with positive or negative affect. One would expect these measures to correlate positively as they are all measures of 'self-disclosure'. Since the three chosen measures in addition specifically stress the affective dimension of the construct it was hypothesized that they would correlate strongly...|$|R
40|$|This {{research}} series {{began as}} a test of an emotion-attribution approach to moral behavior. However, in the early studies, college students who read about morality were subsequently more likely to cheat on a vocabulary test than were control subjects who read materials irrelevant to morality. We hypothesized that resentment toward the <b>test</b> <b>constructors</b> interacted with the moral schemas activated by the reading task. To reduce resentment, in Study III the vocabulary test was presented as the experimenter 2 ̆ 7 s doctoral research. As predicted, compared to controls, those subjects who read about morality cheated less. Study IV was a quasi-experiment that confirmed the hypothesized resentment differences between Study III and the earlier studies. In Study V, while two groups read about morality, one group read an internal emotion-attribution passage and the other read an external version; less cheating was observed in the internal condition than in the external or control conditions. The results indicate that even when moral schemas are elicited under conditions favoring moral behavior, those schemas will lead to reduced cheating most effectively under conditions in which subjects attribute their emotional arousal to their own behavior rather than to external causes. Issues of moral schema activation and emotion-attribution in moral behavior are discussed...|$|R
40|$|Several factors {{combine to}} {{undermine}} {{our ability to}} detect the full incidence of highly gifted children in the population: (1) gifted children are not systematically assessed as are mentally retarded children; (2) {{it is common for}} gifted children to hide their abilities; (3) IQ tests are not constructed with sufficient difficulty to capture the full range of gifted children's abilities; (4) the tests provide no norms beyond the third or fourth standard deviation; (5) mental ages are no longer provided on current tests, which prevents extrapolating ratio IQs for the highly gifted; (6) scores in the highly gifted range are significantly depressed with each new set of test norms. One {{of the authors of the}} Stanford-Binet Revision IV, Elizabeth Hagen, explained in an interview (Silverman, 1986) the constraints faced by <b>test</b> <b>constructors.</b> Their objective is to produce instruments that can adequately appraise the full range of individual differences in a chronological age group from the slowest level of development to the most rapid, but the instruments must be fairly easy to administer within a reasonable amount of time. The compromise is to produce instruments that are most effective for the majority of students. Tasks are eliminated if they are successfully completed by 99. 99 percent of an age group or are failed by 99. 99 percent of an age group. Items that can only be solved by children in classes for the gifted are purposely exclude...|$|R
40|$|The present paper {{looks at}} the issue of {{standardization}} in L 2 oral testing. Whereas external examiners are frequently used globally, some countries opt for test-takers’ own teachers as examiners instead. In the present study, Sweden {{is used as a}} case in point, with a focus on the mandatory, high-stakes, summative, 9 th-grade national test in English (speaking part). The national test has the typical characteristics of standardized tests and its main objective is to contribute to equity in assessment and grading on a national level. However, using teachers as examiners raises problems for standardization. The aim {{of this study is to}} examine teachers’/examiners’ practices and views regarding four aspects of the speaking test – test-taker grouping, recording practices, the actual test occasion, and examiner participation in students’ test interactions – and to discuss findings in relation to issues concerning the normativity and practical feasibility of standardization, taking the perspectives of test-takers, teachers/examiners, and <b>test</b> <b>constructors</b> into account. In order to answer research questions linked to these four aspects of L 2 oral testing, self-report survey data from a random sample of teachers (N = 204) and teacher interviews (N = 11) were collected and quantitative data were analyzed using inferential statistics. Survey findings revealed that despite thorough instructions, teacher practices and views vary greatly across all aspects, which was further confirmed by interview data. Three background variables – teacher certification, work experience, gender – were investigated to see whether they could provide explanations. Whereas certification and gender did not contribute significantly to explaining the findings, work experience bore some relevance, but effect sizes were generally small. The study concludes that using teachers as examiners is a well-functioning procedure in terms of assessment for learning, but raises doubts regarding assessment of learning and standardization; a solution for test authorities could be to frame the test as non-standardized. Testing Tal...|$|R
40|$|Research has {{demonstrated}} that people can and often do consciously manipulate scores on personality <b>tests.</b> Test <b>constructors</b> have responded by using social desirability and lying scales {{in order to identify}} dishonest respondents. Unfortunately, these approaches have had limited success. This study evaluated the use of appropriateness measurement for identifying dishonest respondents. A dataset was analyzed in which respondents were instructed either to answer honestly or to fake good. The item response theory approach classified a higher number of faking respondents at low rates of misclassification of honest respondents (false positives) than did a social desirability scale. At higher false positive rates, the social desirability approach did slightly better. Implications for operational testing and suggestions for further research are provided. Index terms: appropriateness measurement, detecting faking, item response theory, lying scales, person fit, personality measurement...|$|R
40|$|Version 0. 1. 0 New Features New π 0 estimators: BUM, Censored BUM, Flat Grenander New p-value {{adjustment}} methods: Forward Step, Benjamini-Liu BUM {{model for}} simulating p-value distributions Fast isotonic regression Reference list of related software packages in documentation Changes Grenander ECDF estimator Immutable method types Default α value for Two Step <b>constructor</b> <b>Tests</b> use new julia 0. 5 base testing framework Documentation moved to 'docs' directory Removed Test coverage reporting through coveralls Support Requires julia 0. ...|$|R
40|$|This thesis {{focuses on}} the scoring of a {{national}} test of Norwegian as a second language: Språkprøven i norsk for voksne innvandrere, developed by Norsk språktest at the University of Bergen. In order to ensure a fair assessment of the candidates’ oral production, the <b>test</b> <b>constructors</b> make use of trained raters basing their scores on an explicit rating scale (NORS). These two highly recommended procedures in performance testing have traditionally been viewed as means to heighten reliability of test scores. In line with recent developments {{in the field of}} language testing, I argue that the rater variable affects not only reliability, but the very construct validity of test scores. Rater training and development of rating scales are costly and time-consuming enterprises. To establish their effect on test scores is therefore interesting from a test theoretical, as well as from a practical and economical point of view. In the study, four groups of informants are compared: non-linguists (or naïve-native speakers), teachers of Norwegian as a second language without rater training, raters of Språkprøven, and finally a subgroup of the most experienced raters of Språkprøven. The informants score eight candidates’ video recorded performances on a six-point scale. The first four are scored impressionistically, and the next for by informants using the NORS. The quantitative data are used in an investigation of internal agreement (inter-rater reliability) between raters of the distinct groups. Informants are also asked to give written reports of their scores, which are used in an investigation of raters’ underlying criteria for assessing speech. The qualitative data are used firstly in an attempt to explain the results of the reliability study, and thereafter in an investigation of the match between raters’ criteria and the criteria of the NORS (construct validity). The results reveal differences between groups for the scores they give, {{as well as for the}} reasons for these scores. One important conclusion echoes the claim that “quantitative similarities in ratings may mask significant qualitative differences in the reasons for those ratings” (Connor-Linton 1995 : 99) ...|$|R
40|$|The {{purpose of}} this study was to examine the {{possibility}} of differential performance for males and females on a mathematics achievement test for different item arrangements. Previous studies of sex differences on item arrangements on mathematics tests have been limited in scope and sample size. This study used a national, standardized test for the mathematics test and a large, nationwide sample. Three item arrangements were employed in the study: (1) easy-to-hard; (2) easy-to-hard within content; and (3) spiral-cyclical. ^ The 20 -item mathematics achievement test selected for study was constructed from tryout items of the General Educational Development (GED) Experimental Mathematics Test. The three forms were spiraled into the 50 GED experimental units and administered during the 1985 fall testing sites nationally. ^ Differential performance was examined at both the test and item levels. At the test level, Spearman rank-order correlation coefficients were obtained for all pairwise combinations between and within sex. The results indicated that the rank order of item difficulty is essentially the same between male and females on the different item arrangements. However, when looking within sex, for both males and females, the rank order of the item difficulties is relatively lower. ^ At the item level, a chi-square full procedure was performed on each item of each form. None of the items showed significant male-female differences for the easy-to-hard form or the spiral-cyclical form. Five of the 20 items showed a significant difference between males and females on the easy-to-hard within content form. ^ This research describes a procedure that can be used to assess differential performance between groups. The results suggest that the item difficulty statistic may not remain constant regardless of the position of the item within the test, and that item arrangement may be a factor of differential performance between males and females on mathematics achievement tests. These findings have several implications for <b>test</b> <b>constructors</b> and future research. ...|$|R
40|$|Systems that locate {{mentions}} {{of concepts}} from ontologies in free text {{are known as}} ontology concept recognition systems. This paper describes an approach to {{the evaluation of the}} workings of ontology concept recognition systems through use of a structured test suite and presents a publicly available test suite for this purpose. It is built using the principles of descriptive linguistic field work and of software testing. More broadly, we also seek to investigate what general principles might inform the construction of such test suites. The test suite was found to be effective in identifying performance errors in an ontology concept recognition system. The system could not recognize 2. 1 % of all canonical forms and no non-canonical forms at all. Regarding the question of general principles of test suite construction, we compared this test suite to a named entity recognition <b>test</b> suite <b>constructor.</b> We found that they had twenty features in total and that seven were shared between the two models, suggesting that there is a core of feature types that may be applicable to test suite construction for any similar type of application. 1...|$|R
40|$|Thesis (Ed. D.) [...] Boston UniversityStatement of the Problem The {{major problem}} of this {{investigation}} was to determine the most commonly dictated medical terms in hospital records and to provide authors, teachers, and <b>test</b> <b>constructors</b> with scientifically determined lists of technical medical data which may be used in the educational preparation of medical assistants. Summary of Procedures 1. Utilizing the records of three hospitals, two case histories for each of 17 medical systems and specialties were selected for analysis each month for a two-year period. 2. Blakiston's New Gould Medical Dictionary was utilized to identify technical medical terms and the Teacher's Word Book of 30, 000 Words by Thorndike and Lorge was used to delimit the study. 3. A common list of medical specialties offered in the general hospital field was determined to facilitate classification of the medical terms analyzed. 4. The validity of the sampling method utilized vas verified by comparing the proportion of cases discharged from each medical department in the hospital with the sampled cases analyzed. 5. Although the hospitals utilized were in one state, the writer made an effort to show the geographic representativeness of the study by analyzing the distribution of the medical colleges and hospitals attended by the physicians involved in the study. 6. The medical data were arranged alphabetically in eight lists. Those terms with a frequency of five or more were considered commonly dictated words and were listed in rank order according to frequency. 7. A series of tables were constructed for the purpose of guiding teachers and authors in determining the extent of practice that may be devoted to the common technical data reported in the study. Summary of Findings 1. The sampling method employed produced 816 case histories dictated by 289 physicians and represented 17 specialized fields of medicine. 2. The case histories analyzed contained 325, 061 running words which included 41, 798 medical terms, 23, 528 medical phrases, 4, 065 medical abbreviations, 1, 064 weights and measures, 1, 539 medical diseases and operations, and the medical terms contained 19, 139 prefixes and 41, 258 suffixes. 3. The following medical terminology had a frequency of occurrence of five or more in the case histories analyzed and were considered commonly dictated in medical practice: (1) 1, 746 medical terms, (2) 973 medical phrases, (3) 289 medical abbreviations, (4) 15 weights and measures, (5) 51 medical diseases and operations, and (6) 77 prefixes and 80 suffixes. 4. Neurologists dictated the largest number of technical medical terms in each case history, 95, and psychiatrists dictated the largest number of running words in each case history, 1, 057. 5. The percentage of technical medical terms in the 325, 061 running words was 14. 13 per cent. 6. The 289 physicians involved in the study (a) have attended 39 of the 78 medical colleges in the United States and 32 medical colleges in 17 foreign countries, (b) have completed their internship and residency training in 135 hospitals in 22 states, the District of Columbia, and Canada, and (c) have had 4, 205 collective years of active medical practice. Conclusions and Recommendations 1. The common medical vocabulary utilized by physicians is difficult and extensive. 2. The seven types of common technical medical data reported in the study should be considered {{an essential part of the}} medical vocabulary of every medical assistant. 3. Prospective medical assistants should be taught the definitions and the common medical terminology of the 17 areas of medical specialization. 4. As indicated by the wide geographic distribution of the training schools of the physicians participating in the study, it may be concluded that the medical terminology in this study is representative of the terminology utilized by physicians in many parts of the United States and the world. 5. The findings of this study appear to be a reliable basis for the writing and revising of classroom materials and textbooks for the educational preparation of the medical secretary and medical assistants...|$|R
50|$|Meanwhile, in 1954 the {{production}} of Frania, the first post-war mass-produced washing machine in Poland started at SHL. In production until 1971, over 3 million pieces were sold, mostly in Poland. Also, in 1957 after a short hiatus {{the production}} of new SHL brand motorcycles resumed in Kielce and lasted until 1971. Their best moment was the 1960s, when over 180,000 of SHL M11 model left production lines in Kielce. In addition, in 1962 the Escorts group bought a licence to manufacture this model in India, under a brand Rajdoot. The license production lasted until 2005 and outlived their production in Poland by 35 years. Between 1961 and 1968 Eugeniusz Frelich, the factory <b>test</b> driver and <b>constructor,</b> won seven consecutive motorcycle road racing championships of Poland in a self-modified SHL 250 ccm, basically a serial SHL M11 with a larger engine. In 1966 he also came fifth during the Grand Prix motorcycle racing in Sweden {{and two years later}} gained the gold at the European Championships in Italy. During the 1960s it was decided to gradually phase out the construction of new SHL models and convert the factory to a automobile parts production plant. The SHL M-17 Gazela of 1969-1970 was the last SHL motorcycle produced in Poland.|$|R

