488|136|Public
25|$|<b>Text</b> <b>Generation</b> Framework: Visual Studio {{includes}} a full <b>text</b> <b>generation</b> framework called T4 which enables Visual Studio to generate text files from templates {{either in the}} IDE or via code.|$|E
25|$|The Word Hoard, the {{collection}} of manuscripts that produced Naked Lunch, also produced parts of the later works The Soft Machine (1961), The Ticket That Exploded (1962), and Nova Express (1964). These novels feature extensive use of the cut-up technique that influenced all of Burroughs's subsequent fiction to a degree. During Burroughs's friendship and artistic collaborations with Brion Gysin and Ian Sommerville, the technique was combined with images, Gysin's paintings, and sound, via Somerville's tape recorders. Burroughs was so dedicated to the cut-up method that he often defended {{his use of the}} technique before editors and publishers, most notably Dick Seaver at Grove Press in the 1960s and Holt, Rinehart & Winston in the 1980s. The cut-up method, because of its random or mechanical basis for <b>text</b> <b>generation,</b> combined with the possibilities of mixing in text written by other writers, deemphasizes the traditional role of the writer as creator or originator of a string of words, while simultaneously exalting the importance of the writer's sensibility as an editor. In this sense, the cut-up method may be considered as analogous to the collage method in the visual arts. New restored editions of The Nova Trilogy (or Cut-Up Trilogy), edited by Oliver Harris and published in 2014, included notes and materials to reveal the care with which Burroughs used his methods and the complex histories of his manuscripts.|$|E
2500|$|Improvements {{in speech}} {{recognition}} software mean that live captioning may be fully or partially automated. BBC Sport broadcasts use a [...] "respeaker": a trained human who repeats the running commentary (with careful enunciation and some simplification and markup) for input to the automated <b>text</b> <b>generation</b> system. This is generally reliable, though errors are not unknown.|$|E
5000|$|Iterature, a {{collection}} of pieces or documentations of performances which use the text from the web as material. Many of the pieces are search engines hacks (primarily Google). They get hold of text floating around the web {{and use it as}} raw material for various re-workings, cut-ups, algorithmic <b>text</b> <b>generations,</b> visualizations, cartographies and so forth.|$|R
50|$|Principles of Economics is {{a leading}} {{political}} economy or economics textbook of Alfred Marshall (1842-1924), first published in 1890. It ran into many editions and was the standard <b>text</b> for <b>generations</b> of economics students.|$|R
50|$|Cataloging, Image tagging, <b>Text</b> Analysis, Lead <b>Generation,</b> Merchandising Audits, Mystery Shopping, Offline data collection.|$|R
50|$|<b>Text</b> <b>Generation</b> Framework: Visual Studio {{includes}} a full <b>text</b> <b>generation</b> framework called T4 which enables Visual Studio to generate text files from templates {{either in the}} IDE or via code.|$|E
5000|$|From a {{computational}} {{point of}} view, {{it provides a}} characterization of text relations that has been implemented in different systems and for applications as <b>text</b> <b>generation</b> and summarization.|$|E
5000|$|Microsoft's Text Template Transformation Toolkit (usually {{referred}} to as [...] "T4") is a template based <b>text</b> <b>generation</b> framework included with Visual Studio. T4 source files are usually denoted by the file extension [...] ".tt".|$|E
40|$|Machine {{learning}} {{models are}} powerful but fallible. Generating adversarial examples - inputs deliberately crafted to cause model misclassification or other errors - can yield important insight into model assumptions and vulnerabilities. Despite significant recent work on adversarial example generation targeting image classifiers, relatively little work exists exploring adversarial example <b>generation</b> for <b>text</b> classifiers; additionally, many existing adversarial example generation algorithms require {{full access to}} target model parameters, rendering them impractical for many real-world attacks. In this work, we introduce DANCin SEQ 2 SEQ, a GAN-inspired algorithm for adversarial <b>text</b> example <b>generation</b> targeting largely black-box text classifiers. We recast adversarial <b>text</b> example <b>generation</b> as a reinforcement learning problem, and demonstrate that our algorithm offers preliminary but promising steps towards generating semantically meaningful adversarial text examples in a real-world attack scenario...|$|R
40|$|Abstract. This paper {{provides}} a first introduction to some basic issues in sentence generation. It describes how systemic-functional grammars {{deal with the}} problems of sentence generation and gives a brief overview of the use of systemic-functional grammars in sentence generation research. Contents: • What is sentence generation? • Putting sentences together into <b>texts</b> • <b>Generation</b> as ‘functionally motivated choice...|$|R
40|$|The KANT system (Knowledge-based, Accurate Natural-language Translation) {{is a set}} of {{software}} tools for automatic and interactive analysis of source <b>text</b> and <b>generation</b> of target <b>text</b> (Mitamura, et al., 1991). It has been primarily targeted towards the translation of technical text in controlled subdomains (Mitamura and Nyberg, 1995). Initially an outgrowth of research ideas following the completion of th...|$|R
50|$|The Origins section gives a {{detailed}} account of the progression of digital poetry from early computer experiments and tests, performed by a select few individuals, into the earliest forms of computer permutation writing and <b>text</b> <b>generation.</b>|$|E
50|$|The theorem {{concerns}} a thought experiment which cannot be fully {{carried out in}} practice, since it is predicted to require prohibitive amounts of time and resources. Nonetheless, it has inspired efforts in finite random <b>text</b> <b>generation.</b>|$|E
5000|$|William C. [...] "Bill" [...] Mann (died August 13, 2004, aged 69) was a {{computer}} scientist and computational linguist, the originator of Rhetorical Structure Theory (RST) and {{a president of}} the Association for Computational Linguistics (1987-1988). He is especially well known for his work in <b>text</b> <b>generation.</b>|$|E
40|$|The term Requirements Engineering {{refers to}} this part of a {{database}} development cycle that involves investigating the problems and requirements of the users community and developing a conceptual specification of the future system. Natural language plays an important role during this stage that has proved to be crucial in the development of computerized systems. The required acquisition of application domain knowledge is achieved either through documents and texts analysis or by means of interviews i. e through language manipulation. Similarly validation of the specification is made via oral discussions with users. The paper proposes that Requirements Engineering (R. E) should be supported by a CASE tool based on a linguistic approach. It presents a R. E support environment that generates the conceptual specification from a description of the problem space provided through natural language statements. Complementary, validation is based on <b>texts</b> <b>generation</b> from the conceptual specificatio [...] ...|$|R
50|$|XTM: XML-Based <b>Text</b> Memory Next <b>Generation</b> Web 2.0 Computer Assisted Translation {{software}} (CAT) is {{a language}} translation program from XTM-INTL based on OAXAL open architecture for XML authoring and localization.|$|R
500|$|Until {{at least}} 1930, Sitti Nurbaya {{was one of}} Balai Pustaka's most popular works, often being {{borrowed}} from lending libraries. After Indonesia's independence, Sitti Nurbaya was taught as a classic of Indonesian literature; {{this has led to}} it being [...] "read more often in brief synopsis than as an original <b>text</b> by <b>generation</b> after generation [...] of Indonesian high school students". , it has seen 44 printings.|$|R
50|$|One {{approach}} to knowledge acquisition investigated {{was to use}} natural language parsing and generation to facilitate knowledge acquisition. Natural language parsing could be performed on manuals and other expert documents and an initial first pass at the rules and objects could be developed automatically. <b>Text</b> <b>generation</b> was also extremely useful in generating explanations for system behavior. This greatly facilitated the development and maintenance of expert systems.|$|E
50|$|Arria NLG plc is a UK-based company {{offering}} Artificial Intelligence {{technology in}} data analytics and information delivery. It {{is one of}} the pioneering companies in the space of automatic <b>text</b> <b>generation,</b> and when it floated on London's Alternative Investment Market (AIM) in December 2013, it was valued at over £160 million. Arria's technology is based on three decades of scientific research in the field of Natural Language Generation (NLG).|$|E
50|$|In 1983 {{he took a}} {{position}} as research linguist at the Institute, where {{he worked on the}} application and development of systemic theory and descriptions for <b>text</b> <b>generation,</b> including the maintenance and expansion of a systemic grammar of English for <b>text</b> <b>generation.</b> It was during this time that he worked with Bill Mann and Sandra Thompson in the development of Rhetorical Structure Theory. In 1988 he moved to the University of Sydney, where he was lecturer, then senior lecturer until 1994. During this period he worked on multilanguage generation, speech generation, English grammar, semantics and discourse, and systemic functional theory. In 1994 he moved to Macquarie University's Department of Linguistic, first as associate professor. In 2002 he took up a chair at Macquarie until 2008, when he was appointed chair and head of the Department of English at the Hong Kong Polytechnic University. From 2009 to mid-2012, he was also associate dean of the Faculty of Humanities at PolyU. Since May 2011, he has been honorary professor at Beijing Normal University, Beijing, and guest professor at the University of Science and Technology, Beijing.|$|E
30|$|Genest and Lapalme [8] {{presented}} a method with three main modules: (i) information extraction determined several candidate rules for each aspect of verbs and nouns; (ii) content selection selected the best rule for each aspect; (iii) summary generation formed the output <b>text</b> using <b>generation</b> patterns. With this method, the researchers created summaries with greater information. On the other hand, {{they had to}} make a lot of effort to manually write all the rules and patterns.|$|R
40|$|Non-expository {{texts are}} not usually read from cover to cover. Readers are helped in such {{circumstances}} by providing selective access to text excerpts as needed. Text themes can be identified representing areas of importance in a text, and summaries can be constructed automatically. In this study, <b>text</b> theme <b>generation</b> and <b>text</b> summarization are related to text struture. It is shown that useful text derivatives are obtainable for texts with diverse structural characteristics...|$|R
50|$|<b>Generation</b> <b>text</b> {{is a term}} {{referring}} to those members of society who {{have grown up with}} cell phones from a young age. Furthermore, it can be applied to adolescents and teenagers who feel that they are unable to function without a cell phone.|$|R
50|$|Rhetorical Structure Theory (RST) was {{originally}} formulated by William Mann and Sandra Thompson of the University of Southern California's Information Sciences Institute (ISI) in 1988. This theory was developed {{as part of}} studies of computer based <b>text</b> <b>generation.</b> Natural language researchers later began using RST in text summarization and other applications. RST addresses text organization by means of relations that hold between parts of text. It explains coherence by postulating a hierarchical, connected structure of texts.|$|E
50|$|Software CGs run on {{standard}} off-the-shelf computer hardware {{and are often}} integrated into video editing software such as non-linear editing system (NLE). Some stand-alone character generator products are available, however, for applications that do not even attempt to offer <b>text</b> <b>generation</b> on their own, as high-end video editing software often does, or whose internal CG effects are not flexible and powerful enough. Some software CGs {{can be used in}} live production with special software and computer video interface cards. In that case, they are equivalent to hardware generators.|$|E
50|$|Matthiessen {{was born}} and raised in Sweden. His mother, Christine Matthiessen, is {{credited}} with starting his interest in language as an object of study. His father, Martin Edmond, was a painter. Matthiessen completed his undergraduate degree at Lund University in 1980, where he studied English, Arabic, and philosophy. His Master of Arts was taken at UCLA, with a dissertation on English tense. In 1989 he completed a PhD at the same institution: <b>Text</b> <b>generation</b> as a linguistic research task. While studying at UCLA, he worked first as a teaching assistant. From 1980 to 1983 he was a research assistant at the Information Sciences Institute at the University of Southern California.|$|E
50|$|SRW:J's {{original}} characters {{finally made}} their {{debut in the}} Original Series in the titled SRW game called Moon Dwellers. Unique amongst this is that Moon Dwellers is the first SRW game to have full English <b>text</b> since Original <b>Generation</b> 2 on the Game Boy Advance.|$|R
40|$|This release {{provides}} enhancements and {{bug fixes}} for login, list view selection, formatted (Markdown) text display and online help <b>text,</b> and JSON-LD <b>generation.</b> It also implements internal structural and data layout {{changes to the}} software, and enhancements to data migration support. Release notes: [URL] (see "history" for details...|$|R
40|$|This paper {{presents}} all {{the essential}} issues {{in developing the}} text-to-speech synthesis for Thai - <b>text</b> analysis, prosody <b>generation</b> and speech synthesis. In the text analysis, problems in Thai text processing can be decomposed into the models of sentence extraction, phrase boundary determination and grapheme-to-phoneme conversion. The syllable duration and F 0 contour generation rules {{are included in the}} prosody generation. This is to realize the synthetic speech in the suprasegmental level. In the speech synthesis, the definition and the construction of acoustic inventory structure `demisyllable' are presented. Furthermore, three signal-processing algorithms, amplitude normalization, the segment boundary smoothing and prosodic modification, are also presented in this topic. KEY WORDS [...] Thai text-to-speech synthesis, <b>text</b> analysis, prosody <b>generation,</b> speech synthesis, demisyllable #s# [...] -n#####s#oeoe#-oe [...] . o-###oe"r"-S#r-## [...] -S [...] - [...] ...|$|R
5000|$|Racter, {{short for}} raconteur, {{was written by}} William Chamberlain and Thomas Etter. The {{existence}} {{of the program was}} revealed in 1983 in a book called The Policeman's Beard Is Half Constructed (...) , which was described as being composed entirely by the program. According to Chamberlain's introduction to the book, the program apparently ran on a CP/M machine; it was written in [...] "compiled BASIC on a Z80 micro with 64K of RAM." [...] This version, the program that allegedly wrote the book, was not released to the general public. The sophistication claimed for the program was likely exaggerated, as could be seen by investigation of the template system of <b>text</b> <b>generation.</b>|$|E
50|$|Another concept used {{to provide}} {{intelligent}} assistance was automatic <b>text</b> <b>generation.</b> Early research at ISI investigated the feasibility of extracting formal specifications from informal natural language text documents. They determined that the approach was not viable. Natural language is by nature simply too ambiguous {{to serve as a}} good format for defining a system. However, natural language generation was seen to be feasible as a way to generate textual descriptions that could be read by managers and non-technical personnel. This was especially appealing to the air force since by law they required all contractors to generate various reports that describe the system from different points of view. Researchers at ISI and later Cogentext and Andersen Consulting demonstrated the viability of the approach by using their own technology to generate the documentation required by their air force contracts.|$|E
50|$|The {{popular media}} has paid the most {{attention}} to NLG systems which generate jokes (see computational humor), but from a commercial perspective, the most successful NLG applicationshave been data-to-text systems which generate textual summaries of databases and data sets; thesesystems usually perform data analysis as well as <b>text</b> <b>generation.</b> In particular, several systems havebeen built that produce textual weather forecasts from weather data. The earliest such system to bedeployed was FoG, which was used by Environment Canada to generate weather forecasts in French and English in the early 1990s. The success of FoG triggered other work, both research and commercial. Recent {{research in this area}} include an experiment whichshowed that users sometimes preferred computer-generated weather forecasts to human-written ones,in part because the computer forecasts used more consistent terminology, and a demonstration that statistical techniquescould be used to generate high-quality weather forecasts.Recent applications include the UK Met Office's text-enhanced forecast.|$|E
40|$|The {{automatic}} detection of emotions in <b>texts</b> and the <b>generation</b> of <b>texts</b> that express emotions {{is important for}} applications such as natural language interfaces, e-learning environments, and educational or entertainment games. These aspects are also important in opinion mining and sentiment analysis, and in the larger area of affective computing...|$|R
50|$|Mirabilia Urbis Romae ("Marvels of the City of Rome") is a much-copied medieval Latin <b>text</b> {{that served}} <b>generations</b> of pilgrims and {{tourists}} {{as a guide}} to the city of Rome. The original, which was written by a canon of St Peter's, dates from the 1140s. The text survives in numerous manuscripts.|$|R
5000|$|The two volume Church and Society in Eighteenth-century France {{published}} in 1998 [...] "represents an enormous achievement" [...] {{as reported by}} Raymond Mentzer of Montana State University. It is two volumes, more than 1600 pages of <b>text</b> documenting four <b>generations</b> of pre-revolutionary France and the culmination of more than 50 years of research.|$|R
