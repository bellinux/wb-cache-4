0|38|Public
5000|$|... #Caption: A <b>thread</b> {{restoring}} <b>file,</b> {{one type}} of thread restorer ...|$|R
50|$|XFS excels in the {{execution}} of parallel input/output (I/O) operations due to its design, {{which is based on}} allocation groups (a type of subdivision of the physical volumes in which XFS is used- also shortened to AGs). Because of this, XFS enables extreme scalability of I/O <b>threads,</b> <b>file</b> system bandwidth, and size of files and of the file system itself when spanning multiple physical storage devices.|$|R
50|$|One type {{of thread}} restorer is a <b>thread</b> {{restoring}} <b>file.</b> There are {{other types of}} thread restoring tools as well.|$|R
40|$|AbstractThis paper {{presents}} a pattern-oriented Platform Independent Model {{for the management}} and efficient usage of resources such as memory, <b>threads,</b> <b>files,</b> or network connections considering the common underlying architecture of different software and hardware dependent mobile network technologies and common resource management needs of these systems to benefit educational services that can be transformed to different Platform Specific Models. To illustrate the proposed method, a PIM was designed to a key resource management pattern. The evaluation results of proposed PIM shows the improvements in accurate and agile designing and maintenance of the underlying architecture...|$|R
5000|$|The QtCore module {{contains}} the core non-GUI classes, including the event loop and Qt's signal and slot mechanism. It also includes platform independent abstractions for Unicode, <b>threads,</b> mapped <b>files,</b> shared memory, regular expressions, and user and application settings.|$|R
5000|$|A <b>File</b> <b>Thread</b> Record stores {{just the}} name of the file and the CNID of its parent directory.|$|R
40|$|Library {{records and}} their {{utilization}} are described {{and the various}} types of file organization available are examined. The serial file with a series of inverted indexes is preferred to the simple serial <b>file</b> or a <b>threaded</b> list <b>file.</b> It is shown how various records should be stored, according to their utilization, in the available storage devices in order to achieve optimum cost-performance...|$|R
40|$|Abstract: The paper compares core kernel {{architecture}} and functionality of two modern open source systems. The subsystems examined are scheduling, memory management, and file system architecture. These subsystems {{are common to}} any operating system (not just Unix and Unix-like systems), {{and they tend to}} be the most wellunderstood components of the operating system. One of the more interesting aspects concerning the Linux and Solaris Operating Systems (OS), is the amount of similarities between them. Ignoring the different naming conventions, both of them utilize similar approaches toward implementing the different concepts. Each OS supports time-shared scheduling of threads, demand paging with a not-recently-used page replacement algorithm, and a virtual file system layer to allow the implementation of different file system architectures. The paper concludes that both the Linux and the OpenSolaris kernel can offer robust and powerful computing environments both at the server application areas and as well at the desktop and workstation ones. Key–Words: Scheduling, memory management, <b>threads,</b> <b>file</b> systems, paging...|$|R
40|$|These days a Smartphone {{is just as}} {{important}} as a computer. It is essential that users select a device that has the best performance possible. This thesis used simulation to compare the performance of an application on two of the most popular mobile operating systems, Android and Windows Phone 7. An application was developed using the Android SDK/Java and Silverlight’s C#/XAML respectively. This application tested the performance of <b>threads,</b> <b>file</b> I/O, database loads, buffering, and caching on both operating systems. Both simulation devices were set up with the exact same specifications. The performance was measured using response times, CPU, and memory usage. This work shows that there were major differences in the performance results. With the response times Windows Phone 7 out performed Android in every scenario except buffering. Windows Phone 7 also used the least memory in every situation except loading when a web page without caching. However when it came to CPU usage Android used half of what Windows Phone 7 did...|$|R
5000|$|Enduro/X {{introduced}} {{new set of}} API function named tplog. This allows user to use {{the same}} C based high-performance debug logging facilities which Enduro/X uses internally. It is possible to redirect all logging by <b>thread</b> bases <b>files.</b> Also there are APIs for [...] "request logging", which means that user and Enduro/X can do logging in separate files for each request (e.g. log by session id, log by username, etc.) ...|$|R
40|$|The {{widely used}} FreeBSD UNIX-like {{operating}} system provides a mature, stable and customisable platform, suitable for many tasks including telecommunications research. FreeBSD {{is being used}} as part of CAIA’s NewTCP project [1] to evaluate up and coming TCP congestion control algorithms. Part of this work has involved the customisation of the FreeBSD kernel, {{in the form of}} a loadable kernel module named SIFTR (Statistical Information For TCP Research). This report aims to capture the knowledge learnt during this process. Whilst FreeBSD 6. 2 -RELEASE was used as the basis for this development, most of the technical information in this report will be applicable to all FreeBSD 6. x releases, and possibly to earlier (5. x and 4. x to a lesser extent) or up and coming (7. x and beyond) releases. Topics covered include FreeBSD kernel module programming, the sysctl interface, packet filtering, data structures, character devices, <b>threading,</b> <b>file</b> writing and debugging. The report draws together our personal experiences and sources of further information to create a useful reference for those that are new to FreeBSD kernel programming...|$|R
50|$|The POrtable COmponents (POCO) C++ Libraries are {{computer}} software, a set {{of class}} libraries for developing computer network-centric, portable applications in the programming language C++. The libraries cover functions such as <b>threads,</b> <b>thread</b> synchronizing, <b>file</b> system access, streams, shared libraries and class loading, Internet sockets, and network communications protocols (HTTP, FTP, SMTP, etc.), and include an HTTP server, and an XML parser with SAX2 and DOM interfaces and SQL database access. The modular and efficient design and implementation makes the libraries well suited for embedded system development.|$|R
5000|$|JPicus is a JavaTM I/O {{analysis}} tool {{created by}} Pavel Genevski and other SAP employees in 2009. Its main {{purpose is to}} analyze I/O operations inside a Java application, providing JVM internal information, such as which <b>thread</b> opened a <b>file.</b> JPicus consists of the following major parts: ...|$|R
40|$|This paper {{presents}} {{our experiences}} in porting selected parts of the. NET Compact Framework to Symbian smartphones. Our port includes support for basic {{services such as}} <b>threading</b> and <b>file</b> access, low-level networking modules as well as Web Services. We also present a portable. NET GUI for the Symbian platform. The paper shows how the programming models of. NET can be efficiently mapped to the runtime structures provided by operating systems for resource-constrained devices such as the Symbian OS. In a detailed analysis, we compare the performance of our port to that of Java and native code. ...|$|R
50|$|The Filazers', Exigenters' and Clerk of the Outlawries' Office for the Court of King's Bench was here. These {{officers}} were so {{called from the}} French word Fil, or <b>thread,</b> because they <b>filed</b> or <b>threaded</b> the writs. Thomas Kenyon was Filazer, Exigenter and Clerk of the Outlawries, and Andrew Edge was Filazer for Essex and Monmouthshire.|$|R
50|$|Agent's Usenet {{features}} include access to multiple news servers, import/export of NZB <b>files,</b> <b>threaded</b> discussions {{and a highly}} configurable user interface which has been criticized as difficult to use. It has long supported yEnc {{as well as many}} other coding schemes, and has the capability of joining incomplete binary attachments, which is useful in the event of posting errors.|$|R
50|$|Threads {{were born}} {{from the idea}} that the most {{efficient}} way for cooperating processes to exchange data would be to share their entire memory space. Thus, threads are effectively processes that run in the same memory context and share other resources with their parent processes, such as open <b>files.</b> <b>Threads</b> are described as lightweight processes because switching between threads does not involve changing the memory context.|$|R
5000|$|The Catalog File {{is another}} B-tree that {{contains}} records {{for all the}} files and directories stored in the volume. It stores four types of records. Each file consists of a <b>File</b> <b>Thread</b> Record and a File Record while each directory consists of a Directory Thread Record and a Directory Record. Files and directories in the Catalog File are located by their unique Catalog Node ID (or CNID).|$|R
40|$|Jülich Supercomputing Centre {{has offered}} Extreme Scaling Workshops since 2009, {{with the latest}} edition in February 2015 giving seven {{international}} code teams an opportunity to (im) prove the scaling of their applications to all 458, 752 cores of the JUQUEEN IBM Blue Gene/Q. Each of them successfully adapted their application codes and datasets to the restricted compute-node memory and exploit the massive parallelism with up to 1. 8 million processes or threads. They thereby qualified {{to be members of}} the High-Q Club which now has over 24 codes demonstrating extreme scalability. Achievements in both strong and weak scaling are compared, and complemented with a review of program languages and parallelisation paradigms, exploitation of hardware <b>threads,</b> and <b>file</b> I/O requirements...|$|R
5000|$|Most OS objects, such as <b>files,</b> <b>threads,</b> {{graphics}} devices, {{and window}} controls (or widgets), and so on, are implemented using {{a thin layer}} that talks directly to the specific native object. UI controls are not emulated for the most part, instead the framework uses native widgets in most places. Direct access to the underlying OS [...] "handle" [...] is allowed in cases where platform specific functionality is needed, allowing the developer to easily call the platform's native APIs.|$|R
40|$|Abstract — This paper {{describes}} {{instruction set}} extensions for {{a variant of}} multi-threading called micro-threading for the LEON 3 SPARCv 8 processor. We show an architecture of the developed processor and its key blocks- cache controller, register <b>file,</b> <b>thread</b> scheduler. The processor has been implemented in a Xilinx Virtex 2 Pro FPGA. The extensions are evaluated in terms of extra resources needed, and the overall performance of the developed processor is evaluated on a simple DSP computation typical for embedded systems. I...|$|R
40|$|Java {{is a key}} {{technology}} {{for the next generation}} of data acquisition systems(DAQ). Java has excellent features, but its performance is a weak point. Weevaluated various Javabenchmark programs with a Java interpreter, Java Just-In-Time compilers (JIT), a Java native code compiler (TowerJ) which generates native code, and a C compiler. Thebenchmark programs include high performance computation, inter-process communication, <b>thread</b> switching, <b>file</b> I/O, readout (VMEbus and CAMAC), distributed object technologies (HORB etc.), object oriented database access and so on. The benchmark programs showed the following: HORB had the best performance among the distributed object technologies, TowerJ had good performance, the JITs were fast in some cases, and readout with the Javalanguage was useful enough. The feasibility of Java-based DAQ has been discussed from the point of view of performance. INTRODUCTION Java[1] is a {{key technology}} of the next generation of DAQ. Java is a pure object oriented l [...] ...|$|R
40|$|The {{primary goal}} of Discussion Search is to {{identify}} a discussion about a topic. A secondary goal is {{to determine whether a}} given message expresses pro or con arguments with respect to the discussion. We employed a combination of POS-driven query expansion and a textclassification technique from [6]. The results of those previous experiments indicated that the technique best performed in extracting protein-protein interaction pairs from MEDLINE. The original email corpus was extremely heterogeneous. We first applied the Tidy HTML parser to strip tags and to identify data such as the sender, thread history, and subject of the messages. We then linked messages into threads in two ways. The corpus provides <b>thread</b> index <b>files</b> for email communications. These thread indexes are composed of hieratically structured multiple discussion threads and single thread. For multiple discussion threads, we unified them into a thread document. We also combined single documents when they had the same subject. 1. 1 Method This is a supervised because the system was developed a small set of training data by taking a subset of discussion threads and manually tagging them. We developed a discussion threa...|$|R
50|$|MapReduce {{achieves}} reliability by parceling out {{a number}} of operations on the set of data to each node in the network. Each node is expected to report back periodically with completed work and status updates. If a node falls silent for longer than that interval, the master node (similar to the master server in the Google File System) records the node as dead and sends out the node's assigned work to other nodes. Individual operations use atomic operations for naming file outputs as a check to ensure that there are not parallel conflicting <b>threads</b> running. When <b>files</b> are renamed, it is possible to also copy them to another name in addition to the name of the task (allowing for side-effects).|$|R
40|$|Part 3 : FORENSIC TECHNIQUESInternational audienceDue to the {{increasingly}} {{massive amounts of}} data {{that need to be}} analyzed in digital forensic investigations, it is necessary to automatically recognize suspect files and filter out non-relevant files. To achieve this goal, digital forensic practitioners employ hashing algorithms to classify files into known-good, known-bad and unknown files. However, a typical personal computer may store hundreds of thousands of files and the task becomes extremely time-consuming. This paper attempts to address the problem using a framework that speeds up processing by using multiple threads. Unlike a typical multithreading approach, where the hashing algorithm is performed by multiple threads, the proposed framework incorporates a dedicated prefetcher <b>thread</b> that reads <b>files</b> from a device. Experimental results demonstrate a runtime efficiency of nearly 40 % over single threading...|$|R
40|$|This article {{presents}} results from an experimental evaluation {{study of the}} HP Exemplar file system. Our experiments consist of simple micro-benchmarks that study the impact of various factors on the file system performance. These factors include I/O request/buffer sizes, vectored/non-vectored access patterns, read-ahead policies, multi-threaded (temporally irregular) requests, and architectural issues (cache parameters, NUMA behavior, etc.). Experimental {{results indicate that the}} Exemplar file system provides high I/O bandwidth, both for single- and multi-threaded applications. The buffer cache, with prioritized buffer management and large buffer sizes, is effective in exploiting temporal and spatial access localities. The performance of non-contiguous accesses can be improved by either using vectored I/O interfaces or tuning the read-ahead facilities. The file system performance depends on the relative locations of the computing <b>threads</b> and the <b>file</b> system, and also on various Exemplar design parameters such as the NUMA architecture, TLB/data cache management and paging policies...|$|R
40|$|With the {{development}} of multi-/many-core processors, applications need to be written as parallel programs to improve execution efficiency. For data-intensive applications that use multiple <b>threads</b> to read/write <b>files</b> simultaneously, an I/O sub-system can easily become a bottleneck when too many {{of these types of}} threads exist; on the contrary, too few threads will cause insufficient resource utilization and hurt performance. Therefore, programmers must pay much attention to parallelism control to find the appropriate number of I/O threads for an application. This paper proposes a parallelism control mechanism named IOPA that can adjust the parallelism of applications to adapt to the I/O capability of a system and balance computing resources and I/O bandwidth. The programming interface of IOPA is also provided to programmers to simplify parallel programming. IOPA is evaluated using multiple applications with both solid state and hard disk drives. The results show that the parallel applications using IOPA can achieve higher efficiency than those with a fixed number of threads...|$|R
40|$|We {{propose a}} soft {{processor}} programming model and architecture inspired by graphics processing units (GPUs) that are well-matched to {{the strengths of}} FPGAs, namely, highly parallel and pipelinable computation. In particular, our soft processor architecture exploits multithreading, vector operations, and predication to supply a floating-point pipeline of 64 stages via hardware support for up to 256 concurrent thread contexts. The key new contributions of our architecture are mechanisms for managing <b>threads</b> and register <b>files</b> that maximize data-level and instruction-level parallelism while overcoming the challenges of port limitations of FPGA block memories as well as memory and pipeline latency. Through simulation {{of a system that}} (i) is programmable via NVIDIA's high-level Cg language, (ii) supports AMD's CTM r 5 xx GPU ISA, and (iii) is realizable on an XtremeData XD 1000 FPGA-based accelerator system, we demonstrate the potential for such a system to achieve 100 % utilization of a deeply pipelined floating-point datapath...|$|R
40|$|There is {{building}} interest in using FPGAs as accelerators for high-performance computing, but existing systems for programming them {{are so far}} inadequate. In this {{paper we propose a}} soft processor programming model and architecture inspired by graphics processing units (GPUs) that are well-matched to the strengths of FPGAs, namely highly-parallel and pipelinable computation. In particular, our soft processor architecture exploits multithreading and vector operations to supply a floating-point pipeline of 64 stages via hardware support for up to 256 concurrent thread contexts. The key new contributions of our architecture are mechanisms for managing <b>threads</b> and register <b>files</b> that maximize data-level and instruction-level parallelism while overcoming the challenges of port limitations of FPGA block memories, as well as memory and pipeline latency. Through simulation of a system that (i) supports AMD’s CTM r 5 xx GPU ISA [1], and (ii) is realizable on an XtremeData XD 1000 FPGA-based accelerator system, we demonstrate that our soft processor can achieve 100 % utilization of the deeply-pipelined floating-point datapath. 1...|$|R
40|$|Abstract—During a {{forensic}} investigation, an investigator might {{be required to}} analyze the content of a personal computer. Due to huge amounts of data, it becomes necessary to recognize suspect files and automatically filter out non-relevant files. To achieve this goal, an investigator can resort to hashing algorithms in order to classify files into known-to-be-good, known-to-be-bad and unknown files. The working steps are quite simple: hash the file, compare the resulting hashes against a database {{and put it in}} one of the categories. Typically personal computers nowadays store several hundred thousand files on their hard disk and thus this operation becomes time consuming. The paper at hand demonstrates a framework that speeds up this proceeding as it uses multiple threads for different tasks. Besides the typical multi-threading where the hashing algorithm is performed by multiple threads, we use a dedicated <b>thread</b> for reading <b>files</b> from the device, a prefetcher. Compared to single threading we improved the run time efficiency by nearly 40 %. Keywords-Digital forensics; hashing; cryptographic hash func-tions; performance; run time efficiency; file handling; prefetching. I...|$|R
2500|$|Romoşan, who {{had earlier}} denied {{involvement}} with the Securitate, claimed that Negoiţescu had actually both been recruited as an agent since their release from prison in the 1960s, and had spied for the Securitate's foreign bureau during his time in Germany. Speaking after Corlăţan's article, he admitted having functioned as a Securitate informer, but not before 1987, when his wife, writer Adina Kenereş, was threatened with losing her travel privileges. He indicated that his signature on any other such documents was obtained {{with the use of}} violence and intimidation. He argued: [...] "I presently think that I was being used by the Securitate, which destroyed my reputation in order to provide Negoiţescu with a cover", and claims that Negoiţescu himself apologized to him for [...] "all the harm" [...] during a chance meeting in the early 1990s. According to Nistorescu's assessment: [...] "When the <b>threads</b> of Negoiţescu's <b>file</b> will come loose, perhaps I'll understand something from [...] adventure." [...] In contrast, Morar and Ştefan Agopian both assessed that Romoşan's own flight abroad was part of a Securitate diversion. Literary critic Dan C. Mihăilescu gave Romoşan's claims {{the benefit of the doubt}} and urged for the Negoiţescu file to be publicized in its entirety, but also asserted that Romoşan had lost his credibility.|$|R
5000|$|Alan Milne, {{a writer}} who was born and lived in London, bought a country retreat {{for himself and his}} family at Cotchford Farm, near Hartfield, East Sussex, in 1925. This old {{farmhouse}} was situated on the banks of a tributary of the River Medway and lay just beyond the northern boundary of Ashdown Forest, about a mile from the ancient forest entrance at Chuck Hatch. The family would stay at Cotchford Farm at weekends and in the Easter and summer holidays. It was easy to walk from the farmhouse up onto the forest, and these walks were frequently family occasions which would see Milne, his wife, Dorothy, his son, Christopher Robin, and his son's nanny, Olive, going [...] "in single <b>file</b> <b>threading</b> the narrow paths that run through the heather". Christopher, who was an only child born in 1920 and whose closest childhood relationship was with his nanny, spent his early years happily exploring the forest. It is the Ashdown Forest landscape, and Christopher's reports of his experiences and discoveries there, that provided inspiration and material for A.A. Milne's stories. As Christopher Milne wrote later: [...] "Anyone who has read the stories knows the forest and doesn't need me to describe it. Pooh’s Forest and Ashdown Forest are identical".|$|R
5000|$|Romoşan, who {{had earlier}} denied {{involvement}} with the Securitate, claimed that Negoiţescu had actually both been recruited as an agent since their release from prison in the 1960s, and had spied for the Securitate's foreign bureau during his time in Germany. Speaking after Corlăţan's article, he admitted having functioned as a Securitate informer, but not before 1987, when his wife, writer Adina Kenereş, was threatened with losing her travel privileges. He indicated that his signature on any other such documents was obtained {{with the use of}} violence and intimidation. He argued: [...] "I presently think that I was being used by the Securitate, which destroyed my reputation in order to provide Negoiţescu with a cover", and claims that Negoiţescu himself apologized to him for [...] "all the harm" [...] during a chance meeting in the early 1990s. According to Nistorescu's assessment: [...] "When the <b>threads</b> of Negoiţescu's <b>file</b> will come loose, perhaps I'll understand something from Romoşan's adventure." [...] In contrast, Morar and Ştefan Agopian both assessed that Romoşan's own flight abroad was part of a Securitate diversion. Literary critic Dan C. Mihăilescu gave Romoşan's claims {{the benefit of the doubt}} and urged for the Negoiţescu file to be publicized in its entirety, but also asserted that Romoşan had lost his credibility.|$|R
5000|$|The biggest knock against NLTSS {{during its}} {{production}} lifetime was performance. The one performance issue that affected users most was file access latency. This generally wasn't {{a significant problem}} for disk I/O, but the systems that NLTSS ran on also supported a significant complement of very low latency solid state disks with access times under 10 microseconds. The initial latencies for file operations under NLTSS were comparable to the latency for solid state disk access and {{significantly higher than the}} LTSS latency for such access. To improve file access latency under NLTSS the implementation was changed significantly to put the most latency sensitive processes (in particular the file server) [...] "in the kernel". This effort wasn't as significant as it might at first sound as all NLTSS servers worked on a multithreading model. What this change really amounted to was to move the <b>threads</b> responsible for <b>file</b> server services from a separate file server process into the kernel [...] "process". Communication to users was unchanged (still through buffer tables, LINCS tokens, etc.), but file operations avoided some significant context changes that were the primary cause of the higher latencies over what the older LTSS and the competing Cray Time Sharing System provided. This change did significantly (~3x) improve the latency of file I/O operations, but it also meant that the file server became a trusted part of the kernel (by implementation, not by design).|$|R
40|$|A well-designed cache {{system has}} {{positive}} {{impacts on the}} 3 D real-time rendering engine. As the amount of visualization data getting larger, the effects become more obvious. They are {{the base of the}} 3 D real-time rendering engine to smoothly browsing through the data, which is out of the core memory, or from the internet. In this article, a new kind of caches which are based on multi <b>threads</b> and large <b>file</b> are introduced. The memory cache consists of three parts, the rendering cache, the pre-rendering cache and the elimination cache. The rendering cache stores the data that is rendering in the engine; the data that is dispatched according to the position of the view point in the horizontal and vertical directions is stored in the pre-rendering cache; the data that is eliminated from the previous cache is stored in the eliminate cache and is going to write to the disk cache. Multi large files are used in the disk cache. When a disk cache file size reaches the limit length（ 128 M is the top in the experiment）, no item will be eliminated from the file, but a new large cache file will be created. If the large file number is greater than the maximum number that is pre-set, the earliest file will be deleted from the disk. In this way, only one file is opened for writing and reading, and the rest are read-only so the disk cache {{can be used in a}} high asynchronous way. The size of the large file is limited in order to map to the core memory to save loading time. Multi-thread is used to update the cache data. The threads are used to load data to the rendering cache as soon as possible for rendering, to load data to the pre-rendering cache for rendering next few frames, and to load data to the elimination cache which is not necessary for the moment. In our experiment, two threads are designed. The first thread is to organize the memory cache according to the view point, and created two threads: the adding list and the deleting list, the adding list index the data tha...|$|R

