6|21|Public
50|$|Although {{including}} synonyms, a thesaurus {{should not}} be taken as a complete list of all the synonyms for a particular word. The entries are also designed for drawing distinctions between similar words and assisting in choosing exactly the right word. Unlike a dictionary, a <b>thesaurus</b> <b>entry</b> does not give the definition of words.|$|E
40|$|Distributional thesauri are {{now widely}} used in {{a large number of}} Natural Language Processing tasks. However, they are far from {{containing}} only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identification of the neighbors of a <b>thesaurus</b> <b>entry</b> that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies. ...|$|E
40|$|Controling {{metadata}} through authority lists or thesauri {{can provide}} many benefits: (1) multilingual: coded metadata {{can be translated}} following the language of users (2) quality control: spelling can be checked (3) exhaustivity: users can retrieve records using synonyms or translations (4) assistance: auto-complete and facetted browsing help users specify their queries and drill-down their results (5) integration: normalizing concepts insures that different applications can be interlinked with precision and exhaustivity: a <b>thesaurus</b> <b>entry</b> can be the keyword to index bibliographic records, {{the title of a}} Wiki Page which explains the concept, the code of a chemical substance in a database (or a code in any external database). Full integration of authority lists in DSpace involves changes in submitting, cataloguing, displaying, searching and browsing. We demonstrate in depth the result of this work with www. WindMusic. org, the database of music scores and recordings for Wind Bands and Orchestras...|$|E
25|$|The Britannica Ultimate Reference Suite 2012 DVD {{contains}} over 100,000 articles. This includes regular Britannica articles, as well {{as others}} drawn from the Britannica Student Encyclopædia, and the Britannica Elementary Encyclopædia. The package includes a range of supplementary content including maps, videos, sound clips, animations and web links. It also offers study tools and dictionary and <b>thesaurus</b> <b>entries</b> from Merriam-Webster.|$|R
40|$|NCI <b>Thesaurus</b> <b>entries</b> {{reference}} corresponding nodes in the UMLS Semantic Network (SN). Adapting {{a process}} previously used to refine relationship definitions in the UMLS Metathesaurus, we used these Thesaurus-to-Network references to analyze {{alignment of the}} Thesaurus with the OBO Relations Ontology {{and at the same}} time validate and improve Thesaurus structure. Given this experience, we offer suggestions for enhancement of the UMLS SN so that it can be even more useful in the future...|$|R
40|$|This article {{offers the}} ways to achieve {{adequate}} codification of the Russian particles in the <b>thesaurus’s</b> <b>entries.</b> To achieve {{the object of the}} topic, there have been solved following problems: 1) the reasons of wrong codifications of the Russian particles in the Russian thesaurus are being established; 2) ways of formation of the derivative particles are revealed; 3) structural types of Russian particles are identified; 4) elements of attributes of the Russian particles in the entry are being established...|$|R
40|$|Current {{methods for}} {{information}} processing and retrieval {{used at the}} NASA Scientific and Technical Information Facility are reviewed. A more cost effective computer aided indexing system is proposed which automatically generates print terms (phrases) from the natural text. Satisfactory print terms can be generated in a primarily automatic manner to produce a thesaurus (NASA TERMS) which extends all the mappings presently applied by indexers, specifies the worth of each posting term in the thesaurus, and indicates the areas of use of the <b>thesaurus</b> <b>entry</b> phrase. These print terms enable the computer to determine which of several terms in a hierarchy is desirable and to differentiate ambiguous terms. Steps in the NASA TERMS algorithm are discussed and the processing of surrogate entry phrases is demonstrated using four previously manually indexed STAR abstracts for comparison. The simulation shows phrase isolation, text phrase reduction, NASA terms selection, and RECON display...|$|E
40|$|Dyvik (1998 a-b and 2002) {{presents}} {{a method for}} deriving lexical sense divisions, and feature descriptions of each sense, automatically from translational data taken from a parallel corpus (ENPC). The senses are grouped in semantic fields based on shared translational properties. The feature sets assigned to the senses in a field form a semilattice based on inclusion and overlap relations among the feature sets. An {{example of such a}} derived semilattice is shown below: According to the hypothesis behind the method the dominance relations in the lattice express hyperonymy/hyponymy relations among the senses, and distance in the lattice generally reflects semantic distance. Hence the lattices should contain some of the information generally found in word nets and thesauri. This is restricted to relations of semantic similarity (hyperonymy/hyponymy and synonymy), while, e. g., antonymy and meronymy probably cannot be derived from the translational data. The present paper discusses the further derivation of thesaurus entries from the information in the feature lattices. Much of the detail in the derived lattices is probably the result of accidental biases and gaps in the parallel corpus from which they were derived. The derivation of a <b>thesaurus</b> <b>entry,</b> in which words are grouped as 'hyperonyms', 'hyponyms', 'synonyms ' and 'related words ' of an entry, therefore needs to abstract away from some of the detail in the lattices, and this can obviously be done in more than one way. The following entry for the Norwegian noun 'rett ' has been derived automatically from the translationally based lattices by the principles to be described. Sense 4 corresponds to the lattice shown above: 2 Extended base...|$|E
40|$|The {{rapid growth}} of {{publicly}} accessible databases has increased the difficulty in knowing about their existence, their contents {{and the way to}} access them. So it would be very useful to put the information about available databases online, having databases catalogued according to their contents. To support end users in retrieving such information, accessing the catalogue of database should be very powerful. On the other hand the maintenance of the information contained in the catalogue cannot be performed in a centralized site due to the natural distribution of information around the world. The standard OSI X. 500 is considered a suitable mean to maintain a distributed catalogue of online source of information. This paper proposes a new X. 500 object class, named "onlineDatabase" (subclass of the object class "onlineInforn 1 ationResource") to map any database information entry as well as a directory schema suitable to deploy the information on a world wide basis while leaving the maintenance of the information to its producer or as near as possible to him. The papers describes how, using the object class "documentSeries", a hierarchical subject index mapping the UNESCO thesaurus has been implemented. This Thesaurus is used by UNESCO to index (by arguments) and retrieve all documents and publications processed through the computerized Documentation System of the UNESCO libraries. Any database entry can be referred by more than one <b>thesaurus</b> <b>entry.</b> For any entry of the thesaurus it 2 ̆ 7 s possible to select any database dealing a specific argument. A prototype implementation of the system is presented. The prototype is composed by a X. 5 OO DSA, an end-user interface running on UNIX system, and a catalogue of databases presently available through the ASTRA service. The end-user interface is able to retrieve the entry of the database of interest by accessing the subjects subtree, or by directly selecting the database entry. Any database entry contains some information such as: the content, the logon and logoff procedures, the database producers, distributors and contact persons, the network access procedures, the source from which the database has been implemented, the cost-of-use, the database management system and where to find much more information...|$|E
40|$|Vocabulary {{knowledge}} {{contributes to}} reading comprehension, and {{is linked to}} academic success (National Reading Panel, 2000). Those of us who enjoy reading, and who read frequently, are constantly learning new words. Vocabulary growth takes place as children encounter unknown words in oral and written language. Yet many children with reading problems read very little and may hear only limited numbers of words; they rarely confront new, unfamiliar words through reading. Children benefit from learning strategies in six specific areas that often are problematic for word meaning. These include learning about available clues such as (a) context clues and (b) structural clues, (c) dictionary definitions and <b>thesaurus</b> <b>entries,</b> (d) semantic relationships such as antonyms and synonyms, (e) multiple meanings of many words, and (f) appreciation of figurative language such as metaphors, idioms, similes, and analogies. Context Clues Teachers often point out context clues to children. Yet Miller & Gildea (1987...|$|R
40|$|The 400 million word Corpus of Contemporary American English (COCA) [1990 - 2009] is {{the only}} large, balanced, {{up-to-date}} corpus of English that is publicly available. There are many features in this corpus that allow learners of English to quickly and easily perform semantically-oriented queries. These include the following: 1) one-step collocates (with limiting by part of speech and sorting and limiting by Mutual Information score), 2) comparing collocates across genres (e. g. collocates of “chain” in fiction and academic), 3) comparison of collocates of two words (e. g. sheer / utter) 4) use of integrated <b>thesaurus</b> (<b>entries</b> for 60, 000 + words) to see frequency of all synonyms (including by genre) and to create more powerful queries (e. g. all forms of all synonyms of “clean” + a noun in a particular semantic domain) and 5) customized wordlists (including {{hundreds or thousands of}} words in a semantic domain) ...|$|R
40|$|A {{browsing}} interface to {{a document}} collection {{can be constructed}} automatically by identifying the phrases that recur in {{the full text of}} the documents and structuring them into a hierarchy based on lexical inclusion. This provides a good way of allowing readers to browse comfortably through the phrases (all phrases) in a large document collection. A subject-oriented thesaurus provides a different kind of hierarchical structure, based on deep knowledge of the subject area. If all documents, or parts of documents, are tagged with thesaurus terms, this provides a very convenient way of browsing through a collection. Unfortunately, manual classification is expensive and infeasible for many practical document collections. This paper describes a browsing scheme that gives {{the best of both worlds}} by providing a phrase-oriented browser and a thesaurus browser within the same interface. Users can switch smoothly between the phrases in the collection, which give access to the actual documents, and the <b>thesaurus</b> <b>entries,</b> which suggest new relationships and new terms to seek...|$|R
40|$|To {{test the}} effects of a {{vertical}} versus a horizontal arrangement of brief structured text entries on known-item search times, 2 groups of 12 women university graduates with mean ages of 71 years and 33 years respectively took part in an experiment. Subjects were classified as either having or not having library reference experience. They were asked to locate and answer factual questions concerning <b>thesaurus</b> <b>entries</b> arranged in each format. The important data unit for analysis was the difference in time for each subject rather than the actual times. Results indicated that differences in search times were proportionately the same for both age groups. Significantly slower search times occurred under the horizontal arrangement, and no participant judged the horizontal arrangement to be easier to use than the vertical arrangement. Those younger people with reference experience preformed more slowly with horizontal presentation than did those younger people without experience. No effect of age was found upon disorientation caused by unanticipated arrangement of test entries...|$|R
40|$|The {{hypertext}} system "HyperMan" {{provides for}} an automatic conversion of linear machine [...] readable documents into hypertexts by applying text partitioning and link generation methods. After {{completion of the}} generation process, the graphical user interface of the system enables users to browse through the converted documents very easily. To determine whether user actions allow conclusions to be drawn about a generated hypertext, a special component that records user actions has been integrated into the system. In this way sequences of actions can be identified that provide hints of relationships between two document passages or between two terms {{that occur in the}} text. Then, the relationships can be stored as links or <b>thesaurus</b> <b>entries,</b> respectively, in the system's data base and can be made available to subsequent users. In addition to acquiring relationships, the user observation component also provides for hints about the acceptance of system components. These hints can serve as a bas [...] ...|$|R
40|$|Abstract. The “Semantic Mirrors Method ” (Dyvik, 1998) is a {{means for}} {{automatic}} derivation of <b>thesaurus</b> <b>entries</b> from a word-aligned parallel corpus. The method {{is based on the}} construction of lattices of linguistic features. This paper models the Semantic Mirrors Method with Formal Concept Analysis. It is argued that the method becomes simpler to understand with the help of FCA. This paper then investigates to what extent the Semantic Mirrors Method is applicable if the linguistic resource is not a high quality parallel corpus but, instead, a medium quality bilingual dictionary. This is a relevant question because medium quality bilingual dictionaries are freely available whereas high quality parallel corpora are expensive and difficult to obtain. The analysis shows that by themselves, bilingual dictionaries are not as suitable for the Semantic Mirrors Method but that this can be improved by applying conceptual exploration. The combined method of conceptual exploration and Semantic Mirrors provides a useful toolkit specifically for smaller size bilingual resources, such as ontologies and classification systems. The last section of this paper suggests that such applications are of interest in the area of ontology engineering. ...|$|R
40|$|Thesauri, such as Roget’s Thesaurus, {{show the}} {{semantic}} relationships among terms and concepts. Understanding these relationships {{can lead to}} a greater understanding of linguistic structure and could be applied to creating more efficient natural-language recognition and processing programs. A general assumption is that focus and context displays of hyperbolic trees accelerate browsing ability over conventional trees. It is believed that allowing the user to visually browse the thesaurus will be more effective than keyword searching, especially when the terms in the thesaurus are not known in advance. The visualization can also potentially provide insight into semantic structure of terms. The novelty of this visualization lay in its implementation of various direct manipulation functions, tightly coupled windows, and how data is read into visualization. The direct manipulation functions allow the user to customize the appearance of the tree, to view the density of terms associated with particular concepts, and to view the <b>thesaurus</b> <b>entries</b> associated with each term. Input data is in an XML file format. The extensibility and ability to model complex hierarchies made XML a convenient choice. The object-oriented design of the code allows for displaying virtually any hierarchical data if it is in the XML format...|$|R
40|$|The "Semantic Mirrors Method" (Dyvik, 1998) is a {{means for}} {{automatic}} derivation of <b>thesaurus</b> <b>entries</b> from a word-aligned parallel corpus. The method {{is based on the}} construction of lattices of linguistic features. This paper models the Semantic Mirrors Method with Formal Concept Analysis. It is argued that the method becomes simpler to understand with the help of FCA. This paper then investigates to what extent the Semantic Mirrors Method is applicable if the linguistic resource is not a high quality parallel corpus but, instead, a medium quality bilingual dictionary. This is a relevant question because medium quality bilingual dictionaries are freely available whereas high quality parallel corpora are expensive and difficult to obtain. The analysis shows that by themselves, bilingual dictionaries are not as suitable for the Semantic Mirrors Method but that this can be improved by applying conceptual exploration. The combined method of conceptual exploration and Semantic Mirrors provides a useful toolkit specifically for smaller size bilingual resources, such as ontologies and classification systems. The last section of this paper suggests that such applications are of interest in the area of ontology engineering...|$|R
5000|$|Users {{can include}} {{semantic}} {{information from a}} 60,000 <b>entry</b> <b>thesaurus</b> directly {{as part of the}} query syntax (e.g. frequency and distribution of synonyms of 'beautiful', synonyms of 'strong' occurring in fiction but not academic, synonyms of 'clean' + noun ('clean the floor', 'washed the dishes') ...|$|R
40|$|In {{this paper}} we explore the use of selectional {{preferences}} for detecting noncompositional verb-object combinations. To characterise the arguments in a given grammatical relationship we experiment with three models of selectional preference. Two use WordNet and one uses the entries from a distributional thesaurus as classes for representation. In previous work on selectional preference acquisition, the classes used for representation are selected according to the coverage of argument tokens rather than being selected according to the coverage of argument types. In our distributional thesaurus models {{and one of the}} methods using WordNet we select classes for representing the preferences by virtue of the number of argument types that they cover, and then only tokens under these classes which are representative of the argument head data are used to estimate the probability distribution for the selectional preference model. We demonstrate a highly significant correlation between measures which use these ‘typebased’ selectional preferences and compositionality judgements from a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired <b>thesaurus</b> <b>entries</b> produced the best results. The correlation for the thesaurus models is stronger than any of the individ...|$|R
40|$|In {{order to}} {{transfer}} the Chinese Classified Thesaurus (CCT) into a machine-processable format and provide CCT-based Web services, a pilot study has been conducted in which a variety of selected CCT classes and mapped <b>thesaurus</b> <b>entries</b> are encoded with SKOS. OWL and RDFS are also used to encode the same contents {{for the purposes of}} feasibility and cost-benefit comparison. CCT is a collected effort led by the National Library of China. It is an integration of the national standards Chinese Library Classification (CLC) 4 th edition and Chinese Thesaurus (CT). As a manually created mapping product, CCT provides for each of the classes the corresponding thesaurus terms, and vice versa. The coverage of CCT includes four major clusters: philosophy, social sciences and humanities, natural sciences and technologies, and general works. There are 22 main-classes, 52, 992 sub-classes and divisions, 110, 837 preferred thesaurus terms, 35, 690 entry terms (non-preferred terms), and 59, 738 pre-coordinated headings (Chinese Classified Thesaurus, 2005) Major challenges of encoding this large vocabulary comes from its integrated structure. CCT {{is a result of the}} combination of two structures (illustrated in Figure 1) : a thesaurus that uses ISO- 2788 standardized structure and a classification scheme that is basically enumerative, but provides some flexibility for several kinds of synthetic mechanism...|$|R
40|$|The word-space {{model is}} a {{computational}} model of word meaning that utilizes the distributional patterns of words collected over large text data to represent semantic similarity between words in terms of spatial proximity. The model {{has been used for}} over a decade, and has demonstrated its mettle in numerous experiments and applications. It is now on the verge of moving from research environments to practical deployment in commercial systems. Although extensively used and intensively investigated, our theoretical understanding of the word-space model remains unclear. The question this dissertation attempts to answer is: what kind of semantic information does the word-space model acquire and represent? The answer is derived through an identification and discussion of the three main theoretical cornerstones of the word-space model: the geometric metaphor of meaning, the distributional methodology, and the structuralist meaning theory. It is argued that the word-space model acquires and represents two different types of relations between words – syntagmatic and paradigmatic relations – depending on how the distributional patterns of words are used to accumulate word spaces. The difference between syntagmatic and paradigmatic word spaces is empirically demonstrated in a number of experiments, including comparisons with <b>thesaurus</b> <b>entries,</b> association norms, a synonym test, a list of antonym pairs, and a record of part-of-speech assignments. För att köpa boken skicka en beställning till exp@ling. su. se/ To order the book send an e-mail to exp@ling. su. s...|$|R
40|$|This paper {{describes}} {{the construction of}} a synonym <b>thesaurus</b> or <b>entry</b> vocabulary for the SUNY Biomedical Communication Network, which will permit the user greater ease of access to the MeSH-indexed material without previously consulting a printed list of indexing terms. In order to discover the actual terminology used by a researcher, words were extracted from titles of articles appearing in Index Medicus, and compared with the subject heading under which they appeared. As well as strict synonyms, grammatical variants were also included. Work is continuing on relating other indexing vocabularies, such as Excerpta Medica and Current Medical Terminology, used in the biomedical world to MeSH terms...|$|R
40|$|Abstract: This paper {{follows from}} {{previous}} research on the relatedness or un-relatedness, in Roget's International <b>Thesaurus</b> (RIT), of <b>entries</b> which have identical spellings. A comparison is made between the output from research on a VAX 11 / 780 computer-accessible version of RIT, the RIT text, and a micro-computer database of the hierarchical and cross-reference information of RIT. The author believes that all meaning is defined by associations and that {{the context of a}} word (the words associated with it) define its meaning in that context. Roget's Thesaurus is a culturally validated instantiation of an abstract thesaurus in which words are organized according to their relatedness...|$|R
40|$|Wikidata is {{the newest}} project of the Wikimedia Foundation (WMF), the {{non-profit}} U. S. -based foundation that runs Wikipedia and its sister projects. Officially released on October 30, 2012, and developed by Wikimedia Deutschland (WMF’s German counterpart), Wikidata is a free knowledge base that can be read and edited by humans and machines alike, published under a free license, allowing to everyone the reuse of the stored data in many different scenarios. The main goal of Wikidata is to centralise access to and management of structured data about every subject covered by Wikipedia and its sister projects (i. e. {{the exact number of}} inhabitants of a country, the apparent magnitude of a celestial body, the birthplace of a Chinese Emperor, and so on). Data are organised into “statements,” i. e. a property-value pair such as “Location: Germany,” with optional qualifiers and with their original sources (such as books, reviews, authority databases…). Wikidata is run by a community of voluntary editors, who decide on the rules of content creation and management, and is based on open-source software MediaWiki – the same software of Wikipedia – with the addition of the Wikibase extensions, specifically created for this project. At the moment, Wikidata is already a test-field for several collaborations with Italian and international libraries: for example, it has already 30 + properties related to National Libraries’ authority files, included a property that links to Italy’s Servizio Bibliotecario Nazionale. Acting as a hub of properties and identifiers, Wikidata is so becoming a “super” authority control. Moreover, there is also an official cooperation between the Association Wikimedia Italia and the National Central Library of Florence, in order to connect BNCF’s thesaurus to Wikipedia articles through Wikidata. As of November 21, 2013, more than 1600 SBN authority codes have been imported to Wikipedia, as long as around 10, 000 BNCF’s <b>thesaurus</b> <b>entries.</b> Both data are still growing. There are also plans for future collaborations: an informal “task force” for books (both as sources and as “items”) has been set up on Wikidata, in order to uniform Wikidata properties to the Dublin Core metadata scheme and allowing the basic description of documents. An Italian mailing list has been also set up, to which some Wikipedia and Wikidata users {{and a growing number of}} librarians subscribed, in order to discuss how librarians can contribute to WMF projects. This led to the organisation of a series of meetings, called “biblio-hackathons” (a portmanteau for “biblioteca,” “hacking,” and “marathon”), in which librarians are asked and “incited” to get their hands on and improve Wikipedia and its sister projects, with the help of some Wikipedian volunteers...|$|R
40|$|CL Research's {{question-answering system}} (DIMAP-QA) for TREC- 9 {{significantly}} extends its semantic relation triple (logical form) technology in which documents are fully parsed and databases built around discourse entities. This extension further exploits parsing output, most notably appositives and relative clauses, which are quite useful for question-answering. Further, DIMAP-QA integrated machine-readable lexical resources: a full-sized dictionary and a <b>thesaurus</b> with <b>entries</b> linked to specific dictionary definitions. The dictionary's 270, 000 definitions were fully parsed and semantic relations extracted {{to provide a}} MindNet-like semantic network; the thesaurus was reorganized into a WordNet file structure. DIMAP-QA uses these lexical resources, along with other methods, to support a just-in-time design that eliminates preprocessing for named-entity extraction, statistical subcategorization patterning, anaphora resolution, ontology development, and unguided query expansion. (All of these techniques are implicit in DIMAP-QA.) The best official scores for TREC- 9 are 0. 296 for sentences and 0. 135 for short answers, based on processing 20 of the top 50 documents provided by NIST, 0. 054 and 0. 083 below the TREC- 9 averages. The initial post-hoc analysis suggests a more accurate assessment of DIMAP-QA's performance in identifying answers is 0. 485 and 0. 196. This analysis also suggests that many failures {{can be dealt with}} relatively straightforwardly, as was done in improving performance for TREC- 8 answers to 0. 803 and 0. 597 for sentences and short answers, respectively...|$|R
40|$|The {{focus of}} my paper is the {{emergence}} of conceptual art in the United States between 1967 - 1972. In particular I {{want to look at the}} innovative strategies and new possibilities for exhibiting this art devised by dealer/organiser and independent curator Seth Siegelaub in his launch of the international careers of conceptual artists Robert Barry, Douglas Heubler, Joseph Kosuth and Lawrence Weiner. This focus I believe will address some of the issues raised in the conference theme The creation and curation of ephemera. From 1966 Siegelaub operated without a gallery and marketed art, which very often had no physical presence. For instance the exhibition January 5 - 31, 1969 consisted of a catalogue and traces of works in an empty office space (leased by Siegelaub for the month of January) at 44 East 52 nd Street, New York. There were two rooms, one with a receptionist at a desk, a telephone, a couch, a coffee table and catalogues for perusal. In the second room there were two works by each artist. Barry's works were imperceptible FM and AM radio waves. Technical details were given on typed labels in a space on the wall. Kosuth's works were pages from newspapers where he had placed <b>thesaurus</b> <b>entries</b> (as advertisements) for the words 'Existence' (New York Times, Museum News, the Nation, Artforum) and 'Time' (The London Times, the Daily Telegraph, the Financial Times, the Daily Express, the Observer). Huebler placed on the windowsill thirteen photographs of the ground taken every 50 miles along the 650 miles between three cities: Haverhill, Windham, New York. There were also polaroid photographs taken every half hour over a period of six hours, of a dispersing rectangle of sawdust that had been placed in the hallway in the morning the exhibition opened. Weiner removed a 36 inch square of plaster from the wall and poured bleach on the carpet to create An amount of bleach poured onto the rug and allowed to bleach. In his efforts to devise how to exhibit this art, how to market it, how it could be owned and later how could it be resold, Siegelaub developed all sorts of tactics that tuned into the new forms of communication and information distribution of advanced capitalism. Siegelaub seems to have recognised the synergy here between the major culture changes taking place and the emergent conceptual art. Siegelaub's influence was to go further than exhibitions. His transformation of art marketing and distribution parallelled the changes in art production then occurring. Much of Siegelaub's significant impact on the art world was due to his deep understanding of the nature and possibilities of conceptual art. Unlike his dealer peers, he seems to be excited by the challenge to sell the idea of art. There was also the appeal of being involved with radical ideals of cultural and political critique through conceptual art's disregard of privilege, hierarchy, established conventions and its desire to reach new audiences. Using Pierre Bourdieu's concept of cultural fields of power, I want to discuss the dynamics by which Siegelaub creates cultural legitimacy for the 'impossible' art he showed and for his own independent curatorial practice. Arts, Education & Law Group, Queensland College of ArtNo Full Tex...|$|R

