14|9|Public
50|$|Quantum error {{correction}} is a quantum algorithm {{for protection from}} errors. The algorithm operates on the relevant qubits (which operate within the computation) and needs a supply of new pure qubits for each round. This requirement can be weakened to purity above a certain <b>thershold</b> instead of requiring fully pure qubits. For this, algorithmic cooling {{can be used to}} produce qubits with the desired purity for quantum {{error correction}}.|$|E
40|$|The {{coal dust}} {{came from the}} process of {{destruction}} and separation of coal that causes health problems such as conjungtivitis. The sources of exposure to coal dust in PT. Indo Acidatama, Tbk came from the work process in unit crusher. The {{purpose of this study}} was to know the differences symptomps of conjungtivitis in employees exposed by coal dust above and below the <b>thershold</b> value in PT. Indo Acidatama Tbk. The method used observational research with cross sectional approach. Sample’s Technique used sample total sampling method with 24 Respondences which were consisted 12 employees from crusher and 12 employees from mechanical workshop. The result of this study used the mann-whitney statistic test showed significant (p 0. 000 < 0. 05), its means there was differences symptomps of conjungtivitis in employees exposed by coal dust above and below the <b>thershold</b> value in PT. Indo Acidatama Tbk. Based on the result, in crusher unit was found 50...|$|E
40|$|Abstract—Routing {{algorithm}} {{is a challenge}} for a mobile ad hoc network (MANET), but current routing protocols for MANET consider the path with minimum number of hops as the optimal path to a given destination. This strategy does not balance the traffic load over a MANET, and may result in some disadvantages such as creating congested area, depleting power faster and enlarging time delay in the nodes with heavy duties. In this paper we proposed routing metric can reflect not only the load of the path, but also the load distribution along the path. Simulation results show effectiveness of the routing scheme on balancing the load over all nodes in the network. Index Terms—Ad hoc, Load balance, <b>Thershold</b> valu...|$|E
40|$|Image can be {{represented}} {{in the area of}} Digital Processing, which is an optically formed duplicate or other reproduction of an object. In this paper images are being used in analysing of an Object. Noise means unwanted signal in an Image. Every Image is represented by noise. Salt and Pepper noise has been represented by bright pixels in dark regions and dark pixels in bright regions. Implementation of Noise removal filter filters the unwanted disturbances in the Image. In this work color image segmentation has been extensively applied using K-means Clustering. Color segmentation is done in by color features in the image in order to classify different colors in an Image. <b>Thersholding</b> is the basic approach in Segmenting an image. In this work Otsu <b>Thersholding</b> and Multiple <b>thersholding</b> has been used to segment the Image. According to the intensity value of the pixels the images are being segmented...|$|R
40|$|Lattice QCD {{approach}} {{to study the}} hadronic resonances and exotic hadrons is described at an introductory level. The main challenge is that these states decay strongly via one or more decay channels, and they often lie near <b>thersholds.</b> Specific results for conventional and exotic hadrons are shown to illustrate the current status...|$|R
40|$|This papre {{proposes a}} method for {{determining}} the coefficient of Inputs for superconductive logic circuits with two <b>thersholds.</b> First, all the target logic functions are divided into the group of NPN-equivalent logic function realizable with the same composition. Next, the input-and-output characteristic of a representation function is expressed geometrically, and it asks for the boundary which separates the domain of an output 0 and output 1. Finally, the example of application in two vriable logic function is shown...|$|R
40|$|Statistical {{analysis}} {{has to be}} taken into consideration in the design process of submicron integrated circuits. The variation of characteristic performance values depend upon the variation of many design parameters. In this paper we present a scheme for accurately and efficiently determining such dependencies at cell level. Additionally we employ an approximation for the performance distribution which does not only rely on a few low-order moments and therefore also performs well in the tails of the distribution. A typical problem that can be handled in this way is for instance the computation of the probability that the leakage current of a cell or circuit is greater than a given <b>thershold</b> value. The same question occurs for power and delay dependencies. Furthermore, sensivities of such marginal probabilities can be determined. Thus, this approach may potentially assist parameter monitoring and optimisation in the fabrication process...|$|E
40|$|This paper {{integrates}} {{the analysis}} of the decision to undertake R&D activities with {{the analysis of}} the decision on the level of the R&D investment when this investment is carried out. The theoretical framework assumes the existence of minimun required R&D expenditures, brought about by the indivisibility of some resources. This assumption, combined with demand characteristics and technological opportunites, determine <b>thershold</b> levels of R&D expenditures under which firms do not find it profitable to invest. This framework leads naturally to a Tobit-type econometric model aimed at estimating thresholds, which we applu to a Spanish manufacturing representative micropanel sample that includes observations on more than 2000 firms of all size, many without R&D expenditures. Our conclusions are that sizable threshold exist, which are systematically related to a list of demand and technological factors. The results strongly suggest that there is an advantage of the biggest firms in undertaking innovative activites...|$|E
40|$|A {{new method}} for {{analysis}} of benzene, toluene, and xylene (BTX) using High Performance Liquid Chromatography-UV detection (HPLC-UV) is described and compared to the gas chromatography (GC) method. A charcoal adsorption tube connected to a small pump was used to obtain samples from an atmosphere chamber standard. Samples were extracted with methanol and analyzed by HPLC-UV. Chromatography was isocratic in a mobile phase consisting of water-methanol (30 - 70). The flow rate was set at 1 ml/min. The analyses were completely separated and were quantified using both methods. The results demonstrated {{no statistically significant differences}} between BTX concentrations between the two analytical methods with a correlation coefficient of 0. 98 - 0. 99. The GC method provided higher sensitivity than HPLC, but the HPLC determination of BTX were applicable to real samples because its sensivity was lower than the <b>thershold</b> limit recommended by the American Conference of Governmental Industrial Hygienist (ACGIH) for an 8 -hour workday...|$|E
40|$|Lattice QCD {{approach}} {{to study the}} hadronic resonances and exotic hadrons is described at an introductory level. The main challenge is that these states decay strongly via one or more decay channels, and they often lie near <b>thersholds.</b> Specific results for conventional and exotic hadrons are shown to illustrate the current status. Comment: 8 pages, invited talk at QCD@Work 27 - 30 June 2016 Martina Franca, Italy, some overlap with 1508. 07322, reference added. arXiv admin note: text overlap with arXiv: 1508. 0732...|$|R
40|$|Metal oxide wide bandgap {{semiconducting}} nanostructures lie at {{the heart}} of many technologies including dye-sensitized solar cells (DSCs), they act as charge separation and transport medium; and therefore, understanding and controlling charge transport is key to increase the performance level of corresponding devices beyond current <b>thersholds.</b> Unlike bulk semiconductors, in which the electrons diffuses in the conduction band from one transport state to another, electron transport in nanostructured materials occurs through the localized surface energy states that lie within the bandgap. This article focus on the advances made on controlling the charge transport in electrospun metal oxide nanostructures...|$|R
40|$|Problem statement: Machine {{vision system}} {{is based on}} digital image {{processing}} and {{is found to be}} the best sensor detection as its operation is similar to the human eye. The purpose of machine vision is the desire to provide real time machines with visual abilities. Approach: A real time system is developed and is interfaced with the mechanical structure to be used in automobile industry. Dynamic <b>thersholding</b> is used and image pre-processing techniques are considered and implemented namely ellipse and circle algorithm. Results: The developed machine vision system consists of a mechanical structure, DIO card for communication and pneumatic components. Conclusion: The proposed model is tested on real time for segregation and mix up of automobile nut samples...|$|R
40|$|Running title: Anti nociceptive, Pelargonium graveolens Abstract: Introduction: Pain is a {{somatosensory}} {{sense that}} as an indicator to diagnoses for diseases. Pelargonium graveolens {{is known as a}} medicinal plant with use in traditional medicine. The aim {{of this study is to}} evaluate the analgesic effect of Pelargonium graveolens leaves extract (PGE) in male mice Material and Methods: In this experimental study, 42 male mice were divided in 6 groups (n= 7). The control group(taking normal saline, 0. 25 ml, i. p), morphine group(1 mg/kg, i. p), treated groups with PGE at doses of 100, 200 and 600 mg/kg and group induced with naloxone (0. 1 mg/kg, i. p) + 200 mg/kg of PGE. In order to evaluate the analgesic effects of PGE the tail flick and writhing tests were used. The data for each test were compared with One-way ANOVA and Tukey's post test. Results: Our results showed that PGE (200 and 600 mg/kg) increased pain <b>thershold</b> compared with control group in writhing and tail flick test significantly (P< 0. 001). Conclusion: The PGE has anitinociceptive effects in male mice. This analgesic effect of Pelargonium graveolens extract probably related to its flavonoids composition which has effect on opioid system...|$|E
40|$|PURPOSE. The aim of {{this study}} is to discuss {{personal}} and demographic factors that influence the relationship between physical activity and awareness of one's own body, as well as the pain response (threshold and tolerance of pain), situational anxiety and personality. In the study 38 healthy individual- volunteers, students in Trakia University - Stara Zagora were selected. All participants were divided into two groups: actively involved in individual or team sport (n = 19) and healthy normaly active subjects (non-athletes, n = 19). The age of the study participants ranged between 18 and 39 years, while the gender breakdown was as follows: men - 22 women – 16. Methods: Psychological Questionnaires: Body Awareness Questionnaire that asks subjects to rate, on a 4 point scale, the degree to which they were currently experiencing symptoms of sympathetic arousal, State Trait Anger Scale, and State Trait Anxiety Scale. Objective methods (cold pressure test) are used only to determine the pain sensation and pain tolerance thresholds. The results of investigation support significant differences between athletes and non-athletes in pain <b>thershold,</b> body awareness and anxiety. The study conclusions discuss body awareness as an increasing factor for pain resistance in athletes and as an integral part of the learning process among them...|$|E
40|$|The pairing {{interaction}} {{is one of}} the most important contribution of the residual interaction and then, it is of major importance for the study of many-body systems. One can get solutions of the pairing Hamiltonian throught the Bardeen-Cooper-Schieffer (BCS) or the Lipkin-Nogami (LN) approximations but, the pairing Hamiltonian admit exact solution worked out by Richardson. Nuclei far away from the stability line have important correlations with the continuum part of the energy spectrum, due that the Fermi level is very close to the contiuum <b>thershold.</b> The correlations with the continuum can be included in the many-body description through the complex energy states, called Gamow states. In this work we compare the approximates and exact solutions of the pairing Hamiltonian in real and complex-energy representations. In the application of this formulation to the symmetric Lipkin model, we found that the LN solution is in a good agreement with the exact one; besides, the extension of the BCS solution to the complex energy plane gives solution even for strength below the critical one, which is purely imaginary. Comment: Published in the Anales de la Asociación Física Argentina. Vol. 28 N. 3, 87 (2017). six pages, seven figures, twenty references. Keywords: Lipkin-Nogami, Richardson, complex energy, Lipkin model, exact solution, pairin...|$|E
40|$|If x is a {{predictor}} variable and y {{is a response}} variable of the regression model y = f (x) +  with f is a regression function which not yet been known and  is independent random variable with mean 0 and variance, hence function f can be estimated by parametric and nonparametric approach. In this paper function f is estimated with a nonparametric approach. Nonparametric approach that used is a wavelet shrinkage or a wavelet threshold method. In the function estimation with a wavelet threshold method, the value of threshold has the most important role to determine level of smoothing estimator. The small threshold give function estimation very no smoothly, while the big value of threshold give function estimation very smoothly. Therefore the optimal value of threshold should be selected to determine the optimal function estimation. One of the methods to determine the optimal value of threshold by minimize a cross validation function. The cross validation method that be used is two-fold cross validatiaon. In this cross validation, it compute the predicted value by using a half of data set. The original data set is split into two subsets of equal size : one containing only the even indexed data, and the other, the odd indexed data. The odd data {{will be used to}} predict the even data, and vice versa. Based on the result of data analysis, the optimal threshold with cross validation method is not uniq, but they give the uniq of wavelet <b>thersholding</b> regression estimation. Keywords : Nonparametric Regression, Wavelet Threshold Estimator, Cross Validation. ...|$|R
40|$|Long range {{weather radar}} {{observations}} are regularly used for short term forecasting and severe weather warning systems. Here, data acquired by portable and short range X-band mini weather radar are used, {{in order to}} try to forecast storms which are moving within the radar visibility area. Key step in developing an adequate storm forecasting system is the storm identification, because the overall system accuracy strictly depends on it. A storm is a contiguous region within a meteorological radar map where radar reflectivity is greater than a certain threshold and its area is greater than a certain areal threshold. Most of the storm identification techniques are manual or semi-automated and many of them are based on single or multi-level threshold(s). These techniques are facing the problems of choosing a suitable threshold value, and the one related to the so called "false merger problem". Moreover they are hardly able to indentify sub-storms within a cluster of storms. To cope with these issues, an automated multi-level <b>thersholding</b> technique is introduced where the initial threshold is automatically calculated by using two different techniques. After that, multileveling of threshold solves the problem of identifying substorms within the cluster of storms. The false merger problem has been instead solved by exploiting the mathematical morphology erosion concept. Storms are approximated and modeled by ellipses and their properties such as centroids, area, and major and minor axis length are calculated. Results show that all of the problems faced by previous systems in storm identification phase seem to be mitigated. Preliminary results about storm tracking and forecasting and preliminary are also presented in this paper...|$|R
40|$|One of the {{practical}} handicaps {{for the application of}} the percolation theory to estimate the percolation threshold of drugs in controlled release systems {{is the fact that the}} dissolution studies must be carried out so that only one surface of the tablet is exposed to the dissolution medium. The aim of this work is to estimate the percolation threshold of the antiarthritic drug lobenzarit dissodium (LBD) in inert matrices prepared with the excipients Ethocel® 100 and Eudragit® RS-PO (10 — 75 % w/w). Release assays were performed using the paddle method. The whole surface of the tablets was exposed to the dissolution medium. For the first time, a new mathematical method is developed to transform the amount of drug released in amount released per surface area in order to calculate the percolation <b>thershold</b> of LBD. The mathematical method proposed allows to calculate, using a new equation, the evolution of the mean surface area (O(t)). The new method was validated and three novel results were achieved: A constant value of (O(t)) at critical time (q) in the matrices (O(q) 1. 272 cm 2); a linear relationship between initial surface area (O(0)) and critical time; and a linear relationship between O(t) and time. Employing the values of O(t), it was possible to calculate for the first time, the percolation threshold (pc 1) for LBD in Ethocel® 100 (pc 1 0. 2800. 102) and Eudragit® RS-PO (pc 1 0. 3440. 07) matrices...|$|E
40|$|The DMFT (S) index, {{created at}} a time when caries was thought to be an {{irreversible}} condition of the tooth structures, proves now to be insufficiently informative under the present situation. The non-operative preventive therapeutic approach is highly regarded now, which is an approach requiring additional information on the reversible carious pathology so that an adequate treatment could be applied. The epidemiological examination of a 1000 children aged 6 - 15 has been conducted in Sofia and Rousse – two big Bulgarian cities in which no public prophylactic programme has been carried out. On the basis of predetermined diagnostic criteria for the determination of the reversible phases {{in the development of the}} dental caries and by accepting an early diagnostic <b>thershold,</b> an IR reversibility index was created. This index is very helpful in supplementing the information obtained by means of the DMFT and DMFS indexes, allowing us to determine the correlation between the reversible and irreversible lesions within the DMF index and making possible adequate therapeutic decision-making. The IR reversibility index is there for the sake of preventive approach in modern cariesology and can successfully be used in epidemiological studies. Epidemiological studies are conducted in the world all the time, aimed at monitoring the level of development of the caries distribution (1, 3, 4, 5, 6, 20). Hence the possibility for comparison between the different countries, geographic regions, groups and populations. The studies reflect the medical therapy conducted, the need for therapy as well as the volume of the social and group prophylactic measures taken (2, 8, 9, 11, 14, 15). In order that comparisons could be reliable, a unification of the indexes applied as well as a unification of the diagnostic bands and criteria use...|$|E
40|$|Background {{and purpose}} : Dextromethorphan is a non-competitive NMDA {{receptor}} antagonist in the glutamatergic system with over 47 years of clinical usage {{experience as an}} over-the counter antitussive drug. We previously demonstrated that dextromethorphan modulates the pain threshold in the mouse acetic acid (0. 6 %,intraperitonealy) -induced writhing test (a tonic and chemical model for chronic pain) and naloxone-induced withdrawal signs in morphine-dependent mice. Because opioid and dopaminergic mechanisms of dextromethorphan have not been evaluated in the acute and phasic pain models, the effect of dextromethorphan on the pain response-induced by hot plate (a phasic and thermal model for acute pain) was investigated in mice. Materials and methods : The effects of dextromethorphan and other drugs on pain <b>thershold</b> were investigated using a hot plate apparatus (Harvard, UK). The hot plate temperature set thermostatically at 52. 5 ± 0. 5 ºC. The latency to licking or kicking of the fore or hind paws was recorded at various times after drug injection. A cut-off time of 45 s was imposed to avoid tissue damage. The integrity of motor coordination was assessed with a rota rod apparatus (Harvard, UK). Results : Dextromethorphan (30 mg/kg, i. p.) increased the pain threshold in the mouse hot plate test. This dose of dextromethorphan was ineffective in the rota rod test, thus it could {{be considered as a}} real antinociceptive effect. Dextromethorphan also potentiated the antinociceptive effect of morphine. The antinociceptive effect of dextromethorphan and the potentiation of morphine antinociception, were antagonized by the opioid receptor antagonist, naloxone. The antinociceptive effect of dextromethorphan was also antagonized by dopamine mixed D 1 /D 2 receptor agonist, apomorphine. The inhibitory effect of apomorphine on antinociceptive response of dextromethorphan was blocked by the dopamine D 1 receptor antagonist, SCH 23390, but not by the dopamine D 2 receptor antagonist, sulpiride nor by the peripheral dopamine receptor antagonist, domperidone. Conclusion : These results suggest that, opioid and central dopamine D 1 receptor mechanisms may in part modulate dextromethorphan antinociception in the mouse hot plate test...|$|E
40|$|AbstractObjectives. This {{study was}} {{designed}} to determine in patients with advanced coronary disease whether prediction of recovery of mechanical function after coronary revascularization could be accomplished more effectively by positron emission tomography (PET) with Carbon- 11 (11 C) -acetate than by PET with fluorine- 18 (18 F) -fluorodeoxyglucose. Background. Results of previous studies have demonstrated that preservation of myocardial oxidative metabolism (measured by PET with 11 C-acetate) is necessary for recovery of systolic function after coronary revascularization. Methods. Myocardial oxidative metabolism was quantified before revascularization in 34 patients by the analysis of the rate of myocardial clearance of 11 C-acetate. Metabolism of glucose was assessed by analysis of uptake of 18 F-fluorodeoxyglucose. Receiver operating characteristic curves for predicting functional recovery were derived for the measurements of oxidative metabolism and glucose metabolism. In addition, criteria for prediction of recovery of function based on measurements of oxidative metabolism and glucose metabolism were developed and compared. Results. Analysis of receiver operating characteristic curves indicated that estimates of oxidative metabolism were more robust in predicting functional recovery than were estimates of glucose metabolism (p < 0. 02). Moreover, <b>thershold</b> criteria with 11 C-acetate exhibited superior positive and negative predictive values (67 % and 89 %, respectively) than did the criteria with 18 F-fluorodeoxyglucose (52 % and 81 %, respectively), p < 0. 01. In segments with initially severe dysfunction, estimates of oxidative metabolism tended to be more robust than estimates of glucose metabolism in predicting functional recovery. Moreover, in such segments, the threshold criteria with 11 C-acetate to exhibit superior positive and negative predictive values (85 % and 87 %, respectively) than did the criteria with 18 F-fluorodeoxyglucose (72 % and 82 %, respectively), although statistical significance was not achieved. Conclusions. In patients with advanced coronary artery disease, the extent to which functional recovery can be anticipated after coronary revascularization can be delineated accurately by quantification of regional oxidative metabolism by PET with 11 C-acetate...|$|E
40|$|The {{purpose of}} this report is to present {{a study on the}} water quality of the Drinking water-treatment plant of Luleå (Sweden) : to give an {{explanation}} to the observed difference in temperature, using both sampling period data and historical data found: finally some alternatives will be suggested to try to improve this quality. With respect to the water quality, three aspects have been differentiated, studying them separately with the purpose of locating the points to improve: • Turbidity and conductivity: Generally the quality of the water sent to the distribution system is within Swedish standards (0. 5 NTU for the turbidity and conductivity < μs/cm), but in some points these values are surpassed. • Particle size distribution: it has been drawn up a map of the particle distribution, classified by large vs. number, done to the three wells with more turbidity of the system. Approximately the 91 % of particles are less than 5 μm of equivalent diameter. • Organic matter: a study was made for three wells with a high turbidity, where the organic matter relatively the inorganic one –as volatile carbon (loss) - was under (approx.) to 30 % in the considered cases. In order to explain the observed temperature differences throughout the years, three hypotheses have been studied: • Different retention times: from the time the water enters the infiltration ponds until it leaves by the wells, an average of two months passes. • Soil as a heat exchanger: by means of an energy balance it has been seen that the Earth absorbs energy from the water when it’s warms, cooling it and vice versa. • Inner heat of the Earth: it has been obtained an approximate temperature gradient of the deepest point of the wells using the thermal gradient. Last, some tests were carried out to see the viability of introducing a chemical precipitation in the process to decrease the turbidity, but there was no perceivable coagulation in any studied cases. With respect to the possibility of introducing membrane filtration technology, the most viable filtration for the type of particles we have is the microfiltration. Therefore, the conclusion is that there are problems with the high ionic content in some of the wells which provoke salinity tastes on the water, and also turbidity exceedes the <b>thershold</b> values according to the Swedish Standards for drinking water in some cases. The organic matter is not high, hence there is no need of introducing a biological treatment in the plant and finally as the particle size distribution test showed, the best alternative would be introducing microfiltration, since it would retain the 90 % of the particles. Validerat; 20101217 (root...|$|E

