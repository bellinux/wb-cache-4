51|10000|Public
5|$|The first metagenomic studies {{conducted}} using high-throughput sequencing used massively parallel 454 pyrosequencing. Three other technologies commonly applied to environmental sampling are the Ion Torrent Personal Genome Machine, the Illumina MiSeq or HiSeq and the Applied Biosystems SOLiD system. These techniques for sequencing DNA generate shorter fragments than Sanger sequencing; Ion Torrent PGM System and 454 pyrosequencing typically produces ~400bp reads, Illumina MiSeq produces 400-700bp reads (depending on whether paired end options are used), and SOLiD produce 25-75bp reads. Historically, these read lengths were significantly {{shorter than the}} typical Sanger sequencing read length of ~750bp, however the Illumina technology is quickly coming close <b>to</b> <b>this</b> <b>benchmark.</b> However, this limitation is compensated for by the much larger number of sequence reads. In 2009, pyrosequenced metagenomes generate 200–500megabases, and Illumina platforms generate around 20–50gigabases, but these outputs have increased by orders of magnitude in recent years. An additional advantage to high throughput sequencing is that this technique does not require cloning the DNA before sequencing, removing {{one of the main}} biases and bottlenecks in environmental sampling.|$|E
50|$|The Beta (β) of a {{stock or}} {{portfolio}} is a number describing the volatility of an asset {{in relation to the}} volatility of the benchmark that said asset is being compared <b>to.</b> <b>This</b> <b>benchmark</b> is generally the overall financial market and is often estimated via the use of representative indices, such as the S&P 500.|$|E
50|$|The first metagenomic studies {{conducted}} using high-throughput sequencing used massively parallel 454 pyrosequencing. Three other technologies commonly applied to environmental sampling are the Ion Torrent Personal Genome Machine, the Illumina MiSeq or HiSeq and the Applied Biosystems SOLiD system. These techniques for sequencing DNA generate shorter fragments than Sanger sequencing; Ion Torrent PGM System and 454 pyrosequencing typically produces ~400 bp reads, Illumina MiSeq produces 400-700bp reads (depending on whether paired end options are used), and SOLiD produce 25-75 bp reads. Historically, these read lengths were significantly {{shorter than the}} typical Sanger sequencing read length of ~750 bp, however the Illumina technology is quickly coming close <b>to</b> <b>this</b> <b>benchmark.</b> However, this limitation is compensated for by the much larger number of sequence reads. In 2009, pyrosequenced metagenomes generate 200-500 megabases, and Illumina platforms generate around 20-50 gigabases, but these outputs have increased by orders of magnitude in recent years. An additional advantage to high throughput sequencing is that this technique does not require cloning the DNA before sequencing, removing {{one of the main}} biases and bottlenecks in environmental sampling.|$|E
50|$|The Exemplar phase {{has been}} awarded Bioregional’s One Planet Living status, one of nine {{developments}} in the world <b>to</b> achieve <b>this</b> <b>benchmark.</b>|$|R
50|$|Due <b>to</b> <b>this</b> proliferation, <b>benchmarks</b> between Intel and AMD {{processors}} increasingly {{reflect the}} cleverness or implementation {{quality of the}} divergent code paths rather than {{the strength of the}} underlying platform.|$|R
25|$|Service {{is a major}} revenue {{generator}} of the company. Most of {{the service}} stations are managed on franchise basis, where Maruti Suzuki trains the local staff. Other automobile companies {{have not been able}} <b>to</b> match <b>this</b> <b>benchmark</b> set by Maruti Suzuki. The Express Service stations help many stranded vehicles on the highways by sending across their repair man to the vehicle.|$|R
40|$|Mobile {{phones are}} ubiquitous, however they are vastly underpowered {{compared}} to their desktop counterparts. We propose a technique to play potentially resource intensive games over a network, and provide a prototype system called RemoteME which implements this technique. We also explore the responsiveness requirement for systems of this nature, establish benchmarks for responsiveness via user studies. We evaluate our implementation by measuring its responsiveness and comparing it <b>to</b> <b>this</b> <b>benchmark...</b>|$|E
40|$|International audienceThe " 3 D {{anisotropy}} Benchmark" ([URL] {{addresses a}} three-dimensional anisotropic diffusion problem, which is discretized on general, possibly non-conforming meshes. In most cases, the diffusion tensor {{is taken to}} be anisotropic, and at times heterogeneous and/or discontinuous. This paper presents numerical results <b>to</b> <b>this</b> <b>benchmark</b> for a 3 D Cell and Vertex centered DDFV scheme. The method applies to very general 3 D meshes including non conformal ones. Its princples and construction is presented {{as well as its}} implementation for diffusion problems...|$|E
40|$|Thrombolysis for {{patients}} with acute myocardial infarction (AMI) is of greatest benefit when treatment is commenced {{as soon as possible}} after symptom onset. The British Heart Foundation (BHF) recently set a benchmark recommending that eligible patients with AMI receive thrombolytic therapy less than 90 min after calling for medical assistance. The {{purpose of this study was}} to compare the performance of an urban emergency service <b>to</b> <b>this</b> <b>benchmark.</b> A secondary objective was to determine whether patients treated outside this time were at a greater risk of mortality...|$|E
40|$|Total Benchmarking is a {{standard}} issued by Directorate General of Taxation {{which is used to}} appraise Taxpayers on the fairness of Financial Report and the obedience to comply with Tax Laws. The Benchmark consists of 14 Ratios that differ among industries. Until now, The Directorate General of Taxation has published Total Benchmark for 100 industries. If there are differences between taxpayer 2 ̆ 7 s ratio and the benchmark, do not mean that the taxpayer didn 2 ̆ 7 t fulfill his obligation according to the Tax Laws. The analyst (Account Representative) should use <b>this</b> <b>benchmark</b> as a tool to plan for further analysis. The data used <b>to</b> construct <b>this</b> <b>benchmark</b> taken from samples in each industry for the period of 2005, 2006, and 2007 which is differ from current condition. Therefore the writer suggest adjustment for <b>this</b> <b>benchmark...</b>|$|R
40|$|Parallel {{systems are}} {{traditionally}} evaluated considering only the performance. However, {{the importance of}} power consumption in these systems is increasing. Benchmarks are programs commonly used for evaluating the performance of parallel systems. In this paper, we use benchmarks LU, FT, CG and EP from NAS Parallel Benchmarks to present a performance analysis considering the power consumption in a multi-core parallel system. We vary the number of threads used <b>to</b> run <b>this</b> <b>benchmarks</b> and identify how much this variation implies in the power consumption. 1...|$|R
40|$|High-Performance-Computing {{to define}} a new {{standard}} for measuring system performance. One part of this research project is {{the design of a}} distributed architecture for execution of benchmarks on High Performance Computers. Its objective is to guide benchmarkers along a mostly automated benchmarking process cycle in compiling and executing benchmarks, and finally gathering and presenting the measured results on a central website. In this paper we present a distributed software architecture which allows immediate analysis of the results, is robust and flexible <b>to</b> support <b>this</b> <b>benchmarking</b> process for the benchmark community. 1...|$|R
40|$|A {{depletion}} calculation benchmark {{devoted to}} MOx fuel {{is an ongoing}} objective of the OECD/NEA WPRS following the study of depletion calculation concerning UOx fuels. The objective of the proposed benchmark is to compare existing depletion calculations obtained with various codes and data libraries applied to fuel and back-end cycle configurations. In the present work the deterministic code NEWT/ORIGEN-S of the SCALE 6 codes package was {{used to calculate the}} masses of inventory isotopes. Then a methodology to apply the Monte Carlo based code MONTEBURNS 2. 0 <b>to</b> <b>this</b> <b>benchmark</b> is presented...|$|E
40|$|Page-based Linear Genetic Programming (GP) is {{proposed}} and implemented with two-layer Subset Selection {{to address a}} two-class intrusion detection classification problem {{as defined by the}} KDD- 99 benchmark dataset. By careful adjustment of the relationship between subset layers, over fitting by individuals to specific subsets is avoided. Moreover, efficient training on a dataset of 500, 000 patterns is demonstrated. Unlike the current approaches <b>to</b> <b>this</b> <b>benchmark,</b> the learning algorithm is also responsible for deriving useful temporal features. Following evolution, decoding of a GP individual demonstrates that the solution is unique and comparative to hand coded solutions found by experts...|$|E
40|$|This report {{provides}} {{details of}} the comparative analysis between calculated and measured results. Comparison with experimental results identified the origins of discrepancies between calculations and measurements and enabled the quantitative comparison of {{the relative merits of}} the different calculation methods. As the benchmark was two-dimensional, a benchmark based on threedimensional VENUS- 2 experimental results will be launched for a more thorough investigation into the calculation methods used for MOX-fuelled systems. Acknowledgements The Secretariat expresses its sincere gratitude to the participants who devoted their time and effort <b>to</b> <b>this</b> <b>benchmark</b> exercise, as well as to SCK. CEN (Mol, Belgium) for the release of the valuable experimental dat...|$|E
40|$|A clear {{need was}} {{established}} by the aero-engine manufacturers and the certifying authorities for a re-assessment of the published rules governing engine certification for large flocking birds. A task group was set up <b>to</b> address <b>this</b> need {{at the beginning of}} 2000. Early in this program, it was determined that a statistical approach to the rule making was required and the Monte-Carlo technique was proposed and accepted. This paper discusses the implementation of the Monte-Carlo technique to simulate bird strike events from the Rolls-Royce viewpoint and describes the various refinements that have been made in order to ensure an adequate comparison with observed service data. Subsequent <b>to</b> <b>this</b> <b>benchmarking</b> process, the results from the analysis have been used to calculate engine shut-down rates for various proposed large bird rule scenarios ultimately leading to the acceptance of a new flocking bird certification requirement for engines of inlet area of 2. 5 m 2 and above. In addition, the analysis has been used extensively within Rolls-Royce to conduct theoretical bird strike studies...|$|R
40|$|The paper proposes {{the use of}} {{the growth}} optimal {{portfolio}} for the construction of financial market models with unobserved factors that have <b>to</b> be filtered. <b>This</b> <b>benchmark</b> approach avoids any measure transformation for the pricing of derivatives. The suggested framework allows to measure the reduction of the variance of derivative prices for increasing degrees of available information. financial modelling; filter methods; benchmark approach; growth optimal portfolio...|$|R
50|$|Maruti Suzuki has 1,820 sales outlets across 1,471 {{cities in}} India. The company aims to double its sales network to 4,000 outlets by 2020. It has 3,145 service {{stations}} across 1,506 cities throughout India. Maruti’s dealership network {{is larger than}} that of Hyundai, Mahindra, Honda, Tata, Toyota and Ford combined. Service is a major revenue generator of the company. Most of the service stations are managed on franchise basis, where Maruti Suzuki trains the local staff. Other automobile companies {{have not been able}} <b>to</b> match <b>this</b> <b>benchmark</b> set by Maruti Suzuki. The Express Service stations help many stranded vehicles on the highways by sending across their repair man to the vehicle.|$|R
40|$|This article {{outlines}} recent {{methods and}} applications directed at understanding the profit and consumer welfare implications of increasingly prevalent price discrimination {{strategies in the}} service sector. These industries are typically characterized by heterogeneity in consumers’ valuation and usage of the service, resale constraints, and a focus on price as the service’s key attribute. The article focuses on how firms use nonlinear pricing or bundling strategies {{to benefit from the}} heterogeneity in consumer demand. We describe the basic economic model commonly used in the literature to analyze such strategic choices and present recent methodological improvements <b>to</b> <b>this</b> <b>benchmark.</b> A discussion of existing applications and future research opportunities concludes the article...|$|E
40|$|Abstracts: In {{this paper}} we point out various factors {{which should be}} taken into {{consideration}} when assessing market power in the evolving electricity markets. Of particular interest are the effects of the electricity market design rules, type of software employed when computing electricity clearing price, and the generation technology-specific costs, such as start up and shut down costs. In this paper we propose a method of estimating a benchmark price which accounts for these factors unique to electricity markets; when assessing market power the actual price is compared <b>to</b> <b>this</b> <b>benchmark</b> price. The pros and cons of assessing market power using such aggregate benchmark price and/or more direct analysis of individual bids are illustrated using the New England market data...|$|E
40|$|International audienceA {{specific}} benchmark {{has been}} developed by the french research group MoMas {{in order to improve}} numerical solution methods applied by reactive transport models, i. e. codes which couple hydrodynamic flow and mass transport in porous media with geochemical reactions. The HYTEC model has been applied <b>to</b> <b>this</b> <b>benchmark</b> exercise and this paper summarizes some of the principal results. HYTEC is a general-purpose code, applied by industrials and research groups {{to a wide variety of}} domains, including soil pollution, nuclear waste storage, cement degradation, water purification systems, storage of CO 2 and valorization of stabilized wastes. The code has been applied to the benchmark test-cases without any specific modification. Apart from the benchmark imposed output, additional information is provided to highlight the behavior of HYTEC specifically and the simulation results in particular...|$|E
40|$|There is a {{discussion}} whether PCTE {{is suitable for}} fine grained data or not, but {{there are very few}} test results. To test the performance of PCTE we have used a benchmark that was originally developed to test whether an OMS is suitable to support an SDE, which is based on syntax directed tools. <b>This</b> <b>benchmark</b> is defined in abstract terms, so that portability between different data models could be achieved. This paper introduces the Merlin benchmark and describes some relevant aspects and design decisions for our PCTE implementation. Then we discuss how <b>to</b> implement <b>this</b> <b>benchmark</b> on top of a PCTE 1. 5 implementation. Results of the benchmark are given, too. Finally, we discuss the process of porting the benchmark to forthcoming ECMA PCTE implementations. The results of the benchmark can be used to determine whether PCTE can provide the necessary performance for supporting finegrained data and to highlight some performance problems that PCTE has with some operations...|$|R
40|$|Abstract. Within the {{programming}} domain of XML processing, {{we set up}} a benchmark for API migration. The benchmark is a suite of XML processing scenarios that are implemented in terms of different XML APIs. We suggest that a relatively general technique for API migration should be capable of pro-viding source-to-source translations between the different implementations. The benchmark involves APIs that are different enough to require more than just local rewrites for the migration. We make different attempts at API migration: wrap-ping, rewriting, and protocol-based translation. None of our attempts are entirely satisfactory, and we hope <b>to</b> provide <b>this</b> <b>benchmark</b> as a challenge to the broader programming language and automated software engineering communities...|$|R
40|$|Greater {{openness}} {{has become}} an almost universal feature of modern, developed economies. This paper develops a workhorse international model, and explores the role of standard monetary policy rules applied to an open economy. For this purpose, I build a two-country DSGE model with monopolistic competition, sticky prices, and pricing-to-market. I also derive the steady state and a log-linear approximation of the equilibrium conditions. The paper provides a lengthy explanation of the steps required <b>to</b> derive <b>this</b> <b>benchmark</b> model, and a discussion of: (a) how to account for certain well-known anomalies in the international literature, and (b) how to start "thinking" about monetary policy in this environment. Monetary policy; Equilibrium (Economics); Globalization; Macroeconomics; International finance; Mathematical models...|$|R
40|$|International audienceWe present here {{a number}} of test cases and meshes which were {{designed}} to form a benchmark for finite volume schemes and give a summary {{of some of the}} results which were presented by the participants <b>to</b> <b>this</b> <b>benchmark.</b> We address a two-dimensional anisotropic diffusion problem, which is discretized on general, possibly non-conforming meshes. In most cases, the diffusion tensor is taken to be anisotropic, and at times heterogeneous and/or discontinuous. The meshes are either triangular or quadrangular, and sometimes quite distorted. Several methods were tested, among which finite element, discontinous Galerkin, cell centred and vertex centred finite volume methods, discrete duality finite volume methods, mimetic methods. The results given by the participants to the benchmark range from the number of unknowns, the errors on the fluxes or the minimum and maximum values and energy, to the order of convergence (when available) ...|$|E
40|$|Focuses on the {{benchmark}} control problems for seismically excited nonlinear buildings defined by Ohtori et al. (2000). This benchmark study focuses on three typical steel structures, 3 -, 9 - and 20 -storey buildings {{designed for the}} SAC project for Los Angeles in the California region. The first stage of applying the fuzzy controller <b>to</b> <b>this</b> <b>benchmark</b> study for the 3 -storey building is reported. The main advantage of the fuzzy controller is its inherent robustness and ability to handle the non-linear behaviour of the structure. This benchmark study {{is based on a}} number of evaluation criteria and control constraints and these limitations are considered {{in the design of the}} fuzzy controller. The performance of the controller is validated through the computer simulation on MATLAB. The results of the simulation show a good performance of the fuzzy controller to reduce the response of the building under different earthquake excitation...|$|E
40|$|We {{estimate}} {{a monetary}} policy reaction function for the Bundesbank {{and use it}} as a benchmark to assess the monetary policy of the ECB since the launch of the euro in January 1999. We find that euro interest rates are low relative <b>to</b> <b>this</b> <b>benchmark.</b> We consider several possible reasons for this, including the divergence between core and headline inflation, inflation having turned out to be higher than could have been foreseen by the ECB and the possibility that the ECB is focussing only on macroeconomic conditions in a subset of member countries. We argue that these potential explanations cannot account for the difference between recent interest rates and our estimated Bundesbank benchmark. Our results suggest that the reaction function of the ECB features a high weight on the output gap relative to the weight on inflation, compared to the Bundesbank. European Central Bank; Monetary policy...|$|E
40|$|The IPACS-project (Integrated Performance Analysis of Computer Systems) {{was founded}} by the Federal Department of Education, Science, Research and Technology (BMBF) in the program High-Performance-Computing to define a new {{standard}} for measuring system performance. One part of this research project is {{the design of a}} distributed architecture for execution of benchmarks on High Performance Computers. Its objective is to guide benchmarkers along an mostly automated benchmarking process cycle in compiling and executing benchmarks, and finally gathering and presenting the measured results on a central website. In this paper we present a distributed software architecture which allows immediate analysis of the results, is robust and flexible <b>to</b> support <b>this</b> <b>benchmarking</b> process for the benchmark community...|$|R
40|$|Lattice Quantum ChromoDynamics (QCD), and by {{extension}} its parent field, Lattice Gauge Theory (LGT), make up a significant fraction of supercomputing cycles worldwide. As such, it would be irresponsible not to evaluate machines' suitability for such applications. <b>To</b> <b>this</b> end, a <b>benchmark</b> has been developed to assess the performance of LGT applications on modern HPC platforms. Distinct from previous QCD-based <b>benchmarks,</b> <b>this</b> allows probing the behaviour {{of a variety of}} theories, which allows varying the ratio of demands between on-node computations and inter-node communications. The results of testing <b>this</b> <b>benchmark</b> on various recent HPC platforms are presented, and directions for future development are discussed. Comment: 6 pages, 5 figures; version as presented at High Performance Computing and Simulation, HPCS 201...|$|R
40|$|The {{ultimate}} {{objective of}} the scheduling competition is to drive us developing frameworks that tackle real-life problems better. Whatever form we gave <b>to</b> <b>this</b> competition, <b>benchmark</b> problems will be the essential tools for identifying the best algorithms and frameworks. So, {{the future of the}} competition will be greatly influenced by the problems we choose. Oversimplified ones will produce a distortion from the competition’s final objective. In this paper, we present three real-life scheduling problems from space mission operations. We extracted their core features to build a rich benchmark scheduling problem {{to be used in the}} competition...|$|R
40|$|The {{last thirty}} years have {{witnessed}} the appearance and rapid expansion of Islamic banking {{both inside and outside}} the Islamic world. Islamic banks provide financial products that do not violate Sharia, the Islamic law of human conduct. The Islamic principles upon which the banks claim to operate give an important role to social issues. Applying these principles, we develop a benchmark set of social disclosures appropriate to Islamic banks. These are then compared, using a disclosure index approach, the actual social disclosures contained in the annual reports of twenty-nine Islamic banks (located in sixteen countries) <b>to</b> <b>this</b> <b>benchmark.</b> In addition, content analysis is undertaken to measure the volume of social disclosures. Our analysis suggests that social reporting by Islamic banks falls significantly short of our expectations. The results of the analysis also suggest that banks required to pay the Islamic religious tax Zakah provide more social disclosures than banks not subject to Zakah...|$|E
40|$|Context. The {{radiative}} {{transport of}} photons through arbitrary three-dimensional (3 D) structures of dust is a challenging problem {{due to the}} anisotropic scattering of dust grains and strong coupling between different spatial regions. The radiative transfer problem in 3 D is solved using Monte Carlo or Ray Tracing techniques as no full analytic solution exists for the true 3 D structures. Aims. We provide the first 3 D dust radiative transfer benchmark composed of a slab of dust with uniform density externally illuminated by a star. This simple 3 D benchmark is explicitly formulated to provide tests of the different components of the radiative transfer problem including dust absorption, scattering, and emission. Methods. The details of the external star, the slab itself, and the dust properties are provided. This benchmark includes models {{with a range of}} dust optical depths fully probing cases that are optically thin at all wavelengths to optically thick at most wavelengths. The dust properties adopted are characteristic of the diffuse Milky Way interstellar medium. This benchmark includes solutions for the full dust emission including single photon (stochastic) heating as well as two simplifying approximations: One where all grains are considered in equilibrium with the radiation field and one where the emission is from a single effective grain with size-distribution-averaged properties. A total of six Monte Carlo codes and one Ray Tracing code provide solutions <b>to</b> <b>this</b> <b>benchmark.</b> Results. The solution <b>to</b> <b>this</b> <b>benchmark</b> is given as global spectral energy distributions (SEDs) and images at select diagnostic wavelengths from the ultraviolet through the infrared. Comparison of the results revealed that the global SEDs are consistent on average to a few percent for all but the scattered stellar flux at very high optical depths. The image results are consistent within 10 %, again except for the stellar scattered flux at very high optical depths. The lack of agreement between different codes of the scattered flux at high optical depths is quantified for the first time. Convergence tests using one of the Monte Carlo codes illustrate the sensitivity of the solutions to various model parameters. Conclusions. We provide the first 3 D dust radiative transfer benchmark and validate the accuracy of this benchmark through comparisons between multiple independent codes and detailed convergence tests...|$|E
40|$|This thesis {{addresses}} several {{topics in}} finance {{and consists of}} two parts. The central theme is formed by economic agents making investment decisions for financial markets using available past and present information. Part I is of a more analytical nature and discusses the influence of ambiguity and transmutability on the investment decisions and argues that limits to rational expectations offer {{a better understanding of}} financial market anomalies than limits to arbitrage. It also introduces a modification of the traditional CAPM in which beliefs of economic agents are determined endogenously. Testable predictions of this model are derived. Using these, we test our version of the CAPM, and we also confront it with the traditional multi-factor models. In part II a benchmark multi-asset artificial financial market model is introduced in which economic agents trade in an artificial market, generating prices and return dynamics. In addition, several extensions <b>to</b> <b>this</b> <b>benchmark</b> model are introduced and it is shown that these models can reproduce several well-known stylized facts of asset returns such as heavy tails, volume/volatility correlation and volatility clustering. ...|$|E
40|$|New {{transport}} infrastructure has {{a myriad of}} short and long run effects. The effects on population and economic activity are most difficult <b>to</b> estimate. <b>This</b> paper introduces three different models to estimate the impacts of new infrastructure on labour supply and demand, and carefully explains how {{the interaction between the}} models and their outcomes should be handled. The methodology is applied to a proposal for a magnetic levitation rail system from Groningen across the Afsluitdijk <b>to</b> Schiphol. <b>This</b> <b>benchmark</b> it is then used to derive a qualitative assessment for different trajectories and slower type of new rail infrastructure all using the Afsluitdijk. Finally, this paper discusses the remarkable differences in the quantitative outcomes with a comparable Maglev proposal that does not use the Afsluitdijk but runs through the polders of the former Zuiderzee. ...|$|R
40|$|A well-coordinated {{assembly}} system {{plays an}} important role in an enterprises strategic management. As the uncertainty of the supply chain is increasing, coordinating the assembly system effectively under supply disruption becomes a critical success factor for a global supply chain system. In this study, we consider an assembly system with two suppliers and one manufacturer and formulate the model of each partner with bonus policy under supply disruption. The optimal decision of each partner is analyzed under decentralized decision. Then we optimize the total cost of assembly system under centralized decision which is regarded as the <b>benchmark.</b> Furthermore, <b>to</b> achieve <b>this</b> <b>benchmark,</b> the compensation mechanism is designed to coordinate the assembly system under supply disruption. At last, the sensitivity is analyzed and the numerical analysis shows that our compensation mechanism is reasonable and effective...|$|R
40|$|AbstractThe {{performance}} of computing devices is increasing day by day, the storage devices are not capable <b>to</b> reach with <b>this</b> <b>benchmark.</b> The compute and storage resources are forming {{two parts of}} the shared network. This paper is presenting a storage middleware called HyCache+ to reduce the high bi-section bandwidth of the parallel computing systems. The middleware uses Programmable Operating System (POSIX) interface to the end users to swap the data with the high capacity network attached storage. The system uses a 2 -layer scheduling approach. The HyCache+ can be deployed on the IBM Blue Gene / P supercomputers for the performance comparison...|$|R
