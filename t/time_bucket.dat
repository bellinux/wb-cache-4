23|76|Public
5000|$|Determining {{the number}} and length of each {{relevant}} time interval (<b>time</b> <b>bucket)</b> ...|$|E
5000|$|Slotting every asset, {{liability}} and {{off-balance sheet items}} into corresponding <b>time</b> <b>bucket</b> based on effective or liquidity duration maturity ...|$|E
50|$|To place {{these funds}} in the longest-dated <b>time</b> <b>bucket</b> as {{deposits}} remain historically stable over time due to large numbers of depositors.|$|E
40|$|Breathing <b>Time</b> <b>Buckets</b> and <b>Time</b> Warp are two {{different}} approaches to optimistic parallel discrete event simulation. To explore their relative performance we ran two simple simulations using transputer-based implementations of the two algorithms. To exercise the two implementations thoroughly, we varied {{the value of the}} following parameters for one of the simulations, a queuing network: the number of objects involved; the granularity of events; and the distance into the future that the new events are scheduled. Both simulators provide speedup. The Breathing <b>Time</b> <b>Buckets</b> algorithm can perform better than the Time Warp based simulator for most cases. However, this latter behaves more consistently as it is not affected by either the distance into the future that new events are scheduled, nor by the network topology used. On the other hand, the performance of Breathing <b>Time</b> <b>Buckets</b> is sensitive to both of these parameters. INTRODUCTION The most important principle that is encountered in [...] ...|$|R
5000|$|Retail {{operations}} however, tend {{to manage}} in weekly <b>time</b> <b>buckets</b> which correspond to consumer shopping cycles. Operational reporting thus tends to use comp criteria where store sales are assessed in weekly buckets.|$|R
40|$|This paper {{deals with}} the problem of {{interpolation}} of discount factors between <b>time</b> <b>buckets.</b> The problem occurs when price and interest rate data of a market segment are assigned to discrete <b>time</b> <b>buckets.</b> A simple criterion is developed in order to identify arbitrage-free robust interpolation methods. Methods closely examined include linear, exponential and weighted exponential interpolation. Weighted exponential interpolation, a method still preferred by some banks and also offered by commercial software vendors, creates several problems and therefore makes simple exponential interpolation a more logical choice. Linear interpolation provides a good approximation of exponential interpolation for a sufficiently dense time grid. [...] ...|$|R
5000|$|Put {{limits for}} each <b>time</b> <b>bucket</b> and monitor {{to stay within}} a {{comfortable}} level around these limits (mainly expressed as a ratio where mismatch may not exceed X% of the total cash outflows for a given time interval) ...|$|E
50|$|Funding report summarises {{the total}} funding needs and sources with the {{objective}} to dispose of a global view where the forward funding requirement lies {{at the time of}} the snapshot. The report breakdown is at business line level to a consolidatedone on the firm-wide level. As a widespread standard, a 20% gap tolerance level is applied in each <b>time</b> <b>bucket</b> meaning that gap within each time period defined can support no more than 20% of total funding.|$|E
40|$|This paper {{addresses}} {{the problem of}} the cost effectiveness of equal lot splitting approaches for various lenghts of the <b>time</b> <b>bucket</b> in planning systems. The results of the research indicate that an equal number of subbatches (lots) per <b>time</b> <b>bucket</b> for all products is inefficient. The magnitude of these inefficiencies depends on the <b>time</b> <b>bucket</b> length of the planning system. An appropriate variable number of subbatches for each product leads to lower costs. We present a heuristic procedure to find such an appropriate number of subbatches per product, given a desired <b>time</b> <b>bucket</b> size...|$|E
5000|$|Spread the {{liability}} maturity profile across many time intervals to avoid concentration {{of most of}} the funding in overnight to few days <b>time</b> <b>buckets</b> (standard prudent practices admit that no more than 20% of the total funding should be in the overnight to one-week period) ...|$|R
50|$|More {{realistic}} {{problems would}} {{also need to}} consider other factors, generally including: smaller <b>time</b> <b>buckets,</b> the calculation of taxes (including the cash flow timing), inflation, currency exchange fluctuations, hedged or unhedged commodity costs, risks of technical obsolescence, potential future competitive factors, uneven or unpredictable cash flows, and a more realistic salvage value assumption, {{as well as many}} others.|$|R
40|$|This paper {{compares the}} very {{fundamental}} concepts behind two approaches to optimistic parallel discrete-event simulation on BSP computers [10]. We refer to (i) asynchronous simulation time advance {{as it is}} realised in the BSP implementation of Time Warp [3], and (ii) synchronous time advance as it is realised in the BSP implementation of Breathing <b>Time</b> <b>Buckets</b> [8]. Our results suggest that asynchronous time advance can potentially lead to more efficient and scalable simulations in BSP...|$|R
40|$|We {{address the}} problem of lot {{splitting}} for various <b>time</b> <b>bucket</b> lengths in MRP systems. Two approaches for lot splitting can be applied: either use the same (equal) or a variable number of subbatches. Equal subbatching strategies have logistical and computational advantages. Literature states that variable batching strategies are only marginal better. However, these results do {{not take into account the}} sensitivity for changes in <b>time</b> <b>bucket</b> length. Managers have reduced <b>time</b> <b>bucket</b> lengths in planning systems. We examine the sensitivity of lot splitting for these changes. Our study reveals that it is not cost-effective to disregard <b>time</b> <b>bucket</b> length when deciding on the number of subbatches. Using the same number of subbatches per <b>time</b> <b>bucket</b> for all products results in substantial cost-differences, where the magnitude is affected by the discontinuity of the total cost curve. For a given <b>time</b> <b>bucket</b> length, a cost difference with a variable number of subbatches per operation of only 2. 1 % can be obtained if an appropriate, equal number of subbatches for each product can be found. Other equal subbatching strategies show much larger cost differences on average, ranging from 4 - 11 %. In order to obtain these results, a new variable subbatch heuristic has been designed. ...|$|E
40|$|Som-theme A Primary {{processes}} within firms We {{address the}} problem of lot splitting for various <b>time</b> <b>bucket</b> lengths in MRP systems. Two approaches for lot splitting can be applied: either use the same (equal) or a variable number of subbatches. Equal subbatching strategies have logistical and computational advantages. Literature states that variable batching strategies are only marginal better. However, these results do {{not take into account the}} sensitivity for changes in <b>time</b> <b>bucket</b> length. Managers have reduced <b>time</b> <b>bucket</b> lengths in planning systems. We examine the sensitivity of lot splitting for these changes. Our study reveals that it is not cost-effective to disregard <b>time</b> <b>bucket</b> length when deciding on the number of subbatches. Using the same number of subbatches per <b>time</b> <b>bucket</b> for all products results in substantial cost-differences, where the magnitude is affected by the discontinuity of the total cost curve. For a given <b>time</b> <b>bucket</b> length, a cost difference with a variable number of subbatches per operation of only 2. 1 % can be obtained if an appropriate, equal number of subbatches for each product can be found. Other equal subbatching strategies show much larger cost differences on average, ranging from 4 - 11 %. In order to obtain these results, a new variable subbatch heuristic has been designed...|$|E
40|$|The {{effect of}} <b>time</b> <b>bucket</b> length on {{the choice of}} a lot-splitting {{approach}} is studied. Due to the continuing pressure to reduce throughput times and increase efficiency, managers apply various measures, such as lot splitting and cycle time reduction programmes, that change the length of the <b>time</b> <b>bucket</b> in their planning systems. It is shown that the choice for a suitable lot-splitting approach depends on the <b>time</b> <b>bucket</b> length used, so these measures cannot be treated independently as costs may increase rapidly without substantial effect on throughput times. Moreover, the current belief in the lot-splitting literature that the improvements of a variable lot-splitting approach are only marginal compared with an equal sub-batch approach is challenged. Some papers estimate the possible flow time improvement to be less than 5 %. It is shown that such statements are not valid as the improvement depends strongly on the type of equal sub-batch strategy applied and the length of the <b>time</b> <b>bucket.</b> Moreover, flow time improvements are accompanied by extra transfer costs, so a cost perspective that includes both effects is more appropriate. The paper shows that equal sub-batching is only effective if information on the <b>time</b> <b>bucket</b> length and on product characteristics is included. At <b>time</b> <b>bucket</b> lengths between one and three days, costs are very sensitive for the type of sub-batching strategy applied. Simulation experiments show that variable sub-batching leads to average cost improvements from 3 to 11 %, depending on the <b>time</b> <b>bucket</b> length and the equal sub-batch strategy applied. Lower costs are due to throughput time reductions and transfer cost savings...|$|E
25|$|Heijunka: {{production}} smoothing presupposes a level {{strategy for the}} MPS and a final assembly schedule developed from the MPS by smoothing aggregate production requirements in smaller <b>time</b> <b>buckets</b> and sequencing final assembly to achieve repetitive manufacturing. If these conditions are met, expected throughput can be equaled to the inverse of takt time. Besides volume, heijunka also means attaining mixed model production, which however may only be feasible through set-up reduction. A standard tool for achieving this is the Heijunka box.|$|R
40|$|Abstract. This paper {{presents}} {{some important}} alternatives for modelling Lot-Sizing and Scheduling Problems. First, {{the accuracy of}} models can improved by using short <b>time</b> <b>buckets,</b> which allow more detailed planning but lead to higher computational effort. Next, valid inequalities make the models tighter but increase their size. Sometimes {{it is possible to}} find a good balance between the size and tightness of a model by limiting a priori the number of valid inequalities. Finally, a special normalization of the variables simplifies the presentation of results and validation of models...|$|R
40|$|Adaptive, parallel, discrete-event-simulation-synchronization algorithm, Breathing <b>Time</b> <b>Buckets,</b> {{developed}} in Synchronous Parallel Environment for Emulation and Discrete Event Simulation (SPEEDES) operating system. Algorithm allows parallel simulations to process events optimistically in fluctuating time cycles that naturally adapt while simulation in progress. Combines best of optimistic and conservative synchronization strategies while avoiding major disadvantages. Algorithm processes events optimistically in time cycles adapting while simulation in progress. Well suited for modeling communication networks, for large-scale war games, for simulated flights of aircraft, for simulations of computer equipment, for mathematical modeling, for interactive engineering simulations, and for depictions of flows of information...|$|R
40|$|This paper {{presents}} a top-down mechanism for coordinating Distributed Discrete Event Simulation (DDES) models using an MRP/ERP system as the federation coordinator. The same MRP/ERP system, which is typically {{used as a}} coordination tool for interactions between complex highly variable manufacturing systems, serves to coordinate and synchronize complex highly variable simulation models of these same systems. This research focuses on enabling each system entity modeled by DDES models to constantly correct its performance with respect to reference trajectories which consist of planned orders {{and the size of}} a <b>time</b> <b>bucket</b> generated by an MRP/ERP system, and trigger a global coordinator which consists of the MRP/ERP system and adapter if necessitated by any discrepancies observed by the entity through simulation models. A global coordinator can synchronize timing of DDES models and provide adaptive time buckets using the cost-based mathematical model and corrected plans using the updated <b>time</b> <b>bucket...</b>|$|E
40|$|This paper {{discusses}} {{an integrated}} model of batch production and maintenance scheduling on a deteriorating machine producing multiple items {{to be delivered}} at a common due date. The model describes the trade-off between total inventory cost and maintenance cost as the increase of production run length. The production run length is a <b>time</b> <b>bucket</b> between two consecutive preventive maintenance activities. The objective function of the model is to minimize total cost consisting of in process and completed part inventory costs, setup cost, preventive and corrective maintenance costs and rework cost. The problem {{is to determine the}} optimal production run length and to schedule the batches obtained from determining the production run length in order to minimize total cost...|$|E
40|$|The {{research}} {{described in}} this paper is directed toward increasing productivity of draglines through automation. In particular, it focuses on the swing-to-dump, dump, and return-to-dig phases of the dragline operational cycle by developing a swing automation system. In typical operation the dragline boom can be in motion for up to 80 % of the total cycle time. This provides considerable scope for improving cycle time through automated or partially automated boom motion control. This paper describes machine vision based sensor technology and control algorithms under development {{to solve the problem}} of continuous real <b>time</b> <b>bucket</b> location and control. Incorporation of this capability into existing dragline control systems will then enable true automation of dragline swing and dump operations...|$|E
50|$|The third {{point is}} that the work of music is defined not only by its content but also by the {{behavior}} it elicits from the audience. In the case of Stravinsky's Rite of Spring, this would consist of widespread dissatisfaction leading up to violent riots. In Cage's 4′33″, the audience felt cheated by having to listen to no composed sounds from the performer. Nevertheless, in 4′33″ the audience contributed the bulk of the musical material of the piece. Since the piece consists of exclusively ambient noise, the audience's behavior, their whispers and movements, are essential elements that fill the above-mentioned <b>time</b> <b>buckets.</b>|$|R
40|$|AbstractThe main {{problems}} of the design of large-scale call center intelligent scheduling system were given firstly; we proposed and testified that using four BP Neural Network models separately in different <b>time</b> <b>buckets</b> can improve the precision of telephone traffic greatly. And the {{problems of}} the classes who have discontinuous working time when applied in Particle Swarm Optimization (PSO) were addressed in this paper. The critical codes were protected by encapsulation as dynamic link library (DLL), the difficulties of using Java Native Access calling C++ codes in java environment were solved too. Practices showed that the large-scale calling center intelligent scheduling system can allocate the human resources scientifically, improve the standard of service and productivity greatly...|$|R
40|$|Abstract. Speed and {{timeliness}} {{are required}} {{more than ever}} to achieve operational excellence in manufacturing and logistics systems. This can only be achieved with quality real-time information, accessible at all relevant stages in the value chain, to improve decision making. Recent development and application of technology allows the real-time capture and dissemination of information. Most planning concepts today are based on batches and <b>time</b> <b>buckets</b> in the planning and scheduling phases. This paper will discuss opportunities in real-time control and focus specifically on {{the link between the}} manufacturing processes and the supply chain processes, and how intelligent support in decision making can contribute. Cases from the manufacturing, food and retail sectors facilitate the discussion...|$|R
40|$|This paper {{investigates the}} effect of demand {{aggregation}} on the performance measures of an inventory system controlled by a (r, Q) policy. Demand usage data is available at different time scales, i. e., daily, weekly, monthly etc., and forecasting is based on these time scales. Using forecasts, appropriate lead time demand models are constructed and used in optimization procedures. The question being investigat-ed is what effect the forecasting <b>time</b> <b>bucket</b> has {{on whether or not}} the inventory control model meets planned performance. A simulation model is used to compare performance under different demand ag-gregation levels. The simulation model of the optimized (r, Q) inventory system is run for the planning horizon and the supply chain operational performance measures like ready rate, expected back order etc., are collected. Subsequently, {{the effect of}} aggregating the demand and planning accordingly is analyzed based on the simulated supply chain’s operational performance. ...|$|E
40|$|The key success {{factor for}} {{material}} requirements planning (MRP), synchronous manufacturing (SYN), and just-in-time (JIT) {{is found in}} the management of capacity. Two case studies of bottleneck management are presented, which are highly simplified situation sketches of real-life problems. Three companies located in Belgium were closely followed during the introduction of JIT. All simulations are based on the XCELL Software. The first case study involves reducing the <b>time</b> <b>bucket</b> in an MRP system. The simulations demonstrate that the success of JIT-kanban depends on how well a company is able to manage capacity. The 2 nd case study explores kanban and changes in the master production schedule. This case illustrates that a JIT-kanban system is very sensitive to changes in the production plan. The kanban planner has to react to problems after they arise, whereas SYN would have discovered the bottleneck problem in advance so that the planner could anticipatestatus: publishe...|$|E
40|$|Temporal demand {{aggregation}} {{has been}} shown in the academic literature to be an intuitively appealing and effective approach to deal with demand uncertainty for fast moving and intermittent moving items. There are two different types of temporal aggregation: non-overlapping and overlapping. In the former case, the time series are divided into consecutive non-overlapping buckets of time where the length of the <b>time</b> <b>bucket</b> equals the aggregation level. The latter case is similar to a moving window technique where the window's size is equal to the aggregation level. At each period, the window is moved one step ahead, so the oldest observation is dropped and the newest is included. In a stock-control context, the aggregation level is generally set to equal the lead-time. In this paper, we analytically compare the statistical performance of the two approaches. By means of numerical and empirical investigations, we show that unless the demand history is short, there is a clear advantage of using overlapping blocks instead of the non-overlapping approach. It is also found that the margin of this advantage becomes greater for longer lead-times...|$|E
40|$|One way to {{organize}} workers that lies between traditional assembly lines, where workers are specialists, and craft assembly, where workers are generalists, are "bucket brigades. " We describe how one firm used bucket brigades {{as an intermediate}} strategy to migrate from craft assembly to assembly lines. The adoption of bucket brigades led to a narrowing of tasks for each worker and thus accelerated learning. The increased production more than compensated for the time lost when workers walk back to get more work, which was significant in this implementation. To understand the trade-offs in migrating from craft to assembly lines, we extend the standard model of bucket brigades to capture hand-off and walk-back <b>times.</b> <b>bucket</b> brigades, assembly line, work sharing, dynamical system, self-organizing system...|$|R
40|$|Large spatial {{datasets}} often {{represent a}} number of spatial point processes generated by distinct entities or classes of events. When crossed with covariates, such as discrete <b>time</b> <b>buckets,</b> this can quickly result in a data set with millions of individual density estimates. Applications that require simultaneous access to a substantial subset of these estimates become resource constrained when densities are stored in complex and incompatible formats. We present a method for representing spatial densities along the nodes of sparsely populated trees. Fast algorithms are provided for performing set operations and queries on the resulting compact tree structures. The speed and simplicity of the approach is demonstrated on both real and simulated spatial data. Comment: 9 pages, 3 figures, 5 table...|$|R
40|$|A novel access {{structure}} for similarity search in metric data, called Similarity Hashing (SH), is proposed. Its multi-level hash structure of separable buckets on each level supports easy insertion and bounded search costs, because at most one bucket {{needs to be}} accessed at each level for range queries up to a pre-dened value of search radius. At the same time, the number of distance computations is always signicantly reduced by use of pre-computed distances obtained at insertion <b>time.</b> <b>Buckets</b> of static les can be arranged {{in such a way}} that the I/O costs never exceed the costs to scan a compressed sequential file. Experimental results demonstrate that the performance of SH is superior to the available tree-based structures. Contrary to tree organizations, the SH structure is suitable for distributed and parallel implementations...|$|R
40|$|Synchronous Parallel Environment for Emulation and Discrete-Event Simulation (SPEEDES) is {{a unified}} {{parallel}} simulation environment. It supports multiple-synchronization protocols without requiring users to recompile their code. When a SPEEDES simulation runs on one node, all the extra parallel overhead is removed automatically at run time. When the same executable runs in parallel, the user preselects the synchronization algorithm {{from a list}} of options. SPEEDES currently runs on UNIX networks and on the California Institute of Technology/Jet Propulsion Laboratory Mark III Hypercube. SPEEDES also supports interactive simulations. Featured in the SPEEDES environment is a new parallel synchronization approach called Breathing Time Buckets. This algorithm uses some of the conservative techniques found in <b>Time</b> <b>Bucket</b> synchronization, along with the optimism that characterizes the Time Warp approach. A mathematical model derived from first principles predicts the performance of Breathing Time Buckets. Along with the Breathing Time Buckets algorithm, this paper discusses the rules for processing events in SPEEDES, describes the implementation of various other synchronization protocols supported by SPEEDES, describes some new ones for the future, discusses interactive simulations, and then gives some performance results...|$|E
40|$|The {{introduction}} of mother-bonded calf rearing into organic dairy farming faces some problems. Regarding the cow, the disturbed milk ejection {{seems to be}} the most important one. We compared two groups of cows, which had permanent (Kp, n= 11) or temporarily (Kt,n= 13) contact to their calves until the 90 th day p. p. with cows separated from their offspring within 24 h after calving (Ko, n= 24). Since suckling following the machine milking strongly influences milk ejection, the Kt-cows were allowed to meet their calves 15 minutes before the usual milking times (milking interval 10 : 14 hours) in a separated area. Milk flow curves were recorded at the 6 th day after calving and afterwards every fortnight at least until the 90 th day of lactation. At the same <b>time</b> <b>bucket</b> milk samples of each cow were collected. Results showed clear differences between the groups with and without calf contact. In these groups, the milk yield gained by machine milking was much lower (up to 10 kg per milking, p< 0. 001) and contained less fat (- 1...|$|E
40|$|Starting from a {{discrete}} event simulation model of real emergency facilities, {{the present paper}} deals with allocation scheduling in operating theatres framed to day care patient’s arrival rate and pathological circumstances. It studies the impact of forecast and resources rationalization and calls and optimal arrangement for bed assignment and operating room services. Different types of patient and multiple servers, which are either specialist and/or cross trained, will be analyzed. The real-life data of a daily based typical surgical care centre at the University Hospital of Cork (Ireland) have been tested and evaluated. A weekly <b>time</b> <b>bucket</b> has been analyzed in planning. Moreover, strategic system and human resources flexibility {{are going to be}} investigated making comparison with the industrial counterpart. The impact of replacing dedicated servers by flexible ones will be evaluated. Optimal assessments will be proposed. Moreover, the authors are going to suggest a viable tool, i. e., synoptic prospect, which can help managers to identify the right level of flexibility in health care services, hence enabling them to improve system performance in accordance with strategic objectives and forecast and overcrowding...|$|E
40|$|Abstract—an {{analysis}} of the supply chain model and solution for IC industry, including foundry, assembly, and testing, is studied. The studied system focus at contract manufacturing service (CMS) in Taiwan and the theme of supply chain management is building upon the detail execution level but under the long term planning. A perspective contrast between <b>time</b> <b>buckets</b> with <b>time</b> line is propounded for distinguishing the difference of the existing techniques for analyzing and the fissures of the derived solutions. In this study we also propose a scheduling approach of objective driven simulation which contains the properties of Adjust and Revise’’. The approach conducts for rescheduling to achieve the goal of bridging the gap between planning and execution, or theoretical and practical. The related research report has been accomplished in an accompany paper [1]. Keywords—contract manufacturing service, rescheduling, objective driven simulation...|$|R
5000|$|The {{second point}} made by 4′33″ {{concerns}} duration. According to Cage, duration {{is the essential}} building block of all of music. This distinction is motivated {{by the fact that}} duration is the only element shared by both silence and sound. As a result, the underlying structure of any musical piece consists of an organized sequence of [...] "time buckets". They could be filled with either sounds, silence or noise; where neither of these elements is absolutely necessary for completeness. In the spirit of his teacher Schoenberg, Cage managed to emancipate the silence and the noise to make it an acceptable or perhaps even integral part of his music composition. 4′33″ serves as a radical and extreme illustration of this concept, asking that if the <b>time</b> <b>buckets</b> are the only necessary parts of the musical composition, then what stops the composer from filling them with no intentional sounds? ...|$|R
40|$|We {{report on}} a {{comparative}} study {{of the performance of}} shared and distributed memory parallel simulation algorithms on a large-scale military logistics simulation, and describe the nature of the application and its parallelisation in some detail. We demonstrate that the patterns of communication in the simulation were such that a standard implementation of Breathing <b>Time</b> <b>Buckets</b> (BTB) was unable to achieve any speed-up. New variants ofBTB were designed and implemented, {{and we were able to}} achieve a speed-up of 7. 4 compared to a critical path limit of 16. 9. The logistics simulation contained a number of complex data structures and its parallel implementation was highly memory-intensive. The resulting patterns of memory access significantly degraded the shared memory simulation performance relative to the equivalent distributed memory version. These results cast doubt on the effectiveness of the current generation of shared memory parallel computers in dealing with optimistic simulations of large, dynamic scenarios. ...|$|R
