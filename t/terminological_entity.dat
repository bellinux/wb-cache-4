1|7|Public
40|$|After an {{introduction}} containing {{definitions of the}} <b>terminological</b> <b>entity</b> ‘object’ from national and international standards, a classification of types of objects is suggested followed by a brief statement {{of the nature of}} objects and their forms of representation. Hereafter the question ‘what can we know about objects’ is posed and the different types of objects are described and illustrated by examples. Another central question is why knowledge about objects should be stored in a terminological databank (TDB). As it is argued, objects are units of knowledge and as such, they are central units in professional communication in an array of fields of knowledge such as history and architecture. Finally, possible data categories are proposed to meet the requirements of professional knowledge storage and transfer. In the conclusion, the main points of the paper are presented in summarised form...|$|E
40|$|This paper {{examines}} an idea we call terminological acquaintance, which {{considers the}} importance of contextual information for various applications in NLP. The importance of contextual information is not new to NLP, but it has rarely been considered in relation to <b>terminological</b> <b>entities.</b> <b>Terminological</b> contexts, however, are vast information sources waiting to be tapped, and are even more useful for domain-specific applications than for general language ones. We focus specifically on the relationships between these contexts and their neighbouring terms, and on how the relevant information from them can best be captured. We give examples of how such information can be clustered, and how this {{can be used for}} applications such as domain-specific lexical ontology development...|$|R
40|$|Abstract Background There {{have been}} a number of recent efforts (e. g. BioCatalogue, BioMoby) to {{systematically}} catalogue bioinformatics tools, services and datasets. These efforts rely on manual curation, making it difficult to cope with the huge influx of various electronic resources that have been provided by the bioinformatics community. We present a text mining approach that utilises the literature to automatically extract descriptions and semantically profile bioinformatics resources to make them available for resource discovery and exploration through semantic networks that contain related resources. Results The method identifies the mentions of resources in the literature and assigns a set of co-occurring <b>terminological</b> <b>entities</b> (descriptors) to represent them. We have processed 2, 691 full-text bioinformatics articles and extracted profiles of 12, 452 resources containing associated descriptors with binary and tf*idf weights. Since such representations are typically sparse (on average 13. 77 features per resource), we used lexical kernel metrics to identify semantically related resources via descriptor smoothing. Resources are then clustered or linked into semantic networks, providing the users (bioinformaticians, curators and service/tool crawlers) with a possibility to explore algorithms, tools, services and datasets based on their relatedness. Manual exploration of links between a set of 18 well-known bioinformatics resources suggests that the method was able to identify and group semantically related entities. Conclusions The results have shown that the method can reconstruct interesting functional links between resources (e. g. linking data types and algorithms), in particular when tf*idf-like weights are used for profiling. This demonstrates the potential of combining literature mining and simple lexical kernel methods to model relatedness between resource descriptors in particular when there are few features, thus potentially improving the resource description, discovery and exploration process. The resource profiles are available at [URL] </p...|$|R
40|$|In {{this paper}} we {{put forward a}} {{mathematical}} and statistical model for measuring the “compatibility” between jobs and university degrees {{on the basis of}} professional competences. The model is aimed at comparing the work requirements and professional counterparts achievable at school. Even if the basic concepts might be valid for the analysis of outer educational realms, our model is devoted to the analy-sis of highly qualified jobs, and in special those “from technical to managerial positions” that may be taught at university. Our model is enough general, too, to frame various studies on the job market and clarify the <b>terminological</b> and opera-tional <b>entities</b> of occupations...|$|R
40|$|The lack of {{parallel}} corpora and linguistic resources for many languages and domains {{is one of}} the major obstacles for the further advancement of automated translation. A possible solution is to exploit comparable corpora (non-parallel bi- or multi-lingual text resources) which are much more widely available than parallel translation data. Our presented toolkit deals with parallel content extraction from comparable corpora. It consists of tools bundled in two workflows: (1) alignment of comparable documents and extraction {{of parallel}} sentences and (2) extraction and bilingual mapping of terms and named entities. The toolkit pairs similar bilingual comparable documents and extracts parallel sentences and bilingual <b>terminological</b> and named <b>entity</b> dictionaries from comparable corpora. This demonstration focuses on the English, Latvian, Lithuanian, and Romanian languages...|$|R
40|$|Abstract. There {{have been}} a number of recent efforts (e. g. BioCatalogue, BioMOBY, etc.) to {{systematically}} catalogue bioinformatics tools, services and datasets. These efforts mostly rely on manual curation and are unable to cope with the huge influx of various electronic resources, which consequently result in their unavailability to the community. We present a text mining approach that utilizes the literature to extract and semantically profile bioinformatics resources. Our method identifies the mentions of resources in the literature and assigns a set of co-occurring <b>terminological</b> and ontological <b>entities</b> (descriptors) to represent them. Since such representations can be extremely sparse, we use kernel metrics based on lexical term/descriptor similarities to identify semantically related resources. Resources are then either clustered or linked into a network, providing the users (bioinformaticians and service/tool crawlers) with a possibility to explore tools, services and datasets based on their relatedness, thus potentially improving the resource discovery process...|$|R
40|$|QWERTY is a knowledge?based {{system which}} maps {{syntactic}} representations of sentences cinto corresponding semantic structures; the mapping process is based: {{on the notion}} of "case?frame". The main feature of this process is the "intelligent" (knowledge?based) selection of the case?frame which drives the representation process for the sentence at hand: the selection process is caccomplished through previous testing of a "conceptual dictionary", where real?world entities corresponding to lexical entries are represented together with their semantic properties in a network?like structure. This allows a case-frame to be chosen according to the linguistic context in which the verb appears; the case?frame is thus representative not of the verb itself, but of a contextually bound instance of the verb. As a side-effect, those forms of verb sense ambiguities which are reflected in case-structure difference can then be resolved. The linguistic domain of application of the system is that of technical?descriptive language. The system is bui 1 t on the observation that paradigmatic sentences of technical descriptive language convey <b>terminological</b> knowledge about <b>entities</b> of the domain of discourse through a description of "spatio?temporally permanent states of affairs" where properties are true of these entities. As a result of the interpretation, the knowledge conveyed by these sentences is encoded in a language specifically oriented to the representation of terminological knowledge. This work {{is part of a larger}} research project whose long term goal is the construction of a system for automatic knowledge acquisition from descriptive texts such as technical reports or appliances user 2 ̆ 7 s guides...|$|R
40|$|The {{identification}} and normalisation of biomedical entities from {{the scientific literature}} {{has a long tradition}} and a number of challenges have contributed to the development of reliable solutions. Increasingly patient records are processed to align their content with other biomedical data resources, but this approach requires analysing documents in different languages across Europe [1, 2]. The CLEF-ER challenge has been organized by the Mantra project partners to improve entity recognition (ER) in multilingual documents. Several corpora in different languages, i. e. Medline titles, EMEA documents and patent claims, have been prepared to enable ER in parallel documents. The participants have been ask to annotate entity mentions with concept unique identifiers (CUIs) in the documents of their preferred non-English language. The evaluation determines the number of correctly identified entity mentions against a silver standard (Task A) and the performance measures for the identification of CUIs in the non-English corpora. The participants could make use of the prepared <b>terminological</b> resources for <b>entity</b> normalisation and of the English silver standard corpora (SSCs) as input for concept candidates in the non-English documents. The participants used different approaches including translation techniques and word or phrase alignments apart from lexical lookup and other text mining techniques. The performances for task A and B was lower for the patent corpus in comparison to Medline titles and EMEA documents. In the patent documents, chemical entities were identified at higher performance, whereas the other two document types cover a higher portion of medical terms. The number of novel terms provided from all corpora is currently under investigation. Altogether, the CLEF-ER challenge demonstrates the performances of annotation solutions in different languages against an SSC...|$|R

