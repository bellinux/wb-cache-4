734|1239|Public
25|$|The uniform {{distribution}} {{is useful for}} sampling from arbitrary distributions. A general method is the inverse transform sampling method, which uses the cumulative distribution function (CDF) of the target random variable. This method is very useful in theoretical work. Since simulations using this method require inverting the CDF of the <b>target</b> <b>variable,</b> alternative methods have been devised for the cases where the cdf is not known in closed form. One such method is rejection sampling.|$|E
2500|$|Mental illness rarely {{stands alone}} when {{analyzing}} {{the risk factors}} associated with incarceration and recidivism rates. The American Psychological Association recommends a holistic approach to reducing recidivism rates among offenders by providing [...] "cognitive– behavioral treatment focused on criminal cognition" [...] or [...] " [...] services that <b>target</b> <b>variable</b> risk factors for high-risk offenders" [...] due to the numerous intersecting risk factors experienced by mentally ill and non-mentally ill offenders alike.|$|E
5000|$|Given an {{observed}} variable [...] and a <b>target</b> <b>variable</b> , a generative model {{defines the}} probabilities [...] and [...] separately. In order {{to predict the}} unobserved <b>target</b> <b>variable,</b> , Bayes’ theorem, ...|$|E
40|$|Many {{existing}} explanation {{methods in}} Bayesian networks, such as Maximum a Posteriori (MAP) assignment and Most Probable Explanation (MPE), generate complete assignments for <b>target</b> <b>variables.</b> A priori, {{the set of}} <b>target</b> <b>variables</b> is often large, {{but only a few}} of them may be most relevant in explaining given evidence. Generating explanations with all the <b>target</b> <b>variables</b> is hence not always desirable. This paper addresses the problem by proposing a new framework called Most Relevant Explanation (MRE), which aims to automatically identify the most relevant <b>target</b> <b>variables.</b> We will also discuss in detail a specific instance of the framework that uses generalized Bayes factor as the relevance measure. Finally we will propose an approximate algorithm based on Reversible Jump MCMC and simulated annealing to solve MRE. Empirical results show that the new approach typically finds much more concise explanations than existing methods...|$|R
40|$|In the {{conventional}} approaches for supervised parametric learning, relations be-tween data and <b>target</b> <b>variables</b> are provided through training sets consisting of pairs of corresponded data and <b>target</b> <b>variables.</b> In this work, we describe a new learning scheme for parametric learning, {{in which the}} <b>target</b> <b>variables</b> y can be modeled with a prior model p(y) and the relations between data and <b>target</b> <b>variables</b> are estimated with p(y) {{and a set of}} uncorresponded data X in train-ing. We term this method as learning with target priors (LTP). Specifically, LTP learning seeks parameter θ that maximizes the log likelihood of fθ(X) on a uncorresponded training set with regards to p(y). Compared to {{the conventional}} (semi) supervised learning approach, LTP can make efficient use of prior knowl-edge of the <b>target</b> <b>variables</b> in the form of probabilistic distributions, and thus re-moves/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efficiently imple-mented and deployed in tasks where running efficiency is critical. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video. ...|$|R
50|$|According to O'Brien and Marakas, {{optimization}} {{analysis is}} a more complex extension of goal-seeking analysis. Instead of setting a specific target value for a variable, {{the goal is to}} find the optimum value for one or more <b>target</b> <b>variables,</b> given certain constraints. Then one or more other variables are changed repeatedly, subject to the specified constraints, until you discover the best values for the <b>target</b> <b>variables.</b>|$|R
5000|$|The arcs from {{terminal}} nodes to the <b>target</b> <b>variable</b> nodes are weighted (terminal nodes are nodes directly {{connected to}} the <b>target</b> <b>variable</b> nodes). The weight is the conditional mutual information due to the arc.|$|E
5000|$|In {{the case}} in of a {{training}} set has two predictor variables, x and y and the <b>target</b> <b>variable</b> has two categories, positive and negative. Given a new case with predictor values x=6, y=5.1, how is the <b>target</b> <b>variable</b> computed? ...|$|E
50|$|In {{data mining}} tools (for multivariate {{statistics}} and machine learning), the depending variable is assigned {{a role as}} <b>target</b> <b>variable</b> (or in some tools as label attribute), while a dependent variable may be assigned a role as regular variable. Known values for the <b>target</b> <b>variable</b> are provided for the training data set and test data set, but should be predicted for other data. The <b>target</b> <b>variable</b> is used in supervised learning algorithms but not in non-supervised learning.|$|E
30|$|More in detail, in 3 D box {{detection}} approaches [5, 6], since 2 D BB and 3 D box are detected, {{there are}} totally 8 annotated <b>target</b> <b>variables,</b> i.e., 4 for 2 D BB, 4 for 3 D box dimension and orientation, height, width, length and orientation at Y-axis in camera coordinate. Meanwhile, our proposed method estimates only 2 D BB and distance, {{and there are}} only 5 <b>target</b> <b>variables,</b> i.e., (x_min^i, y_min^i, x_max^i, y_max^i, z).|$|R
40|$|We {{describe}} {{a set of}} supervised machine learning experiments centering {{on the construction of}} statistical models of WH-questions. These models, which are built from shallow linguistic features of questions, are employed to predict <b>target</b> <b>variables</b> which represent a user's informational goals. We report on different aspects of the predictive performance of our models, including the influence of various training and testing factors on predictive performance, and examine the relationships among the <b>target</b> <b>variables...</b>|$|R
40|$|The {{introduction}} of inflation targeting {{has led to}} major progress in practical monetary policy. Nevertheless, inflation-targeting central banks can make substantial additional progress by being more specific, systematic, and transparent about their operational objectives (in the form of using an explicit intertemporal loss function), their forecasts (in the form of deciding on optimal projections of the instrument rate and the <b>target</b> <b>variables),</b> and their communication (in the form of announcing optimal projections of the instrument rate and <b>target</b> <b>variables)</b> ...|$|R
5000|$|Introduced in CART, {{variance}} {{reduction is}} often employed {{in cases where}} the <b>target</b> <b>variable</b> is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied. The variance reduction of a node [...] is defined as the total reduction of the variance of the <b>target</b> <b>variable</b> [...] due to the split at this node: ...|$|E
50|$|Assuming {{positive}} orientation, a {{scoring rule}} {{is considered to}} be strictly proper, if the value of the expected score loss is positive for all possible forecasts. In other words, based on a strictly proper score rule, a forecasting scheme must score best, if it suggests the <b>target</b> <b>variable</b> as the forecast, and if it scores best, the suggested forecast must be the <b>target</b> <b>variable.</b>|$|E
5000|$|The {{expected}} score loss is {{the difference}} between the expected score for the <b>target</b> <b>variable</b> and the forecast: ...|$|E
5000|$|They {{have to be}} {{projected}} {{during the}} IMF Financial Programming exercise in order to set the desired levels for the <b>target</b> <b>variables</b> which are: ...|$|R
50|$|One high-dimensional {{generalization}} scheme which maximizes {{the mutual}} information between the joint distribution and other <b>target</b> <b>variables</b> {{is found to}} be useful in feature selection.|$|R
40|$|Abstract—In this paper, the {{implementation}} of a rule-based intuitive reasoner is presented. The implementation included two parts: the rule induction module and the intuitive reasoner. A large weather database was acquired as the data source. Twelve weather variables from those data were chosen as the “target variables” whose values were predicted by the intuitive reasoner. A “complex” situation was simulated by making only subsets of the data available to the rule induction module. As a result, the rules induced were based on incomplete information with variable levels of certainty. The certainty level was modeled by a metric called "Strength of Belief", which was assigned to each rule or datum as ancillary information about the confidence in its accuracy. Two techniques were employed to induce rules from the data subsets: decision tree and multi-polynomial regression, respectively for the discrete and the continuous type of <b>target</b> <b>variables.</b> The intuitive reasoner was tested for its ability to use the induced rules to predict the classes of the discrete <b>target</b> <b>variables</b> and the values of the continuous <b>target</b> <b>variables.</b> The intuitive reasoner implemented two types of reasoning: fast and broad where, by analogy to human thought, the former corresponds to fast decision making and the latter to deeper contemplation [...] For reference, a weather data analysis approach which had been applied on similar tasks was adopted to analyze the complete database and create predictive models for the same 12 <b>target</b> <b>variables.</b> The values predicted by the intuitive reasoner and the reference approach were compared with actual data. The intuitive reasoner reached near- 100 % accuracy for two continuous <b>target</b> <b>variables.</b> For the discrete <b>target</b> <b>variables,</b> the intuitive reasoner predicted at least 70 % as accurately as the reference reasoner. Since the intuitive reasoner operated on rules derived from only about 10 % of the total data, it demonstrated the potential advantages in dealing with sparse data sets as compared with conventional methods. Keywords—Artificial intelligence, intuition, knowledge acquisition, limited certainty. I...|$|R
50|$|Terms are now {{split into}} {{acronyms}} of CONTROL VARIABLE + BREATH SEQUENCE + TARGETING SCHEME. As in PC-CMV, Pressure Controlled Continuous Mandatory Ventilation. The term trigger (commonly flow or pressure) denotes the criteria that starts inspiration and cycle denotes the criteria that stops it. The <b>target</b> <b>variable</b> {{should not be}} confused with the cycle variable or the control variable. The <b>target</b> <b>variable</b> only sets an upper limit for pressure, volume or flow.|$|E
5000|$|Decision rules can be {{generated}} by constructing association rules with the <b>target</b> <b>variable</b> on the right. They can also denote temporal or causal relations.|$|E
5000|$|...Here Information is maximized {{about one}} <b>target</b> <b>variable</b> but {{minimized}} about another, allowing {{to learn a}} representation that is informative about selected aspects of data. Formally ...|$|E
40|$|This study {{argues that}} {{although}} scenic beauty, preference, and restoration are correlated {{due to their}} functional significance over evolution, they still can be distinguished from one another within natural landscapes. A total of 274 undergraduate students reported their responses with scenic beauty, prefer-ence, and restoration as the <b>target</b> <b>variables</b> while viewing 48 landscape slides of six biomes. In addition, {{a group of three}} judges evaluated three physical features presented in the landscape slides as controlling and descrip-tor variables, described as “complexity, ” “openness, ” and “water features. ” Statistical analyses showed that (a) the three <b>target</b> <b>variables</b> were all signifi-cantly and highly (rs> 0. 94, p < 0. 05) correlated; (b) the only mediated relationship among the <b>target</b> <b>variables</b> was preference mediating scenic beauty and restoration; and (c) scenic beauty and preference versus restora-tion could be distinguished from each other with respect to the types of natu-ral landscapes and the three physical features...|$|R
50|$|The {{distinction}} between {{the various types of}} monetary policy lies primarily with the set of instruments and <b>target</b> <b>variables</b> that are used by the monetary authority to achieve their goals.|$|R
40|$|It {{is argued}} that {{inflation}} targeting is best understood as a commitment to a targeting rule rather than an instrument rule, either a general targeting rule (explicit objectives for monetary policy) or a specific targeting rule (a criterion for (the forecasts of) the <b>target</b> <b>variables</b> to be fulfilled), essentially the equality of the marginal rates of transformation and substitution between the <b>target</b> <b>variables.</b> <b>Targeting</b> rules allow the use of judgment and extra-model information, are more robust and easier to verify than optimal instrument rules, and they can nevertheless bring the economy close to the socially optimal equilibrium. ...|$|R
5000|$|The {{increase}} in conditional MI of the <b>target</b> <b>variable</b> after building the net equals to {{the sum of}} the {{increase in}} conditional MI in all layers.|$|E
5000|$|Expected {{score is}} the {{expected}} {{value of the}} scoring rule over all possible values of the <b>target</b> <b>variable.</b> For example, for a continuous random variable we have ...|$|E
50|$|Decision tree {{learning}} is a method commonly used in data mining. The {{goal is to create}} a model that predicts the value of a <b>target</b> <b>variable</b> based on several input variables. An example is shown in the diagram at right. Each interior node corresponds to one of the input variables; there are edges to children for each of the possible values of that input variable. Each leaf represents a value of the <b>target</b> <b>variable</b> given the values of the input variables represented by the path from the root to the leaf.|$|E
40|$|The Extended Global Cardinality Constraint (EGCC) is an {{important}} component of constraint solving systems, since it is very widely used to model diverse problems. The literature contains many different versions of this constraint, which trade strength of inference against computational cost. In this paper, I focus on the highest strength of inference usually considered, enforcing general-ized arc consistency (GAC) on the <b>target</b> <b>variables.</b> This work is an extensive empirical survey of algo-rithms and optimizations, considering both GAC on the <b>target</b> <b>variables,</b> and tightening the bounds of the cardinality variables. I evaluate a number of key techniques from the literature, and report im...|$|R
40|$|Abstract — Factor graphs are {{a general}} {{estimation}} framework {{that has been}} widely used in computer vision and robotics. In several classes of problems a natural partition arises among variables involved in the estimation. A subset of the variables are actually of interest for the user: we call those <b>target</b> <b>variables.</b> The remaining variables are essential for the for-mulation of the optimization problem underlying maximum a posteriori (MAP) estimation; however these variables, that we call support variables, are not strictly required as output of the estimation problem. In this paper, we propose a systematic way to abstract support variables, defining optimization problems that are only defined over the set of <b>target</b> <b>variables.</b> This abstraction naturally leads {{to the definition of}} smart factors, which correspond to constraints among <b>target</b> <b>variables.</b> We show that this perspective unifies the treatment of heteroge-neous problems, ranging from structureless bundle adjustment to robust estimation in SLAM. Moreover, it enables to exploit the underlying structure of the optimization problem and the treatment of degenerate instances, enhancing both computa-tional efficiency and robustness. I...|$|R
40|$|A static computable general {{equilibrium}} {{model of}} South Africa is adapted to compare new taxes on water demand by two industries, namely forestry, and irrigated field crops. Comparisons are made with respect to both the short and the long run, in terms of three <b>target</b> <b>variables,</b> namely (i) the environment; (ii) the economy; and (iii) equity. Since the taxes on the two industries do not raise {{the same amount of}} revenue, the <b>target</b> <b>variables</b> are calculated per unit of real government revenue raised by the new taxes (also referred to as the marginal excess burdens of the taxes). The model results are robust for moderate values of the water elasticity of demand in the two industries, in both the long and the short run. The tax on irrigated field crops performs better in terms of all three the <b>target</b> <b>variables</b> in the short run. In the long run the tax on irrigated filed crops is better in terms of water saving, but reduces real GDP and the consumption by poor households. ...|$|R
50|$|Decision tree {{learning}} uses {{a decision}} tree (as a predictive model) {{to go from}} observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It {{is one of the}} predictive modelling approaches used in statistics, data mining and machine learning. Tree models where the <b>target</b> <b>variable</b> can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the <b>target</b> <b>variable</b> can take continuous values (typically real numbers) are called regression trees.|$|E
5000|$|Each {{number has}} its own {{precision}} (in bits since MPFR uses radix 2). The floating-point results are correctly rounded to the precision of the <b>target</b> <b>variable,</b> {{in any of the}} four IEEE-754 rounding modes.|$|E
5000|$|It may be {{evaluated}} using the [...] or [...] key, with the user being prompted to enter {{values for the}} included variables. In {{the case of an}} assignment, the <b>target</b> <b>variable</b> receives the result.|$|E
40|$|We {{investigate}} the asymptotic construction of constant-risk Bayesian predictive densities under the Kullback–Leibler risk when the distributions {{of data and}} <b>target</b> <b>variables</b> are different and have a common unknown parameter. It is known that the Kullback–Leibler risk is asymptotically equal to a trace of the product of two matrices: the inverse of the Fisher information matrix for the data and the Fisher information matrix for the <b>target</b> <b>variables.</b> We assume that the trace has a unique maximum point {{with respect to the}} parameter. We construct asymptotically constant-risk Bayesian predictive densities using a prior depending on the sample size. Further, we apply the theory to the subminimax estimator problem and the prediction based on the binary regression model...|$|R
40|$|Does {{simultaneous}} {{classification of}} multiple <b>target</b> <b>variables</b> perform better than building a classifier {{for each of}} the <b>target</b> <b>variables</b> independently? To answer this question we implemented a set of classification techniques for multi-target classification and integrated them into Orange Multitarget, an add-on for Orange, an open-source machine learning framework. Performance of both multi-target (clustering trees, neural networks, PLS) and single-target techniques (e. g. random forests) was tested on multiple datasets, which included datasets with binary class variables and datasets with multinomial class variables. The results do not show an advantage for either of the techniques. We have also observed that increased correlation between class variables does not increase the performance of multi-target techniques when compared to single-target techniques...|$|R
40|$|An {{economic}} tracking portfolio (ETP) is {{a portfolio}} of financial assets whose returns are correlated with some macroeconomic variable of interest. Specifically, an ETP is designed to track revisions to investors' expectations about the <b>target</b> macroeconomic <b>variable.</b> This paper evaluates whether ETPs provide information about expectations of future macroeconomic outcomes, and are thus {{a useful tool for}} conjunctural economic assessment. A set of ETPs is estimated using UK equity returns for three target variables: inflation, industrial production growth, and growth in the volume of retail sales. In sample, it is possible to track all three of the <b>target</b> <b>variables</b> with equity returns. But the out-of-sample results are poor. Although some ETPs retain significant explanatory power, most do not, and in all cases there is a substantial deterioration in the relationship between the ETPs and the <b>target</b> <b>variables.</b> Covariances between equity returns and macroeconomic variables appear to change substantially over time, and the consequent instability in portfolio weights significantly diminishes the usefulness of ETPs for conjunctural analysis. ...|$|R
