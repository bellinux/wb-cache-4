22|99|Public
50|$|In some databases, such as PostgreSQL, when a FROM clause is present, what {{essentially}} {{happens is}} that the <b>target</b> <b>table</b> is joined to the tables mentioned in the fromlist, and each output row of the join represents an update operation for the <b>target</b> <b>table.</b> When using FROM, one should ensure that the join produces at most one output row for each row to be modified. In other words, a target row shouldn't join {{to more than one}} row from the other table(s). If it does, then only one of the join rows will be used to update the target row, but which one will be used is not readily predictable.|$|E
5000|$|Assume {{the marble}} type table above is named marbles_seed and the <b>target</b> <b>table</b> is named marbles. The code that generates the needed 270 rows is:insert into marbles(m.texture, m.appearance, m.shape, m.color_actual)select m.texture, m.appearance, m.shape, m.color_actual from marbles_seed m, numbers n where m.quantity > n.n ...|$|E
5000|$|This {{can occur}} when range locks are not {{acquired}} on performing a SELECT ... WHERE operation.The phantom reads anomaly {{is a special}} case of Non-repeatable reads when Transaction 1 repeats a ranged SELECT ... WHERE query and, between both operations, Transaction 2 creates (i.e. INSERT) new rows (in the <b>target</b> <b>table)</b> which fulfill that WHERE clause.|$|E
50|$|The {{preamble}} also reiterates Germany's 2010 climate <b>targets</b> (see <b>table)</b> and its 2016 Paris Agreement commitment.|$|R
50|$|Security Administrator Tool for Analyzing Networks (SATAN) {{was a free}} {{software}} vulnerability scanner for analyzing networked computers. SATAN {{captured the attention of}} a broad technical audience, appearing in PC Magazine and drawing threats from the United States Department of Justice. It featured a web interface, complete with forms to enter <b>targets,</b> <b>tables</b> to display results, and context-sensitive tutorials that appeared when a vulnerability had been found.|$|R
5000|$|Disable {{triggers}} (disable trigger ...) in the <b>target</b> database <b>tables</b> {{during the}} load: simulate their effect {{as a separate}} step ...|$|R
5000|$|The [...] "quantity" [...] column {{represents}} {{how many}} marbles {{of that type}} we have. The task {{is to create a}} second table holding one row for each marble of that type. Thus, the <b>target</b> <b>table</b> would have the four text columns, and a total of 40 + 20 + 20 + 10 + ... + 10 + 5 = 270 rows.|$|E
50|$|Hashes are {{unsigned}} 32 bit integers, {{and start}} with a value of 5381. For each byte of the key, the current hash is multiplied by 33, then XOR'ed with the current byte of the key. Overflow bits are discarded. Slots and tables are trivially computed from hashes. The <b>target</b> <b>table</b> is simply the lowest eight bits of the hash (i.e. hash modulo 256), and the slot within the table is the remaining bits of the hash modulo the table length (i.e. hash divided by 256 modulo table length).|$|E
50|$|RedBeanPHP is {{different}} from other ORM systems because it requires no configuration in XML, YAML or JSON. It adapts the database schema based {{on the needs of}} the program. All tables and columns are created on-the-fly, without upfront configuration or mapping. It automatically adds columns to tables if necessary and changes the type of the column to match its content requirements. When the developer is done developing and no more schema changes are expected, the schema can be frozen for deployment to production environments. After freezing the database no more schema alterations take place. Relations among tables are mapped in the same way: by convention. For instance, to create a one-to-many relationship between two tables one assigns an array to the property bearing the name of the <b>target</b> <b>table.</b> This automatically creates the table as well as the required columns.|$|E
5000|$|Do all {{validation}} in the ETL layer {{before the}} load: disable integrity checking (disable constraint ...) in the <b>target</b> database <b>tables</b> during the load ...|$|R
40|$|Abstract-This paper proposes an {{efficient}} algorithm for tech-nology mapping <b>targeting</b> <b>table</b> look-up (TLU) blocks. It {{is capable of}} minimizing either the number of TLU’s used or {{the depth of the}} produced circuit. Our approach consists of two steps. First a network of super nodes, is created. Next a Boolean function of each super node with an appropriate don’t care set is decomposed into a network of TLU’s. To minimize the circuit’s depth, several rules are applied on the critical portion of the mapped circuit...|$|R
30|$|These days {{urban design}} of open spaces is {{strongly}} related to bioclimatic techniques and practices. It is here presented the procedure of a bioclimatic study {{by the use of}} simulation tools. The area of an open market place is characterized of decreased human thermal comfort conditions during summer time. The employment of computational fluid dynamics has contributed in the understanding of what interventions should be made at the open space in order to succeed the defined thermal related <b>targets.</b> <b>Table</b> of the proposed rehabilitation explains what the interventions would contribute in the improvement of the local environment.|$|R
5000|$|DROP TABLE IF EXISTS docs;CREATE TABLE docs (line STRING);LOAD DATA INPATH 'input_file' OVERWRITE INTO TABLE docs;CREATE TABLE word_counts ASSELECT word, count(1) AS count FROM (SELECT explode(split(line, '\s')) AS word FROM docs) tempGROUP BY wordORDER BY word;A brief {{explanation}} {{of each of}} the statements is as follows:DROP TABLE IF EXISTS docs;CREATE TABLE docs (line STRING);Checks if table [...] exists and drops it if it does. Creates a new table called [...] with a single column of type [...] called [...]LOAD DATA INPATH 'input_file' OVERWRITE INTO TABLE docs;Loads the specified file or directory (In this case “input_file”) into the table. [...] specifies that the <b>target</b> <b>table</b> to which the data is being loaded into is to be re-written; Otherwise the data would be appended.CREATE TABLE word_counts ASSELECT word, count(1) AS count FROM(SELECT explode(split(line, '\s')) AS word FROM docs) tempGROUP BY wordORDER BY word;The query [...] creates a table called [...] with two columns: [...] and [...] This query draws its input from the inner query [...] This query serves to split the input words into different rows of a temporary table aliased as [...] The [...] groups the results based on their keys. This results in the [...] column holding the number of occurrences for each word of the [...] column. The [...] sorts the words alphabetically.|$|E
30|$|Yakout et al. [19] {{present a}} system called InfoGather for augmenting {{entities}} in a database with information gathered from web tables. As the input data is already structured, no extraction is needed. Their {{ultimate goal is}} to supply new values for existing attributes or to supply new attributes with values for existing entities. For this, the authors propose a strategy that first identifies web tables that match a given <b>target</b> <b>table</b> to be augmented and then selects values in this table {{that can be used to}} supply values and attributes to the entities in the <b>target</b> <b>table</b> based on several similarity models.|$|E
40|$|This paper {{presents}} {{findings from}} interpretative phenomenological interviews about the UX of interactive climate management with six growers and crop consultants. A model of UX of interactive climate management is presented. The findings {{are reported in}} a UX <b>target</b> <b>table,</b> which can {{be the basis for}} future research on UX at work in this domain...|$|E
40|$|The most {{well-known}} origin-destination estimation methods involve entropy maximization or information minimization. However, these methods often require much {{more information than}} is available in reliable form, namely, <b>target</b> trip <b>tables</b> {{and the extent to}} which each origin-destination path uses each link. In this paper, a second version of the Shortest Augmenting Path Estimation method (SHAPE- 2) is introduced, in which <b>target</b> trip <b>tables</b> and path use proportions are not required. In tests to date, SHAPE- 2 has produced good results. It has demonstrated good sensitivity to changes in link loadings on small test networks and, on a real network of special interest to the authors, it performed better than any method previously applied. ...|$|R
50|$|The native Versant Query Language (VQL) {{is similar}} to SQL92. It is a string based {{implementation}} which allows parameterized runtime binding. The difference is that instead of <b>targeting</b> <b>tables</b> and columns, it targets classes and attributes. Other object-oriented elements apply to query processing. For example, a query targeting a super class will return all instances of concrete subclasses that satisfy the query predicate. VOD is a distributed database: a logical database can be composed of many physical database nodes, with queries are performed in parallel. Versant query support includes most of the core concepts found in relational query languages including: pattern matching, join, set operators, orderby, existence, distinct, projections, numerical expressions, indexing, cursors, etc.|$|R
50|$|MALDI is an {{ionization}} technique where {{laser energy}} is absorbed by a matrix to create ions from large molecules without fragmentation. The matrix, typically in excess, {{is mixed with}} the analyze molecule and deposited on a <b>target.</b> A <b>table</b> of matrix compounds, their structures, laser wavelengths typically used, and typical application is shown below.|$|R
40|$|AbstractWhen {{clustering}} the tuples in the <b>target</b> <b>table</b> {{which is}} in a relational database, the prior task is to exactly and effectively calculate the relational distance between tuples. A lot of methods are used today, such as the relational distance measuring based on RIBL 2. However, all these methods fail to consider the differences of similarity between the objects in both non-target table and <b>target</b> <b>table,</b> which stopped them from getting a high clustering accuracy. Using canonical correlation analysis in this paper and setting a weight for each table in the relational database, the weight indicated {{its role in the}} calculation of the distance among target tables. In addition, when calculating the distance between the two clusters to find the center of each cluster, turn the calculation of the distance between clusters into a distance between center points. Experiments show that this method ensures clustering efficiency and improved clustering accuracy...|$|E
40|$|Abstract—Existing {{methods of}} data mining cannot be applied on spatial data because they require spatial {{specificity}} consideration, as spatial relationships. This paper {{focuses on the}} classification with decision trees, which {{are one of the}} data mining techniques. We propose an extension of the C 4. 5 algorithm for spatial data, based on two different approaches Join materialization and Querying on the fly the different tables. Similar works have been done on these two main approaches, the first- Join materialization- favors the processing time in spite of memory space, whereas the second- Querying on the fly different tables- promotes memory space despite of the processing time. The modified C 4. 5 algorithm requires three entries tables: a <b>target</b> <b>table,</b> a neighbor table, and a spatial index join that contains the possible spatial relationship among the objects in the <b>target</b> <b>table</b> and those in the neighbor table. Thus, the proposed algorithms are applied to a spatial data pattern in the accidentology domain. A comparative study of our approach with other works of classification by spatial decision trees will be detailed...|$|E
30|$|Since {{the results}} {{generated}} by sampling are approximate, {{it is important}} to calculate an associated confidence interval. For a simple aggregate query on one table, like SELECT op (expression) FROM table WHERE predicate, it’s easy to achieve online aggregation by continuously sampling from the <b>target</b> <b>table.</b> Standard statistical formulas can help us get unbiased estimators and estimate the confidence interval. A lot of previous work [13, 14, 15, 16] have made great contributions on this problem.|$|E
40|$|The dynamic {{aperture}} of the LHC optics version 5 at injection {{energy has}} been calculated for an opti-mistic error table, {{the so called}} <b>target</b> error <b>table,</b> in which erect and/or skew octupolar components were increased up to values close to realistic estimates. Correction strategies, using octupole spool pieces or the lattice octupoles, have been tested so as to recover, as much as possible, the loss in dynamic aperture...|$|R
50|$|If the {{resulting}} four-kilobyte table size {{is too large}} for a given <b>target</b> platform, the <b>table</b> lookup operation can be performed with a single 256-entry 32-bit (i.e. 1 kilobyte) table {{by the use of}} circular rotates.|$|R
30|$|Workload Source Identification. A user {{identifies}} any existing {{sources of}} workload data, for example, from database monitoring tools. The {{goal is to}} produce a sequence of measurements containing: data being read or written, and the <b>target</b> (i.e., database <b>table).</b>|$|R
40|$|We {{evaluate}} {{the influence of}} mechanical tolerances on the field quality in the LHC dipoles. We show that the most relevant effect is due to tolerances on the coil and on the internal part of the collars. The sensitivities of the field error multipoles on the mechanical tolerances are worked out using a finite element model of the dipole cross section. A MonteCarlo method is used to simulate the overall effect of both collar and coil tolerances on field quality. Correlation between random multipoles is worked out, and a comparison with the <b>target</b> <b>table</b> of the LHC field errors is given...|$|E
40|$|The West Area at the CERN SPS has {{recently}} been rebuilt to provide two versatile secondary and/or tertiary test beams, the X 5 and X 7, which each have been upgraded to a top momentum of 250 GeV/c. In this note we describe the design, operational modes and performance of these new West Area beams. This is a revision of a previous report CERN SL- 99 - 013 EA, updated following the displacement of the T 1 primary <b>target.</b> <b>TABLE</b> OF CONTENTS 1. Introduction [...] . 3 2. General organisation of the West Area [...] . 4 3. The H 3 secondary beam [...] . 4 4. The X 5 beam [...] 11 5. The X 7 beam [...] . ...|$|E
40|$|International audienceIn multi-relational data mining, {{data are}} {{represented}} in a relational form where the individuals of the <b>target</b> <b>table</b> are potentially related to several records in secondary tables in one-to-many relationship. In this paper, we introduce an itemset based framework for constructing variables in secondary tables and evaluating their conditional information for the supervised classification task. We introduce a space of itemset based models in the secondary table and conditional density estimation of the related constructed variables. A prior distribution is defined on this model space, resulting in a parameter-free criterion to assess {{the relevance of the}} constructed variables. A greedy algorithm is then proposed in order to explore the space of the considered itemsets. Experiments on multi-relationalal datasets confirm the advantage of the approach...|$|E
5000|$|A right join is {{employed}} over the <b>Target</b> (the INTO <b>table)</b> and the Source (the USING table / view / sub-query)--where Target is the left table and Source {{is the right}} one. The four possible combinations yield these rules: ...|$|R
40|$|A large {{class of}} data mining {{applications}} involvesdata sets that pertain to multiple entities andrelationship. This {{has led to}} the suggestion of multirelationaldata mining (MRDM) that aims toincorporate and exploit the heterogeneous andsemantically rich relationships that exist amongentity types. Specially, given a database consisting ofmultiple tables linked through foreign key joins, atarget table (that typically represents a certain realworldentity type) and, optionally, a target attribute(e. g. a class label attribute), MRDM aims to discoverpatterns and models spanning all the tables and linksthat either describe or predict the target entity orattribute. In this paper, we study one class of MRDMtask, namely multi-relational association rulemining. For this task, we distinguish two types oftarget tables, namely entity and relationship targettables. For the case of entity <b>target</b> <b>tables,</b> we castthe computation of frequent patterns as multirelationaliceberg cube com-putation and propose anef cient algorithm for it. Then, we study theapplication and peculiar requirements of tar-gettables that are relationship tables. This study revealsa new mining task, dubbed linkage mining, where themere instances of relationships are the objects ofmining. We then show how our multi-relationaliceberg computation algorithm is extended to dolinkage mining. In the end, we present performancestudies of our algorithms...|$|R
30|$|For functions, we have {{developed}} a bytecode rewriting engine {{as well as an}} HTTP based remote call library that takes the partitioning plan generated by the analyzer, injects remote call code at each cut-point, and serializes data between the two locations. This remote call instrumentation is essentially a simplified version of J-Orchestra [15] implemented over HTTP (but is not yet as complete as the original J-Orchestra work). In order to allow for distribution of data entities, we have taken advantage of Oracle’s distributed database management system (DDBMS). This allows for tables remote to a local Oracle DBMS, to be identified and queried for data through the local Oracle DBMS. This is possible by providing a database link (@dblink) between the local and the remote DBMS systems. Once a bidirectional dblink is established, the two databases can execute SQL statements <b>targeting</b> <b>tables</b> from one another. This allows us to use the distribution plan from our analyzer system to perform vertical sharding at the level of database tables. Note that the distributed query engine acts on the deployment of a system after a decision about the placement of tables has been made by our partitioning algorithm. We have provided an Eclipse plugin implementation of the analyzer framework available online [16].|$|R
40|$|Spatial {{data mining}} {{requires}} {{the analysis of}} the interactions in space. These interactions can be materialized using distance tables, reducing spatial data mining to multi-table analysis. However, conventional data mining algorithms consider only one input table where each row is an observation to analyze. Simple relational joins between these tables does not resolve the problem and mislead the results because of the multiple counting of observations. We propose three alternatives of multi-table data mining in the context of spatial data mining. The first makes a hard modification in the conventional algorithm in order to consider those tables. The second is an optimization of the first approach. It pre-computes all join operations and adapts the conventional algorithm. The third re-organizes data into a unique table by completing -not joining- the <b>target</b> <b>table</b> using the existing data in the other tables, then applies any standard data mining algorithm without modification. This article presents these three alternatives. It describes their implementation for classification algorithms and compares their performances. Pages: 127 - 14...|$|E
30|$|We {{discovered}} that a single point mutation in the CH 3 domain and two mutations at cysteine residues within the IgG hinge region could result in Halfbodies as well, {{similar to the ones}} generated with multiple mutations at the CH 3 domain (Table S 1 and Fig. S 1). With the Halfbody format, we demonstrated the conversion of agonistic or partial antagonist molecules to pure antagonists against the cell surface <b>target</b> (<b>Table</b> S 2). In addition, we extended the Halfbody technology to DVD-Ig format to generate Half DVD-Ig molecules for monovalent CD 3 binding. The monovalent CD 3 containing Half DVD-Ig maintained the ability to bind CD 3 but was conditional with regard to their ability to initiate immune synapse formation and mediate T cell activation upon binding to tumor-associated antigen which greatly reduced non-specific cytokine release for CD 3 -mediated T cell redirected cytotoxicity in vitro. Additionally, the half DVD-Ig molecule has good expression in mammalian cells, can be purified to homogeneity, and has improved half-life compared to other monovalent antibody variants (e.g., scFv, BiTE, and Fab). Therefore, we believe that Half DVD-Ig molecular format may provide an additional option for exploring cancer treatment using redirected T cell cytotoxicity with more practical dosing regimens.|$|E
40|$|Classification {{is one of}} {{the most}} popular data mining tasks with a wide range of applications, and lots of {{algorithms}} have been proposed to build accurate and scalable classifiers. Most of these algorithms only take a single table as input, whereas in the real world most data are stored in multiple tables and managed by relational database systems. As transferring data from multiple tables into a single one usually causes many problems, development of multi-relational classification algorithms becomes important and attracts many researchers ’ interests. Existing works about extending Naïve Bayes to deal with multi-relational data either have to transform data stored in tables to mainmemory Prolog facts, or limit the search space to only a small subset of real world applications. In this work, we aim at solving these problems and building an efficient, accurate Naïve Bayesian classifier to deal with data in multiple tables directly. We propose an algorithm named Graph-NB, which upgrades Naïve Bayesian classifier to deal with multiple tables directly. In order to take advantage of linkage relationships among tables, and treat different tables linked to the <b>target</b> <b>table</b> differently, a semantic relationship graph is developed to describe the relationship and to avoid unnecessary joins. Furthermore, to improve accuracy, a pruning strategy is given to simplify the graph to avoid examining too many weakly linked tables. Experimental study on both realworld and synthetic databases shows its high efficiency and good accuracy...|$|E
40|$|We {{present the}} design of a system for {{assembling}} a table from a few example rows by harnessing the huge corpus of information-rich but unstructured lists on the web. We developed a totally unsupervised end to end approach which given the sample query rows — (a) retrieves HTML lists relevant to the query from a pre-indexed crawl of web lists, (b) segments the list records and maps the segments to the query schema using a statistical model, (c) consolidates the results from multiple lists into a unified merged table, (d) and presents to the user the consolidated records ranked by their estimated membership in the target relation. The key challenges in this task include construction of new rows from very few examples, and an abundance of noisy and irrelevant lists that swamp the consolidation and ranking of rows. We propose modifications to statistical record segmen-tation models, and present novel consolidation and ranking techniques that can process input tables of arbitrary schema without requiring any human supervision. Experiments with Wikipedia <b>target</b> <b>tables</b> and 16 million unstructured lists show that even with just three sample rows, our system is very effective at recreating Wikipedia tables, with a mean runtime of around 20 s. 1...|$|R
40|$|The Kepler Mission Science Operations Center (SOC) {{performs}} several critical functions including {{managing the}} ~ 156, 000 target stars, associated <b>target</b> <b>tables,</b> science data compression tables and parameters, {{as well as}} processing the raw photometric data downlinked from the spacecraft each month. The raw data are first calibrated at the pixel level to correct for bias, smear induced by a shutterless readout, and other detector and electronic effects. A background sky flux is estimated from ~ 4500 pixels {{on each of the}} 84 CCD readout channels, and simple aperture photometry is performed on an optimal aperture for each star. Ancillary engineering data and diagnostic information extracted from the science data are used to remove systematic errors in the flux time series that are correlated with these data prior to searching for signatures of transiting planets with a wavelet-based, adaptive matched filter. Stars with signatures exceeding 7. 1 sigma are subjected to a suite of statistical tests including an examination of each star's centroid motion to reject false positives caused by background eclipsing binaries. Physical parameters for each planetary candidate are fitted to the transit signature, and signatures of additional transiting planets are sought in the residual light curve. The pipeline is operational, finding planetary signatures and providing robust eliminations of false positives. Comment: 8 pages, 3 figure...|$|R
5000|$|The shakehand grip is so-named {{because the}} racket is grasped {{as if one}} is {{performing}} a handshake. Though it is {{sometimes referred to as}} the [...] "tennis" [...] or [...] "Western" [...] grip, it bears no relation to the Western tennis grip, which was popularized on the West Coast of the United States in which the racket is rotated 90°, and played with the wrist turned so that on impact the knuckles face the <b>target.</b> In <b>table</b> tennis, [...] "Western" [...] refers to Western nations, for this is the grip that players native to Europe and the Americas have almost exclusively employed.|$|R
