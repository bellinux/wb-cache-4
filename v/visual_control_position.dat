0|8483|Public
40|$|The {{research}} {{presented in}} this paper focuses on high performance <b>visual</b> servoing for <b>controlled</b> <b>positioning</b> on micrometer scale. Visual servoing means that the positioning of a x-y table is performed with a camera instead of encoders and one camera image and image processing should determine the position in the next iteration. With a frame rate of 1000 [fps], a maximum processing time of 1 millisecond is allowed for each image (132 × 45 pixels). The visual servoing task is performed on an OLED substrate, which can be found in displays, with a typical size of 80 by 220 µm. The repetitive pattern of the OLED substrate itself is used as an encoder and thus closing the control loop. We present the design choices made for the image processing algorithms and the experimental setup as well as their limitations...|$|R
40|$|International audienceThe paper {{evaluates the}} {{performance}} and tries to identify the strategies of human operators (HO's) teleoperating a robot along 2 -D-trajectories in simulations of place-like tasks in an obstacle encumbered environment. The experiments utilize computer graphic simulations of a remote robot whose end-effector displacements are dynamically controlled in the X - Y plane by the HO's hand displacements. Four different modes are selected to provide suggested trajectories to the HO's that begin at the initial point of the robot's end-effector and end in the landing zone. For each mode, two different visual frames of reference are used to present the remote scene viewed by the HO: a stationary view of the environment, or {{a view from the}} end-effector of the robot, which produces the sensation of a mobile environment. The performance is described in the various displayed conditions in terms of movement duration, spatial and temporal errors, and energy. The data show that performance depends {{on the nature of the}} cues and instructions provided to the operators. Comparative analysis of the various conditions suggests that <b>visual</b> feedback <b>position</b> <b>control</b> is used when continuous static or dynamic information about the trajectory is provided, while feedforward control, corrected by sampled visual information, is adopted when a self-generated movement can be planned and executed. The data also show that for a given set of execution cues, an equivalent performance is achieved in both visual frames of reference. A fifth mode of trajectory presentation, suggested by the results of the former analysis, has been tested. This mode, which presents a few specific spatial cues (gates) on the screen during the execution of the movement, combines the features for trajectory planning and sampled <b>visual</b> feedback <b>position</b> <b>control.</b> This last mode yields higher performance...|$|R
40|$|This paper {{presents}} direct {{methods for}} vision-based {{control for the}} application of industrial inkjet printing. In this, <b>visual</b> <b>control</b> is designed with a direct coupling between camera measurements and joint motion. Traditional visual servoing commonly has a slow visual update rate and needs an additional local joint controller to guarantee stability. By only using the product as reference and sampling with a high update rate, direct visual measurements are sufficient for <b>controlled</b> <b>positioning.</b> The proposed method is simpler and more reliable than standard motor encoders, despite the tight real-time constraints. This direct <b>visual</b> <b>control</b> method is experimentally verified with a 2 D planar motion stage for micrometer positioning. To achieve accurate and fast motion, a balance is found between frame rate and image size. With a frame rate of 1600 fps and an image size of 160 × 100 pixels we show the effectiveness of the approach...|$|R
40|$|In {{this paper}} we present an {{extensive}} experimental evaluation of adaptive and non-adaptive visual servoing in 3, 6 and 12 Degrees of Freedom, {{comparing it to}} traditional joint feedback control. While the purpose of experiments in most other work have been {{to show that the}} particular algorithm presented indeed also works in practice, we do not focus on the algorithm, but rather on properties important to visual servoing in general. Our main results are: Positioning of a 6 axis PUMA 762 arm is up to 5 times more precise under <b>visual</b> <b>control,</b> than under joint <b>control.</b> <b>Positioning</b> of a Utah/MIT dextrous hand is better under <b>visual</b> <b>control,</b> than under joint control alone. We also found that a trust region based adaptive visual feedback controller is very robust. For m tracked visual features the algorithm can successfully estimate online the m Θ 3, m 3 image Jacobian without any prior information, while carrying out a 3 DOF manipulation task. For 6 and higher DOF manipulation, a reas [...] ...|$|R
40|$|Proposes {{a method}} for <b>visual</b> <b>control</b> of a robotic system which {{does not require the}} {{formulation}} of an explicit calibration between image space and the world coordinate system. Calibration is known to be a difficult and error prone process. By extracting control information directly from the image, the authors free their technique from the errors normally associated with a fixed calibration. The authors demonstrate this by performing a peg-in-hole alignment using an uncalibrated camera to <b>control</b> the <b>positioning</b> of the peg. The algorithm utilizes feedback from a simple geometric effect, rotational invariance, to <b>control</b> the <b>positioning</b> servo loop. The method uses an approximation to the image Jacobian to provide smooth, near-continuous control...|$|R
40|$|Sensors play an {{important}} role in providing feedback to robotic control systems. Vision can be an effective sensing modality due to its speed, low cost, and flexibility. It can also serve as an external sensor that can provide control information for devices that lack internal sensing. Many robot grippers do not have internal sensors. These grippers can be prone to error as they operate under open loop control and usually require a precise model of the environment to be effective. This paper describes 2 <b>visual</b> <b>control</b> primitives that can be used to provide <b>position</b> <b>control</b> for a sensorless robot system. The primitives use a simple visual tracking and correspondence scheme to provide real-time feedback control in the presence of imprecise camera calibrations. Experimental results are shown for the positioning task of locating, picking up, and inserting a bolt into a nut under <b>visual</b> <b>control.</b> Results are also presented for the <b>visual</b> <b>control</b> of a bolt tightening task. 1 Introduction In [...] ...|$|R
50|$|<b>Visual</b> <b>control</b> methods aim to {{increase}} the efficiency and effectiveness of a process by making the steps in that process more visible. The theory behind <b>visual</b> <b>control</b> is that if something is clearly visible or in plain sight, it is easy to remember and keep {{at the forefront of the}} mind. Another aspect of <b>visual</b> <b>control</b> is that everyone is given the same visual cues and so are likely to have the same vantage point.|$|R
50|$|<b>Visual</b> <b>controls</b> {{are meant}} to display the {{operating}} or progress status of a given operation in an easy to see format and also to provide instruction and to convey information. A <b>visual</b> <b>control</b> system must have an action component associated {{with it in the}} event that the visually represented procedures are not being followed in the real production process. Therefore <b>visual</b> <b>controls</b> must also have a component where immediate feedback is provided to workers.|$|R
50|$|Use <b>visual</b> <b>control</b> so no {{problems}} are hidden.|$|R
5000|$|AIP: <b>visual</b> <b>control</b> of {{grasping}} and manipulating hand movements ...|$|R
40|$|In {{this paper}} we present an {{experimental}} evaluation of adaptive and non-adaptive visual servoing in 3, 6 and 12 {{degrees of freedom}} (DOF), comparing it to traditional joint feedback control. While the purpose of experiments in most other work has been {{to show that the}} particular algorithm presented indeed also works in practice, we do not focus on the algorithm, but rather on properties important to visual servoing in general. Our main results are: positioning of a 6 axis PUMA 762 arm is up to 5 times more precise under <b>visual</b> <b>control,</b> than under joint <b>control.</b> <b>Positioning</b> of a Utah/MIT dextrous hand is better under <b>visual</b> <b>control</b> than under joint control by a factor of 2. We also found that a trust-region-based adaptive visual feedback controller is very robust. For £ tracked visual features the algorithm can successfully estimate online the £¥ ¤ 3 (£§ ¦ 3) image Jacobian (J) without any prior information, while carrying out a 3 DOF manipulation task. For 6 and higher DOF manipulation, a rough initial estimate of ¨ is beneficial. We also verified that redundant visual information is valuable. Errors due to imprecise tracking and goal specification were reduced as the number of visual features, £, was increased. Furthermore highly redundant systems allow us to detect outliers in the feature vector, and deal with partial occlusion...|$|R
5000|$|There {{are many}} {{different}} techniques {{that are used to}} apply <b>visual</b> <b>control</b> in the workplace. Some companies use <b>visual</b> <b>control</b> as an organizational tool for materials. A clearly labeled storage board lets the employee know exactly where a tool belongs and what tools are missing from the display board. Another simple example of a common <b>visual</b> <b>control</b> is to have reminders posted on cubicle walls so that they remain in plain sight. Visual signs and signals communicate information that is needed to make effective decisions. These decisions may be safety oriented or they may give reminders as to what steps should be taken to resolve a problem. Most companies use <b>visual</b> <b>controls</b> in one degree or another, many of them not even realizing that the <b>visual</b> <b>controls</b> that they are making have a name and a function in the workplace. Whether it is recognized by the name of [...] "visual control" [...] or not, the fact is that replacing text or number with graphics makes a set of information easier to understand with only a glance, making it a more efficient way of communicating a message.|$|R
5000|$|T.133 - Audio <b>Visual</b> <b>Control</b> Services - Controls for realtime data streams.|$|R
50|$|The Heijunka box allows {{easy and}} <b>visual</b> <b>control</b> of a {{smoothed}} production schedule.|$|R
40|$|Abstract- Unstable {{control systems}} and {{computing}} are two close entities but always very less combined when modeling or designing {{a system which}} needs a good feedback capability. Computer Vision or Machine Vision is field which can exploit the functionality of being real time similar to human eye and the SoC(System on Chip) systems which comes with CPU, GPU and FPGA elements pre-fabricated makes computing real-time algorithm faster than before. Even cameras which are currently being manufactured are being integrated easily into many systems for passive observation or remote viewing. Here the investigation is basically to see if systems can be integrated {{together to form a}} feedback system without external measurement units like IMU(Inertial Measurement Units), accelerometers etc and form a part of the embedded system already present thus reducing more circuitry and increase more stand alone automated control. The main aim is to have the processing unit act as a digital comparator and camera act as an inertial measurement unit and the system dynamics which include servos are controlled through I/O ports of the processing unit. We shall see many different methods which can be used for controlling a self-balancing robot which is an example of an inverted pendulum and inverted pendulum {{is one of the most}} basic forms of an unstable control system. The examples which apply to this principle includes in-line production robots, UAVs, applications based on <b>visual</b> <b>controls</b> for example <b>positioning</b> solar panels in a solar power plant according to the direction of the sun and many others. I...|$|R
5000|$|There are {{two groups}} and seven types of {{application}} in <b>visual</b> <b>controls.</b> Displays group and controls group.|$|R
5000|$|Use {{performance}} metrics and <b>visual</b> <b>controls</b> {{to provide}} rapid feedback to improve real-time decision-making and problem-solving; and ...|$|R
25|$|The gallery, {{which is}} closed to public access, housed the Gallery Organ and the Audio <b>Visual</b> <b>control</b> room.|$|R
50|$|Inspections include pre-dispatch inspection, {{dimensional}} <b>control,</b> <b>visual</b> <b>control,</b> {{and damage}} control. Documentation and certificates {{should also be}} reviewed.|$|R
40|$|We {{propose a}} method for <b>visual</b> <b>control</b> of a robotic system which {{does not require the}} {{formulation}} of an explicit calibration between image space and the world coordinate system. Calibration is known to be a difficult and error prone process. By extracting control information directly from the image, we free our technique from the errors normally associated with a fixed calibration. We demonstrate this by performing a peg-in-hole alignment using an uncalibrated camera to <b>control</b> the <b>positioning</b> of the peg. The algorithm utilizes feedback from a simple geometric effect, rotational invariance, to <b>control</b> the <b>positioning</b> servo loop. The method uses an approximation to the Image Jacobian to provide smooth, near-continuous control. 1 INTRODUCTION In many real world applications, there exists a need to align one object with another. Many researchers have concentrated their efforts on recovering the true position (the world coordinates) of the object to be manipulated. They used these results [...] ...|$|R
5000|$|<b>Visual</b> <b>controls</b> or <b>visual</b> {{management}} where management processes (e.g. checking) use simple graphics to show {{problems at}} a glance ...|$|R
30|$|The {{success of}} the {{intubation}} was secondarily confirmed by an experienced practitioner by a <b>visual</b> <b>control</b> and a lungs’ auscultation.|$|R
40|$|Abstract—This paper {{presents}} the <b>visual</b> <b>control</b> flow support of Visual Modeling and Transformation System (VMTS), which facilitates composing complex model transformations out of simple transformation steps and executing them. The VMTS <b>Visual</b> <b>Control</b> Flow Language (VCFL) uses stereotyped activity diagrams to specify control flow structures and OCL constraints {{to choose between}} different control flow branches. This work discusses the termination properties of VCFL and provides an algorithm to support the termination analysis of VCFL transformations...|$|R
40|$|We describ e {{a system}} that {{integrates}} real-time computer vision with a sensorless gripper to provide closed loop feedback control for grasping and manipulation tasks. Many hand-eye coordination skills {{can be thought of}} as sensory-control loops, where specialized reasoning has been embodied as a feedback or control path in the loop's construction. Our framework captures the essence of these hand-eye coordination skills in simple <b>visual</b> <b>control</b> primitives, which are a key component of the software integration. The primitives use a simple visual tracking and correspondence scheme to provide real-time feedback control in the presence of imprecise camera calibrations. Experimental results are shown for the positioning task of locating, picking up, and inserting a bolt into a nut under <b>visual</b> <b>control.</b> Results are alsopresented for the <b>visual</b> <b>control</b> of abolt tightening task. ...|$|R
40|$|Visual Management (VM) is a {{fundamental}} element of the lean production system that relies {{on the effectiveness of}} sensory communication to realise the lean production/construction goals. VM can be implemented by following certain frameworks in an incremental fashion. <b>Visual</b> <b>controls</b> are one of the main tools of VM that are used to limit and guide process outcomes. They are generally adopted in production management, process, safety, quality, and maintenance management efforts. VM and <b>visual</b> <b>controls</b> have extensively been an important part of lean construction initiatives for a while. Their implementation is also envisioned to be relevant to highway construction and maintenance projects. A set of critical points, general recommendations and possible areas of implementation for VM and <b>visual</b> <b>controls</b> identified from both research and practical experience are presented in this report...|$|R
40|$|Performance in a non-symbolic {{comparison}} task {{in which}} participants are asked to indicate the larger numerosity of two dot arrays, {{is assumed to be}} supported by the Approximate Number System (ANS). This system allows participants to judge numerosity independently from other visual cues. Supporting this idea, previous studies indicated that numerosity can be processed when <b>visual</b> cues are <b>controlled</b> for. Consequently, distinct types of <b>visual</b> cue <b>control</b> are assumed to be interchangeable. However, a previous study contrasting different types of <b>visual</b> cue <b>control</b> showed that the type of <b>visual</b> cue <b>control</b> affected performance using a simultaneous presentation of the stimuli in numerosity comparison. In the current study, we explored whether the influence of the type of <b>visual</b> cue <b>control</b> on performance disappeared when sequentially presenting each stimulus in numerosity comparison. While the influence of the applied type of <b>visual</b> cue <b>control</b> was significantly more evident in the simultaneous condition, sequentially presenting the stimuli did not completely exclude the influence of distinct types of <b>visual</b> cue <b>control.</b> Altogether, these results indicate that the implicit assumption {{that it is possible to}} compare performances across studies with a differential <b>visual</b> cue <b>control</b> is unwarranted and that the influence of the type of <b>visual</b> cue <b>control</b> partly depends on the presentation format of the stimuli...|$|R
50|$|An {{operator}} {{is usually}} {{dedicated to the}} <b>visual</b> <b>control</b> of the effect but new systems allow to use the instant replay operator.|$|R
40|$|This paper {{deals with}} active {{tracking}} of 3 D moving targets. Visual tracking {{is presented as}} a regulation control problem. The performance and robustness in <b>visual</b> <b>control</b> of motion depends both on the vision algorithms and the control structure. Delays and system latencies substantially affect the performance of visually guided systems. In this paper we discuss ways to cope with delays while improving system performance. Model predictive control strategies are proposed to compensate for the mechanical latency in <b>visual</b> <b>control</b> of motion. 1...|$|R
50|$|William H. Warren, Jr. is an American psychologist, {{currently}} the Chancellor's Professor at Brown University, focusing with perception and action, <b>visual</b> <b>control</b> of locomotion, spatial navigation.|$|R
40|$|Camera motion {{filtering}} {{is important}} in many applications, such as camera-based <b>visual</b> <b>control</b> interface on a mobile device. In this paper, we first present a robust global motion estimation algorithm based on optical flow and RANSAC, and a strategy that decompose camera motions into individual components frame-by-frame. A motion filtering strategy is then proposed to detect the most dominant motion within each individual component. Acceleration strategies are proposed to make it run real-time. Applications in video analysis and a mobile <b>visual</b> <b>control</b> interface are discussed. 1...|$|R
40|$|OBJECTIVE: To examine novice {{and expert}} {{differences}} in <b>visual</b> <b>control</b> strategies while performing a virtual reality transurethral resection {{of the prostate}} (TURP) task and to determine if these differences could provide a novel method for assessing {{construct validity of the}} simulator. SUBJECTS AND METHODS: A total of 11 novices (no TURP experience) and 7 experts (> 200 TURPs) completed a virtual reality prostate resection task on the TURPsim (Simbionix USA Corp, Cleveland, OH) while wearing an eye tracker (ASL, Bedford, MA). Performance parameters and the surgeon's <b>visual</b> <b>control</b> strategy were measured and compared between the 2 groups. RESULTS: Experts resected a greater percentage of prostate than novices (p < 0. 01) and had less active diathermy time without tissue contact (p < 0. 01). Experts adopted a target-locking visual strategy, employing fewer visual fixations (p < 0. 05) with longer mean fixation duration (p < 0. 005). With multiple learning trials, novices' performance improved and the adoption of a more expertlike gaze strategy was observed. CONCLUSION: Significant differences between experts and novices in both performance and <b>visual</b> <b>control</b> strategy were observed. The study of <b>visual</b> <b>control</b> strategies may be a useful adjunct, alongside measurements of motor performance, providing a novel method of assessing the construct validity of surgical simulators...|$|R
40|$|Information about limb {{positions}} and movements consists of input from visual, vestibular, cutaneous, muscular, tendinous and joint receptors, but the relative contribution from each type {{and location of}} receptors is not known. The {{aim of this study}} was: a) to measure the contribution from <b>visual</b> <b>control</b> on extremity function, as measured with a one-leg hop test in healthy persons, in patients with an asymptomatic ACL injury, after non-operative treatment and in patients with a stable knee after an ACL reconstruction, b) to investigate if there was any relation between proprioception from the extremity, as measured with the threshold for detecting passive motion of the knee, and the one-leg hop test with a gradual decrease in <b>visual</b> <b>control.</b> There was a decrease in hop-length when the subjects were deprived of <b>visual</b> <b>control</b> that was significant when the dominant eye or both eyes were blinded, both in the 2 patient groups and the reference population. The magnitude of the length reduction did not differ between the groups or between injured and healthy limbs. In all 4 threshold tests performed as a measure of peripheral proprioception, a stronger relation to hop-length was recorded for the blinded hop than with full <b>visual</b> <b>control</b> in the patients with nonoperated ACL injuries. The coefficients of correlation between hop-length and the proprioceptive recordings in the injured limb were of the same magnitude as on the healthy side...|$|R
40|$|Adjustments to gait were {{examined}} when positioning the foot within a narrow target {{at the end}} of an approach for two impact conditions, hard and soft. Participants (6 M, 6 F) ran toward a target of three lengths along a 10 -m walkway consisting of two marker strips with alternating black and white 0. 5 -m markings. Five trials were conducted for each target length and impact task, with trials block randomized between the 6 participants of each gender. A 50 -Hz digital video camera panned and filmed each trial from an elevated position adjacent to the walkway. Video footage was digitized to deduce the gait characteristics. A linear speed/accuracy tradeoff between target length and approach time was found for both impact tasks (hard, r = 0. 99, p < 0. 01; soft, r = 0. 96, p < 0. 05). For the hard-impact task, <b>visual</b> <b>control</b> time increased linearly (r = 0. 99, p < 0. 05) when whole-body approach velocity decreased. <b>Visual</b> <b>control</b> time was unaffected by whole-body approach velocity in the soft-impact task. A constant tau-margin of 1. 08 describes the onset of <b>visual</b> <b>control</b> when approaching a target while running, with the control of braking during <b>visual</b> <b>control</b> described by a tau-dot of – 0. 85. Further research is needed to examine the control of braking in different targeting tasks. <br /...|$|R
500|$|The {{secondary}} armament {{was primarily}} controlled by directors mounted {{on each side}} of the bridge. They were supplemented by two additional <b>control</b> <b>positions</b> in the fore-top, which were provided with [...] rangefinders, fitted in 1924–25. The anti-aircraft guns were controlled by a simple high-angle [...] rangefinder mounted on the aft <b>control</b> <b>position,</b> fitted in 1926–27. Three torpedo-control towers were fitted, each with a [...] rangefinder. One was {{on each side of}} the amidships control tower and the third was on the centreline [...] the aft <b>control</b> <b>position.</b>|$|R
40|$|A {{method for}} {{performance}} envelope boundary cueing for a {{vehicle control system}} comprises the steps of formulating a prediction system for a neural network and training the neural network to predict values of limited parameters {{as a function of}} current <b>control</b> <b>positions</b> and current vehicle operating conditions. The method further comprises the steps of applying the neural network to the control system of the vehicle, where the vehicle has capability for measuring current <b>control</b> <b>positions</b> and current vehicle operating conditions. The neural network generates a map of current <b>control</b> <b>positions</b> and vehicle operating conditions versus the limited parameters in a pre-determined vehicle operating condition. The method estimates critical control deflections from the current <b>control</b> <b>positions</b> required to drive the vehicle to a performance envelope boundary. Finally, the method comprises the steps of communicating the critical control deflection to the vehicle control system; and driving the vehicle control system to provide a tactile cue to an operator of the vehicle as the <b>control</b> <b>positions</b> approach the critical control deflections. Georgia Tech Research Corporatio...|$|R
5000|$|... #Caption: Royce Graciedemonstrating defense {{from the}} side <b>control</b> <b>position.</b>|$|R
