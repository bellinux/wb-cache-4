87|29|Public
500|$|Though Mya's {{character}} was {{generally regarded as}} impressive, the underlying technology was described by Peggy Albright in Wireless Week as not surprising; Albright said Motorola was [...] "latest company in recent weeks to introduce a voice-activated virtual assistant", as Mya was announced shortly after Microsoft had announced their MiPad, and Lucent had launched their Mobile <b>Voice</b> <b>Activated</b> Dialing software. However, Tobey Grummet spoke highly {{of the program in}} anticipation of its release, and Mya was described by Elliot Drucker of Wireless Week as a solution to the limitations of accessing the Internet on a mobile phone, without a keyboard or large colour display. While regarding the program with interest, John Sullivan doubted that Mya would persuade people who were not already on the Internet to start using it, and stated that if Mya could only read emails and not actually converse with him, he would rather just read his emails himself. Dawn Chmielewski from the Orange County Register called Mya a [...] "crude interpretation of things to come", noting that speech technology at the time was not without its limitations.|$|E
2500|$|In , Doc builds another DeLorean into a time machine, {{restoring}} {{most of its}} features, including Mr. Fusion and {{the hover}} conversion (Doc either rebuilds the one destroyed {{at the end of}} Part III or he simply builds a new one). [...] He also seemingly adds the capability to travel through space in addition to time (i.e., appear at a different location from the one it departed), similar to the TARDIS from Doctor Who. The cartoon DeLorean time machine has many add-ons, including a back seat in normal two-door mode, the ability to transform into a four-door, a pop-out covered wagon top, a blimp, a rear video screen, and a <b>voice</b> <b>activated</b> time input.|$|E
2500|$|Digital audio, {{programmable}} control: Both {{the audio}} circuit and the additional control circuits are fully digital. The hearing professional programs {{the hearing aid}} with an external computer temporarily connected to the device and can adjust all processing characteristics on an individual basis. Fully digital circuitry allows implementation of many additional features not possible with analog circuitry, {{can be used in}} all styles of hearing aids and is the most flexible; for example, digital hearing aids can be programmed to amplify certain frequencies more than others, and can provide better sound quality than analog hearing aids. Fully digital hearing aids can be programmed with multiple programs that can be invoked by the wearer, or that operate automatically and adaptively. These programs reduce acoustic feedback (whistling), reduce background noise, detect and automatically accommodate different listening environments (loud vs soft, speech vs music, quiet vs noisy, etc.), control additional components such as multiple microphones to improve spatial hearing, transpose frequencies (shift high frequencies that a wearer may not hear to lower frequency regions where hearing may be better), and implement many other features. Fully digital circuitry also allows control over wireless transmission capability for both the audio and the control circuitry. Control signals in a hearing aid on one ear can be sent wirelessly to the control circuitry in the hearing aid on the opposite ear to ensure that the audio in both ears is either matched directly or that the audio contains intentional differences that mimic the differences in normal binaural hearing to preserve spatial hearing ability. Audio signals can be sent wirelessly to and from external devices through a separate module, often a small device worn like a pendant and commonly called a “streamer”, that allows wireless connection to yet other external devices. This capability allows optimal use of mobile telephones, personal music players, remote microphones and other devices. With the addition of speech recognition and internet capability in the mobile phone, the wearer has optimal communication ability in many more situations than with hearing aids alone. This growing list includes <b>voice</b> <b>activated</b> dialing, <b>voice</b> <b>activated</b> software applications either on the phone or on the internet, receipt of audio signals from databases on the phone or on internet, or audio signals from television sets or from global positioning systems. The first practical, wearable, fully digital hearing aid was invented by Maynard Engebretson, Robert E Morley, Jr. and Gerald R Popelka. Their work resulted in US Patent 4,548,082, [...] "Hearing aids, signal supplying apparatus, systems for compensating hearing deficiencies, and methods" [...] by A Maynard Engebretson, Robert E Morley, Jr. and Gerald R Popelka, filed in 1984. This patent formed the basis of all subsequent fully digital hearing aids from all manufacturers, including those produced currently.|$|E
5000|$|The Talking Adventure Team Commander {{featured}} eight new <b>voice</b> commands, <b>activated</b> by a pull-string {{through the}} chest. The voice commands were: ...|$|R
50|$|In September 2016 Hive {{announced}} it will partner with Amazon Echo, allowing customers to use Alexa <b>voice</b> control to <b>activate</b> lighting, plugs and heating.|$|R
50|$|Voice Control was {{introduced}} as an exclusive {{feature of the}} iPhone 3GS and allows for the controlling of the phone and music features of the phone by voice. There {{are two ways to}} <b>activate</b> <b>Voice</b> Control: hold the Home button while in the home screen for a few seconds; or, change the effect of what double-clicking the home button does so it will <b>activate</b> <b>Voice</b> Control (only on iOS 3.x; on iOS 4 or later, double clicking the Home button opens the multitasking bar).|$|R
5000|$|<b>Voice</b> <b>Activated</b> Controls : The Knight 4000's {{dashboard}} is less [...] "cluttered looking" [...] than KITT's original dash {{with most}} of the controls now being completely <b>voice</b> <b>activated</b> rather than having to push a button.|$|E
50|$|Electric Jukebox {{controller}} built-in microphone provides {{access to}} the full catalogue of music. <b>Voice</b> <b>activated</b> functionality is provided by Nuance.|$|E
5000|$|<b>Voice</b> <b>Activated</b> Lumia — A 6 ft sphere {{that would}} respond with light effects to the guests voice or sounds.|$|E
50|$|The {{children}} {{escape and}} awaken Shane, who sends {{the kids to}} get the police while {{he goes to the}} vault to help Julie. Mr. Chun follows them in his car; with Zoe at the wheel, the kids force him to crash into a car dealership. Shane gets past the security system using a dance Howard had used to make Peter go to sleep each night. Julie knocks out Mrs. Chun, and Shane's <b>voice</b> <b>activates</b> the final vault, knocking out Bill with the door. By then, the children have lured a large crowd of police to the house. Mr. Chun arrives and holds all of them at gunpoint. Shane notices school principal and love interest Claire Fletcher right behind him, having followed the chase when she saw it pass by the school. Shane distracts Mr. Chun with the aide of Gary the duck, and Claire knocks him unconscious.|$|R
5000|$|In {{the film}} Spider-Man: Homecoming, a <b>voice</b> changer is <b>activated</b> when Spider-Man turns on his Tech Suit's {{interrogation}} mode, making him {{speak in a}} deep, frightening voice to a potential suspect.|$|R
50|$|Luminor: An {{automated}} {{lighting system}} that responds to <b>voice</b> commands to <b>activate</b> and change intensity of illumination. This {{is an example}} of fluorescent lighting or cold light, but the term fluorescent lighting is not used in the story.|$|R
50|$|A <b>voice</b> <b>activated</b> GPS system {{displayed}} the current location on a display while additional safety was supplied by two door mounted airbags.|$|E
50|$|This {{can range}} from using the tongue, lips, mouth, or {{movement}} of the head to <b>voice</b> <b>activated</b> interfaces utilizing speech recognition software and a microphone or bluetooth technology.|$|E
5000|$|Personalisation: the MOOC {{platforms}} must {{be capable}} of user's personalisation, in terms of: profile selection, colors, enough contrast, <b>voice</b> <b>activated</b> assistants) that will help to capture student's attention.|$|E
50|$|Xiaomi {{released}} the Yi (also called Ants or Xiaoyi) Smart Webcam for CNY 149 on 29 October 2014. It has 720p resolution, a 111 o wide-angle lens with 4x zoom, {{and the ability}} to make two-way <b>voice</b> calls. <b>Activated</b> and viewable via the smartphone, it doubles as both a webcam for chatting and a security camera with recording capabilities. The camera automatically records whenever it detects movement in view. In June 2015, Xiaomi launched a night vision edition of the Yi Camera with a 940 nm infrared sensor.|$|R
40|$|Successful {{sentence}} comprehension requires {{listeners to}} efficiently attend to and integrate numerous cues {{in the speech}} stream while concurrently ignoring acoustic cues irrelevant to word identity. But how do listeners decide what cues are (not) relevant? Talker-related information is an interesting case where helpful and unhelpful cues occur simultaneously: talker-related acoustic variation may obfuscate cross-speaker similarities in speech, while higher-order information about talker-identity may assist in semantic interpretation. In this study, we focus on whether and how adult and child listeners use talker-identity cues to swiftly interpret speech. Recent work has shown that adult and child listeners can rapidly activate a speaker’s color preference during sentence comprehension (Creel, in press) and that adults swiftly evaluate talker-related consistency with a sentence-level message (Van Berkum et. al. 2008). Additionally, adult and child listeners can activate information about explicitly-named agents (in combination with actions) to generate predictions about upcoming sentential themes (Borovsky, et. al., in press). However, {{it is unclear whether}} listeners predictively generate inferences about a speaker’s potential role in an action as a sentence unfolds. We explore this question by asking whether <b>voices</b> <b>activate</b> role information in an eye-tracked sentence comprehension task with college-aged adults (N= 50) and children (N= 11, aged 3; 8 - 7; 1). The experiment consisted of six blocks of two interleaved tasks: 1) Talker familiarization, and 2) Sentence comprehension...|$|R
40|$|This article {{explores the}} legal/human rights {{dimension}} of the evolving role of the State in activating not only media voices - the typical focus of media pluralism discussions - but {{a wider range of}} non-media voices that ought to be heard in public debate. European human rights law - specifically the European Convention on Human Rights and relevant case-law of the European Court of Human Rights - has developed a number of principles that could guide States in their task of <b>activating</b> <b>voices.</b> The article pays particular attention to the nature and scope of the obligation on States to take positive (policy and regulatory) measures to <b>activate</b> <b>voices.</b> The article aims to provide useful initial input into a broader, multi-stranded policy discussion on how the State can best activate a diverse range of voices in an increasingly digitized world...|$|R
50|$|Maluuba {{was founded}} by two {{undergraduate}} students from the University of Waterloo, Sam Pasupalak and Kaheer Suleman. Their initial proof of concept was a <b>voice</b> <b>activated</b> travel tool that allowed users to search for flights using their voice.|$|E
50|$|In 1999, Feather River {{became the}} first {{hospital}} in Northern California to house an Endoscopy Suite complete with <b>voice</b> <b>activated,</b> hands free surgical equipment. It {{is one of only}} four hospitals in the region with an anticoagulation clinic, and funds a wide range of preventative treatment programs.|$|E
50|$|In {{partnership}} with the Royal Blind Institute, London, UK, it has undertaken to provide the latest modified gadgets to allow the inclusion into information society i.e., Braille keyboards, screens, <b>voice</b> <b>activated</b> devices, special videos and toys that promoted learning for children {{with varying degrees of}} blindness.|$|E
5000|$|He {{provided}} the voice for a speaking [...] "motivational action figure" [...] depicting him in USMC drill instructor uniform with programmed <b>voice</b> messages <b>activated</b> by pressing {{a button on}} the figure. The figures' dialogue comes in two versions, one with (somewhat) family-friendly language and one with [...] "extra-salty" [...] dialogue which includes profanity, the latter of which is packaged with an R rating as a warning to consumers. One of these figurines also appears on occasion on Mail Call, wherein it {{is often referred to as}} 'Mini-Lee' by the host, sometimes seen berating a G.I. Joe figurine.|$|R
3000|$|In addition, the teacher’s {{perceptions}} generally paralleled {{with those}} of the students. As indicated by the teacher, “the fact that one’s <b>voice</b> can <b>activate</b> the desired PowerPoint slides in accordance to the speaker’s cues allows not only the instructor but also other students to interact with the course contents. This naturally increases students’ in class participations”. The teacher also indicated that since his teaching style “involves a great deal of moving around in the classroom,” this system can “indeed meet teaching needs” by unrestricting him from the teaching platform and enabling him to “interact with the system and the students at the same time.” [...]...|$|R
50|$|Microphones (4)—three {{microphones}} {{are used}} to hear sharp sounds and one is used to discriminate voice. The robot can localize {{the direction of a}} sharp sound and move towards it. I-Cybies can recognize spoken commands and respond in a specific manner. Voice recognition features biometric authentication. <b>Voice</b> recognition is <b>activated</b> by the head contact sensor or by the remote control unit.|$|R
50|$|The {{notion of}} Siri was firmly planted at Apple 25 years ago though “Knowledge Navigator” {{with the voice}} of the {{assistant}} was only a concept prototype. In October 2011, Apple relaunched Siri, a <b>voice</b> <b>activated</b> personal assistant software vaguely similar to that aspect of the Knowledge Navigator.|$|E
5000|$|Super: This TURBO Mode {{resembles a}} superhero costume, which Max finds too ridiculous, even though Steel loves it. In this mode, Max can fly, has heat vision, {{increased}} strength, speed, durability and other abilities Max hasnt unlocked yet but are apparently, <b>voice</b> <b>activated.</b> This {{was the first}} mode accessed by Steel using Mimic Mode.|$|E
50|$|Chumenwenwen is a <b>voice</b> <b>activated</b> AI {{assistance}} app {{available on}} Android and iOS devices. It enables searches in over 60 vertical fields of interest. It provides users ability {{to ask it}} for directions, restaurant suggestions, news, weather information, among many other options. In 2014, Chumenwenwen became the official voice service provider for Android Wear users in China.|$|E
40|$|Recognizing the {{identity}} of other individuals across different sensory modalities is critical for successful social interaction. In the human brain, face- and voice-sensitive areas are separate, but structurally connected. What kind of information is exchanged between these specialized areas during cross-modal recognition of other individuals is currently unclear. For faces, specific areas are sensitive to identity and to physical properties. It {{is an open question}} whether <b>voices</b> <b>activate</b> representations of face identity or physical facial properties in these areas. To address this question, we used functional magnetic resonance imaging in humans and a voice-face priming design. In this design, familiar voices were followed by morphed faces that matched or mismatched with respect to identity or physical properties. The results showed that responses in face-sensitive regions were modulated when face identity or physical properties did not match to the preceding voice. The strength of this mismatch signal depended on the level of certainty the participant had about the voice identity. This suggests that both identity and physical property information was provided by the voice to face areas. The activity and connectivity profiles differed between face-sensitive areas: (i) the occipital face area seemed to receive information about both physical properties and identity, (ii) the fusiform face area seemed to receive identity, and (iii) the anterior temporal lobe seemed to receive predominantly identity information from the voice. We interpret these results within a prediction coding scheme in which both identity and physical property information is used across sensory modalities to recognize individuals...|$|R
50|$|The LifeChat LX-3000 is a wired headset {{that connects}} to a PC by a USB cable. The headset {{features}} a microphone on an adjustable boom that can rotate up and down. The headset {{also has an}} in-line volume control switch with a mute button and a Windows Live Call button that <b>activates</b> <b>voice</b> calls in Microsoft Windows. The LX-3000 uses the CM108 USB audio chip from C-Media.|$|R
5000|$|Once her <b>voice</b> {{circuits}} are <b>activated,</b> The Machine provides spoken {{feedback to}} the player {{on his or her}} shots. The Machines voice is provided by Chicago-based singer Stephanie Rogers. [...] Due to the sexual overtones in some of her speech, the game includes a [...] "modesty" [...] setting that prevents some clips from being played. The original synthetic voice of Pin-Bot is also featured in the game.|$|R
50|$|Options, {{depending}} on the trim level, included HID Xenon headlamps, Dynamic Laser Cruise Control, parking sensors, a convex rear view mirror to enable the driver to see the passengers, a voice-activated navigation system (not <b>voice</b> <b>activated</b> for the 2004 model year) which included a backup camera, 10-speaker JBL audio and rear-seat DVD entertainment system with two 110V outlets.|$|E
50|$|Some {{adaptive}} technological accommodations {{may include}} {{but are not}} limited to: Adaptive computer technology (including <b>voice</b> <b>activated</b> and speech output), Assistive listening devices, Films/videotapes about disabilities, Kurzweil personal reader, Large print software,Print enlargers (CCTV), Raised-line drawing kit, Tactile map of campus, Talking calculators, Tape recorders/APH Talking Book Machine, TDD for hearing impaired, Wheelchair, Wheelchair access maps.|$|E
50|$|Garcia {{delivered}} the 1.5 million GSIS members into the Digital Age by embarking on the ambitious eCard Plus System, the GSIS Wireless Automated Processing System (G-W@PS) and the GSIS <b>Voice</b> <b>Activated</b> Processing System (G-V@PS). These programs enabled GSIS members {{to enjoy their}} GSIS benefits and loan privileges anytime and anywhere—using automated, paperless, secure, and wireless systems.|$|E
40|$|In {{this paper}} we present our {{researches}} regarding automat parsing of audio recordings. These recordings are obtained from children with dyslalia and {{are necessary for}} an accurate identification of speech problems. We develop the ADM algorithm and we analyze the complexity of this solution. We utilize a digital voice recorder in High Quality mode and with VCVA (Variable Control <b>Voice</b> Actuator) <b>activated.</b> The record format is IMA-ADPCM, 16 KHz and 4 bits (16 bits PCM). A microphone was placed at 10 cm from mouth {{in order to minimize}} environment noise. A software set of classes (C#) was created for handling audio stream (read, conversion between different format, write). We also propose an original solution for placing markers in audio stream. These markers are needed for a correct parsing af full recoding. Comment: 6 pages, 3 figures, in Romania...|$|R
40|$|We {{propose a}} new method to measure {{distances}} between different I-positions in internal dialogue. Subjects facing and then making a major life decision via internal dialogue can indicate the places of different {{voices in the}} dialogical self’s structure. The subjects’ task is to assign a place to themselves (narrator I) and their imaginary interlocutors at a round table. The Dialogical Self's Round Table (DSRT) task, a modified form of the Semantic Distance Task (SDT; Bartczak & Bokus, 2013, 2017), was designed so that the distances between the different I-positions could be coded numerically. Presenting the method of the DSRT, we will {{answer the question of}} which <b>voices</b> are <b>activated</b> the most often in internal dialogues, and which voices can be heard the most often from different locations at the round table. We will also analyze where the subjects place the voices they consider to be the most important...|$|R
40|$|This paper {{considers}} {{instances of}} biliterate educational practice in contexts of indigenous language revitalization involving Quechua in the South American Andes, Guarani in Paraguay, and Maori in Aotearoa/New Zealand. In these indigenous contexts of sociohistorical and sociolinguistic oppression, {{the implementation of}} multilingual language policies through multilingual education brings with it choices, dilemmas, and even contradictions in educational practice. I consider examples of such contentious educational practices from an ecological perspective, using the continua of biliteracy {{and the notion of}} voice as analytical heuristics. I suggest that the biliterate use of indigenous children’s own or heritage language as medium of instruction alongside the dominant language mediates the dialogism, meaning-making, access to wider discourses, and taking of an active stance that are dimensions of voice. Indigenous <b>voices</b> thus <b>activated</b> can be a powerful force for both enhancing the children’s own learning and promoting the maintenance and revitalization of their languages...|$|R
