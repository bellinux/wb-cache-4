30|19|Public
5000|$|Best Electronic Design 2008 for Best Automotive Design, for the EyeQ2 <b>Vision</b> <b>Processor.</b> Electronic Design, December 2008.|$|E
50|$|A camera can be {{anything}} from a standard compact camera system with integrated <b>vision</b> <b>processor</b> to more complex laser sensors and high resolution high speed cameras. Combinations of several cameras to build up 3D images of an object are also available.|$|E
50|$|Among Keisler's {{graduate}} students, several {{have made}} notable mathematical contributions, including Frederick Rowbottom who discovered Rowbottom cardinals. Several others {{have gone on}} to careers in computer science research and product development, including: Michael Benedikt, a professor of computer science at the University of Oxford, Kevin J. Compton, a professor of computer science at the University of Michigan, Curtis Tuckey, a developer of software-based collaboration environments; Joseph Sgro, a neurologist and developer of <b>vision</b> <b>processor</b> hardware and software, and Edward L. Wimmers, a database researcher at IBM Almaden Research Center.|$|E
5000|$|As an {{outgrowth}} of his work in neurophysiology, while still working as a post-doctoral fellow and {{an assistant professor of}} neurology, Sgro founded Alacron, Inc. (formerly Corteks, Inc.until 1990) in 1985 to manufacture technologies relevant to his neurological research. In 1989 he commercialized this technology and began developing array <b>processors,</b> frame grabbers, <b>vision</b> <b>processors,</b> and most recently supported advances in BSI sensor technology. Extending his work in machine vision technology, in 2002, Sgro founded FastVision, LLC, a maker of smart cameras, as a subsidiary of Alacron, Inc [...]|$|R
40|$|Visual sensors {{combined}} with video analysis algorithms can enhance applications in surveillance, healthcare, intelligent vehicle control, human-machine interfaces, etc. Hardware solutions exist for video analysis. Analog on-sensor processing solutions [1] feature image sensor integration. However, the precision loss of analog signal processing prevents those solutions from realizing complex algorithms, and they lack flexibility. <b>Vision</b> <b>processors</b> [2, 3] realize high GOPS numbers by combining a processor array for parallel operations and a decision processor for other ones. Converting from parallel {{data in the}} processor array to scalar in the decision processor creates a throughput bottleneck. Parallel memory accesses also lead to high power consumption. Privacy is a critical issue in setting up visual sensors because {{of the danger of}} revealing video data from image sensors or processors. These issues exist with the above solutions because inputting or outputtin...|$|R
40|$|This paper {{describes}} what I {{have done and}} learned in robot-vision research, and speculates what is necessary and promising in the fu-ture. In early artificial intelligence research, it was proved {{in the context of}} recognition of polyhedra that a hierarchical method in which a higher process assumes an ideal output of lower processes has limitations. A similar approach has been effective for vision processes that use the result of color segmentation, stereo vision, optical flow, or correspondence between multiple images. It is often more effective for recognition of difficult scenes to increase avail-able input data than to try to make a clever procedure. There was an attempt to approach more flexible vision systems like human vi-sion, which include reliable stereo vision and integration of multiple visual cues. Now a tightly coupled perception-action paradigm is an important issue, where significant research themes are person tracking and recognition, flexible real-time <b>vision</b> <b>processors,</b> and planning of perception-action considering the planning cost. KEY WORDS—vision, sensor fusion, active vision, uncer-tainty, real-time vision, plannin...|$|R
5000|$|In 2002, Sgro {{launched}} FastVision, LLC. FastVision builds high-speed megapixel-plus digital cameras, {{based on}} CMOS and CCD image sensors [...] The company's {{goal is to}} produce smart cameras, i.e. cameras with high-speed scalable integrated image processing capabilities built into the same package housing the opto-electronics. Like most smart camera vendors, FastVision’s suite includes FPGA processing and memory subsystems to enable in-camera image processing. When integrated with a high powered frame grabber or <b>vision</b> <b>processor</b> board (or a host subsystem), the resulting system capabilities can be expanded beyond simple image compression. The smart camera subsystem can be integrated with disk or non-volatile semiconductor storage inside or outside the camera to hold sustained real-time data acquisition, a valuable aid to system effectiveness when network connectivity is overloaded or is unavailable.|$|E
5000|$|Despite {{their speed}} and low power consumption, {{there are some}} {{significant}} drawbacks to analog CNN processors. First, analog CNN processors can potentially create erroneous results due to environment and process variation. In most applications, these errors are not noticeable, but there are situations where minor deviations can result in catastrophic system failures. For example, in chaotic communication, process variation will change the trajectory of a given system in phase space, resulting in a lost of synchronicity/stability. Due to {{the severity of the}} problem, there is considerable research being performed to ameliorate the problem. Some researchers are optimizing templates to accommodate greater variation. Other researchers are improving the semiconductor process to more closely match theoretical CNN performance. Other researchers are investigating different, potentially more robust CNN architectures. Lastly, researchers are developing methods to tune templates to target a specific chip and operating conditions. In other words, the templates are being optimized to match the information processing platform. Not only does process variation limit what can be done with current analog CNN processors, it is also a barrier for creating more complex processing units. Unless this process variation is resolved, ideas such as nested processing units, non-linear inputs, etc. cannot be implemented in a real-time analog CNN processor. Also, the semiconductor [...] "real estate" [...] for processing units limits the size of CNN processors. Currently the largest AnaVision CNN-based <b>vision</b> <b>processor</b> consists of a 4K detector, which is significantly less than the megapixel detectors found in affordable, consumer cameras. Unfortunately, feature size reductions, as predicted by Moore’s Law, will only result in minor improvements. For this reason, alternate technologies such as Resonant Tunneling Diodes and Neuron-Bipolar Junction Transistors are being explored. Also, the architecture of CNN processors is being reevaluated. For example, Star-CNN processors, where one analog multiplier is time-shared between multiple processor units, have been proposed and are expected to result in processor unit reduction size of eighty percent.|$|E
40|$|Abstract. In this paper, we outline an {{architecture}} for supporting real time autonomous vision in small devices. Our <b>vision</b> <b>processor</b> performs data reduction {{at the point}} of image capture, using re-configurable hardware devices to extract and describe image features. By implementing our design on FPGAs, the <b>vision</b> <b>processor</b> can support highly parallel vision processing structures as well as more traditional pipelined or sequential algorithms. Because the processor hardware can be reconfigured, it becomes a flexible prototyping tool. The resulting platform is well suited for research in reactive robot control based on vision, as well as research into biomorphic vision structures. ...|$|E
40|$|Vision {{processing}} {{is a topic}} traditionally {{associated with}} neurobiology; known to encode, process and interpret visual data most effectively. For example, the human retina; an exquisite sheet of neurobiological wetware, is amongst {{the most powerful and}} efficient <b>vision</b> <b>processors</b> known to mankind. With improving integrated technologies, this has generated considerable research interest in the microelectronics community in a quest to develop effective, efficient and robust vision processing hardware with real-time capability. This thesis describes the design of a novel biologically-inspired hybrid analogue/digital vision chip ORASIS 1 for centroiding, sizing and counting of enclosed objects. This chip is the first two-dimensional silicon retina capable of centroiding and sizing multiple objects 2 in true parallel fashion. Based on a novel distributed architecture, this system achieves ultra-fast and ultra-low power operation in comparison to conventional techniques. Although specifically applied to centroid detection, the generalised architecture in fact presents a new biologically-inspired processing paradigm entitled: distributed asynchronous mixed-signal logic processing. This is applicable to vision and sensory processing applications in general that require processing of large numbers of parallel inputs, normally presenting a computational bottleneck. Apart from the distributed architecture, the specific centroiding algorithm and vision chip other original contributions include: an ultra-low power tunable edge-detection circuit, an adjustable threshold local/global smoothing network and an ON/OFF-adaptive spiking photoreceptor circuit. Finally, a concise yet comprehensive overview of photodiode design methodology is provided for standard CMOS technologies. This aims to form a basic reference from an engineering perspective, bridging together theory with measured results. Furthermore, an approximate photodiode expression is presented, aiming to provide vision chip designers with a basic tool for pre-fabrication calculations. EThOS - Electronic Theses Online ServiceToumaz Technology Ltd,GBUnited Kingdo...|$|R
40|$|Robotic {{applications}} impose hard real-time {{demands on}} their vision components. To accommodate the realtime constraints, the visual component of robotic systems are often simplified by narrowing {{the scope of}} the vision system for a particular task. Another option as to build a generalized <b>vision</b> (sensor) <b>processor</b> and provides multiple interfaces, of differing scales and content, to other modules in the robot. Both options can be implemented in many ways, depending on computational resources. The tradeofls among these alternatives become clear when we study the vision process as a server whose clients request information about the world. We model the interface on client-server relations in user interfaces and operating systems. We examine the relation of this model to robot and vision sensor architecture and explore its application to a variety of vision sensor implementations. ...|$|R
40|$|Feature {{extraction}} is {{the part}} of pattern recognition, where the sensor data is transformed into a more suitable form for the machine to interpret. The purpose of this step is also {{to reduce the amount of}} information passed to the next stages of the system, and to preserve the essential information in the view of discriminating the data into different classes. For instance, in the case of image analysis the actual image intensities are vulnerable to various environmental effects, such as lighting changes and the feature extraction can be used as means for detecting features, which are invariant to certain types of illumination changes. Finally, classification tries to make decisions based on the previously transformed data. The main focus of this thesis is on developing new methods for the embedded feature extraction based on local non-parametric image descriptors. Also, feature analysis is carried out for the selected image features. Low-level Local Binary Pattern (LBP) based features are in a main role in the analysis. In the embedded domain, the pattern recognition system must usually meet strict performance constraints, such as high speed, compact size and low power consumption. The characteristics of the final system can be seen as a trade-off between these metrics, which is largely affected by the decisions made during the implementation phase. The implementation alternatives of the LBP based feature extraction are explored in the embedded domain in the context of focal-plane <b>vision</b> <b>processors.</b> In particular, the thesis demonstrates the LBP extraction with MIPA 4 k massively parallel focal-plane processor IC. Also higher level processing is incorporated to this framework, by means of a framework for implementing a single chip face recognition system. Furthermore, a new method for determining optical flow based on LBPs, designed in particular to the embedded domain is presented. Inspired by some of the principles observed through the feature analysis of the Local Binary Patterns, an extension to the well known non-parametric rank transform is proposed, and its performance is evaluated in face recognition experiments with a standard dataset. Finally, an a priori model where the LBPs are seen as combinations of n-tuples is also presente...|$|R
40|$|Neuromorphic <b>vision</b> <b>processor</b> is an {{electronic}} implementation of vision algorithm processor on semiconductor. To image the world, a low-power CMOS {{image sensor array}} is required in the <b>vision</b> <b>processor.</b> The image sensor array is typically formed through photo diodes and {{analog to digital converter}} (ADC). To achieve low power acquisition, a low-power mid-resolution ADC is necessary. In this paper, a 1. 8 V, 8 -bit, 166 MS/s pipelined ADC was proposed in a 0. 18 um CMOS technology. The ADC used operational amplifier sharing architecture to reduce power consumption and achieved maximum DNL of 0. 24 LSB, maximum INL of 0. 35 LSB, at a power consumption of 38. 9 mW. When input frequency is 10. 4 MHz, it achieved an SNDR 45. 9 dB, SFDR 50 dB, and an ENOB of 7. 33 bit...|$|E
40|$|This paper {{proposes a}} new concept for image {{recognition}} and understanding whereby both overall and local image processing are executed in parallel. Using this concept, we have developed an advanced <b>vision</b> <b>processor</b> that can recognize moving objects and handle moving pictures. The <b>vision</b> <b>processor</b> cames out feature extraction for image recognition by means of several local processors in parallel from time varying images on which an overall image processing unit has performed low-level operations such as data conversion and filtering. Special characteristics of the system are that each local processor processes only its region of interest, the division of regions being determined from the image feature and contents. A balloon-juggling robot has been developed to demonstrate the capability of the system. The system recognizes the balloon and measures its 3 -D position in real time. The robot hits the balloon by using the 3 -D data. 1...|$|E
40|$|Abstract—iVisual, an {{intelligent}} visual sensor SoC integrating 2790 fps CMOS image sensor and 76. 8 GOPS, 374 mW <b>vision</b> <b>processor,</b> is implemented on a 7. 5 mm 9. 4 mm {{die in a}} UMC 0. 18 m CMOS Image Sensor process. Light-in, answer-out SoC architecture is adopted to avoid possible privacy problems. A feature processor is designed to eliminate the dataflow mismatch between processor array and scalar processor to increase 36 % of average throughput. To increase hardware utilization, an inter-processor synchronization scheme is adopted to increase 23 % of average throughput. Memory access is reduced by 94 % to save 726 mW of power consumption. A bitplane-based single port memory structure is adopted to reduce SRAM area. The 205 GOPS/W power efficiency and 1. 16 GOPS/mm 2 area efficiency are therefore achieved by use of the proposed techniques. Index Terms—GOPS, intelligent visual sensor, SIMD, single-instruction multiple-data, video analysis, <b>vision</b> <b>processor.</b> I...|$|E
50|$|Further public {{displays}} of gestigon’s technologies include Audi at CES 2015 and CES 2016; Volkswagen and Infineon. Gestigon launched its Virtual Reality solution Carnival at the TechCrunch Disrupt in San Francisco in September 2015 using an Oculus Rift and different depth sensors. The first demo using a mobile device was done at the CES 2015. Gestigon has partnered with several companies that develop hardware solutions, especially depth sensors, to provide sensing solutions. In 2015 Gestigon partnered with Inuitive, a 3D computer <b>vision</b> and image <b>processors</b> developer, to create a VR unit. The system was presented at CES 2016 assembled on an Oculus Rift development kit.|$|R
40|$|This paper {{presents}} a new processing cell circuit, {{suitable for use}} in massively parallel fine-grain processor arrays, oriented towards image processing applications. The design, based on dynamic logic, is efficient for both local and global operations. In this paper we discuss design trade-offs and provide {{detailed description of the}} architecture. A cellular processor array based on the presented design can operate in both discrete- and continuous-time domains. Asynchronous execution of global operations significantly increases overall performance. Simulation results indicate the performance in the range from 1. 1 (unsigned products) to 2900 (asynchronous binary processing) MOPS/cell. KEY WORDS <b>Vision</b> chips, cellular <b>processor</b> arrays, asynchronous processing, SIMD arrays, wave propagations. I...|$|R
40|$|Markov random field models {{provide a}} robust {{formulation}} of low-level vision problems. Among the problems, stereo vision {{remains the most}} investigated field. The belief propagation provides accurate result in stereo vision problems, however, the algorithm remains slow for practical use. In this paper we examine and extract the parallelisms in the belief propagation method for stereo <b>vision</b> on multicore <b>processors.</b> The results show that with parallelization exploration on multicore processors, the belief propagation algorithm can have a 13. 5 times speedup compared to the single processor implementation. The experimental results also indicate that the parallelized belief propagation algorithm on multicore processors is able to provide a frame rate in 6 frames per second. ...|$|R
40|$|Complex vision tasks, e. g., face {{recognition}} or wavelet based image compression, impose severe demands on computational resources {{to meet the}} real-time requirements of the applications. Clearly, the bottleneck in computation can be identified in the first processing steps, where basic features are computed from full size images, as motion cues and Gabor or wavelet transform coefficients. This paper presents an architectural study of a <b>vision</b> <b>processor,</b> which was particulary designed to overcome this bottleneck...|$|E
40|$|Loral Defense Systems (LDS) {{developed}} a 3 D Laser Radar <b>Vision</b> <b>Processor</b> system capable of detecting, classifying, and identifying small mobile targets {{as well as}} larger fixed targets using three dimensional laser radar imagery for use with a robotic type system. This processor system is designed to interface with the NASA Johnson Space Center in-house Extra Vehicular Activity (EVA) Retriever robot program and provide to it needed information so it can fetch and grasp targets in a space-type scenario...|$|E
40|$|Abstract. Complex vision tasks, e. g., face {{recognition}} or wavelet based image compression, impose severe demands on computational resources {{to meet the}} real-time requirements of the applications. Clearly, the bottleneck in computation can be identified in the first processing steps, where basic features are computed from full size images, as motion cues and Gabor or wavelet transform coefficients. This paper presents an architectural study of a <b>vision</b> <b>processor,</b> which was particulary designed to overcome this bottleneck. ...|$|E
40|$|Vector {{extensions}} {{for general}} purpose processors are an efficient feature {{to address the}} growing performance demand of multimedia and computer <b>vision</b> applications. Embedded <b>processors</b> are the most widespread architectures for such applications. While providing sufficient computing power for these applications, they {{must take into account}} power, area and real-time constraints. In this paper, we propose two hardware optimization techniques to address those constraints: RISCization and instruction set customization. Experimental results show that those techniques both reduce time and power consumption by up to 50 % when compared to the original ISA. Keywords SIMD instruction set; altivec; vectorization; embedded systems;processor;customization; high performance image processing; power efficient architectures 20 Altivec Vector Unit Customization for Embedded Systems 21 1...|$|R
40|$|The UGV#Demo II {{program was}} {{established}} to develop and mature, within 5 years, the navigation and automatic target recognition technologies critical for minimally supervised, semi-autonomous, unmanned ground vehicles for military scout missions. The program, initiated in 1992, focuses on and exploits the arti #cial intelligence, computer <b>vision,</b> and advanced <b>processor</b> development sponsored under ARPA's science and technology program using militarily relevant scenarios. The vehicles are HMMWV's out#tted with on-board sensors and processing, and conduct reconnaissance, surveillance, and target acquisition #RSTA# operations supervised byahuman operator over a low-bandwidth radio link. FLIR, LADAR, and CCD camera sensors are used to detect, track, and identify targets using on-board parallel processing architectures. An operator monitors and initiates RSTA activities using a remote graphical user interface, and veri#es detected targets from transmitted image #chips"...|$|R
40|$|Designing {{real-world}} applications can involve coordinating many pieces of hardware and integrating multiple software components. Increased processing power has allowed complex {{real-world applications}} to be designed, {{and there has}} been increasing interest in the issues involved in designing both the applications and their support. In this paper we describe the issues involved in designing the application. The shepherding application we have chosen is representative of many real-world applications. This report focuses on technical details. We describe the underlying hardware, including the camera, <b>vision</b> processing boards, <b>processors,</b> and puma robot arm. We then discuss the software components we designed to integrate the hardware components in real-time. At each stage we describe the trade-offs between the different possibilities and why the ones chosen were best suited for our environment. We also present results supporting our selection. At appropriate points we indicate underlying su [...] ...|$|R
40|$|This paper {{presents}} an energy-efficient computer <b>vision</b> <b>processor</b> for a navigation device for the visually impaired. Utilizing a shared parallel datapath, out-of-order processing and co-optimization with hardware-oriented algorithms, the processor consumes 8 mW at 0. 6 V while processing 30 fps input data stream in real time. The test chip fabricated in 40 nm is demonstrated as a core {{part of a}} navigation device based on a ToF camera, which successfully detects safe areas and obstacles. Texas Instruments Incorporate...|$|E
40|$|A 5 -layer neuromorphic <b>vision</b> <b>processor</b> whose {{components}} communicate spike events asychronously {{using the}} address-eventrepresentation (AER) is demonstrated. The system includes a retina chip, two convolution chips, a 2 D winner-take-all chip, a delay line chip, a learning classifier chip, {{and a set}} of PCBs for computer interfacing and address space remappings. The components use a mixture of analog and digital computation and will learn to classify trajectories of a moving object. A complete experimental setup and measurements results are shown. ...|$|E
40|$|Abstract — When {{dealing with}} images, {{denoising}} is an inevitable preprocessing part. The key in denoising {{also depends on}} the type of noises present in the image. There are different methods for denoising and the method suitable for computer vision application is chosen to be LOWESS and Savitzky-Golay smoothening techniques. Analyzing an image to extract the features of its contents require edge enhancement. Among the various types of techniques for enhancing the edges, Unsharp Masking is selected to suit the requirement for it gives good results. These methods can be used for the preprocessing phase of a <b>vision</b> <b>processor</b> or similar applications...|$|E
40|$|The UGV / Demo II program, {{begun in}} 1992, {{developed}} and matured those navigation and automatic target recognition technologies {{critical for the}} development of supervised, autonomous ground vehicles capable of performing military scout missions with a minimum of human oversight. The program culminated with a highly successful series of field exercises performed by soldiers at Ft. Hood, Texas over three weeks in May/June 1996. This paper provides an introduction to the UGV / Demo II program. 1. UGV / Demo II Program Concept The objective of the UGV / Demo II program was to develop and mature those navigation and automatic target recognition technologies critical for the development and demonstration of supervised, autonomous ground vehicles capable of performing military scout missions with a minimum of human oversight. The intent was to focus on and exploit the artificial intelligence, computer <b>vision,</b> and advanced <b>processor</b> developments sponsored under the Defense Advanced Research P [...] ...|$|R
40|$|International audienceAbstract: In this article, we {{describe}} speciﬁc constraints of vision {{systems that are}} dedicated to be embedded in mobile robots. If PC based hardware architecture is convenient in this ﬁeld because of its versatility, its ﬂexibility, its performance and its cost, current real-time operating systems are not completely adapted to long processings with varying duration, and it is often necessary to oversize the system to guarantee fail-safe functioning. Also, interactions with other robotic tasks having more priority are diﬃcult to handle. To answer this problem, we have developed a dynamically reconﬁgurable vision processing system, based on the innovative features of Cl´eopatre real-time applicative layer concerning scheduling and fault tolerance. This framework allows to deﬁne emergency and optional tasks to ensure a minimal quality of service for the other subsystems of the robot, while allowing to adapt dynamically vision processing chain to an exceptional overlasting <b>vision</b> process or <b>processor</b> overload. Thus it allows a better cohabitation of several subsystems in a single hardware, and to develop less expensive but safe systems, as they will be designed for the regular case and not rare exceptional ones. At last, it brings {{a new way to}} think and develop vision systems, with pairs of complementary operators...|$|R
40|$|Abstract. Query {{processing}} {{is one of}} {{the most}} important mechanisms for data management, and there exist mature techniques for effective query optimization and efficient query execution. The vast majority of these techniques assume workloads of rather small transactional tasks with strong requirements for ACID properties. However, the emergence of new computing paradigms, such as grid and cloud computing, the increasingly large volumes of data commonly processed, the need to support data driven research, intensive data analysis and new scenarios, such as processing data streams on the fly or querying web services, the fact that the metadata fed to optimizers are often missing at compile time, and the growing interest in novel optimization criteria, such as monetary cost or energy consumption, create a unique set of new requirements for query processing systems. These requirements cannot be met by modern techniques in their entirety, although interesting solutions and efficient tools have already been developed for some of them in isolation. Next generation query processors are expected to combine features addressing all of these issues, and, consequently, lie at the confluence of several research initiatives. This paper aims to present a <b>vision</b> for such <b>processors,</b> to explain their functionality requirements, and to discuss the open issues, along with their challenges. ...|$|R
40|$|Abstract- This paper {{describes}} existing {{designs and}} future design trends of in-vehicle vision processors for driver assistance systems. First, requirements of vision processors for driver assistance systems are summarized. Next, {{the characteristics of}} vision tasks for safety are described. Then several in-vehicle <b>vision</b> <b>processor</b> LSI implementations are reviewed, and the design approach of one of them, the IMAPCAR highly parallel processor, is further described in detail. Finally, future trends of in-vehicle vision processors focusing on their architectures and application coverage expansion such as integration of vision for safety, Digital TV codec, and 3 D graphics functions of future car navigation, are discussed. ...|$|E
40|$|Abstract — In {{this paper}} {{we present a}} <b>vision</b> <b>processor,</b> which {{incorporates}} a 160 × 80 SIMD array of pixel-processors. The processor operates with a 100 MHz clock and 1. 8 V supply. The device provides 640 GOPS (binary) and 23 GOPS (greyscale) consuming 0. 5 W. The chip occupies 50 mm 2 and is fabricated in a standard 0. 18 μm CMOS process. The I/O interface supports 200 MPixels/s (greyscale), 1. 6 GPixels/s (binary) and 40 MPixels/s (address-event readout) data rate, and PE-parallel image sensing mode for embedded high-speed vision applications. Experimental {{results indicate that the}} performance of the presented chip approaches the efficiency of recently reported application-specific vision processors, while providing full programmability and thus being adjustable {{to a wide range of}} applications. I...|$|E
40|$|International audienceA {{real-time}} trinocular stereo <b>vision</b> <b>processor</b> {{is proposed}} which combines a window matching architecture with a classification architecture. A pair wise segmented window matching {{for both the}} center-right and center-left image pairs as their scaled down image pairs is performed. The resulting cost functions are combined which results into nine different cost curves. A multi level hierarchical classifier is used to select the most promising disparity value. The classifier makes use of features provided by the calculated cost curves and the pixels’ spatial neighborhood information. Evaluation and classifier training has been performed using an indoor dataset. The system is prototyped on an FPGA board equipped with three CMOS cameras. Special care has been taken to reduce the latency and the memory footprint...|$|E
40|$|Abstract: In this article, we {{describe}} specific constraints of vision {{systems that are}} dedicated to be embedded in mobile robots. If PC based hardware architecture is convenient in this field because of its versatility, its flexibility, its performance and its cost, current real-time operating systems are not completely adapted to long processings with varying duration, and it is often necessary to oversize the system to guarantee fail-safe functioning. Also, interactions with other robotic tasks having more priority are difficult to handle. To answer this problem, we have developed a dynamically reconfigurable vision processing system, based on the innovative features of Cléopatre real-time applicative layer concerning scheduling and fault tolerance. This framework allows to define emergency and optional tasks to ensure a minimal quality of service for the other subsystems of the robot, while allowing to adapt dynamically vision processing chain to an exceptional overlasting <b>vision</b> process or <b>processor</b> overload. Thus it allows a better cohabitation of several subsystems in a single hardware, and to develop less expensive but safe systems, as they will be designed for the regular case and not rare exceptional ones. At last, it brings {{a new way to}} think and develop vision systems, with pairs of complementary operators...|$|R
40|$|Abstract—This paper {{presents}} a CMOS chip for the parallel acquisition and concurrent analog processing of two-dimensional (2 -D) binary images. Its processing function {{is determined by}} a reduced set of 19 analog coefficients whose values are programmable with 7 -b accuracy. The internal programming signals are analog, but the external control interface is fully digital. Onchip nonlinear digital-to-analog converters (DAC’s) map digitally coded weight values into analog control signals, using feedback to predistort their transfer characteristics in accordance to {{the response of the}} analog programming circuitry. This strategy cancels out the nonlinear dependence of the analog circuitry with the programming signal and reduces the influence of interchip technological parameters random fluctuations. The chip includes a small digital RAM memory to store eight sets of processing parameters in the periphery of the cell array and four 2 -D binary images spatially distributed over the processing array. It also includes the necessary control circuitry to realize the stored instructions in any order and also to realize programmable logic operations among images. The chip architecture is based on the cellular neural/nonlinear network universal machine (CNN-UM). It has been fabricated in a 0. 8 - m single-poly double-metal technology and features 2 - s operation speed (time required to process an image) and around 7 -b accuracy in the analog processing operations. Index Terms — Analog array processors, cellular neural networks, focal plane <b>processors,</b> <b>vision</b> chips. I...|$|R
40|$|With the {{advances}} of robotics, computer science, {{and other related}} areas, home service robots attract much attention from both academia and industry. Home service robots present interesting technical challenges to the community in {{that they have a}} wide range of potential applications, such as home security, patient caring, cleaning, etc., and that the services provided by the robots in each application area are being defined as markets are formed and, therefore, they change constantly. Without architectural considerations to address these chal-lenges, robot manufacturers often focus on developing tech-nical components (e. g., <b>vision</b> recognizer, speech <b>processor,</b> and actuator) and then attempt to develop service robots by integrating these components. When prototypes are de-veloped for a new application, or when services are added, modified, or removed from existing robots, unexpected, un-desirable, and often dangerous side-effects, which are known as feature interaction problem, happen frequently. Reengi-neering of such robots can make a serious impact in delivery time and development cost. In this paper, we present our experience of re-engineering a prototype of a home service robot developed by Samsung Advanced Institute of Technology. First, we designed a mod-ular and hierarchical software architecture that makes inter-action among the components visible. With the visibility of interactions, we could assign functional responsibilities to each component clearly. Then, we re-engineered existing codes to conform to the new architecture using a reactive language Esterel. As a result, we could detect and solve feature interaction problems and alleviate the difficulty of adding or updating components. Categories and Subject Descriptor...|$|R
