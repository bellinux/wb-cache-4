2|46|Public
25|$|Adding data validation, using <b>validation</b> <b>formulae,</b> and {{conditional}} formatting features without manually writing code.|$|E
40|$|Four {{circular}} cylindrical grids {{were tested}} under axial compression {{to investigate the}} validity of a theoretical formula for the critical load for such structures. The chief result of the investigation was to throw light on some difficulties connected with the experimental <b>validation</b> of such <b>formulas...</b>|$|R
5000|$|One journal {{article has}} been written {{on the topic of}} Wilks <b>formula</b> <b>validation.</b> Based on the men's and women's world record holders and the top two {{performers}} for each event in the IPF's 1996 and 1997 World Championships (a total of 30 men and 27 women for each lift), it concluded: ...|$|R
40|$|SummaryBackgroundCardiac {{magnetic}} resonance imaging (MRI) is the key examination for patients with tetralogy of Fallot, but it remains challenging. The MRI report should at least mention left (L) and right (R) ventricle end-diastole volumes (V), ejection fraction (EF) and pulmonary regurgitation (PR). These variables are linked by basic physiology rules and (V×EF) L=(V×EF) R(1 −PR). AimsTo investigate this formula as a quality control of Fallot MRI. MethodsA total of 98 consecutive Fallot MRI were included retrospectively. Examinations that failed the formula (with a 10 % tolerance) constituted the invalid group and were compared with a control group of the same size. MRIs of both groups were randomly submitted to a senior observer for blinded reassessment. The initial and new reports were compared. The inter-observer limits of agreement were calculated for the different variables within both groups. ResultsTwelve examinations failed to pass the <b>validation</b> <b>formula.</b> From the 24 reanalysed examinations (12 invalid+ 12 controls), four failed to pass the formula (all from the invalid group). Two examinations had significant artefacts in the aorta or pulmonary trunk due to sternal wires. The quality check detected two other patients with atypical anatomy (persistent septal defects), which were not known by the MRI physician and were not detected during the examination. The inter-observer disagreements within the invalid group concerned essentially VR (P< 0. 02). ConclusionThe quality control detected questionable MRI examinations, in which 83 % corresponded to unreliable right ventricle volumes due to questionable manual contours or unreliable output flow due to artefacts...|$|R
30|$|The derived {{predictive}} formula was {{feasible for}} Group Dec and could retrospectively determine the examinations with theoretically excessive acquisition window. As a next step, {{we would like}} to prospectively apply this formula on patients with decreased HR during breath hold to predict the HR at scan and to determine the width of acquisition window. Further <b>validation</b> of the <b>formula</b> would be possible by comparing image qualities between experience-based and formula-based cardiac CT scans.|$|R
40|$|PurposeCardiac MRI (CMR) {{is the key}} exam for Fallot {{patients}} {{but remains}} tricky. The CMR report should at least mention left and right ventricle end-diastole volumes (Vl and Vr), ejection fraction (EFl and EFr) and pulmonary regurgitation (PR). Obviously, these variables are linked together by basic physiology rules and indeed Vl×EFl=Vr×EFr×(1 -PR). We investigated the interest of using such formula as quality check during Fallot CMR exams in our center. Methods 98  consecutive CMR examinations for Fallot (or Fallot-like) cardiopathy between 2010 and 2014 were retrospectively included. The exams failing to pass the formula (with a 10 % tolerance) constituted the Invalid-group and a control group {{of the same size}} was also constituted. CMR of both groups were randomly submitted to a blinded senior observer. The inter-observer limits of agreements were compared for the different variables within both groups. Results(Fig. 1) 12  CMR (12 %) failed to pass the <b>validation</b> <b>formula.</b> From the 24 reanalyzed CMR, only 4 failed to pass the formula (all from the Invalid-group). Two had persistent defect (VSD or ASD) which were not mentioned to the radiologist and not detected during the CMR. Two had significant artefacts in the aorta or pulmonary trunk due to sterna wires. The inter-observer disagreements for the 8 other CMR of the Invalid-group concerned the Right ventricle end-diastole volume (P< 0. 05). ConclusionThe use of a simple formula as quality check of CMR examinations for Fallot patients was useful to detect a total of 12 % of CMR with issues. 8 % of the CMR corresponded to uncertain right ventricle contours, 2 % to persistent septal defects that should have been noticed during the examination and 2 % to unreliable aortic or pulmonary flow due to artifacts. The formula could have permitted either to detect the anomalies or at least to conclude that the quality of the exams was impaired...|$|R
40|$|Thermodynamic {{properties}} calculator is {{an engineering}} tool built by MS Excel addin which is properties of water, air and superheated vapor. The {{main purpose of}} this project is to excel file that can simply obtain thermodynamic properties without manually reading from thermodynamic properties table. The project scopes werefluids {{that will be used}} as a reference were water, superheated vapor and air, thermodynamic properties such as enthalpy, entropy, specific heat, thermal conductivity, specific volume, relative pressure ratio, specific volume ratio and internal energy were the main properties that need to be referred. All the properties were based on thermodynamic properties table and result <b>validation.</b> <b>Formulas</b> that suitable to the formula of interface created using excel file were identified and studied. All unknown for the common formula that used to do interpolation manually were listed out and identified its function. Then, formula for each unknown created using excel formula and lastly all the formulas created for each unknown were combined according to the common formula used for manual interpolation. The new formula created using excel formulas was tested and evaluated. The value of properties from the interface created was compared to the value of properties from manual interpolation using calculator in order to check the validation of the data. In the nutshell, interface of a simplest way to read thermodynamics properties table using excel software was successfully created and was checked by data validation. In order to improve this project, the function of changing the units of thermodynamics properties to other standard, for example into American Standard, should be developed and the software should also be able to run on other operating systems such as MAC for Apple and LINUX for Linus Torvalds so that people using other operating systems will have the benefits of using the software...|$|R
40|$|We derive {{two types}} of Akaike {{information}} criterion (AIC) -like model-selection formulae for the semiparametric pseudo-maximum likelihood procedure. We first adapt the arguments leading to the original AIC formula, related to empirical estimation of a certain Kullback–Leibler information distance. This gives a significantly different formula compared with the AIC, which we name the copula information criterion. However, we show that such a model-selection procedure cannot exist for copula models with densities that grow very fast {{near the edge of}} the unit cube. This problem affects most popular copula models. We then derive what we call the cross-validation copula information criterion, which exists under weak conditions and is a first-order approximation to exact cross <b>validation.</b> This <b>formula</b> is very similar to the standard AIC formula but has slightly different motivation. A brief illustration with real data is given...|$|R
40|$|Estimation {{of maximum}} local scour at the pier site is {{necessary}} for safety and economy of the designed bridge. Numerous formulae are available for predicting maximum local scour depth at the pier site. Almost all of these formulae were developed based on laboratory data. <b>Validation</b> of these <b>formulae</b> using field data is necessary to recommend the formulae with a reasonable estimation. In this study, four formulae were selected for validation process using field data recorded from bridges subjected to scour in Pakistan, Canada and India and the data obtained from Qadar [7]. The selected formulae which were used for validation process had been proposed by Colorado State University (CSU), Melville and Sutherland, Jain and Fisher, and Lauren and Toch. Three statistical tests were carried out to determine the formulae equation with minimum prediction errors. They were mean absolute error (MAE), {{root mean square error}} (RMSE), and Theil’s Coefficient (TC). The statistical tests showed that error in the prediction of maximum local scour depth using CSU formula was minimum compared to the errors in the prediction of the other three formulae. For CSU formula, the computed values of MAE, RMSE and TC were found to be 0. 11, 0. 93 and 1. 24 respectively. Keywords: Bridge, Local scour, Field data, <b>Formulae,</b> <b>Validation.</b> <br...|$|R
40|$|Model {{checking}} {{based on}} validating temporal logic formulas has proven practical and e#ective for numerous software engineering applications. As systems {{based on this}} approach have become more mainstream, a need has arisen to deal e#ectively with large batches of formulas over a common model. Presently, most systems validate formulas one at a time, {{with little or no}} interaction between <b>validation</b> of separate <b>formulas.</b> This is the case despite the fact that, {{for a wide range of}} applications, a certain level of redundancy between domain-related formulas can be anticipated. This pape...|$|R
40|$|This paper {{deals with}} {{permeability}} measurement {{in the context}} of Resin Transfer Moulding (RTM). A new approach to two-dimensional radial flow permeability measurement with constant inlet pressure is proposed. It allows principal permeability to be measured even if the experimental axes are not aligned with the principal direction. This part of the paper looks at the underlying theory of the new approach while part B reports on <b>validation</b> experiments. <b>Formulae</b> are derived which allow to calculate principal permeability and the orientation of the principal axes from flow front measurements in three directions. Transient effects of the developing flow front caused by the circular inlet are discussed and its influence on the measured permeability is illustrated. Numerical studies are performed which show that the shape of the flow front is dependent only {{on the size of the}} inlet diameter and the degree of anisotropy. This leads to the development of a formula for estimating the minimum required mould size for permeability measurement...|$|R
40|$|This is the author’s accepted, {{refereed}} {{and final}} manuscript to the article. LOCKED until 2016 - 02 - 01 due to copyright restrictions. We derive {{two types of}} Akaike information criterion (AIC) -like model-selection formulae for the semiparametric pseudo-maximum likelihood procedure. We first adapt the arguments leading to the original AIC formula, related to empirical estimation of a certain Kullback–Leibler information distance. This gives a significantly different formula compared with the AIC, which we name the copula information criterion. However, we show that such a model-selection procedure cannot exist for copula models with densities that grow very fast {{near the edge of}} the unit cube. This problem affects most popular copula models. We then derive what we call the cross-validation copula information criterion, which exists under weak conditions and is a first-order approximation to exact cross <b>validation.</b> This <b>formula</b> is very similar to the standard AIC formula but has slightly different motivation. A brief illustration with real data is given. 2, Forfatterversjo...|$|R
40|$|AMS Subject Classification: 62 G 99 In Ponzini et al. (2006) a new {{approach}} has been proposed for estimat-ing in a reliable way blood flow rate from velocity Doppler measurements. In that paper, basic features of the approach and some “in silico ” test cases 1 were furnished. Here, we give more insights of this approach by perform-ing a sensitivity analysis of the formulae relating blood flow rate to blood velocity. In particular we analyze their sensitivity to the physiological pa-rameters {{in comparison with the}} standard formula proposed in Doucette et al. (1992). A first glance at “in vivo ” <b>validation</b> of the <b>formulae</b> is given too. ...|$|R
30|$|The high {{incidence}} of PRRT-related hyperkalemia in patients treated for NET has been recently reported [13]. The {{aim of this study}} was to identify pre-therapeutic parameters for prediction of AA-induced hyperkalemia in patients undergoing PRRT. Combinations of serum parameters were retrospectively identified, and their predictive value was prospectively evaluated. No single serum parameter could reliably predict the occurrence of hyperkalemia. In the prospective <b>validation</b> cohort, a <b>formula</b> including five routine biochemical serum parameters (potassium, BUN, sodium, phosphate, LDH; Formula 2) correctly identified 8 of 9 patients with (sensitivity: 88.9 %) and 23 of 29 patients without hyperkalemia > 6.0  mmol/l (specificity: 79.3 %); resulting in an accuracy of 31 of 38 (81.6 %).|$|R
40|$|Model {{checking}} {{based on}} validating temporal logic formulas has proven practical and effective for numerous software engineering applications. As systems {{based on this}} approach have become more mainstream, a need has arisen {{to deal effectively with}} large batches of formulas over a common model. Presently, most systems validate formulas one at a time, with little or no interaction between <b>validation</b> of separate <b>formulas.</b> This is the case despite the fact that, {{for a wide range of}} applications, a certain level of redundancy between domain-related formulas can be anticipated. This paper presents an optimizing compiler for batches of temporal logic formulas. A component of the Carnauba model checking system, this compiler addresses the need to handle batches of temporal logic formulas by leveraging th...|$|R
40|$|Integration of {{distributed}} generation (DG) in distribution network has significant effects on voltage profile for both customers and distribution network service providers (DNSPs). This impact may manifest itself positively or negatively, depending on {{variation of the}} voltage {{and the amount of}} DG that can be connected to the distribution networks. This paper presents a way to estimate the voltage variation and the amount of the DG that can be accommodated into the distribution networks. To do this, a voltage rise formula is used with some approximation and the <b>validation</b> of this <b>formula</b> is checked by comparing with the existing power systems simulation software. Using the voltage variation formula, the worst case scenario of the distribution network with DG is used to estimate the amount of voltage variation and maximum permissible DG. No Full Tex...|$|R
40|$|Abstract. Capturing and {{understanding}} mathematics from print form {{is an important}} task in translating written mathematical knowledge into electronic form. While the problem of syntactically recognising mathematical formulas from scanned images has received attention, very little {{work has been done}} on semantic validation and correction of recognised formulas. We present a first step towards such an integrated system by combining the Infty system with a semantic analyser for matrix expressions. We applied the combined system in experiments on the semantic analysis of matrix images scanned from textbooks. While the first results are encouraging, they also demonstrate many ambiguities one has to deal with when analysing matrix expressions in different contexts. We give a detailed overview of the problems we encountered that motivate further research into semantic <b>validation</b> of mathematical <b>formula</b> recognition. ...|$|R
40|$|The left {{ventricular}} ejection fraction {{is useful in}} characterizing cardiac performance and evaluating prognosis in patients with known or suspected cardiac disease. The {{purpose of this study}} was to determine if simple, quantitative clinical information generated as part of a routine patient evaluation could be used to predict ejection fraction determined by radionuclide ventriculography. Multiple regression analysis was used to study a group of 64 patients selected to represent the full range of ejection fraction values. All patients had undergone cardiac catheterization and standard chest radiography in addition to resting and exercise radionuclide ventriculography. Using easily determined clinical variables, a regression formula was developed that predicted the radionuclide ventriculographic ejection fraction (r = 0. 73). Plain film heart volume, heart rate, pulse pressure, and thoracic width were highly significant terms in the optimal regression equation. For <b>validation,</b> the <b>formula</b> was applied to a second, independent verification data set composed of 41 cases and revealed similar correlation (r = 0. 78). A radionuclide ventriculographic ejection fraction below 40 was identified in the verification data set with a sensitivity of 87 percent and specificity of 83 percent. Use of this method, requiring only direct heart rate, blood pressure, and chest radiographic measurements and simple calculations, may assist physicians in patient management and facilitate the optimal use of more invasive and expensive studies...|$|R
40|$|Because of {{measuring}} {{problems related to}} evaluation of urban roughness parameters, a new approach using a roughness mapping tool has been tested: evaluation of roughness length z(o) and zero displacement z(d) from cadastral databases. Special attention needs {{to be given to}} the validation of the tool, and the roughness formulas therein. After a roughness <b>formula</b> <b>validation</b> using laboratory data, three other validation tests have been applied sucessfully, using roughness classifications, comparison with an established roughness model, and spatial averaging consistency tests. The latter is an interesting, new approach which allows to rank the performance of the otherwise largely similar models. Further results of interest are the sensitivity of the Raupach model to some of its parameters, and the need to include the drag of forest, town and obstacle group edges in an effective, spatially averaged roughness. (C) 1998 Elsevier Science Ltd. All rights reserved...|$|R
40|$|In Winkler {{model the}} {{subgrade}} soil {{is assumed to}} behave like infinite numbers of linear elastic springs, which the stiffness of the springs is named as the {{modulus of subgrade reaction}}. This modulus depends on some parameters like soil type, dimension, shape, embedment depth and type of foundation (Flexible or Rigid). The direct method to estimate the modulus of subgrade reaction is the plate load test. In this study the foundations on clayey soil are modeled with finite element software to investigate the <b>validation</b> of Terzaghi’s <b>formula</b> and the effect of different parameters on subgrade reaction modulus. Due to obtained results, Terzaghi's formula does not consider the effect of soil consistency and the shape of foundation; therefore it results in uncertain values. As depth of embedment of foundation is increased, the modulus of subgrade reaction is increased. Flexural rigidity of foundation can improve the status of subgrade reaction modulus...|$|R
40|$|EarthSaw{trademark} is a {{proposed}} technology {{for construction of}} uniform high quality barriers under and around pits and trenches containing buried radioactive waste without excavating or disturbing the waste. The method works by digging a deep vertical trench {{around the perimeter of}} a site, filling that trench with high specific gravity grout sealant, and then cutting a horizontal bottom pathway {{at the base of the}} trench with a simple cable saw mechanism. The severed block of earth becomes buoyant in the grout and floats on a thick layer of grout, which then cures into an impermeable barrier. The ''Interim Report on task 1 and 2 '' which is incorporated into this report as appendix A, provided theoretical derivations, field <b>validation</b> of <b>formulas,</b> a detailed quantitative engineering description of the technique, engineering drawings of the hardware, and a computer model of how the process would perform in a wide variety of soil conditions common to DOE waste burial sites. The accomplishments of task 1 and 2 are also summarized herein Task 3 work product provides a comprehensive field test plan in Appendix B and a health and safety plan in Appendix C and proposal for a field-scale demonstration of the EarthSaw barrier technology. The final report on the subcontracted stress analysis is provided in Appendix D. A copy of the unified computer model is provided as individual non-functional images of each sheet of the spreadsheet and separately as a Microsoft Excel 2000 file...|$|R
40|$|The {{support vector machine}} (SVM) is one of {{the most}} {{successful}} learning methods for solving classification problems. Despite its popularity, SVM has a serious drawback, that is sensitivity to outliers in training samples. The penalty on misclassification is defined by a convex loss called the hinge loss, and the unboundedness of the convex loss causes the sensitivity to outliers. To deal with outliers, robust variants of SVM have been proposed, such as the robust outlier detection algorithm and an SVM with a bounded loss called the ramp loss. In this paper, we propose a robust variant of SVM and investigate its robustness in terms of the breakdown point. The breakdown point is a robustness measure that is the largest amount of contamination such that the estimated classifier still gives information about the non-contaminated data. The main contribution of this paper is to show an exact evaluation of the breakdown point for the robust SVM. For learning parameters such as the regularization parameter in our algorithm, we derive a simple formula that guarantees the robustness of the classifier. When the learning parameters are determined with a grid search using cross <b>validation,</b> our <b>formula</b> works to reduce the number of candidate search points. The robustness of the proposed method is confirmed in numerical experiments. We show that the statistical properties of the robust SVM are well explained by a theoretical analysis of the breakdown point. ...|$|R
40|$|Integration of {{different}} types of distributed energy resources (DERs) in distribution network has significant effects on voltage profile for both customers and distribution network service providers (DNSPs). This impact may manifest itself positively or negatively, depending on the voltage variation and the amount of DERs that can be connected to the distribution networks. This chapter presents a way to estimate the voltage variation and the amount of the DERs in a microgrid. To do this, a voltage rise formula is derived with some approximation and the <b>validation</b> of this <b>formula</b> is checked by comparing with the existing power {pipe}systems simulation software. Using the voltage variation formula, the worst case scenario of microgrid is used to estimate the amount of voltage variation and maximum permissible DERs. The relationship between voltage level, voltage rise, and connection cost of DERs in a microgrid is also described in this chapter. Finally, based on the worst case scenario of microgrid; some recommendations are given to counteract the voltage rise effect...|$|R
40|$|In 1951 Gorlin and Gorlin (Am Heart J 1951; 41 : 1 – 29) {{published}} their {{formula for}} calculating {{the area of}} a stenotic cardiac valve from hemodynamic data. The important concept implicit in this formula is that the hemodynamic evaluation of a stenotic valve requires that the pressure gradient across that valve be examined {{in light of the}} cardiac output passing through the orifice. The concept asserts that if an accurately obtained pressure gradient can be related to an accurate cardiac output by an accurate formula, valve area can be determined. However, recent studies have demonstrated flaws in current practices for obtaining transvalvular gradients and cardiac outputs. Further, new data are available regarding the validation and possible changes in the Gorlin <b>formula,</b> <b>validation</b> and changes that the Gorlins noted might be necessary to make. These new data concerning the three basic requirements of an accurate valve area determination are the subject of this review...|$|R
40|$|A novel {{remote sensing}} method is {{presented}} that accurately predicts 2 % {{run up and}} run down thresholds on a gravel beach under calm (Hs < 2 m) conditions. This overcomes the common problem of ascertaining accurate field measurements in the energetic swash zone of a gravel beach where damage to equipment is commonplace. The optical image intensity from time-exposure and time-variance Argus images is interrogated in order to extract the swash parameters of interest. Predictions are validated against field observation and result in a vertical RMS error of 17 cm for run up and 18 cm for back wash. The method alleviates the need for manual digitisation of swash events as has previously been commonplace, enabling swift creation of large datasets for <b>validation</b> of empirical <b>formulae.</b> The use of time-variance images was also seen {{to increase the number}} of useable images collected from Argus stations in adverse conditions when compared to time-exposure imagery. This paper outlines a solid proof of concept for the method but acknowledges that extensive further field validation is required, specifically under energetic conditions and at other gravel beaches...|$|R
40|$|The paper {{presents}} a new discriminant function (DFA) for sex determination from cranial skeletal material specifically suitable for Swiss alpine populations. The formula was developed {{on a large}} cranial series (n= 637) originating from an ossuary in Poschiavo Switzerland (16 th– 19 th century AD). The accuracy of the new DFA was compared to other published DFA (Giles and Elliot 1963; Henke 1973; Kajanoja 1966; Brùžek and Velemínský 2006) whereas a <b>validation</b> of all <b>formulae</b> was made on skeletal material mainly from Switzerland, southern Germany and Austria (12 series; ca. 1400 individuals; 9 th mil. BC– 19 th century AD). The new DFA proved accurate {{with a high degree}} of concordance with morphognostic sex determination (79 %) compared to the other DFA (69 %– 83 %). Considering also that with the Poschiavo DFA more individuals could be classified based on the small number of necessary measurements, the present DFA proved especially valuable. The high accuracy levels and the fact that few measurements are needed make the proposed DFA suitable for Swiss alpine populations, applicable to a large amount of individuals and less timeconsuming...|$|R
40|$|A simple {{formula to}} {{determine}} the human average whole-body SAR (SAR(wb)) under realistic propagation conditions is proposed in the GHz region, i. e. from 1. 45 GHz to 5. 8 GHz. The methodology is based on simulations of ellipsoidal human body models. Only the exposure (incident power densities) and the human mass are needed to apply the formula. Diffuse scattered illumination is addressed {{for the first time}} and the possible presence of a Line-of-Sight (LOS) component is addressed as well. As <b>validation,</b> the <b>formula</b> is applied to calculate the average whole-body SAR(wb) in 3 D heterogeneous phantoms, i. e. the virtual family (34 year-old male, 26 year-old female, 11 year-old girl, and 6 year-old boy) and the results are compared with numerical ones-using the Finite-Difference Time-Domain (FDTD) method-at 3 GHz. For the LOS exposure, the average relative error varies from 28 % to 12 % (resp. 14 - 12 %) for the vertical polarization (resp. horizontal polarization), depending on the heteregeneous phantom. Regarding the diffuse illumination, relative errors of - 39. 40 %, - 11. 70 %, 10. 70 %, and 10. 60 % are obtained for the 6 year-old boy, 11 year-old girl, 26 year-old female, and 34 year-old male, respectively. The proposed formula estimates well (especially for adults) the SAR(wb) induced by diffuse illumination in realistic conditions. In general, the correctness of the formula improves when the human mass increases. Keeping the uncertainties of the FDTD simulations in mind, the proposed formula might be important for the dosimetry community to assess rapidly and accurately the human absorption of electromagnetic radiation caused by diffuse fields in the GHz region. Finally, we show the applicability of the proposed formula to personal dosimetry for epidemiological research...|$|R
40|$|Winkler {{model is}} one of the most popular models in {{determining}} the modulus of sub grade reaction. In this model the sub grade soil is assumed to behave like infinite number of linear elastic springs. The stiffness of these springs is named as the modulus of sub grade reaction. This modulus is dependent to some parameters like soil type, size, shape, depth and type of foundation. The direct method for estimating the modulus of sub grade reaction is plate load test that is done with 30 - 100 cm diameter circular plate or equivalent rectangular plate. Afterward, we have to extrapolate the test value for exact foundation. In the practical design procedure, Terzaghi's equation is usually used to determine the modulus of sub grade reaction for actual foundation, but there are some uncertainties in utilizing such equation. In this paper the size effect of foundation on sandy sub grade with use of finite element software (Plaxis) is proposed to investigate the <b>validation</b> of Terzaghi's <b>formula</b> on determination of sub grade reaction modulus. Also the comparison between Vesic's equation, Terzaghi's one and obtained results are presented...|$|R
40|$|International audienceThe dual-medium {{approach}} is convenient for simulating flows within two interactingcontinua such as fractured reservoirs, because it greatly simplifies the apparent complexityof the flow problem while offering a conceptual representation of flows within and betweenthe two continua {{that helps the}} understanding of flow responses. Considerable work has beenachieved during the last decades to model the coupling term of such models, that is, matrixfracturetransfers. Whereas pseudo-steady-state transfers taking place at late times can befairly well predicted, the simulation of transient, i. e. early-time, transfers still encountersdifficulties due to intrinsically complex transfer mechanisms that the resolution of diffusionequations entails. Those difficulties are emphasized for tight porous media where transientflow behaviour persists over a durable period of time, for compressible fluids that increaseinaccuracy of linear approximations of transfers, and {{also because of the}} multi-directionalityof transfers. Starting from analytical solutions of diffusivity equations, the present paper proposesamethodology to account for transient effects and for non-linearities inmatrix-fracturetransfer formulas of dual-porosity simulators in order that they can predict the production ofunconventional low-permeability hydrocarbon reservoirs. <b>Validation</b> of these <b>formulas</b> forthe production of very tight fractured media is shown at the matrix block scale and at thescale of a stimulated reservoir volume...|$|R
40|$|Abstract: Estimation {{of maximum}} local scour depth at the bride pier site is {{necessary}} for the safety and economy of the designed bridge. Numerous formulae are available and almost all of these formulae were developed based on laboratory data. <b>Validation</b> of these <b>formulae</b> is necessary in order to ascertain which of the formulae will give a reasonable estimate of the local scour depth. In this study, four commonly cited formulae were selected for the validation process using both the laboratory and field data. They were the Colorado State University (CSU), Melville and Sutherland, Jain and Fisher, and Laursen and Toch formula. The experimental data was obtained from the laboratory model study done at University Putra Malaysia, whilst the field data were obtained from 14 bridges sites. Three statistical tests were carried out to determine the formula that gives minimum prediction errors. Comparison between the predicted and measured depth of scour from the experimental and field data showed that the Laursen and Toch and the CSU formulae appeared to give a reasonable estimate. Whilst the Melville and Sutherland and Jain and Fisher formulae appeared to over-predict the depth of the scour. This observation was supported by the statistical tests...|$|R
40|$|This summary on {{the paper}} on erosion model based on grainsize {{distribution}} ratios of weathering product of Quaternary volcanic deposits in Bandung Basin, West Java, is the latest development in these issues in Indonesia. USLE formula known as formula for annual erosion prediction is no longer accurate, which needs a modification, in term of involvement of soil variables determining erosion for the <b>validation</b> of the <b>formula</b> before being used for the prediction of annual erosion {{in the area of}} distribution of weathered Quaternary volcanic deposits. This modification of USLE was conducted during a research in southern Bandung Basin using deterministic and probabilistic approaches. Based on the ratio between silt to clay content of soil mass, the validation of the erosion formula mentioned above generated the modification of USLE for both silt soils (symbolized MH) EMH = 0. 77 RKLSCP and clay soils (CH) ECH = 0. 51 RKLSCP respectively. These new formulas, possessing the errors about 6 %, represent the grainsize distribution proportion and also cohesion of soil mass which explain the characteristic of its erodibility. Large erodibility of silt soil is also indicated by large correlation coefficient between silt content and coefficient of erosion r = 0. 93...|$|R
40|$|Existing {{equations}} {{to calculate}} the estimated glomerular filtration rate (eGFR) were derived from nondiabetic Caucasian patients with chronic kidney disease. Here, we developed formulas to more accurately predict the eGFR in Chinese patients with type 2 diabetes and validated their performance in 202 type 2 diabetic and 46 nondiabetic individuals. Within the diabetic cohort, 135 {{were randomly assigned to}} a training group, whereas the remaining 67 diabetic and all of the nondiabetic patients were assigned to a validation group. Reference GFR was measured by 51 Cr-EDTA plasma clearance. The new eGFR-estimating formulas, derived using a stepwise regression model, were compared with existing prediction equations in the <b>validation</b> group. The <b>formulas</b> are: 313 × (Age) - 0. 494 (years) × [SCr]- 1. 059 (mg/dl) × [Alb]+ 0. 485 (g/dl) for men, and 783 × (Age) - 0. 489 (years) × [SCr]- 0. 877 (mg/dl) × [SUN]- 0. 150 (mg/dl) for women. Compared with existing equations, the new formulas were more accurate and precise in calculating eGFR in diabetic patients, but, similar to other equations, were less accurate in the nondiabetic cohort. Our newly developed equations are simple to use and can be applied in routine clinical practice to calculate eGFR in Chinese patients with type 2 diabetes...|$|R
40|$|We derive an {{analytic}} phenomenological {{expression that}} predicts the final {{mass of the}} black-hole remnant resulting from the merger of a generic binary system of black holes on quasi-circular orbits. Besides recovering the correct test-particle limit for extreme mass-ratio binaries, our formula reproduces well the results of all the numerical-relativity simulations published so far, both when applied at separations of a few gravitational radii, and when applied at separations {{of tens of thousands}} of gravitational radii. These <b>validations</b> make our <b>formula</b> a useful tool in a variety of contexts ranging from gravitational-wave physics to cosmology. As representative examples, we first illustrate how it can be used to decrease the phase error of the effective-one-body waveforms during the ringdown phase. Second, we show that, when combined with the recently computed self-force correction to the binding energy of nonspinning black-hole binaries, it provides an estimate of the energy emitted during the merger and ringdown. Finally, we use it to calculate the energy radiated in gravitational waves by massive black-hole binaries as a function of redshift, using different models for the seeds of the black-hole population. Comment: 9 pages (emulateapj), 4 figures. Matches version in ApJ but includes slight changes to fig 4 described in Barausse, et al ApJ 786, 76 (2014) (doi: 10. 1088 / 0004 - 637 X/ 786 / 1 / 76), see also [URL]...|$|R
40|$|Background Dual-energy X-ray {{absorptiometry}} (DXA) provides separate {{measurements of}} fat mass, fat-free mass and bone mass, {{and is a}} quick, accurate, and safe technique, yet one that is not readily available in routine clinical practice. Consequently, we aimed to develop statistical formulas to predict fat mass (%) and fat mass index (FMI) with simple parameters (age, sex, weight and height). Methods We conducted a retrospective observational cross-sectional study in 416 overweight or obese patients aged 4 – 18 years that involved assessing adiposity by DXA (fat mass percentage and FMI), body mass index (BMI), sex and age. We randomly divided the sample into two parts (construction and validation). In the construction sample, we developed formulas to predict fat mass and FMI using linear multiple regression models. The formulas were validated in the other sample, calculating the intraclass correlation coefficient via bootstrapping. Results The fat mass percentage formula had a coefficient of determination of 0. 65. This value was 0. 86 for FMI. In the <b>validation,</b> the constructed <b>formulas</b> had an intraclass correlation coefficient of 0. 77 for fat mass percentage and 0. 92 for FMI. Conclusions Our predictive formulas accurately predicted fat mass and FMI with simple parameters (BMI, sex and age) in children with overweight and obesity. The proposed methodology could be applied in other fields. Further {{studies are needed to}} externally validate these formulas...|$|R
40|$|Bottom seated subsea {{pipelines}} {{resting on}} an erodible bed {{are subjected to}} scour and transport processes. These processes potentially undermine pipeline integrity by increasing free-span lengths and resulting in bending stresses. Making predictions of scour is fraught with challenges including: (1) determining when pipeline burial or lowering will occur; (2) unexpected obstacles to scour propagation (e. g. rocks); and, (3) varying incidence angles of fluid flow. Due to these challenges, pipeline surveys remain {{a key part of}} ongoing asset management. Field observations of phenomena such as self-burial and the impact of flow incidence angles and localized water velocities also help us characterize the subsea pipeline environment. The <b>validation</b> of empirical <b>formulas</b> through comparison to field data is essential for high-risk regions. For this investigation scour beneath a relatively deep and relatively shallow section of the Tasmanian Gas Pipeline in south-eastern Australia was examined. Our data were collected by an Autonomous Underwater Vehicle (AUV), a Remotely Operated Vehicle (ROV) and Acoustic Doppler Current Profiler (ADCP). tour findings suggest to refine empirical formulas to better explain the coupled effects of waves and currents. An improvement on these formulas may reduce the frequency of monitoring surveys required. AUV and ROV surveys are powerful resources to better inform decision making of these assets and better understand scour at individual sites...|$|R
40|$|Models {{of complex}} systems, {{composed}} of many heterogeneous interacting components, are challenging to analyse, {{due to the}} size {{and complexity of the}} network of interactions among the individual entities. The analysis becomes even more challenging when the spatio-temporal aspects of the system are to be taken into account. In this thesis, we propose a framework of efficient techniques to validate and analyse the behaviour of complex systems with spatio-temporal dynamics, both in the stochastic and deterministic cases. In particular, we define Signal Spatio-Temporal Logic (SSTL), a spatial extension of Signal Temporal Logic (STL). SSTL presents two new operators: the bounded somewhere and the bounded surround, {{that can be used to}} specify metric and topological properties in a discrete space. Given an SSTL formula, we design efficient monitoring algorithms to check its validity and compute its satisfaction (robustness) score over a spatio-temporal trace. To deal with stochastic systems, we define a stochastic version of the quantitative semantics of STL that we extended later to SSTL. We then combine it with machine learning techniques to define efficient parameter estimation and system design procedures. The specification and <b>validation</b> of SSTL <b>formulae</b> have been implemented in a Java tool, jSSTL. Finally, the expressivity of SSTL and the efficiency of the algorithms developed in this work are showed on interested and challenging case studies, including an epidemic spreading model of a waterborne disease, a pattern formation example for reaction-diffusion systems and a french flag model of the morphogen Bicoi...|$|R
