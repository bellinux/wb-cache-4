27|10000|Public
5000|$|The {{subsequent}} validation or {{verification of}} computer systems targets only the [...] "GxP critical" [...] requirements of computer systems. Evidence (e.g. screen prints) is gathered {{to document the}} validation exercise. In this way it is assured that systems are thoroughly tested, and that <b>validation</b> <b>and</b> <b>documentation</b> of the [...] "GxP critical" [...] aspects is performed in a risk-based manner, optimizing effort and ensuring that computer system's fitness for purpose is demonstrated.|$|E
40|$|Efficient {{control law}} {{analysis}} and design tools which properly {{account for the}} interaction of flexible structures, unsteady aerodynamics and active controls are developed. Development, application, <b>validation</b> <b>and</b> <b>documentation</b> of efficient multidisciplinary computer programs for analysis and design of active control laws are also discussed...|$|E
40|$|A botanical survey {{conducted}} in the Nilgiri Biosphere Reserve, Madukkarai and Kanuvai forests of Tamil Nadu, Southern Western Ghats led to the <b>validation</b> <b>and</b> <b>documentation</b> of many RET plants. The plants are properly identified and documented. Among them 51 selected species belonging to 38 genera, 26 families and two subfamilies are presented here. Short notes about their habit, habitat, locality and endemism are also included along with three colour photos...|$|E
5000|$|... (1986) Software <b>Validation,</b> Verification, Testing, <b>and</b> <b>Documentation</b> (Editor), Petrocelli Books ...|$|R
40|$|Abstract: We {{describe}} some {{tools to}} support distributed cooperative design <b>and</b> <b>validation</b> of software systems. Workers at di erent sites can collaborate on tasks including speci cation, re nement, <b>validation,</b> veri cation, <b>and</b> <b>documentation.</b> A distributed database supports alternative and incomplete activities, {{and can be}} read using any web browser; remote proof execution, animation, and informal explanation are supported, and results are broadcast by a protocol that prevents inconsistencies. The Kumo tool assists with <b>validations</b> <b>and</b> generates <b>documentation</b> websites. A range of formality is supported, from full mechanical proofs to informal of envelope &quot; arguments, using a fuzzy logic for con dence levels. Some conclusions drawn from experiments are reported. ...|$|R
40|$|Since {{the project}} is rapidly nearing conclusion, {{the status of the}} tasks {{outlined}} in the original proposal are briefly outlined. These tasks include: viscous interation and wake curvature effects; code optimization and design methodology studies; methods for the design of isolated regions; program improvement efforts; <b>and</b> <b>validation,</b> testing, <b>and</b> <b>documentation...</b>|$|R
40|$|Software Requirement Patterns (SRP) {{have been}} {{proposed}} as an artifact for fostering requirements reuse. PABRE is a framework that promotes the use of SRP {{as a means for}} requirements elicitation, <b>validation</b> <b>and</b> <b>documentation</b> in the context of IT procurement projects. In this paper, we present a catalogue of non-technical SRP included in the framework and present in detail some of them. We also introduce the motivation to arrive to these patterns. Peer ReviewedPostprint (published version...|$|E
30|$|Reuse {{allows the}} usage of {{acquired}} knowledge or parts developed in previous projects, which increases the chances of success in the elaboration of new projects (Palomares et al. 2013). Moreover, it can contribute to increased productivity since it leads to: a) improvement of product time to market; b) improvement of product quality; and c) reduction of development costs (Chernak 2012). Application of reuse approaches from the requirements phase can assist software engineers in the elicitation, <b>validation,</b> <b>and</b> <b>documentation</b> of software requirements, resulting in a more complete, consistent and unambiguous specification (Ketabchi et al. 2011; Palomares et al. 2011).|$|E
40|$|The {{aboriginal}} {{medical system}} prevalent among traditional healers of Wayanad has demonstrated a good practice, so bright {{future in the}} therapy of type 2 diabetes. Therefore, present study focused on identification <b>validation</b> <b>and</b> <b>documentation</b> such Ethno botanical polypharmacy prevalent in the district. A total of 47 species belonging to 44 genera comes under 29 families were identified being utilized in 23 different compound medicinal recipes for diabetic healthcare in Wayanad. These preparations and the herbal ingredients need scientific evaluation about their mechanism of action in living organism in heath as well as disease condition to confirm their activity against type 2 diabetes...|$|E
40|$|We {{describe}} some {{tools to}} support distributed cooperative design <b>and</b> <b>validation</b> of software systems. Workers at different sites can collaborate on tasks including specification, refinement, <b>validation,</b> verification, <b>and</b> <b>documentation.</b> A distributed database supports alternative and incomplete activities, {{and can be}} read using any web browser; remote proof execution, animation, and informal explanation are supported, and results are broadcast by a protocol that prevents inconsistencies. The Kumo tool assists with <b>validations</b> <b>and</b> generates <b>documentation</b> websites. A range of formality is supported, from full mechanical proofs to informal "back of envelope" arguments, using a fuzzy logic for confidence levels. Some conclusions drawn from experiments are reported. 1 Introduction Software engineering is very difficult. Typical projects have multiple workers, often at multiple sites with different schedules; then {{it is difficult to}} share information <b>and</b> coordinate tasks; <b>documentation</b> may [...] ...|$|R
40|$|This report {{provides}} {{an evaluation of}} the Software Quality Assurance Plan. The Software Quality Assurance Plan is intended to ensure all actions necessary for the software life cycle; verification <b>and</b> <b>validation</b> activities; <b>documentation</b> <b>and</b> deliverables; project management; configuration management, nonconformance reporting and corrective action; and quality assessment and improvement have been planned and a systematic pattern of all actions necessary to provide adequate confidence that a software product conforms to established technical requirements; and to meet the contractual commitments prepared by the sponsor; the Nuclear Regulatory Commission...|$|R
25|$|JSON Schema {{specifies}} a JSON-based {{format to}} define the structure of JSON data for <b>validation,</b> <b>documentation,</b> <b>and</b> interaction control. It provides a contract for the JSON data required by a given application, and how that data can be modified.|$|R
40|$|The {{development}} of computational icing simulation methods {{is making the}} transition form the research to common place use in design and certification efforts. As such, standards of code management, design <b>validation,</b> <b>and</b> <b>documentation</b> must be adjusted to accommodate the increased expectations of the user community with respect to accuracy, reliability, capability, and usability. This paper discusses these concepts with regard to current and future icing simulation code development efforts as implemented by the Icing Branch of the NASA Lewis Research Center {{in collaboration with the}} NASA Lewis Engineering Design and Analysis Division. With the application of the techniques outlined in this paper, the LEWICE ice accretion code has become a more stable and reliable software product...|$|E
40|$|Abstract — Requirements {{engineering}} {{is the fundamental}} aspect of Software Process development. It is modularized into various stages of elicitation, Analysis, <b>Validation</b> <b>and</b> <b>Documentation.</b> The process of building software starts with requirement elicitation. This phase determines what problem needs to be solved or what software has to be developed. Requirements are constraints or demands which are described by the customers and must be met within a specific time period. While, Elicitation is all about gathering necessities and requirements from the end-user’s to recognize what a software system should consist of, acquisition of requirements is done via an interaction between the analysts and the stakeholders. If the need of the stakeholders doesn’t meet then the quality software will not be produced. If an error occurs in Requirement Elicitation stage, then i...|$|E
40|$|This is an {{exploratory}} study of methods for {{the preparation of}} computer curriculum materials. It deals with courseware development procedures for the PLATO IV computer-based education system, and draws on interviews with over 100 persons engaged in courseware production. The report presents a five stage model of development: (1) planning, (2) production, (3) review, (4) <b>validation</b> <b>and</b> <b>documentation,</b> and (5) implementation. This model then serves {{as a basis for}} the discussion of various group structures which range from the independent developer to teas members with special skills. Features of the PLATO system did affect the group productivity as well as quality of lesson material: these influences, both beneficial and detrimental, appear to be important factors to consider in the organisation and management of-courseware development groups. A major objective of this study, beyond the description o...|$|E
50|$|Mathcad is {{computer}} software primarily {{intended for the}} verification, <b>validation,</b> <b>documentation</b> <b>and</b> re-use of engineering calculations. First introduced in 1986 on DOS, {{it was the first}} to introduce live editing of typeset mathematical notation, combined with its automatic computations.|$|R
50|$|JSON Schema {{specifies}} a JSON-based {{format to}} define the structure of JSON data for <b>validation,</b> <b>documentation,</b> <b>and</b> interaction control. It provides a contract for the JSON data required by a given application, and how that data can be modified.|$|R
50|$|The {{electrotechnical}} module in Automation Studio is {{used for}} design, simulation, <b>validation,</b> <b>documentation</b> <b>and</b> troubleshooting of electrical diagrams. It includes multi-line and one-line representation according to the users' choice. The {{different aspects of the}} IEC and NEMA international standards are respected: components’ identification, symbols, ratings, ports names, … etc.|$|R
40|$|Reverse-engineering {{application}} codes back to {{the design}} and specification stage may entail the recreation of lost information for an application, or the extraction of new information. We describe techniques which produce abstractions in object-oriented and functional notations, thus aiding the comprehension of the essential structure and operations of the application, and providing formal design information which may make the code much more maintainable and certainly more respectable. The two types of application considered here are (1) data processing applications written in Cobol [...] of primary importance due to their pre-dominance in present computing practice [...] and (2) scientific applications written in Fortran. These two require somewhat different abstraction approaches. 1 Introduction The Programming Research Group at Oxford University is participating in the ESPRIT II project REDO 1 on the Maintenance, <b>Validation</b> <b>and</b> <b>Documentation</b> of Software Systems. As part of this proje [...] ...|$|E
40|$|The {{importance}} of verification, <b>validation</b> <b>and</b> <b>documentation</b> of simulation models is widely recognised, {{at least in}} principle. However, in practice, inadequate model management procedures can lead to insufficient information being available to allow a model to be applied with confidence or {{for it to be}} re-used without difficulty and much additional effort. The ease with which a model can be understood by someone not involved in its development depends on the transparency of the model development process. This paper reviews ideas associated with transparency and model management. It also includes discussion of some related issues that are believed to be particularly important, such as identifiability and experimental design for model validation. Some recent developments in engineering applications and in physiological and health-care modelling are discussed, along with the responsibilities of the academic community in giving more emphasis to simulation model testing and transparency...|$|E
30|$|Results: After {{ensuring}} that {{the production of a}} new radiopharmaceutical is robust and repeatable and all analytical methods including sterility and endotoxin tests are validated, the process can be validated according to a written validation plan. In addition bioburden (number of bacteria living on the drug solution before sterilization) must be determined. The aseptic processing of operators must also be confirmed by performing media fills (the performance of an aseptic manufacturing procedure using a sterile microbiological growth medium in place of the drug solution). At TPC the process validation includes three consecutive process validation batches which shall fulfill all specifications for the given radiopharmaceutical. This also qualifies the operator for production. Additional batches must be done in order to qualify more operators. Finally, process <b>validation</b> <b>and</b> <b>documentation</b> is compiled. Process validation report, method description for preparation and quality control and master batch record are written.|$|E
50|$|Atmospheric Model Intercomparison Project (AMIP) is a {{standard}} experimental protocol for global atmospheric general circulation models (AGCMs). It provides a community-based infrastructure in support of climate model diagnosis, <b>validation,</b> intercomparison, <b>documentation</b> <b>and</b> data access. Virtually the entire international climate modeling community has participated in this project {{since its inception in}} 1990.|$|R
50|$|Requirements {{engineering}} and software architecture {{can be seen}} as complementary approaches: while software architecture targets the 'solution space' or the 'how', requirements engineering addresses the 'problem space' or the 'what'. Requirements engineering entails the elicitation, negotiation, specification, <b>validation,</b> <b>documentation</b> <b>and</b> management of requirements. Both requirements {{engineering and}} software architecture revolve around stakeholder concerns, needs and wishes.|$|R
40|$|The Use Case, Responsibility Driven Analysis and Design (URDAD) {{methodology}} is {{a methodology}} for technology neutral design generating the Platform Independent Model of the Object Management Group’s Model Driven Architecture. It requires the core modeling {{to be done}} in the problem space by domain specialists and not in the solution space by technology specialists. URDAD allows for formal elements to be added by different role players at different stages of the model refinement, whilst aiming to preserve agility of the outputs and low cost of the process generating the outputs. This paper discusses the semi-formal aspects of URDAD which facilitate model <b>validation</b> <b>and</b> testing, <b>documentation</b> generation <b>and</b> automated implementation mapping as well as aspects which promote agility and low cost...|$|R
40|$|A {{growing body}} of {{empirical}} research has examined large, successful open source software projects such as the Linux kernel, Apache web server, and Mozilla web browser. Do these results extend to small open source efforts involving a handful of developers? A study of the OpenEMR open source electronic medical record project was conducted, {{with the goal of}} understanding how requirements are elicited, documented, agreed, and validated in a small open source software project The results show that the majority of features are asserted by developers, based on either their personal experience, or knowledge of users’ needs. Relatively few were requested directly by users. <b>Validation</b> <b>and</b> <b>documentation</b> took the form of informal discussions via the project’s developer mailing list. These results are consistent with an earlier study of the Firefox web browser, suggesting that there is a common open source requirements approach that is independent of project size...|$|E
40|$|Vocabularies {{typically}} {{reflect a}} consensus among experts in a certain application domain. They are thus implemented in collaboration of domain experts and knowledge engineers. Particularly the presence of domain experts with little technical background requires a low-threshold vocabulary engineering methodology. This methodology should be im- plementable without dependencies on complex software components, it should provide collaborators with comprehensible feedback on syntax and semantics errors in a tight loop, and it should give access to a human- readable presentation of the vocabulary. Inspired by agile software and content development methodologies, we define the VoCol methodology to address these requirements. We implemented a prototype based on a loose coupling of <b>validation</b> <b>and</b> <b>documentation</b> generation components {{on top of a}} standard Git repository. All of these components, even the repository engine, can be exchanged with little effort. By evaluating the usefulness of error feedback of different tools in the realistic setting of an emerging mobility vocabulary we prove, however, that our choice of the crucial validation component is workable...|$|E
40|$|Many BPR {{practitioners}} {{have indicated}} that the application of information technology {{is critical to the}} success of their BPR. ERP is currently one of the most popular information systems being employed to help organisations gain competitive advantage. Companies worldwide have attempted to implement ERP systems, but failure has been experienced by many. Very often this failure has been due to managerial rather than technical issues. In this paper, the managerial and organisational aspects of ERP are elaborated upon. In particular, the experience of ERP implementation in China is emphasised, and the differences in characteristics and strategy between the ERP implementation process and the ERP system are identified. It has been found that the ERP system is a virtual enterprise or model that has its own organisational structure and normative process, and that can be managed scientifically. ERP implementation requires mapping this virtual enterprise to the status of a real entity. To map the virtual enterprise into an organisation, we proposed the IDEF methodology to cater to the macroscopic context of reengineering. IDEF provides a practical vehicle through which management and organisation are connected to the virtual ERP system for customisation, <b>validation</b> <b>and</b> <b>documentation.</b> Department of Industrial and Systems Engineerin...|$|E
40|$|The {{purpose of}} this {{research}} was to produce of learning materials in the form of handout cultural base which valid and good category to sains(physics) learning in junior high school. The type of this research is development with 4 D developt design by Thiagarajan. The method to data collection is <b>validation,</b> post-test, questionnaire, <b>and</b> <b>documentation.</b> The method to define simple area is a simple random sampling. Product from this research is handout with validation quality 4, 2 from validators, students learning outcomes to state clear classical 75...|$|R
40|$|The Terrestrial and Planetary Environments Team at Marshall Space Flight Center (MSFC) {{proposes to}} {{facilitate}} <b>validation,</b> <b>documentation,</b> <b>and</b> adoption of updated Range Reference Atmospheres (RRAs). This viewgraph presentation describes the plan, focusing on seven tasks: 1) Document data sources; 2) Document analytical models; 3) Document data processing procedures; 4) Compare updates to 1983 versions; 5) Compile written documentation; 6) Obtain approval for final volumes; 7) Publish new RRA datasets/documents...|$|R
30|$|Because {{training}} courses {{are not easily}} available worldwide, {{and because of the}} above mentioned limitations (short duration <b>and</b> lack of <b>validation,</b> evaluation, <b>and</b> <b>documentation),</b> the classic apprentice–tutor model is still the most used method for laparoscopic training. In this model of “see one, do one, teach one,” feedback is directly provided during surgery in the OR, and the surgery is learned by the student through simple observation and, later, imitation of the actions of a skilled mentor. This model, which has many advantages and is undoubtedly ideal for specific procedures and at specific stages of the learning process, is currently limited {{by the fact that the}} reported learning curves (i.e., time required for reaching proficiency) are very long [22 – 26], that residents’ working hours are shorter, and a great number of surgical procedures and skilled mentors are required.|$|R
40|$|The "United States Street, Landmark, and Postal Address Data Standard " is a draft data {{standard}} for United States address information. The draft standard defines and specifies elements and structures for organizing address data, defines tests of address data quality, and facilitates address data exchange. The draft standard has four parts: Data Content, Data Classification, Data Quality, and Data Exchange. The Address Standard as now drafted {{goes well beyond}} existing postal and assignment standards in the following respects: 1. It proposes a new definition for addresses: “An address specifies a location by reference to a thoroughfare or landmark; or it specifies a point of postal delivery. ” 2. It defines the address elements and attributes needed for database records, data <b>validation</b> <b>and</b> <b>documentation,</b> and data exchange, {{as well as for}} creation of mailing lists. 3. It classifies addresses by their internal syntax, rather than their business purpose. 4. It provides a simple, complete taxonomy of US address patterns. 5. It introduces the idea of an address scheme (a set of local rules by which new addresses are assigned and old ones checked within a specific area). 6. It provides for an address identifier for each different address...|$|E
40|$|AbstractDuring {{the past}} two decades the use and {{refinements}} of imaging modalities have markedly increased making it possible to image embryos and fetuses used in pivotal nonclinical studies submitted to regulatory agencies. Implementing these technologies into the Good Laboratory Practice environment requires rigorous testing, <b>validation,</b> <b>and</b> <b>documentation</b> to ensure the reproducibility of data. A workshop on current practices and regulatory requirements was held with the goal of defining minimal criteria for the proper implementation of these technologies and subsequent submission to regulatory agencies. Micro-computed tomography (micro-CT) is especially well suited for high-throughput evaluations, and is gaining popularity to evaluate fetal skeletons to assess the potential developmental toxicity of test agents. This workshop was convened to help scientists in the developmental toxicology field understand and apply micro-CT technology to nonclinical toxicology studies and facilitate the regulatory acceptance of imaging data. Presentations and workshop discussions covered: (1) principles of micro-CT fetal imaging; (2) concordance of findings with conventional skeletal evaluations; and (3) regulatory requirements for validating the system. Establishing these requirements for micro-CT examination can provide a path forward for laboratories considering implementing this technology and provide regulatory agencies with a basis to consider the acceptability of data generated via this technology...|$|E
40|$|Site visits {{were made}} to DOE {{beryllium}} handling facilities at the Rocky Flats Plant; Oak Ridge Y- 12 Plant, LLNL; {{as well as to}} the AWE Cardiff Facility. Available historical data from each facility describing its beryllium control program were obtained and summarized in this report. The AWE Cardiff Facility computerized Be personal and area air-sampling database was obtained and a preliminary evaluation was conducted. Further <b>validation</b> <b>and</b> <b>documentation</b> of this database will be very useful in estimating worker Be. exposure as well as in identifying the source potential for a variety of Be fabrication activities. Although all of the Be control programs recognized the toxicity of Be and its compounds, their established control procedures differed significantly. The Cardiff Facility, which was designed for only Be work, implemented a very strict Be control program that has essentially remained unchanged, even to today. LLNL and the Oak Ridge Y- 12 Plant also implemented a strict Be control program, but personal sampling was not used until the mid 1980 s to evaluate worker exposure. The Rocky Flats plant implemented significantly less controls on beryllium processing than the three previous facilities. In addition, records were less available, management and industrial hygiene staff turned over regularly, and less control was evident from a management perspective...|$|E
40|$|Graduation date: 1977 The FLEX/REFLEX {{paradigm}} {{is applied to}} the description of a computer program system. The {{paradigm is}} shown to be relevant and appropriate to computer program systems and to advantageously display and structure the general hierarchical characteristics of computer program systems. Program systems characterized in the paradigm are described both holistically and mechanistically at each hierarchical level. This, together with an explicit description of and distinction between input quantities and output quantities, permits effective and intuitivly logical management of even large systems. The problems of program synthesis, modification, testing <b>and</b> <b>validation,</b> debugging, <b>and</b> <b>documentation</b> may be decomposed into "sub-problems" associated with each module in the hierarchical structure. Only problems relevant to the holistic behavior of the module need to be considered. Problems associated with the behavior of the elements of the module are dealt with at the next lower hierarchical level, when the element itself is considered as a module. The paradigm is also shown to be consistent and compatable with top-down structured programming...|$|R
30|$|Our study {{confronted the}} {{objective}} of most training courses: exposing the trainee to a task (few repetitions) with {{the objective of}} ascertaining the full acquisition of the task (many repetitions). Our results indicate the relevance of many repetitions for reaching an optimal level, similar to previously reports in animal models [8, 21], which is virtually impossible in the classic 2 – 3  day hands-on training courses. In addition, our findings highlight the importance of proper <b>validation,</b> data evaluation, <b>and</b> <b>documentation,</b> which is in sharp contrast with most training courses that are evaluated by participant satisfaction only and with insufficiently documented results.|$|R
40|$|The data {{management}} and processing supplied by ST Systems Corporation (STX) for the Stratospheric Aerosol Measurement 2 (SAM 2) and Stratospheric Aerosol and Gas Experiment (SAGE) experiments {{for the years}} 1983 to 1986 are described. Included are discussions of data <b>validation,</b> <b>documentation,</b> <b>and</b> scientific analysis, {{as well as the}} archival schedule met by the operational reduction of SAM 2 and SAGE data. Work under this contract resulted in the archiving of the first seven years of SAM 2 data and all three years of SAGE data. A list of publications and presentations supported was also included...|$|R
