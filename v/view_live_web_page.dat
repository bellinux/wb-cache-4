0|10000|Public
40|$|Abstract. Web {{annotation}} {{has been}} receiving increased attention re-cently {{with the organization}} of the Open Annotation Collaboration and new tools for open annotation, such as Hypothes. is. In this paper, we in-vestigate the prevalence of orphaned annotations, where a <b>live</b> <b>Web</b> <b>page</b> no longer contains the text that had previously been annotated in the Hypothes. is annotation system (containing 6281 highlighted text anno-tations). We found that about 27 % of highlighted text annotations can no longer be attached to their <b>live</b> <b>Web</b> <b>pages.</b> Unfortunately, only about 3. 5 % of these orphaned annotations can be reattached using the hold-ings of current public web archives. For those annotations that are still attached, 61 % are in danger of becoming orphans if the <b>live</b> <b>Web</b> <b>page</b> changes. This points to the need for archiving the target of annotations at the time the annotation is created...|$|R
40|$|Web {{annotation}} {{has been}} receiving increased attention recently {{with the organization}} of the Open Annotation Collaboration and new tools for open annotation, such as Hypothes. is. We investigate the prevalence of orphaned annotations, where neither the <b>live</b> <b>Web</b> <b>page</b> nor an archived copy of the <b>Web</b> <b>page</b> contains the text that had previously been annotated in the Hypothes. is annotation system (containing 20, 953 highlighted text annotations). We found that about 22 % of highlighted text annotations can no longer be attached to their <b>live</b> <b>Web</b> <b>pages.</b> Unfortunately, only about 12 % of these annotations can be reattached using the holdings of current public web archives, leaving the remaining 88 % of these annotations orphaned. For those annotations that are still attached, 53 % are in danger of becoming orphans if the <b>live</b> <b>Web</b> <b>page</b> changes. This points to the need for archiving the target of an annotation at the time the annotation is created...|$|R
40|$|Scholars are {{increasingly}} citing electronic “web references” {{which are not}} preserved in libraries or full text archives. WebCite is a new standard for citing web references. To “webcite” a document involves archiving the cited <b>Web</b> <b>page</b> through www. webcitation. org and citing the WebCite permalink instead of (or in addition to) the unstable <b>live</b> <b>Web</b> <b>page...</b>|$|R
40|$|Move to {{the next}} level using PowerPoint as an {{authoring}} tool to make your presentations more engaging and interactive to students. Learn how to escape linear presentations using bullets to more interactive presentations and make your presentation truly memorable by incorporating multimedia elements including sound, images, videos (with and without Internet connection), clip art and animations. You can also incorporate <b>live</b> <b>web</b> <b>pages</b> into your presentation for added impact...|$|R
40|$|We {{describe}} Mink, a new {{web browser}} extension that pro-vides a different model for {{integration of the}} <b>live</b> and archived <b>web.</b> While a user browses the <b>live</b> <b>web,</b> Mink actively queries the archives and reports other instances of the page in the archives without requiring active querying by the user. Further, by querying the archives dynamically and asyn-chronously, a user can view {{the extent to which}} the currently <b>viewed</b> <b>page</b> on the <b>live</b> <b>web</b> has been archived and proac-tively submit a request to various archives using an overlay on the <b>live</b> <b>web</b> <b>page</b> and a simple interface...|$|R
50|$|During cobrowsing sessions, some {{solutions}} {{can display}} multiple labeled cursors. Additionally, some modern cobrowsing solutions will also offer observation capabilities whereby a second person can <b>view</b> a <b>live</b> <b>web</b> browsing session, but {{not participate in}} its navigation.|$|R
40|$|Text {{extraction}} from <b>web</b> <b>pages</b> {{has many}} applications, including web crawling optimization and document clustering. Though {{much has been}} written about the acquisition of content from <b>live</b> <b>web</b> <b>pages,</b> content acquisition of archived <b>web</b> <b>pages,</b> known as mementos, remains a relatively new enterprise. In the course of conducting a study with almost 700, 000 <b>web</b> <b>pages,</b> we encountered issues acquiring mementos and extracting text from them. The acquisition of memento content via HTTP is expected to be a relatively painless exercise, but we have found cases to the contrary. We also find that the parsing of HTML, already known to be problematic, can be more complex when one attempts to extract the text of mementos across many web archives, due to issues involving different memento presentation behaviors, as well as the age of the HTML in their mementos. For the benefit of others acquiring mementos across many web archives, we document those experiences here. Comment: 16 pages, 6 figures, 13 listing...|$|R
40|$|Abstract. This paper {{presents}} {{a novel approach}} to successfully predict <b>Web</b> <b>pages</b> that {{are most likely to}} be re-accessed in a given period of time. We present the design of an intelligent predictor that can be implemented on a Web server to guide caching strategies. Our approach is adaptive and learns the changing access patterns of <b>pages</b> in a <b>Web</b> site. The core of our predictor is a neural network that uses a back-propagation learning rule. We present results of the application of this predictor on static data using log files; it can be extended to learn the distribution of <b>live</b> <b>Web</b> <b>page</b> access patterns. Our simulations show fast learning, uniformly good prediction, and up to 82 % correct prediction for the following six months based on a oneday training data. This long-range prediction accuracy is attributed to the static structure of the test Web site. ...|$|R
40|$|Named entity extractors are {{a popular}} means for {{enriching}} documents with semantic annotations. Both the overlap {{and the increasing}} diversity in the capabilities and in the vocabularies of the annotators motivate the need for managing and integrating semantic annotations in a coherent and uniform fashion. ROSEANN is a framework for the management and the reconciliation of semantic annotations. It provides end-users and programmers with a unified view over the results of multiple online and standalone annotators, linking them to an integrated ontology of their vocabularies, and supporting a variety of document formats such as: plain text, <b>live</b> <b>Web</b> <b>pages,</b> and PDF documents. Although ROSEANN provides two pre-defined algorithms for conflict resolution – one supervised, appropriate when representative training data is available, and one unsupervised – it also allows application developers to define their own integration techniques, as well as extending the pool of annotators as new ones become available...|$|R
40|$|Rivet is {{the first}} fully-featured, browser-agnostic re-mote {{debugger}} for web applications. Using Rivet, de-velopers can inspect and modify the state of <b>live</b> <b>web</b> <b>pages</b> that are running inside unmodified end-user web browsers. This allows developers to explore real applica-tion bugs {{in the context of}} the actual machines on which those bugs occur. To make an application Rivet-aware, developers simply add the Rivet JavaScript library to the client-side portion of the application. Later, when a user detects a problem with the application, the user informs Rivet; in turn, Rivet pauses the application and notifies a remote debug server that a debuggable session is avail-able. The server can launch an interactive debugger front-end for a human developer, or use Rivet’s live patching mechanism to automatically install a fix on the client or run diagnostics for offline analysis. Experiments show that Rivet imposes negligible overhead during normal ap-plication operation. At debug time, Rivet’s network foot-print is small, and Rivet is computationally fast enough to support non-trivial diagnostics and live patches. ...|$|R
50|$|The {{attendance}} of 14,974 was the sixth highest ever for a camogie-only final day in Croke Park. An average TV audience of 218,000 tuned in for full match coverage on RTÉ sport (including half and full time analysis), recording a Neilsen rating of 5.22 and a {{market share of}} 23.14, the fifth highest viewership in the game’s history. The final minutes of the game attracted a peak of 303,000 viewers. A further 3,312 <b>viewed</b> on <b>live</b> <b>web</b> feed.|$|R
40|$|Scholars are {{increasingly}} citing electronic ?web references? {{which are not}} preserved in libraries or full text archives. WebCite is a new standard for citing web references. To ?webcite? a document involves archiving the cited <b>Web</b> <b>page</b> through www. webcitation. org and citing the WebCite permalink instead of (or in addition to) the unstable <b>live</b> <b>Web</b> <b>page.</b> Almost 200 journals are already using the system. We discuss the rationale for WebCite, its technology, and how scholars, editors, and publishers can benefit from the service. Citing scholars initiate an archiving process of all cited Web references, ideally before they submit a manuscript. Authors of online documents and websites which {{are expected to be}} cited by others can ensure that their work is permanently available by creating an archived copy using WebCite and providing the citation information including the WebCite link on their Web document(s). Editors should ask their authors to cache all cited Web addresses (Uniform Resource Locators, or URLs) ?prospectively? before submitting their manuscripts to their journal. Editors and publishers should also instruct their copyeditors to cache cited Web material if the author has not done so already. Finally, WebCite can process publisher submitted ?citing articles? (submitted for example as eXtensible Markup Language [XML] documents) to automatically archive all cited <b>Web</b> <b>pages</b> shortly before or on publication. Finally, WebCite can act as a focussed crawler, caching retrospectively references of already published articles. Copyright issues are addressed by honouring respective Internet standards (robot exclusion files, no-cache and no-archive tags). Long-term preservation is ensured by agreements with libraries and digital preservation organizations. The resulting WebCite Index may also have applications for research assessment exercises, being able to measure the impact of Web services and published Web documents through access and Web citation metrics...|$|R
50|$|Windows Vista {{provides}} gadgets {{that the}} user can {{place on the}} Windows Sidebar (Sidebar gadgets), a Windows <b>Live</b> start <b>page</b> (<b>Web</b> gadgets), or an external display, such as the user's mobile phone (SideShow gadgets). The Windows Sidebar was a visible partition in Windows Vista, and was eliminated in Windows 7, along with Sidebar gadgets which were changed to Desktop Gadgets.|$|R
5000|$|Slooh is {{a robotic}} {{telescope}} service {{that can be}} <b>viewed</b> <b>live</b> through a <b>web</b> browser with Flash plug-in. It {{was not the first}} robotic telescope, but it was the first that offered [...] "live" [...] viewing through a telescope via the web. Other online telescopes traditionally email a picture to the recipient. The site has a patent on their live image processing method. Slooh is an online astronomy platform with live-views and telescope rental for a fee. Observations come from a global network of telescopes located in places including Spain and Chile.|$|R
40|$|Many {{people use}} the Web as {{the main source of}} {{information}} in their daily <b>lives.</b> However, most <b>web</b> <b>pages</b> contain noninformative components such as side bars, footers, headers, and advertisements, which are undesirable for certain applications like printing. We demonstrate a system that automatically extracts the informative contents from news- and blog-like <b>web</b> <b>pages.</b> In contrast to many existing methods that are limited to identifying only the text or the bounding rectangular region, our system not only identifies the content but also the structural roles of various content components such as title, paragraphs, images and captions. The structural information enables re-layout of the content in a pleasing way. Besides the article text extraction, our system includes the following components: 1) print-link detection to identify the URL link for printing, and to use it for more reliable analysis and recognition; 2) title detection incorporating both visual cues and HTML tags; 3) image and caption detection utilizing extensive visual cues; 4) multiple-page and next page URL detection. The performance of our system has been thoroughly evaluated using a human labeled ground truth dataset consisting of 2000 <b>web</b> <b>pages</b> from 100 major web sites. We show accurate results using such a dataset...|$|R
40|$|The U. S. Centennial of Flight Commission {{developed}} and maintained a public {{web site that}} included activities related to the centennial of flight celebration {{and the history of}} aviation. The web site, www. centennialofflight. gov, was continually updated with educational and historical information, events, sights and sounds, and Commission information from its inception to June 2004. This DVD contains a 'snap shot' of the web site as of April 2004. The Web site on this DVD can be enjoyed without an Internet connection although in some places, you will be given links to online content. DVD content includes: 1) About the Commission - Information on the legislation, the Commissioners and Advisory Board members, news, the National Plans, meeting minutes and status reports; 2) Calendar of Events - A comprehensive list of activities, symposiums, exhibits, air shows, educational activities and more that took place through March 2004; 3) Wright Brothers History - The Library of Congress bibliography of Wright-related resources as well as the Chronology and Flight Log; the Brunsman articles; interactive learning modules from The Wright Experience; short informative essays and a series of links to other Wright brothers information sources. 4) History of Flight - Essays and images on the history of flight; 5) Sights and Sounds - Images, movies and special collections that capture the accomplishments of the Wright brothers and others who made significant contributions throughout the history of aviation and aerospace. As part of the NASA Art Program, a centennial song, 'Way Up There,' was commissioned; 6) Licensed Products - View collections of souvenirs and gift items to commemorate the 100 th anniversary of the first powered flight; 7) Education - Resources that will help educators and their students celebrate 100 years of flight. Teachers can download Wright brothers posters and a Centennial of Flight bookmark, <b>view</b> <b>live</b> <b>Web</b> casts, and access an Educational Resources Center Matrix representing more than 50 government, industry and labor organizations promoting aviation and aerospace education...|$|R
40|$|Jeanie M. Welch is a Professor and Business Reference Librarian at the University of North Carolina at Charlotte. She holds a B. A. and M. A. L. {{from the}} University of Denver and a Master of International Management from Thunderbird, The Garvin School of International Management. She {{is the author}} of two books and {{numerous}} articles in professional journals and winner of the 1996 Dun & Bradstreet Online Champion Award. Reference service has evolved in the digital age to include email, <b>live</b> chat, and <b>Web</b> <b>pages.</b> This had led to a need to include these new means of providing service into the statistical reporting of reference activity. As traditional numbers for reference service have stagnated or declined, the inclusion of email, <b>live</b> chat, and <b>Web</b> <b>pages</b> in statistical reporting provides a more accurate picture of reference-related activities. This paper discusses going beyond the traditional measures of reference service (i. e., number of reference/directional questions and number of telephone questions received at the reference desk) to include the number of email queries received by individual reference librarians, the number of queries received in live chat sessions (both local and consortium-based group chat), and the number of visits to reference-generated library Web sites. This paper presents the challenges of including these new categories in reference service statistical reporting and proposes a model for their inclusion. These challenges include the need to expand the traditional statistical reporting criteria required by government and professional agencies and the need for standardization in reporting new types of reference transactions, including virtual reference. This paper also discusses the new National Information Standards Organization's NISO Z 39. 7 - 2004 standards that include the reporting of the usage of library-generated <b>Web</b> <b>pages</b> and e-mail and virtual reference queries in a reference department. The discussion includes two specific subcategories in the new NISO standards (7. 3. 1 Virtual Reference Transactions and 7. 7. 1. 5 Virtual Visits) that provide definitions for these types of reference activities. Gathering these new statistics will be discussed as well as the importance of including such statistics provides a more complete of reference activity at a time when traditional forms of reference service have declined...|$|R
50|$|The {{data for}} {{years prior to}} 2016 were {{gathered}} on March 13, 2016 from historical WFS and INAFA <b>web</b> <b>pages</b> provided by the Wayback Machine.The data for 2016 and 2017 were gathered from the <b>live</b> WFS <b>web</b> site on March 4, 2016 and January 3, 2017, respectively.|$|R
40|$|Search {{engines are}} the primary {{gateways}} of information access on the Web today. Behind the scenes, search engines crawl the Web to populate a local indexed repository of <b>Web</b> <b>pages,</b> used to answer user search queries. In an aggregate sense, the Web is very dynamic, causing any repository of <b>Web</b> <b>pages</b> to become out of date over time, which in turn causes query answer quality to degrade. Given the considerable size, dynamicity, and degree of autonomy of the Web as a whole, it is not feasible for a search engine to maintain its repository exactly synchronized with the Web. In this paper we study how to schedule <b>Web</b> <b>pages</b> for selective (re) downloading into a search engine repository. The scheduling objective is to maximize {{the quality of the}} user experience for those who query the search engine. We begin with a quantitative characterization {{of the way in which}} the discrepancy between the content of the repository and the current content of the <b>live</b> <b>Web</b> impacts the quality of the user experience. This characterization leads to a usercentric metric of the quality of a search engine’s local repository. We use this metric to derive a policy for scheduling <b>Web</b> <b>page</b> (re) downloading that is driven by search engine usage and free of exterior tuning parameters. We then focus on the important subproblem of scheduling refreshing of <b>Web</b> <b>pages</b> already present in the repository, and show how to compute the priorities efficiently. We provide extensive empirical comparisons of our user-centric method against prior <b>Web</b> <b>page</b> refresh strategies, using real Web data. Our results demonstrate that our method requires far fewer resources to maintain same search engine quality level for users, leaving substantially more resources available for incorporating new <b>Web</b> <b>pages</b> into the search repository...|$|R
30|$|MBCC-HITS {{algorithm}} is proposed based on hyper-text induced topic selection (HITS) algorithm [16]. Authority score and hub score are defined as the measurements of <b>web</b> <b>page</b> importance in HITS algorithm. Authority score and hub score represent the contributions of a <b>web</b> <b>page</b> to information originality and transmission, respectively [17]. Internet can be abstracted as a directed and unweighted graph according to connection relations among <b>web</b> <b>pages.</b> Each <b>web</b> <b>page</b> corresponds to a node in the directed graph. The directions of edges are given according to the directions of corresponding hyperlinks. Authority score of a <b>web</b> <b>page</b> equals to the sum of hub scores of <b>web</b> <b>pages</b> (i.e. in-linked <b>web</b> <b>pages)</b> which cite this <b>web</b> <b>page.</b> It reflects {{the importance of a}} <b>web</b> <b>page</b> from the perspective of in-degree. Hub score of a <b>web</b> <b>page</b> equals to the sum of authority scores of <b>web</b> <b>pages</b> (i.e. out-linked <b>web</b> <b>pages)</b> which this <b>web</b> <b>page</b> cites. It reflects the importance of a <b>web</b> <b>page</b> from the perspective of out-degree.|$|R
40|$|It is {{attractive}} to extract parts of <b>Web</b> <b>pages</b> for the following two purposes. One is to clip parts of <b>Web</b> <b>pages</b> as we clip articles of newspapers. Another is to utilize information on <b>Web</b> <b>pages</b> by software. In this paper we define operations to extract parts of <b>Web</b> <b>pages,</b> namely path set operations. The operations are for both clipping of parts of <b>Web</b> <b>pages</b> and information extraction from <b>Web</b> <b>pages.</b> <b>Web</b> <b>page</b> clipping is extraction of parts of <b>Web</b> <b>pages</b> keeping their view information. Information extraction from <b>Web</b> <b>pages</b> is transformation of <b>Web</b> <b>pages</b> to tractable structures. We show that we can easily extract parts of <b>Web</b> <b>pages</b> for the two purposes by the operations, and also show that procedures based on the operations are somewhat robust against update of <b>Web</b> <b>pages.</b> 1...|$|R
40|$|The {{main goal}} of <b>web</b> <b>pages</b> ranking {{is to find}} the interrelated pages. In this paper, we {{introduce}} an algorithm called FPR-DLA. In the proposed method learning automata is assigned to each <b>web</b> <b>page</b> which its function is determining the weight of hyperlinks between <b>web</b> <b>pages.</b> Also for determining the weight of each <b>web</b> <b>page</b> parameters such as time duration on a <b>web</b> <b>page</b> and the importance of <b>web</b> <b>pages</b> are considered. Time duration on a <b>web</b> <b>page</b> and the importance of <b>web</b> <b>pages</b> are characterized as a fuzzy linguistic variable. The proposed algorithm calculates the rank of each <b>web</b> <b>page</b> as recursive according to the weights of each <b>web</b> <b>page</b> and hyperlinks between <b>web</b> <b>pages.</b> Experimental results show that the proposed method has a considerable efficiency in determining the rank of <b>web</b> <b>pages...</b>|$|R
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. The Adult Learners' Lives project was a major National Research and Development Centre (NRDC) research study carried out {{by members of the}} Lancaster Literacy Research Centre. The overall aim of the project was to develop understandings of the relationships between learners' lives and the literacy, language and numeracy (LLN) learning in which they were engaged, and to draw out the implications for the Skills for Life strategy. Starting from the perspectives of the adult learners the project focused on issues around motivation, participation, persistence and engagement. The first year of the Adult Learners' Lives project concentrated on college environments. Working with teacher-researchers enabled the research to be embedded in real classrooms and ensured that it had an impact on practice. In the second year of the project the work was on other sites with learners in what has been referred to by others as provision for the 'hard to reach'. This included a drug support and aftercare centre, a young homeless scheme and a domestic violence project. Contact was also maintained with 53 learners who represent the longitudinal cohort of the study. Work was collaborative with practitioners from all sites which allowed for exploration of participation and engagement with learners who frequently have issues in their lives that impact upon learning. The project investigated adult learning at three case study sites of Blackburn, Lancaster and Liverpool. The study consists of a range of interviews with learners and teachers, field notes, observations, focus groups and participants' creative writing and photographs. Further information about the project {{can be found on the}} Adult Learners' <b>Lives</b> <b>web</b> <b>page.</b> Main Topics : Some of the initial outcomes of the research include: relationships matter in learning, including teacher/student and student/student relationships learning environments often offer structure and stability in learners' lives being in control is key motivation for learning health (both physical and mental) is often a barrier to learning there is a need to assess and recognise small gains in LLN skills and the wider benefits to learning and learners. Learners value knowing what progress they have made there is a complex relationship between teaching and learning: learners do not learn what teachers teach there needs to be more effective inter-agency response to the social and learning needs of students seeking asylum in English for Speakers of Other Languages (ESOL) classes, learners often express satisfaction with their classes, but there is a need for more free use of language and 'bringing the outside in' as part of the learning process involving teachers in research projects can have great impact, on the teachers' professional development, on the culture of their workplaces, and on regional networks </ul...|$|R
40|$|Key Words: meta-tag, {{automatic}} classification, <b>web</b> <b>page</b> classification, weka {{naive bayes}} Recently, {{the amount of}} <b>web</b> <b>pages,</b> which include various information, has been drastically increased according to the explosive increase of WWW usage. Therefore, the need for <b>web</b> <b>page</b> classification arose {{in order to make}} it easier to access <b>web</b> <b>pages</b> and to make it possible to search the <b>web</b> <b>pages</b> through the grouping. <b>Web</b> <b>page</b> classification means the classification of various <b>web</b> <b>pages</b> that are scattered on the web according to the similarity of documents or the keywords contained in the documents. <b>Web</b> <b>page</b> classification method can be applied to various areas such as <b>web</b> <b>page</b> searching, group searching and e-mail filtering. However, it is impossible to handle the tremendous amount of <b>web</b> <b>pages</b> on the <b>web</b> by using the manual classification. Also, the automatic <b>web</b> <b>page</b> classification has the accuracy problem in that it fails to distinguish the different <b>web</b> <b>pages</b> written in different forms without classification errors. In this paper, we propose the automatic <b>web</b> <b>page</b> classification system using meta-tag that can be obtained from the <b>web</b> <b>pages</b> in order to solve the inaccurat...|$|R
40|$|Abstract — Today, {{the world}} is moving across the internet. Internet is {{collection}} of <b>Web</b> <b>Pages.</b> <b>Web</b> <b>Page</b> contains a bunch of information. In bunch of information to find or retrieve particular page or information is difficult task. It is difficult for the Search Engine to identify <b>web</b> <b>page.</b> So make this task easy there are different <b>web</b> <b>page</b> classification methods. <b>web</b> <b>page</b> classification is a web mining area. Using this method we can identify <b>web</b> <b>pages.</b> <b>Web</b> <b>page</b> Classification retrieves WebPages based on content and structure of <b>web</b> <b>page.</b> This paper shows results of different Classification methods and comparison of that Index Terms — <b>web</b> <b>page</b> classification, SVM, Naïve Bayes, Extraction, Feature I...|$|R
40|$|The {{growth of}} the Internet has {{generated}} <b>Web</b> <b>pages</b> that are rich in media and that incur significant rendering latency when accessed through slow communication channels. The technique of Web-object prefetching can potentially expedite the presentation of <b>Web</b> <b>pages</b> by utilizing the current <b>Web</b> <b>page’s</b> view time to acquire the Web objects of likely future <b>Web</b> <b>pages.</b> The performance of the Web object prefetcher is contingent on the predictability of future <b>Web</b> <b>pages</b> and quickly determining which Web objects to prefetch during the limited view time interval of the current <b>Web</b> <b>page.</b> The proposed Markov–Knapsack method uses an approach that combines a Multi-Markov Web-application centric prefetch model with a Knapsack Web object selector to enhance <b>Web</b> <b>page</b> rendering performance. The Markov <b>Web</b> <b>page</b> model ascertains the most likely next <b>Web</b> <b>page</b> set based on the current <b>Web</b> <b>page</b> and the <b>Web</b> object Knapsack selector determines the premium Web objects to request from these <b>Web</b> <b>pages.</b> The results presented in the paper show that the proposed methods can be effective in improving a Web browser cache-hit percentage while significantly lowering <b>Web</b> <b>page</b> rendering latency...|$|R
50|$|Since its incubation, BGRC has {{expanded}} throughout the years:1st BGRC (2006)- 3 days conference held in June. All 300 participants reached ‘Shanghai Consensus 2006’, agreeing to {{terms such as}} not working for companies that are irresponsible in an environmental or social context.2nd BGRC (2007)- 3 days conference held in May. Participants included 150 MBA students from over 20 business schools in Asia-Pacific area and over 50 business professional and NGO representatives.3rd BGRC (2008)- 3 day conference held in June. For the first time, there were students from international schools. Attendees included MBA students from 22 domestic and 17 international schools.4th BGRC (2009)- 2 day conference held in June. BGRC Invited 46 local and global top business schools to participate. For the first time, it had an exclusive <b>Live</b> Broadcasting media <b>web</b> <b>page</b> reporting the event (by Tencent). There were also added activities such as a student essay competition and interactive gaming.5th BGRC (2010)- 2 day conference held in May. At this event, Green Campus, a committee geared towards turning CEIBS into an exemplary institution in sustainable and environmental issues, was introduced. Activities also included photographic exhibitions and environment promotion movie shows.|$|R
40|$|This bachelor`s thesis {{deals with}} <b>web</b> <b>pages</b> design and {{structure}} of new <b>web</b> <b>pages</b> after previous analysis of actual <b>web</b> <b>pages.</b> It includes at analysis of negatives and positives of the actual website and new <b>web</b> <b>pages</b> design, functions, design of code structure and content structure of new <b>web</b> <b>pages</b> with possibility of later application...|$|R
40|$|This chapter {{describes}} {{systems that}} automatically classify <b>web</b> <b>pages</b> into meaningful categories. It first defines {{two types of}} <b>web</b> <b>page</b> classification: subject based and genre based classifications. It then describes {{the state of the}} art techniques and subsystems used to build automatic <b>web</b> <b>page</b> classification systems, including <b>web</b> <b>page</b> representations, dimensionality reductions, <b>web</b> <b>page</b> classifiers, and evaluation of <b>web</b> <b>page</b> classifiers. Such systems are essential tools for Web Mining and for the future of Semantic Web...|$|R
40|$|Although {{user access}} {{patterns}} on the <b>live</b> <b>web</b> are wellunderstood, {{there has been no}} corresponding study of how users, both humans and robots, access web archives. Based on samples from the Internet Archive’s public Wayback Machine, we propose a set of basic usage patterns: Dip (a single access), Slide (the same page at different archive times), Dive (different pages at approximately the same archive time), and Skim (lists of what pages are archived, i. e., Time-Maps). Robots are limited almost exclusively to Dips and Skims, but human accesses are more varied between all four types. Robots outnumber humans 10 : 1 in terms of sessions, 5 : 4 in terms of raw HTTP accesses, and 4 : 1 in terms of megabytes transferred. Robots almost always access Time-Maps (95 % of accesses), but humans predominately access the archived <b>web</b> <b>pages</b> themselves (82 % of accesses). In terms of unique archived <b>web</b> <b>pages,</b> there is no overall preference for a particular time, but the recent past (within the last year) shows significant repeat accesses...|$|R
40|$|We {{propose to}} mine {{parallel}} texts from mixedlanguage <b>web</b> <b>pages.</b> We define a mixedlanguage <b>web</b> <b>page</b> as a <b>web</b> <b>page</b> consisting of (at least) two languages. We mined Japanese-English parallel texts from mixedlanguage <b>web</b> <b>pages.</b> We presented the statistics for extracted parallel texts and conducted machine translation experiments. These statistics and experiments showed that mixedlanguage <b>web</b> <b>pages</b> are rich sources of parallel texts. ...|$|R
40|$|In this paper, we will {{introduce}} a new algorithm {{used to determine the}} sink <b>web</b> <b>pages</b> in a <b>web</b> application. The sink <b>web</b> <b>pages</b> are defined by using a partial order relation among the <b>web</b> <b>pages</b> of a <b>web</b> application. Using only these <b>web</b> <b>pages,</b> we will describe a method of determining all the <b>web</b> <b>pages</b> in a <b>web</b> application, containing errors...|$|R
40|$|Optimization of <b>Web</b> <b>page</b> {{search and}} the <b>web</b> <b>page</b> access {{is always the}} major issue for a web user, because of this there is always some scope to improve the <b>web</b> <b>page</b> access based on user requirement. One of such {{approach}} is given by <b>web</b> <b>page</b> pre-fetching. The pre-fetching concept is about to avail the <b>web</b> <b>page</b> to user before the user demand. It means as the user is visiting some page the next page that he visit will be copied to the user cache. In this present work we are performing an intelligent <b>web</b> <b>page</b> prediction approach based on history of <b>web</b> <b>page</b> visit. The proposed approach is three level approach {{in which we have}} combined markov model along with association mining and clustered approach. As the next <b>web</b> <b>page</b> will be predicted it will perform the efficient <b>web</b> <b>page</b> acces...|$|R
40|$|Clustering and {{retrieval}} of <b>web</b> <b>pages</b> dominantly relies on analyzing either {{the content of}} individual <b>web</b> <b>pages</b> or the link structure between them. Some literature also suggests to use the structure of <b>web</b> <b>pages,</b> notably the structure of its DOM tree. However, little work considers the visual structure of <b>web</b> <b>pages</b> for clustering. In this paper (i) we motivate visual structure-based <b>web</b> <b>page</b> clustering and retrieval {{for a number of}} applications, (ii) we formalize a visual box model-based representation of <b>web</b> <b>pages</b> that supports new metrics of visual similarity, and (iii) we report on our current work on evaluating human perception of visual similarity of <b>web</b> <b>pages</b> and applying the learned visual similarity features to <b>web</b> <b>page</b> clustering and retrieval...|$|R
40|$|As {{a part of}} {{this thesis}} {{existing}} <b>web</b> <b>pages</b> about planar geometrical transformations were evaluated and new <b>web</b> <b>pages</b> were created. Pages in Czech and in English language were evaluated shortly verbally. There is a table summarizing findings about existing <b>web</b> <b>pages</b> {{at the end of this}} part. The second part of this thesis contains newly created <b>web</b> <b>pages.</b> Scope of subject matter of created <b>web</b> <b>pages</b> overreaches common secondary school curriculum. There are Java applets used for illustrating findings about geometrical transformations as an integral part of <b>web</b> <b>pages.</b> Solving of exercises is accompanied by step-by-step operation. <b>Web</b> <b>pages</b> contain definitions, theorems with proofs and explanations of constructions...|$|R
40|$|In this paper, {{we present}} a related <b>web</b> <b>page</b> cluster method that not only {{considers}} the corresponding domain keywords on ontology but also analyzes semantic contents of <b>web</b> <b>pages.</b> First, the method embeds the corresponding domain ontology of search keyword to find <b>web</b> <b>pages</b> front the Internet. Next, consider {{the location of the}} keywords in the <b>web</b> <b>pages,</b> and relations between keywords and concepts in the domain ontology to find the features of the <b>web</b> <b>pages.</b> Then, the <b>web</b> <b>pages</b> were clustered based on the similarity values of mapping keywords concept-ontology level relations. Primary experimental results prove that our method is effective to find related <b>web</b> <b>pages...</b>|$|R
