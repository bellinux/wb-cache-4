160|240|Public
5000|$|... where [...] is the {{observer}} index. The first layer observers {{consists of the}} same gain [...] but they differ with the initial state [...] In the second layer all [...] from [...] observers are combined into one to obtain single state <b>vector</b> <b>estimation</b> ...|$|E
40|$|Abstract — Motion <b>vector</b> <b>estimation</b> is {{frequently}} performed {{as a prelude}} to the exploitation of temporal redundancies in video applications. As a result, a large volume of work has been done to develop techniques to avoid the heavy memory access requirements of full search motion <b>vector</b> <b>estimation.</b> Often, these approaches introduce data dependence to the algorithm, leading to memory accesses which cannot be determined at design time. Consequently, this complicates the exploitation of data reuse in hardware. In this work, the cost of data dependence is quantified. Experiments indicate that a data dependent fast motion <b>vector</b> <b>estimation</b> approach is faster than full search by up to 47 % in the absence of data re-use optimisation. However, full search is approximately 16 times faster than the ‘fast ’ motion <b>vector</b> <b>estimation</b> algorithm when a static line buffering scheme and a parallel caching scheme are used respectively to exploit data re-use. Therefore, it is established that data dependence in motion <b>vector</b> <b>estimation</b> is very expensive in terms of hardware performance. I...|$|E
40|$|The {{problem of}} {{estimating}} parameters for frequency domain models is considered. Past approaches have most commonly based estimation criteria upon modulus and phase transformations {{of the model}} and sample frequency response functions. As an alternative, a complex <b>vector</b> <b>estimation</b> criterion is proposed and is implemented in a nonlinear, Gauss-Marquardt optimization algorithm. When compared to the previous methods of fitting modulus and phase transformations, the complex <b>vector</b> <b>estimation</b> methodology has less bias and variance and is more robust...|$|E
40|$|Abstract: The {{system of}} {{development}} unstable processes prediction is given. It {{is based on}} a decision-tree method. The processing technique of the expert information is offered. It is indispensable for constructing and processing by a decision-tree method. In particular data is set in the fuzzy form. The original search algorithms of optimal paths of development of the forecast process are described. This one is oriented to processing of trees of large dimension with <b>vector</b> <b>estimations</b> of arcs...|$|R
40|$|Abstract: Construction of {{algorithm}} {{of extended}} Kalman filter for a nonlinear continuous - discrete problem of an estimation {{of a state}} vector of system is described. It is considered {{the problem of the}} adaptive filter where the estimations of the covariance matrices is carried out for noises of process and measurements with the purpose to increase an accuracy of a state <b>vector's</b> <b>estimation.</b> It is shown the schemes for several types of adaptive algorithms and results of mathematical modelling of their work for a rigid body with given inertia. Note: Publication language:russia...|$|R
40|$|Manufacturing price markup {{equations}} {{are estimated}} for 15 OECD countries using annual data. Firms have CES production technology. The markup depends on demand, competitors' prices and uncertainty. Cointegration is tested with the Pedroni tests and a panel {{version of the}} Johansen test, and evidence found for unique cointegrating <b>vectors.</b> <b>Estimation</b> of the long-run parameters is performed with a pooled mean group method, with short run heterogeneous dynamics. Tests for homogeneity of the long-run parameters do not reject the hypothesis. Markups are pro-cyclical and rise with both competitors' prices and uncertainty. pricing behaviour, markups, panel test for order of integration, panel cointegration, dynamic heterogeneous panels, pooled mean group estimation...|$|R
40|$|GPU（Graphic Processing Unit）是一种市场上很容易获得的低成本、高性能处理器，拥有高度并行性和可编程性的GPU已经广泛应用于各种通用计算领域。本论文利用GPU来实现结构光三维视觉测量和电子稳像系统中的运动矢量估计。结构光三维视觉测量技术因其非接触、动态响应快、精度高等优点被广泛应用于各种表面检测领域。采用结构光视觉对其进行缺陷检测需要占用大量的计算资源，往往难以满足实时检测的要求。电子稳像技术是通过数字图像处理技术对序列图像进行运动估计，进而进行运动补偿来消除图像帧间的诸如抖动、旋转等非正常偏移的一种技术，其中的关键技术是全局运动矢量估计。在整个电子稳像系统中，运动矢量估计需要非常大的计算量，消耗大量的计算资源，几乎占用了整个电子稳像系统大部分的计算时间，因此提高运动矢量估计的计算效率成了提高电子稳像系统实时性的关键。本文首先对基于结构光视觉的钢轨表面缺陷检测作了深入研究，提出了一种结构光中心的快速提取算法——Steger算法的改进算法，并将该算法在GPU中实现。对于钢轨表面缺陷深度和宽度的计算，本文则提出了一种基于形态学的计算方法，并在静态情况下对钢轨表面缺陷检测进行实验，实验结果显示，GPU对于结构光中心的快速提取具有非常好的效果。同时，本文对于运动矢量估计算法进行研究，其中以块匹配算法及其改进算法作为重点研究对象，对于块匹配算法的一种改进算法——GEA算法（Global Elimination Algorithm），进行并行化处理，并利用GPU实现该算法。针对具有平移和小的旋转抖动的航拍视频进行实验，实验结果表明，GPU对于运动矢量估计具有非常好的加速效果，能够满足运动矢量估计的实时处理。GPU (Graphic Processing Unit) {{is a kind}} of {{low cost}} and high {{performance}} processor that is easy to get from market, and GPU with high parallelism and programmability has been widely used in all kinds of general-purpose computing field. In this paper, GPU is used to implement structured light 3 d vision measurement and motion <b>vector</b> <b>estimation.</b> Structured light 3 d vision measuring technique has been widely used in various surface defects detection field because of its advantages of non-contact, fast dynamic response and high precision. Defects inspection using structure light vision takes up a lot of computing resources, and {{it is often difficult to}} meet the requirement of real-time detection. Electronic image stabilization is a technology that uses the digital image processing technology to estimate the motion vectors on image sequences and then compensate them to remove the non-normal movement such as image jitter and rotation, and the key technology is the global motion <b>vector</b> <b>estimation.</b> Throughout the electronic image stabilization system, motion <b>vector</b> <b>estimation</b> requires a very large amount of calculation, consumes large amounts of computing resources, and almost occupies most of the computational time of the whole electronic image stabilization system. Thus improving the computing efficiency of motion <b>vector</b> <b>estimation</b> is the key to real-time electronic image stabilization system. Firstly, in this paper，an intensive study is made on rail surface defects detection based on structure light vision, a fast algorithm of structure light center extraction [...] the improved algorithm of Steger algorithm, is put forward and its parallel algorithm is realized in GPU. For the calculation of the depth and width of the rail surface defects, a calculation method based on morphology is put forward in this paper. Under the static condition, the experiment of rail surface defects detection is made, and the result shows that GPU has a very good effect in extracting structured light center. At the same time, this paper studies motion <b>vector</b> <b>estimation</b> algorithm, with block matching algorithm and its improved algorithm the key research object. As an improved algorithm, GEA algorithm (Global Elimination Algorithm) is paralleled processed and is implemented using GPU. Aerial video with translation and small jitter is used in the experiment, and the result shows that GPU has very good speedup for motion <b>vector</b> <b>estimation,</b> and can meet the real-time processing of motion <b>vector</b> <b>estimation...</b>|$|E
30|$|To {{find the}} {{displacement}} of objects within a scene captured by single camera, we use the motion <b>vector</b> <b>estimation</b> procedure of the H. 264 standard. Since H. 264 motion <b>vector</b> <b>estimation</b> is block-based (i.e., it measures {{the displacement of}} a block and not a point or object), we propose correction steps that reevaluate and refine the estimated motion vectors in order to calculate the motion vectors for the objects within the scene. Then the resulting object motion vectors are transformed to depth information. The following subsections elaborate on different steps of our proposed scheme.|$|E
40|$|The {{technique}} of motion <b>vector</b> <b>estimation</b> can remove the information of temporal redundancy {{to reduce the}} bit rate for video coding. Due to the great computation required in motion <b>vector</b> <b>estimation,</b> many simplified search algorithms have been proposed. In particular, one-bit transform scheme can significantly reduce the search complexity. This paper proposes a dynamic threshold scheme to form the binary images with respective to each macro-block images. Motion vectors are calculated {{through a series of}} Boolean operations, such as “XOR”, “AND”, and/or “OR”. The experimental results show that better improvement on the coding quality can be achieved. 1...|$|E
40|$|This paper {{investigates the}} fiscal deficit {{sustainability}} of Turkey {{over the period}} 1975 – 2008 by using both cointegration and multicointegration methods. In addition to the conventional unit root tests, the minimum LM unit root test with one structural break is {{used to examine the}} time series properties of government expenditures and revenues. Since the series are I(1), the study next implements a cointegration method with an endogenous break. The cointegrating <b>vector</b> <b>estimations</b> from the DOLS procedure indicate that the fiscal deficit is weakly sustainable. This paper also uses one-step multicointegration method to test whether the fiscal process is truly sustainable in Turkey. The results show that the fiscal system in Turkey is multicointegrated and sustainable. fiscal deficit sustainability; structural break; cointegration tests; multicointegration tests; Turkey. ...|$|R
40|$|In most Photogrammetry and {{computer}} vision tasks, finding the corresponding points among images is required. Among many, the Lucas-Kanade optical flow estimation has been employed for tracking interest points {{as well as}} motion <b>vector</b> field <b>estimation.</b> This paper uses the IMU measurements to reconstruct the epipolar geometry and it integrates the epipolar geometry constraint with the brightness constancy assumption in the Lucas-Kanade method. The proposed method has been tested using the KITTI dataset. The results show the improvement in motion <b>vector</b> field <b>estimation</b> {{in comparison to the}} Lucas-Kanade optical flow estimation. The same approach has been used in the KLT tracker and {{it has been shown that}} using epipolar geometry constraint can improve the KLT tracker. It is recommended that the epipolar geometry constraint is used in advanced variational optical flow estimation methods...|$|R
40|$|Fluid motion {{estimation}} from time sequenced images {{is a significant}} image analysis task. Its application is widespread in experimental fluidics research and many related areas like biomedical engineering and atmospheric sciences. In this paper, we present a novel flow computation framework to estimate the flow velocity vectors from two consecutive image frames. In an energy minimization-based flow computation, we propose a novel data fidelity term, which (1) can accommodate various measures, such as cross-correlation or sum of absolute or squared differences of pixel intensities between image patches, (2) has a global mechanism to control the adverse effect of outliers arising out of motion discontinuities, proximity of image borders, and (3) can go hand-in-hand with various spatial smoothness terms. Further the proposed data term and related regularization schemes are both applicable to dense and sparse flow <b>vector</b> <b>estimations.</b> We validate these claims by numerical experiments on benchmark flow data sets...|$|R
40|$|We {{present and}} {{investigate}} methods of displacement <b>vector</b> <b>estimation,</b> {{which can be}} used in predictive image coding or motion-adaptive frame interpolative coding. In both cases, a differential approach is adopted. This means that the displacement <b>vector</b> <b>estimation</b> is based on the measurement of the spatiotemporal gradient of the image sequence. In the case of predictive coding the motion estimator is composed of three parts. The first is a spatial predictor of the displacement vector, which is based on a spatial autoregressive relation of the velocity field. The coefficients of this relation are made intensity-dependent. The presence of discontinuities inherent to the motion and to the instabilities of the estimation algorithm makes necessary a stage of detection of all type of discontinuities. Finally, an a posteriori estimator achieves the task of displacement <b>vector</b> <b>estimation.</b> This last stage is of iterative form. The application of this algorithm in a very noisy image sequence has permitted to obtain a gain of about 40 % in the absolute value of the difference with the predicted displacement vector and 65 % with the estimated one after two iterations. The same structure of displacement field estimation {{can be used to make}} a frame interpolation motionadaptive...|$|E
40|$|We {{propose a}} block-based wavelet codec in which motion <b>vector</b> <b>estimation</b> and motion {{residual}} encoding are performed within a wavelet domain. By interpolation on dyadic wavelet transform, {{we show that}} motion <b>vector</b> <b>estimation</b> in the wavelet domain can achieve sub-pixel precision. To improve PSNR performance at low bit rates, we propose a bit-plane-based rate-distortion (R-D) optimization algorithm. This algorithm allocates an optimal number of bit-planes to each macroblock. Our codec is SNR scalable and our bit-stream syntax is fully compatible with that of H. 263. Experiments show that our block-based wavelet codec outperforms a framebased wavelet codec. Also, compared with H. 263 baseline results, our wavelet codec is competitive at low bit rates, and is superior at higher bit rates. ...|$|E
40|$|A {{self-contained}} {{theory of}} extrema (viz., suprema, maxima, minima, and infima) of differentiable functions of several (possibly infinitely many) variables mapping into finite-dimensional integrally closed directed partially ordered linear spaces is reported. The {{applicability of the}} theory {{to the analysis of}} linear least squares <b>vector</b> <b>estimation</b> problem is demonstrated...|$|E
3000|$|... [...]) {{covariance}} matrix {{and the target}} steering <b>vector,</b> the precise <b>estimation</b> of the {{covariance matrix}} in heterogeneous environments presents {{to be a key}} for the detection tasks.|$|R
3000|$|Motion {{estimation}} - After, the two reference {{frames are}} low-pass filtered {{to obtain a}} more spatially coherent motion <b>vector</b> field. Motion <b>estimation</b> is then performed from X [...]...|$|R
40|$|AbstractUltrasonic imaging {{is often}} used to {{estimate}} blood flow velocity. Estimates are currently performed by Doppler-based techniques but they suffer from some shortcomings. This article compares four <b>vector</b> velocity <b>estimation</b> methods complementary to Doppler. Each method has been applied to six sequences, simulated and experimental, with various flow parameters. Results are presented in several curves and show specificities of each method...|$|R
40|$|In this paper, {{we propose}} a novel {{adaptation}} technique based on coarse/fine training of transfer vectors. We focus on transfer <b>vector</b> <b>estimation</b> of a Gaussian mean from an initial model to an adapted model. The transfer vector is decomposed into a direction vector and a scaling factor. By using tied-Gaussian class (coarse class) estimation for the direction vector, and by using individual Gaussian class (fine class) estimation for the scaling factor, we can obtain accurate transfer vectors {{with a small}} number of parameters. Simple training algorithms for transfer <b>vector</b> <b>estimation</b> are analytically derived using the variational Bayes, maximum a posteriori (MAP) and maximum likelihood methods. Speaker adaptation experiments show that our proposals clearly improve speech recognition performance for any amount of adaptation data, compared with conventional MAP adaptation...|$|E
40|$|In this paper, {{a median}} filtering-based {{hierarchical}} motion <b>vector</b> <b>estimation</b> scheme {{making use of}} a pyramidal data structure is proposed. Compared to the conventional hierarchical motion <b>vector</b> <b>estimation</b> schemes, the proposed scheme overcomes the problem of propagation of false motion vectors across resolutions. Simulation {{studies show that the}} proposed scheme not only improves the prediction accuracy with respect to the prediction mean square error, but also results in a smoother motion field, which can be encoded with less number of coding bits. It is shown that an improvement in the rate distortion performance is achieved with little increase in the computational complexity. It is also shown that Burt and Adelson's pyramidal data structure provides the best performance among a number of the generating kernels considered in our study...|$|E
40|$|Visual {{information}} {{is very much}} important for human to perceive, recognize and understand the surrounding world. As {{we live in the}} age of multimedia video sequences are very useful to us for providing information. Video involves a huge amount of data. So video compression is necessary Motion compensation has lot of computation in total video compression process. Fast motion <b>vector</b> <b>estimation</b> is a key-factor in video coding standard. Full search algorithm is the best algorithm between all the block matching algorithms to estimate the motion <b>vector</b> <b>estimation</b> with a huge computation cost. The challenge is to reduce the computational complexity of Full Search algorithm without losing too much quality at the output. In this paper we propose to implement the fuzzy logic based Four Step Search algorithm which performs better than other block matching algorithms...|$|E
40|$|Precise {{measurement}} {{is crucial}} to science and technology. However, the rule of nature imposes various restrictions on the precision {{that can be achieved}} depending on specific methods of measurement. In particular, quantum mechanics poses the ultimate limit on precision which can only be approached but never be violated. Depending on analytic techniques, these bounds may not be unique. Here, in view of prior information, we investigate systematically the precision bounds of the total mean-square error of <b>vector</b> parameter <b>estimation</b> which contains $d$ independent parameters. From quantum Ziv-Zakai error bounds, we derive two kinds of quantum metrological bounds for <b>vector</b> parameter <b>estimation,</b> both of which should be satisfied. By these bounds, we show that a constant advantage can be expected via simultaneous estimation strategy over the optimal individual estimation strategy, which solves a long-standing problem. A general framework for obtaining the lower bounds in a noisy system is also proposed. Comment: 8 pages, 4 figure...|$|R
40|$|In this paper, a new {{optimised}} {{method of}} coding stereoscopic image sequences is presented and compared with already known methods. Two basic methods of coding a stereoscopic image sequence are the compatible and joint. The first one uses MPEG for coding the left channel and {{takes advantage of}} the spatial disparity redundancy between the two sequences for coding the right channel. The second one employs MPEG for coding the left channel but takes advantage of both temporal redundancy among the right channel frames and spatial redundancy between the corresponding frames of the two channels. The proposed method, which is called IMDE, estimates the P and B type of frames of the right channel by an interpolative scheme that takes in to account both the temporal and disparity characteristics. Investigating the effectiveness of the joint motion and disparity <b>vectors</b> <b>estimation</b> as well as the choice of the weighting factors that participate in the proposed interpolative scheme optimises the whole framework...|$|R
40|$|We {{establish}} {{large sample}} approximations for an arbitray number of bilinear {{forms of the}} sample variance-covariance matrix of a high-dimensional vector time series using ℓ_ 1 -bounded and small ℓ_ 2 -bounded weighting <b>vectors.</b> <b>Estimation</b> of the asymptotic covariance structure is also discussed. The results hold true without any constraint on the dimension, the number of forms and the sample size or their ratios. Concrete and potential applications are widespread and cover high-dimensional data science problems such as tests for large numbers of covariances, sparse portfolio optimization and projections onto sparse principal components or more general spanning sets as frequently considered, e. g. in classification and dictionary learning. As two specific applications of our results, we study in greater detail the asymptotics of the trace functional and shrinkage estimation of covariance matrices. In shrinkage estimation, {{it turns out that}} the asymptotics differs for weighting vectors bounded away from orthogonaliy and nearly orthogonal ones in the sense that their inner product converges to 0. Comment: 42 page...|$|R
40|$|In this paper, {{we propose}} an e#cient motion <b>vector</b> <b>estimation</b> and coding method for H. 263 -based {{low bit rate}} video compression. The method {{exploits}} structural constraints within the motion #eld. The same H. 263 median predictor is used to localize motion estimation, which is directed to the best motion vectors by employing a computation-constrained layered search technique. Moreover, the localized motion #eld is encoded using semi-#xedlength codes. The resulting low bit rate video encoder yields essentially the same levels of rate-distortion performance and subjective qualityachieved byTelenor's implementation of the ITU TMN 5 model. However, our motion <b>vector</b> <b>estimation</b> and coding method provides for substantially higher encoding speed and channel error robustness. # This work {{was supported by the}} Natural Sciences and Engineering Research Council of Canada. 1 1 Introduction Very low bit rate video compression techniques are becoming increasingly important due to present and [...] ...|$|E
40|$|A {{complete}} {{theory of}} necessary and sufficient conditions is discussed for a control {{to be superior}} {{with respect to a}} nonscalar-valued performance criterion. The latter maps into a finite dimensional, integrally closed directed, partially ordered linear space. The applicability of the theory to the analysis of dynamic <b>vector</b> <b>estimation</b> problems and to a class of uncertain optimal control problems is demonstrated...|$|E
3000|$|A {{computationally}} expensive {{solution is}} to perform pixel-based motion <b>vector</b> <b>estimation</b> for the object-border pixels. We propose an alternative solution which detects the border blocks with nonzero motion vectors and, then, classifies each pixel within each of these blocks as a background pixel or an object pixel. This process {{is based on the}} results of the JSEG algorithm (Section 3.3). The estimated [...]...|$|E
40|$|We {{construct}} {{minimum variance}} unbiased estimators of von Mises functionals in estimation problems where no complete sufficient [sigma]-algebra exists. The construction method is instead {{based on the}} higher order tangent structure of the underlying class of distributions. We discuss especially some curved and some noncurved nonparametric examples in the iid case and estimation in nonparametric Markov chain models. k-tangent <b>vector</b> unbiased <b>estimation</b> symmetry model Markov chain...|$|R
30|$|Robust {{signal support}} {{estimation}} is facilitated {{by combining the}} soft information from multiple jointly sparse signal <b>vectors.</b> Support <b>estimation</b> is crucial for quick tag identification, as the support dictates the overhead of the reader-to-tag communication (correct detections lead to correctly read out tags, while false alarms prolong the identification). It was shown that prior knowledge of {{the exact number of}} activated tags is not required for robust support estimation.|$|R
40|$|Abstract — This paper {{describes}} efficient object-based video coding schemes {{suitable for}} content-based multimedia streaming systems. Anew two video coding approaches are presented {{to study the}} effect of the object based motion estimation in video coding and {{the effect of the}} object based video coding on the compression quality, respectively. The first approach is based on a new motion estimation technique; based on arbitrary shaped-regions. The second approach is based on the video object extraction and a new motion estimation technique based on arbitrary shaped-regions. The proposed methods are applied on videos containing a variety of scenarios such as multiple objects undergoing occlusion, splitting, merging, entering and exiting, as well as a changing background. The simulation results are introduced in a comparison form with some of recent video coding method. For all the videos, the proposed approach displays higher Peak Signal to Noise Ratio (PSNR) compared to other methods, and provides comparable or better compression than some of recent video coding techniques. Index Term — Video coding; image segmentation; object extraction; MPEG- 4; video object planes (VOPs); motion <b>vectors</b> <b>estimation</b> I...|$|R
40|$|We derive {{a linear}} minimum {{mean square error}} {{estimator}} for sparse <b>vector</b> <b>estimation</b> from an underdetermined set of linear equations. The derivation of the estimator uses a prior distribution conditioned on the support set of the underlying sparse vector. The estimator is used in the architecture of the standard orthogonal matching pursuit algorithm to achieve a better performance. QC 20140619 </p...|$|E
40|$|We {{propose a}} {{technique}} to recover lost enhancement layer information in scalable video using the information from (1) the current base layer and (2) the previous base- and enhancement-layer together with a decoder motion <b>vector</b> <b>estimation</b> method. An average of 1 dB improvement is reported for the enhancement layer vs. existing techniques. An accelerated method is also demonstrated to make the concealment real-time...|$|E
40|$|In this {{document}} {{a comparison of}} three methods of volumetric data gradient <b>vector</b> <b>estimation</b> (one of which was proposed by the authors) and five methods for triangle mesh vertex normal computation will be described and compared. All the methods {{can be used for}} regular as well as irregular meshes. The tests were focused primarily on the accuracy. However, a comparison of the tempora...|$|E
40|$|Microstructure data {{typically}} {{consist of}} trades and bid and offer quotes for financial securities that are collected at fine sampling intervals (often within the day). This paper reviews approaches taken to modeling these data. The {{emphasis is on}} the techniques of stationary multivariate time series analysis: autoregressive and moving average representations o f standard microstructure models, <b>vector</b> autoregressive <b>estimation,</b> random-walk decompositions and cointegration. The paper also discusses the challenges posed by irregular observation frequencies, discreteness and nonlinearity. ...|$|R
40|$|Doppler {{flow imaging}} for the {{visualisation}} of neonatal intraventricular blood flow currently still has inherent limitations: beam-to-flow angle dependency, aliasing and a too low frame rate. Ultrafast imaging and <b>vector</b> flow <b>estimation</b> may resolve these limitations, yet both still require thorough validation for the pediatric cardiac setting. Hence, a computational modelling {{approach in the}} neonatal left ventricle was employed to investigate (i) diverging wave emission to acquire images at very high frame rate and (ii) subsequent speckle tracking algorithms for <b>vector</b> flow <b>estimation.</b> Single non-tilted diverging waves with an opening angle of 60 ° were transmitted, at a pulse repetition frequency of 9 kHz. Speckle tracking on the acquired ultrasound images provided 2 D intraventricular flow estimates at a frame rate of 180 Hz for both the apical four chamber and parasternal short axis view, and this over an entire cardiac cycle. Overall, the blood flow was reasonably accurately tracked throughout the cardiac cycle, yet several imaging artefacts were observed. Zones of low flow proved very difficult to track due to clutter filtering issues, while high spatial flow gradients caused strong underestimation of systolic outflow...|$|R
40|$|This paper {{proposes a}} Direct Matrix Converter {{operating}} as a Unified Power Flow Controller (DMC-UPFC) with an advanced control method for UPFC, {{based on the}} Lyapunov direct method, presenting good results in power quality assessment. This control method is used for real-time calculation of the appropriate matrix switching state, determining which switching state should be applied in the following sampling period. The control strategy takes into account active and reactive power flow references to choose the vector converter closest to the optimum. Theoretical principles for this new real-time vector modulation and control applied to the DMC-UPFC with input filter are established. The method needs DMC-UPFC dynamic equations to be solved just once in each control cycle, to find the required optimum vector, in contrast to similar control methods that need 27 <b>vector</b> <b>estimations</b> per control cycle. The designed controller’s performance was evaluated using Matlab/Simulink software. Controllers were also implemented using a digital signal processing (DSP) system and matrix hardware. Simulation and experimental results show decoupled transmission line active (P) and reactive (Q) power control with zero theoretical error tracking and fast response. Output currents and voltages show small ripple and low harmonic content...|$|R
