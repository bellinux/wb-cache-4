78|269|Public
5000|$|<b>Vector</b> <b>Computer,</b> Budapest, Textbook Publishers, 1973, 470 p [...] (Peter Tasnádi) ...|$|E
5000|$|<b>Vector</b> <b>Computer,</b> I-III [...] {{conclude}} [...] Budapest, Textbook Publishers, 1980-1983. (Peter Tasnádi and Peter Gnädig rel) ...|$|E
5000|$|In 1976, Cray Research (a company {{supported}} by Seymour Cray's former employer Control Data Corporation), released the Cray-1 <b>vector</b> <b>computer.</b>|$|E
40|$|We {{examine the}} {{performance}} of parallel algorithms for first-order linear recurrence on <b>vector</b> <b>computers,</b> evaluate them quantitatively on a simple model of <b>vector</b> <b>computers,</b> and propose new fast algorithms. We also show a result of performance benchmarking of them on actual <b>vector</b> <b>computers.</b> 1 Introduction First-order linear recurrence is a basic operation often used in many algorithms of numerical analysis such as solving general block diagonal linear system. But {{it is also a}} notorious example such that we cannot get good performance on vector supercomputers. Since naive vectorization of first-order recurrence is impossible, some Japanese supercomputers have special hardware support for it. But the performance is still not so much as that for ordinary element-wise operations. On the other hand, there have been proposed many parallel algorithms for first-order recurrences [4], {{and some of them are}} tuned for <b>vector</b> <b>computers</b> [12, 14]. However, many of them cannot achieve enough perform [...] ...|$|R
40|$|Implicit hybrid {{algorithms}} employing symmetric planar Gauss-Seidel (SPGS) {{relaxation and}} either block-tridiagonally structured coefficient matrices (AF-SPGS) or block-triangular coefficient matrices (LU-SPGS) are derived {{to solve the}} flux-difference-split Navier-Stokes equations for three-dimensional incompressible flow in an upwind scheme. The physical basis of the approach is discussed, and results for problems involving vortex flow around a thin delta wing at Reynolds numbers 900, 000 and 10, 000 are presented graphically. It is found that AF-SPGS converges faster on <b>vector</b> <b>computers</b> which depend on long vector lengths to achieve optimum performance, whereas LU-SPGS is preferable on sequentially operating machines and <b>vector</b> <b>computers</b> using shorter <b>vector</b> lengths...|$|R
50|$|In {{contrast}} to the then ubiqituous, conventional <b>vector</b> <b>computers</b> (e.g. NEC SX architecture, Cray Y-MP), SUPRENUM-1 pursued {{as one of the}} first a massively parallel design. However, competitors like Thinking Machines Corporation were catching up fast.|$|R
5000|$|... 1993 David H. Bailey. [...] "For {{contributions}} to numerical computational science including innovative algorithms for FFT's, matrix multiply and multiple precision arithmetic on <b>vector</b> <b>computer</b> architecture." ...|$|E
50|$|The <b>vector</b> <b>computer</b> CRAY J90 {{was used}} as a file server. It had 12 processors, an {{internal}} memory of 2 gigabytes and boasted a performance of 3 GFLOPS. CRAY J90 was also run on UNICOS 10.0 and it too was decommissioned on June 30, 2005.|$|E
50|$|The <b>vector</b> <b>computer</b> CRAY SV1ex was the {{successor}} of CRAY J90, {{which was in}} operation between 1996 and 2002. It represented the next stage in the computer series of the parallel vector computers with a shared memory, CRAY X-MP, Y-MP and C90.With 16 CPUs and an internal memory of 32 gigabytes, the CRAY SV1ex had a performance of 32 GFLOPS. It was run on the UNICOS 10.0 operating system. This computer was decommissioned on June 30, 2005.|$|E
40|$|AbstractRecently two {{dissection}} algorithms (one-way {{and incomplete}} nested dissection) {{have been developed}} for solving the sparse positive definite linear systems arising from n by n grid problems. Concurrently, <b>vector</b> <b>computers</b> (such as the CDC STAR- 100 and TI ASC) {{have been developed for}} large scientific applications. An analysis of the use of dissection algorithms on <b>vector</b> <b>computers</b> dictates that <b>vectors</b> of maximum length be utilized thereby implying little or no dissection; on the other hand, minimizing operation counts suggest that considerable dissection be performed. In this paper we discuss the resolution of this conflict by minimizing the total time required by vectorized versions of the two algorithms...|$|R
40|$|The GAMM Committee for Numerical Methods in Fluid Mechanics organizes {{workshops}} {{which should}} bring together experts {{of a narrow}} field of computational fluid dynamics (CFD) to exchange ideas and experiences in order to speed-up the development in this field. In this sense {{it was suggested that}} a workshop should treat the solution of CFD problems on <b>vector</b> <b>computers.</b> Thus we organized a workshop with the title "The efficient use of <b>vector</b> <b>computers</b> with emphasis on computational fluid dynamics". The workshop took place at the Computing Centre of the University of Karlsruhe, March 13 - 15, 1985. The participation had been restricted to 22 people of 7 countries. 18 papers have been presented. In the announcement of the workshop we wrote: "Fluid mechanics has actively stimulated the development of superfast <b>vector</b> <b>computers</b> like the CRAY's or CYBER 205. Now these computers on their turn stimulate the development of new algorithms which result in a high degree of vectorization (sca 1 ar/vectorized execution-time). But with 3 -D problems we quickly reach the limit of present <b>vector</b> <b>computers.</b> If we want e. g. to solve a system of 6 partial differential equations (e. g. for u, v, w, p, k, € or for the vectors u, curl u) on a 50 x 50 x 50 grid we have 750. 000 unknowns and for a 4 th order difference method we have circa 60 million nonzero coefficients in the highly sparse matrix. This characterizes the type of problems which we want to discuss in the workshop"...|$|R
40|$|We {{investigate}} {{the use of}} approximate factorization and diagonalizing techniques for solving iteratively fully implicit numerical models of three-dimensional transport-chemistry problems. In particular, we investigate various possibilities that {{can take advantage of}} the parallelization and vectorization facilities offered by parallel <b>vector</b> <b>computers...</b>|$|R
5000|$|In 1994, Convex {{introduced}} {{an entirely new}} design, known as the Exemplar. Unlike the C-series <b>vector</b> <b>computer,</b> the Exemplar was a parallel-computing machine that used HP PA-7200 microprocessors, connected together using SCI. First dubbed MPP, these machines were later called SPP [...] and Exemplar and sold under the SPP-1600 moniker. The expectation was that a software programming model for parallel computing could draw in customers. But the type of customers Convex attracted believed in Fortran and brute force rather than sophisticated technology. The operating system also had terrible performance problems which could not easily be fixed. Eventually, Convex established a working partnership with HP's hardware and software divisions. Initially it was intended that the Exemplar would be binary-compatible with HP's HP-UX operating system but eventually {{it was decided to}} port HP-UX to the platform and sell the platform as standalone servers.|$|E
50|$|First, {{the manual}} {{calculator}} {{was replaced by}} an external box operated by a new crew member. The box contained the inputs needed to drive the vector calculator, as well as copies of the various aircraft instruments displaying required information. The operator simply had to keep the input dials set so their indicators overlapped those on the instruments. This drove the machine to calculate the correct angles, as on the earlier CSBS models, and fed them directly into a remote sighting unit, the sighting head. This provided practically instant updates of the sighting angles. The wire sights of the earlier models were replaced by reflector sights indicating the location the bombs would hit if dropped at that instant. As the sighting head lacked the <b>vector</b> <b>computer</b> it was much smaller than earlier models, which allowed it to be easily mounted on a stabilized platform. This allowed the sights to be used even while the aircraft was maneuvering, and required only 10 seconds to settle.|$|E
40|$|Abstract:- Vector {{computers}} {{are suitable for}} processing vectors and matrices. Nevertheless, sophisticated algorithms are needed to process sparse vectors and matrices. Additionally, vector {{computers are}} not always the best computing platform because of their very high cost. We have designed a special architecture of a <b>vector</b> <b>computer</b> and implemented it within an FPGA. Our main objective is to build multiprocessors and vector machines within FPGAs because they offer a low cost alternative to supercomputers and other parallel machines. We demonstrate here that FPGAs provide a cost effective and reconfigurable means to implement vector computers. The implementation and performance of the above <b>vector</b> <b>computer</b> on an FPGA platform are also discussed...|$|E
40|$|A {{collection}} of examples is presented which demonstrates {{the effects of}} the arithmetic of <b>vector</b> <b>computers</b> on basic numerical methods. It is shown that the results of numerical algorithms which make use of the additional arithmetic operations in vector mode differ from the results computed in scalar mode...|$|R
40|$|<b>Vector</b> <b>computers</b> {{have been}} {{extensively}} used {{for years in}} matrix algebra to treat with large dense matrix problems. However, if matrices are sparse and we use special storage schemes for them, vectorization provides a poor performance due to the great amount of indirections in the code. An alternative option is the utilization of a multiprocessor (or a cluster of workstations); in this case, a data parallel programming model also fails because of the reason pointed out for <b>vector</b> <b>computers.</b> Therefore, the best choice is to parallelize the corresponding algorithms using message passing routines. In order to discuss these features, we will focus on solving sparse linear least squares problems, which appear in several scientific areas such as structural analysis, geodetic survey, molecular structure and many others. Experimental results are obtained for vector and paralle...|$|R
40|$|AbstractA conditionally stable {{explicit}} {{scheme is}} developed {{and applied to}} the ocean acoustic parabolic wave equation. The scheme is leapfrogged in the range variable and centered-differenced in the vertical variable. The theoretical development, the computational aspects and {{the advantage of the}} scheme are discussed. The method is ideal for <b>vector</b> <b>computers...</b>|$|R
40|$|AbstractA quasimolecular, {{particle}} {{model is}} developed for {{and applied to}} the simulation of cavity flow. The approach is formalized as an n-body problem which incorporates classical molecular type forces. Large systems of second order, ordinary differential equations are generated and solved numerically. <b>Vector</b> <b>computer</b> examples are described and discussed...|$|E
40|$|International audienceSome arithmetics for {{numerical}} validation {{have been}} developed on sequential computer architectures: interval arithmetic, discrete stochastic arithmetic or multi-precision arithmetic. Today, a lot of powerful computations are performed on <b>vector</b> <b>computer</b> architectures. We present here the first vector version of the CADNA software based on discrete stochastic arithmetic which combines powerful and reliable computations...|$|E
40|$|AbstractThe {{application}} of a conforming spectral collocation method to certain nonconforming domain decompositions leads to global matrices which have a particular block structure. We study the performance of various direct linear system solvers, some of which exploit this block structure, on a Cray J- 916 <b>vector</b> <b>computer,</b> an SGI Power Challenge 8000, and an IBM RS 6000 workstation...|$|E
40|$|Cluster update {{algorithms}} dramatically reduce critical {{slowing down}} in spin models, {{but unlike the}} standard Metropolis algorithm, it is not obvious how to implement these algorithms efficiently on parallel or <b>vector</b> <b>computers.</b> Here we present two different parallel implementations of the Swendsen-Wang algorithm which give reasonable efficiencies on various MIMD parallel computers. ...|$|R
40|$|Boundary value {{problems}} with nonrectangular domains were solved with unigrid methods. Unigrid was developed, {{its relationship to}} multigrid is described and some simple theory is presented. Its use is illustrated with a North Atlantic basin oceanographic model problem. It is demonstrated how unigrid (and, hence, multigrid) can be used efficiently with <b>vector</b> <b>computers</b> on {{problems with}} irregular domains...|$|R
40|$|This paper {{examines}} the applications most commonly {{run on the}} supercomputers at the Numerical Aerospace Simulation (NAS) facility. It analyzes {{the extent to which}} such applications are fundamentally oriented to <b>vector</b> <b>computers,</b> {{and whether or not they}} can be efficiently implemented on hierarchical memory machines, such as systems with cache memories and highly parallel, distributed memory systems...|$|R
40|$|Relaxed non-stationary multisplitting {{methods are}} studied for the {{parallel}} solution of nonsingular linear systems. Convergence {{results of the}} synchronous and asynchronous versions for H-matrices are presented. Computational results of these methods on a shared memory multiprocessor <b>vector</b> <b>computer</b> are reported. Λ Supported by the ESPRIT III basic research programme of the EC under contract No. 9072 (project GEPPCOM) ...|$|E
40|$|The {{air flow}} past a {{wind tunnel model}} of an Eurocopter BO- 105 fuselage, main rotor and tail rotor {{configuration}} is simulated by solving the time dependent Navier-Stokes equations. The flow solver uses overlapping, block structured grids to discretize the computational domain. The simulation setup and the execution on a parallel NEC-SX 6 <b>vector</b> <b>computer</b> are described. The numerical results are compared with unsteady pressure measurements on the fuselage and the blades. An overall good agreement is found. Differences between predicted and measured data on the main rotor and the tail rotor {{can be explained by}} blade elasticity effects and a different trim law respectively. The computational performance of the flow solver is analyzed for the NEC-SX 6 and NEC-SX 8 <b>vector</b> <b>computer</b> showing a good parallel performance. Modifications of the code structure resulted in a reduction of the execution time for the Chimera procedure by a factor of 6. 6...|$|E
40|$|AbstractParallel chaotic schemes {{based on}} the {{extrapolated}} Jacobi method and a second degree stationary method are studied. Sufficient conditions for the convergence of the above methods for the synchronous and asynchronous models are given. These sufficient conditions are related to properties of the block Jacobi matrix. The schemes are illustrated by computational results on a shared memory multiprocessor <b>vector</b> <b>computer...</b>|$|E
40|$|The paper {{considers}} the 'ijk forms' of LU and Cholesky factorization on certain parallel computers. This extends an earlier analysis for <b>vector</b> <b>computers.</b> Attention {{is restricted to}} local memory systems with processors {{that may or may}} not have vector capability. Special attention is given to bus architectures but qualitative analyses are given for other interconnection systems...|$|R
5000|$|Phase {{portraits of}} planar <b>vector</b> fields: <b>computer</b> proofs, J. Experimental Mathematics 4 (1995), 153-164.|$|R
40|$|A {{large number}} of {{scientific}} and engineering problems require the rapid solution of large systems of simultaneous equations. The performance of parallel computers in this area now dwarfs traditional <b>vector</b> <b>computers</b> by nearly an order of magnitude. This talk describes the major issues involved in parallel equation solvers with particular emphasis on the Intel Paragon, IBM SP- 1 and SP- 2 processors...|$|R
40|$|The TOP 500 {{project was}} {{launched}} in 1993 to provide a reliable basis for tracking and detecting trends in high performance computing. Twice a year, {{a list of the}} sites operating the world’s 500 most powerful computer systems is compiled and released. The best performance on the Linpack benchmark is used as the measurement for ranking the computer systems. The list contains a variety of information including the systems ’ specifications and major application areas. Information on all 30 TOP 500 lists issued to date is available at: www. top 500. org 1. Mannheim Supercomputer Statistics 1986 – 1992 and TOP 500 Project Start in 1993 From 1986 through 1992, the Mannheim supercomputer statistics were presented to participants of the Supercomputer Seminars at Mannheim University, and we noticed an increased interest in this kind of data from year to year. [1] The statistics simply counted the <b>vector</b> <b>computer</b> systems installed in the U. S., Japan and Europe, since in the mid- 80 s a supercomputer was synonymous with a <b>vector</b> <b>computer.</b> Counting the vector computers installed worldwide primarily depended on the input provided by the manufacturers of the systems, which made the statistics less reliable. Whereas we knew well which vector systems existed in the U. S. and Europe, information regarding systems in Japan was much more difficult to collect. We therefore contacted the three Japanese <b>vector</b> <b>computer</b> manufacturers – Fujitsu, NEC and Hitachi – for information on all systems installed in Japan and used their data as the basis for our yearly estimations...|$|E
40|$|AbstractThis paper gives a {{classification}} for the triangular factorization of square matrices. These factorizations {{are used for}} solving linear systems. Efficient algorithms for vector computers are presented on basis of criteria for optimal algorithms. Moreover, the Gauss—Jordan elimination algorithm in a version which admits efficient implementation on a <b>vector</b> <b>computer</b> is described. Comparative experiments in FORTRAN 77 with FORTRAN 200 extensions for the Cyber 205 are reported...|$|E
40|$|Abstract. Different {{types of}} {{synchronous}} and asynchronous two-stage multisplitting algorithms for {{the solution of}} linear systems are analyzed. The different algorithms {{which appeared in the}} literature are reviewed, and new ones are presented. Convergence properties of these algorithms are studied when the matrix in question is either monotone or an H-matrix. Relaxed versions of these algorithms are also studied. Computational experiments on a shared memory multiprocessor <b>vector</b> <b>computer</b> are presented...|$|E
40|$|A {{technique}} for optimizing software is proposed {{that involves the}} use of a standardized set of computational kernels that are common to many iterative methods for solving large sparse linear systems of equations. These kernels, referred to as "Iterative Basic Linear Algebra Subprograms" or "Iterative BLAS", are defined and techniques for their optimization on <b>vector</b> <b>computers</b> are presented. Several sparse matrix storage formats for different classes of matrix problems are proposed that allow the vectorization of fundamental operations in various iterative methods using these kernels. 1 Introduction Many iterative methods perform operations that can be easily optimized on most <b>vector</b> <b>computers,</b> such as the dot product of two vectors and the updating of a vector using another vector. These operations are often used in linear algebra applications, and they have been denoted as Basic Linear Algebra Subprograms or BLAS [23]. In the BLAS library, the calling sequences of these primitive vec [...] ...|$|R
40|$|This thesis {{introduces}} and elaborates {{on specific}} language constructs that allow a simple programming of <b>vector</b> <b>computers</b> {{and help to}} gain a better understanding for these programs. Thereby the emphasis lies on the support of explicitly vectorizable statements as well as on a concept for parameter passing adapted to the needs of numerical applications. <b>Vector</b> <b>computers</b> provide powerful instructions for the processing of whole vectors. The speed of programs is often increasing by orders of magnitude if these programs allow the use of such instructions, i. e. if they are vectorizable. In order to make a program run faster, a compiler usually tries to vectorize its innermost loops. Unfortunately, the dependence analysis required therefore is quite complicated and often cannot be performed completely. The thesis therefore proposes a simple language construct allowing the explicit specification of independence and thus the parallel execution of statements. Hence, this language construct is much easier to vectorize than loops. It improves the readability and security of programs without reducing the quality of the generated code. The main application area of <b>vector</b> <b>computers</b> are numerical applications of linear algebra. A problem arising with those programs is that parts of matrices such as rows, columns or diagonals must be passed as arguments to a subroutine. Yet, most programming languages do not support such a flexible way of parameter passing. Array constructors offer a simple and safe way to solve this problem. The second part of the thesis focuses on the description of an experimental programming language called Oberon-V and of an appropriate cross_compiler for the Cray Y-MP. Oberon-V includes a subset of the language Oberon, which has been extended by the language constructs mentioned above. Compared to traditional compilers for <b>vector</b> <b>computers,</b> the Oberon-V compiler excels by its compactness and efficiency. Detail problems of implementation were solved in a new and more simple way: some of the achievements were a new way of generating symbol files to support separate compilation, the optimization of the generated code by eliminating redundant computations (common subexpression elimination) and the reorganization of instructions to increase the execution rate (instruction scheduling). The thesis finally investigates and judges the code quality of Oberon-V programs in comparison with corresponding Fortran programs. ...|$|R
40|$|A {{practical}} finite element-based solution {{procedure for}} high-speed inviscid compressible flow problems is described. The method provides time-accurate {{solutions to the}} coompressible Euler equations, and is computationally more efficient than the one-step Taylor-Galerkin approach and better suited for implementation on the modern generation of <b>vector</b> <b>computers.</b> The method is coupled to an adaptive mesh refinement process that enables steady state solutions of improved quality to be obtained...|$|R
