0|259|Public
30|$|Equation (30) {{involves}} nine variables (η_d_p, R 1, R 0, N, ρ p, d p, L 1, μ, and Q) and one constant. To {{improve the}} efficiency of dust removal, {{it is necessary to}} optimise these nine variables, and to do this, a <b>simplification</b> <b>procedure</b> will be applied to Eq. (30).|$|R
40|$|Abstract. We {{provide a}} package of {{strategies}} for automation of non-linear arithmetic in PVS. In particular, we describe a <b>simplification</b> <b>procedure</b> for the field of real numbers and a strategy for cancellation of common terms. Key words. Non-linear arithmetic, PVS strategies, real number automation Subject classification. Computer Science 1. Introduction. Whil...|$|R
40|$|Several {{commercial}} {{case-based reasoning}} (CBR) shells now use decision trees to index cases, including REMIND (Cognitive Systems Inc.), KATE (AcknoSoft), THE EASY REASONER (The Haley Enterprise), and KNOWLEDGE BUILDER (ServiceSoft). These trees serve to expedite case retrieval and to generate comprehensible explanations of case retrieval behavior. Unfortunately, induced trees are often large and complex, reducing their explanatory power. To combat this problem, some commercial systems contain {{an option for}} simplifying decision trees. However, while many methods for simplifying decision trees exist, {{they have not been}} systematically compared and most have not been applied to case retrieval. This report builds on our previous survey and initial empirical comparison of tree <b>simplification</b> <b>procedures.</b> In this report, we compare them on a specific, challenging task that is the focus of an existing CBR effort. We examine which tree <b>simplification</b> <b>procedures</b> are useful for this task and sugge [...] ...|$|R
40|$|After a brief {{review of}} the current state of {{simplification}} this paper proposes a classification of simplification rules that may help in the practical implementation of <b>simplification</b> <b>procedures.</b> The last part of the paper is concerned in the mathematical theory of simplification, and the set of expressions formed from rational powers of polynomials is shown to have a canonical form...|$|R
40|$|An {{algorithm}} is developed to generate simplified (skeletal) kinetic mechanisms from a given detailed one. The {{algorithm is}} able to replicate the dynamics of a user-specified set of species (chosen from the original set) when a finite set of sampling points, D, in the chemistry configuration space is given. The <b>simplification</b> <b>procedure</b> involves discarding elementary reactions and species that ar...|$|R
40|$|This paper proposes {{design rules}} {{in order to}} simplify current MRP II {{software}} systems and make them suitable for the small manufacturing firm. Simplification rules apply to production data management by reducing database complexity. This has consequences to medium-term requirements planning which is moved toward the realization of simultaneous material and capacity plannning. The <b>simplification</b> <b>procedure</b> is completed by applyin...|$|R
5000|$|<b>Simplification</b> of <b>procedures</b> through {{removal of}} requests, changes to Rules of Court, and Practice Directions.|$|R
40|$|Abstract A {{method for}} {{detection}} of salient ridges and ravines on surfaces approximated by dense triangle meshes is proposed. The method combines together detection of mesh creases by local differential geometry operators, a mesh <b>simplification</b> <b>procedure</b> preserving main surface creases, and detection of the main surface creases on the original mesh by solving weighted length minimization problems. Presented results demonstrate robustness of the developed approach...|$|R
40|$|A type {{inference}} algorithm for {{the local}} type system for mobile ambients of [CDCGP 04] is presented. It is obtained by applying a combination of techniques to the original typing rules, including a constraint handling and <b>simplification</b> <b>procedure,</b> which is mainly based on unification. The algorithm employs type schemes and therefore is, in a sense, more general than the original system. The inference algorithm is then implemented in Prolog...|$|R
40|$|This paper {{describes}} a topological approach for simplifying continuous functions defined on volumetric domains. The Morse-Smale complex provides a segmentation of the domain into monotonic regions having uniform gradient flow behavior. We present a combinatorial algorithm that simplifies the Morse-Smale complex by repeated application of two atomic operations that removes pairs of critical points. The <b>simplification</b> <b>procedure</b> leaves important critical points untouched, {{and is therefore}} useful for extracting features. We present a visualization of the simplified topology...|$|R
40|$|Abstract. This is a {{proposal}} for demonstration of simba in PROPOR 2012. simba is an extractive multi-document summarization system that aims at producing generic summaries guided by a compression rate defined by the user. It uses a double-clustering approach to find the relevant information {{in a set of}} texts. In addition, simba uses a sentence <b>simplification</b> <b>procedure</b> as a mean to ensure summary compression. Furthermore, simplification seeks to produce simpler and more incisive sentences, containing the amount of information required...|$|R
40|$|This paper {{describes}} {{the use of}} computational geometry concepts in the digital cartography.  It presents an importance of 2 D geometric structures, geometric operations and procedures for automated or semi automated simplification process. This article is focused on automated building <b>simplification</b> <b>procedures,</b> some techniques are illustrated and discussed. Concrete examples with the requirements to the lowest time complexity, emphasis on the smallest area enclosing rectangle, convex hull or self intersection procedures, are given. Presented results illustrate the relationship of digital cartography and computational geometry...|$|R
5000|$|<b>Simplification</b> of {{administrative}} <b>procedures</b> for citizens and businesses; ...|$|R
40|$|Abstract. In this paper, we firstly {{describe}} two topological configurations {{that are}} not considered by Saalfeld’s polyline simplification algorithm: the coincidence topology, concerning the overlapping of two polylines or the overlapping of a feature point and a polyline, and the incidence topology, concerning the incidence of two polylines without having the incidence point represented as a common vertex. Afterwards, we suggest a simple modification in Saalfeld’s algorithm for preserving these topologies. Finally, we give some results of our <b>simplification</b> <b>procedure</b> and compare them to the ones of Saalfeld’s algorithm. 1...|$|R
40|$|In {{the present}} study a {{comparison}} {{has been carried out}} between manually and digitally simplified lines in order to estimate appropriate algorithmic tolerances. For this purpose two algorithms have been used to simplify coastlines, cartographic features of high complexity, over a wide range of scale change. Digitally derived coastlines were compared with a reference data set representing the manual <b>simplification</b> <b>procedure.</b> The tolerances ensure small magnitude of displacement. This method may be followed for producing derived cartographic lines at a wide range of scales from a base map...|$|R
5000|$|It may be {{necessary}} to re-engineer the job process. This could involve redesigning the physical facility, redesign processes, change technologies, <b>simplification</b> of <b>procedures,</b> elimination of repetitiveness, redesigning authority structures.|$|R
2500|$|... (ii) [...] <b>simplification</b> of <b>procedure</b> {{to reduce}} and {{eliminate}} technicalities and devices for delay {{so that it}} operates not {{as an end in}} itself but as a means of achieving justice.|$|R
40|$|AbstractPartial {{discretization}} as a {{means for}} simplifying mathematical models is examined in detail. The main aim of this endeavor is to reconcile the approximate character of this model simplification technique with some fairly recent system theoretic developments, in which valid simplification is characterized in terms of certain preservation relations. To this end, a model is constructed for systems of interest in blood physiology and chemical engineering, then this model is subjected to all sorts of manipulations, including partial discretization. Out of this analysis, there emerges the notion of approximate preservation relation, which charaterizes validity of this approximate <b>simplification</b> <b>procedure...</b>|$|R
40|$|AbstractCreation of {{mathematical}} model of complex radical processes manually {{is a very}} time-consuming process. Steam-cracking model created automatically by automated reaction network generation, on the other hand, becomes very large and complex for bigger molecules. This work was aimed at applying pseudo-steady state assumption automatically to components inside generated reactions network. The details of simplifying procedure are provided and the comparison of experimental data (lab-scale) to simulations by original and simplified model is presented. In overall, the <b>simplification</b> <b>procedure</b> lead only to marginal deviations in the simulated results, but the model ability to simulate bigger molecules has substantially improved...|$|R
40|$|Contingent {{processing}} {{occurs when}} agents select computational procedures for making decisions contingent on specific {{features of the}} decision environment they face. We illustrate contingent processing in a simple strategic situation—the well-known takeover game where the naive model of bidding provides {{an explanation of the}} winner’s curse. We interpret the naïve model as a <b>simplification</b> <b>procedure</b> used when the complexity of the takeover game is sufficiently great in a well-defined way we call type complexity. Experimental variation of type complexity across takeover games changes the frequency of naïve simplification, consistent with contingent processing theories of decision cognition...|$|R
40|$|This paper {{suggests}} a ten-step model for solving ethical dilemmas {{taking into account}} {{a wide spectrum of}} ethical values. The model has a prescriptive content that should help decision-makers to find a solution to ethical dilemmas according to the dictates suggested by moral obligation. For each step of the model, different types of <b>simplification</b> <b>procedures</b> are used in order to guide the decision-maker progressively toward a satisfactory solution. We begin with a discussion of the main characteristics that the model should possess. The paper then gives {{a detailed description of the}} single steps of the model. Lastly, a case study was analysed...|$|R
40|$|Finite Element (FE) meshes {{are usually}} highly refined and dense and, consequently, {{computationally}} very expensive. Therefore, after the preliminary FE mesh generation, {{it is necessary}} to decrease its size by diminishing the total number of nodes and elements while maintaining both geometrical accuracy and a physically-meaningful FE mesh refinement. The aim of this work is to describe an optimized FE mesh <b>simplification</b> <b>procedure</b> based on edge contraction and on a user-defined spatial refinement gradient criterion. The main idea is that, for normal mechanical loadings applied to a motion segment, the vertebrae shall behave almost as an incompressible medium, and only the intervertebral disc (IVD) should undergo relevant strains. In this case, it is acceptable (and desirable) to attain a problem-functional FE mesh, in which the FE mesh should be more refined at the IVD and coarser at vertebrae. On the other hand, an optimized FE mesh should be more refined at the annulus fibrosus than at the nucleus pulposus. In summary, the proposed FE mesh generation and <b>simplification</b> <b>procedure,</b> being based on a user-definable spatial FE mesh refinement gradient, allows the user to define precisely the required refinement and thus to reduce drastically the number of elements in the final FE mesh and consequently to diminish the computation time required for the FE analysis, while keeping the necessary physical meaning on the FE mesh. The aforesaid procedure will be applied to the FE mesh generation (based on [1]) of a motion segment initially characterized by medical imaging...|$|R
40|$|ACL 2 {{was used}} to prove {{properties}} of two <b>simplification</b> <b>procedures.</b> The procedures differ in complexity but solve the same programming problem that arises {{in the context of}} a resolution/paramodulation theorem proving system. Term rewriting is at the core of the two procedures, but details of the rewriting procedure itself are irrelevant. The ACL 2 encapsulate construct {{was used to}} assert the existence of the rewriting function and to state some of its properties. Termination, irreducibility, and soundness properties were established for each procedure. The availability of the encapsulation mechanism in ACL 2 is considered essential to rapid and efficient verification of this kind of algorithm. ...|$|R
40|$|In this paper, {{we present}} a topological {{approach}} for simplifying continuous functions defined on volumetric domains. We introduce two atomic operations that remove pairs of critical points of the function and design a combinatorial algorithm that simplifies the Morse-Smale complex by repeated application of these operations. The Morse-Smale complex is a topological data structure that provides a compact representation of gradient flow between critical points of a function. Critical points paired by the Morse-Smale complex identify topological features and their importance. The <b>simplification</b> <b>procedure</b> leaves important critical points untouched, and is therefore useful for extracting desirable features. We also present a visualization of the simplified topology...|$|R
5000|$|... {{facilitation}} {{of trade}} through <b>simplification</b> of customs <b>procedures</b> and administration of tax break schemes; ...|$|R
40|$|In {{their model}} of induced {{unemployment}} and {{unemployment insurance benefits}} Grubel, Maki, and Sax (1975) break the collinearity between two subsets of regressors by replacing one subset with the residuals from the regression of that subset on the other. This note works out the statistical implications of this orthogonalization procedure in the general linear model. It is shown that a subset of estimators is always biased and inconsistent and that conventional inference is thereby invalidated. Furthermore, we demonstrate that orthogonalization can actually worsen collinearity if measured by its effect on estimated variances. The implications of these results for the model <b>simplification</b> <b>procedure</b> used recently in Baba, Hendry, and Starr (1992) are also discussed. ...|$|R
40|$|This paper {{presents}} {{a method for}} extractive multi-document summarization that explores a two-phase cluster-ing approach that, combined with a sentence <b>simplification</b> <b>procedure,</b> aims to generate more useful summaries. First, sentences are clustered by similarity, and one sentence per cluster is selected, to reduce redundancy. Then, in order to group them according to topics, those sentences are clus-tered considering the collection of keywords. Finally, the summarization process includes a sentence simplification step, which aims not only to create simpler and more in-cisive sentences, but also {{to make room for}} the inclusion of further relevant content in the summary. Evaluation re-veals that the approach pursued produces highly informa-tive summaries, containing relevant data and no repeated information. ...|$|R
40|$|We {{follow the}} <b>simplification</b> <b>procedure</b> {{described}} in previous work to generate simplified kinetic mechanisms for the surrogate fuel proposed by Dagaut for jet fuels. A family of comprehensive mechanisms is produced at different pressures (1, 10, 20, and 40 atm) {{by analyzing the}} constant volume, adiabatic, spatially homogeneous, auto-ignition process of surrogate/air mixture at different equivalence ratios and initial temperatures. We identify the prevailing pathways of surrogate oxidation for low and high temperature kinetics for a lean mixture at 10 atm. We assess how {{the accuracy of the}} prediction of soot precursors changes with different choices of the set of target species. Copyright © 2008 by the American Institute of Aeronautics and Astronautics, Inc...|$|R
40|$|International audienceMesh {{simplification}} is {{an important}} research topic in scientific visualization and virtual reality. The simplification metric is a key issue of a simplification algorithm. In this study, two new simplification metrics based on surface moments and volume moments are proposed, which take {{the difference between the}} moments defined by the original mesh and those of the simplified mesh as the objective function. These metrics were used in an edge collapse scheme in order to prove their usability in the mesh <b>simplification</b> <b>procedure.</b> For a given maximum order and the number of triangles required, the optimal mesh with a minimum moment difference from the original mesh can be determined. The procedures are applied to some models and better results are obtained in comparison with some known algorithms...|$|R
40|$|Handling {{intellectual}} property involves the cognitive process {{of understanding the}} innovation de-scribed {{in the body of}} patent claims. In this paper we present an on-going project on a multi-level text simplification to assist experts in this complex task. Two levels of <b>simplification</b> <b>procedure</b> are de-scribed. The macro-level simplification results in the visualization of the hierarchy of multiple claims. The micro-level simplification includes visualization of the claim terminology, decomposition of the claim complex structure into a set of simple sentences and building a graph explicitly showing the in-terrelations of the invention elements. The methodology is implemented in an experimental text sim-plifying computer system. The motivation underlying this research is to develop tools that could in-crease the overall productivity of human users and machines in processing patent applications. ...|$|R
40|$|In {{this report}} we present an {{implementation}} of a symbolic reachability analyzer, FIXIT, based on SAT-methods. The problem of reachability is to determine for a given transition system, whether a set of bad states is reachable from a set of initial states. We call our approach symbolic, as we use formulas to represent set of states. The representation includes the standard connectives AND, EQUIV, and NOT, plus sharing of common subformulas. We show how the operations of a standard reachability algorithm can be implemented on this representation, and especially how boolean quantification can be performed. Finally, we present some experimental results, and conclude that a straightforward quantification algorithm together with local <b>simplification</b> <b>procedures</b> suffice for making SAT-based methods an interesting alternative to BDD-based methods. Contents...|$|R
40|$|The recent {{continuous}} {{growth in}} the size and availability of large triangular surface models has generated {{a great deal of}} research in multi-resolution progressive representation and data compression. An ongoing challenge is to design an efficient data structure that encompasses both the compactness of a geometric compression scheme and the visual quality of a progressive representation. In this paper we introduce a geometric data-structure and an algorithm to build a compressed representation of a geometric model with attached attributes. This compressed representation encompasses multiple levels of detail that can be transmitted and displayed progressively. The <b>simplification</b> <b>procedure</b> allows one to modify the global topology of the model (number of holes and connected components) achieving guaranteed size coarse levels even for very large and complex models...|$|R
40|$|In {{the quest}} for realism in {{computer}} graphics, highly detailed geometric models are rapidly becoming commonplace. These models, often represented as complex triangle meshes, challenge all aspects of computing, including rendering performance, transmission bandwidth, and storage capacities. In this paper we introduce the progressive mesh (PM) representation, a new scheme for storing and transmitting arbitrary triangle meshes. This efficient, lossless, continuous-resolution representation addresses several practical problems in graphics: smooth geomorphing of level-of-detail approximations, progressive transmission, mesh compression, and selective refinement. In addition, we present a new mesh <b>simplification</b> <b>procedure</b> for constructing a PM representation from an arbitrary mesh. The goal of this optimization procedure is to preserve not just the geometry of the original mesh, but more importantly its overall appearance as defined by its discrete and scalar appearance attributes such as ma [...] ...|$|R
50|$|Among other <b>simplifications</b> of {{government}} <b>procedures,</b> a visa-free regime {{was to be}} implemented, but a draft decree wasn't agreed to by government departments.|$|R
40|$|International audienceIn this paper, we {{introduce}} a flexible {{framework for the}} reconstruction of a surface from an unorganized point set, extending the geometric convection approach introduced by Chaine. Given a dense input point cloud, we first extract a triangulated surface that interpolates {{a subset of the}} initial data. We compute this surface in an output sensitive manner by decimating the input point set on-the-fly during the reconstruction process. Our <b>simplification</b> <b>procedure</b> relies on a simple criterion that locally detects and reduces oversampling. If needed, we then operate in a dynamic fashion for local refinement or further simplification of the reconstructed surface. Our method allows to locally update the reconstructed surface by inserting or removing sample points without restarting the convection process from scratch. This iterative correction process can be controlled interactively by the user or automatized given some specific local sampling constraints...|$|R
40|$|P. B. I. {{determinations}} are {{now being}} replaced in many laboratories by thyroxine determinations, but the latter have previously had the disadvantage of not being automated. The Analmatic preparation unit has been adapted for the large scale determinations of serum thyroxine using the Radiochemical Centre's Thyopac- 4 kits. There is satisfactory correlation between the manual and automated techniques. Both the between-batch and within-batch precision {{were found to be}} better than that for the manual method. Serum thyroxine determinations are generally accepted to be more reliable than protein-bound iodine (P. B. I.) determinations as tests of thyroid function, but the former have the disadvantage of not being capable of automation. However, work <b>simplification</b> <b>procedures</b> for serum thyroxine deter-minations have recently been developed (Badman and Platten, 1973; Seth, 1973 a), and the variables which affect the accuracy and precision of thes...|$|R
