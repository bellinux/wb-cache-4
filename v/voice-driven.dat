51|0|Public
50|$|Raeburn can {{be heard}} {{contributing}} on various BBC Radio 4 programmes and <b>voice-driven</b> advertising features, such as for Ocado in January 2008.|$|E
50|$|Aino is {{a unified}} <b>voice-driven</b> online {{platform}} based in India. It provides resolutions to customer queries and brand interactions with multiple service providers through a single platform. It {{is the first}} unified free customer care calling app in India.|$|E
50|$|Rockbox is a {{free and}} {{open-source}} software replacement for the OEM firmware in various forms of digital audio players (DAPs) with an original kernel. It offers {{an alternative to the}} player's operating system, in many cases without removing the original firmware, which provides a plug-in architecture for adding various enhancements and functions. Enhancements include personal digital assistant (PDA) functions, applications, utilities, and games. Rockbox can also retrofit video playback functions on players first released in mid-2000. Rockbox includes a <b>voice-driven</b> user-interface suitable for operation by visually impaired users.|$|E
5000|$|The word [...] "galant" [...] {{derives from}} French, {{where it was}} in use from at least the 16th century. In the early 18th century, a Galant Homme {{described}} a person of fashion; elegant, cultured and virtuous. The German theorist Johann Mattheson {{appears to have been}} fond of the term. It features in the title of his first publication of 1713, Das neu-eröffnete Orchestre, oder Universelle und gründliche Anleitung wie ein Galant Homme einen vollkommenen Begriff von der Hoheit und Würde der edlen Music erlangen. (Instead of the Gothic type rendered here in italics, Mattheson used Roman to emphasize the many non-German expressions ( [...] ; [...] )). Mattheson was apparently the first to refer to a [...] "galant style" [...] in music, in his Das forschende Orchestre of 1721. He recognized a lighter, modern style, einem galanten Stylo and named among its leading practitioners Giovanni Bononcini, Antonio Caldara, Georg Philipp Telemann, Alessandro Scarlatti, Antonio Vivaldi and George Frideric Handel [...] All were composing Italian opera seria, a <b>voice-driven</b> musical style, and opera remained the central form of galant music. The new music was not as essentially a court music as it was a city music: the cities emphasized by Daniel Heartz, a recent historian of the style, were first of all Naples, then Venice, Dresden, Berlin, Stuttgart and Mannheim, and Paris. Many galant composers spent their careers in less central cities, ones that may be considered consumers rather than producers of the style galant: Johann Christian Bach and Carl Friedrich Abel in London, Giovanni Paisiello in St Petersburg, Georg Philipp Telemann in Hamburg, and Luigi Boccherini in Madrid.|$|E
40|$|When Mike McCue founded Tellme in 1999, {{its initial}} product was a <b>voice-driven</b> {{information}} service, {{what might be}} termed a 2 ̆ 2 voice portal. 2 ̆ 2 Today, the company 2 ̆ 7 s voice-recognition systems power directory assistance services from AT 2 ̆ 6 T, Verizon and Cingular along with automated 800 -number customer help lines at companies like Merrill Lynch and Federal Express. But McCue still harbors dreams of a broader <b>voice-driven</b> web, one which will provide consumers with new ways of using the phone {{to interact with the}} universe of information. He discussed this vision, and others, during a recent interview with Knowledge@Wharton in Tellme 2 ̆ 7 s Mountain View, Calif., offices...|$|E
40|$|In this work, an {{implementation}} of an automatic <b>voice-driven</b> telephone exchange is presented. The main feature {{is that this}} system can accept continuous speech. In addition, {{it is based on}} a low-cost hardware platform which consists of a voice modem and a personal computer using the Linux operating system...|$|E
30|$|Conventional <b>voice-driven</b> wheelchairs usually employ headset {{microphones}} {{that are}} capable of achieving sufficient recognition accuracy, even in the presence of surrounding noise. However, such interfaces require users to wear sensors such as a headset microphone, which can be an impediment, especially for the hand disabled. Conversely, it is also well known that the speech recognition accuracy drastically degrades when the microphone is placed far from the user. In this paper, we develop a noise robust speech recognition system for a <b>voice-driven</b> wheelchair. This system can achieve almost the same recognition accuracy as the headset microphone without wearing sensors. We verified the effectiveness of our system in experiments in different environments, and confirmed that our system can achieve almost the same recognition accuracy as the headset microphone without wearing sensors.|$|E
40|$|Abstract—This paper {{presents}} {{a system for}} controlling audio mosaicing with a voice signal, which {{can be interpreted as}} a further step in <b>voice-driven</b> sound synthesis. Compared to <b>voice-driven</b> instrumental synthesis, it increases the variety in the synthesized timbre. Also, it provides a more direct interface for audio mosaicing applications, where the performer voice controls rhythmic, tonal and timbre properties of the output sound. In a first step, voice signal is segmented into syllables, extracting a set of acoustic features for each segment. In the concatenative synthesis process, the voice acoustic features (target) are used to retrieve the most similar segment from the corpus of audio sources. We implemented a system working in pseudo-realtime, which analyzes voice input and sends control messages to the concatenative synthesis module. Additionally, this work raises questions to be further explored about mapping the input voice timbre space onto the audio sources timbre space. I...|$|E
40|$|The {{objective}} of the EU-funded SpeechDat project was to create large-scale speech databases for <b>voice-driven</b> teleservices. This paper deals with the design of two such Swedish resources: 5000 speakers over the fixed telephone network, and 1000 over the mobile network. It also reports on experiences from speaker recruitment and presents statistics on speaker distribution. Results regarding orthographic labelling of pronunciation, pronunciation errors and non-speech events are also included...|$|E
40|$|HOMER is a <b>voice-driven</b> {{system for}} Slovenian text-to speech {{synthesis}} developed for blind or partially sighted persons for reading Slovenian newspapers. The system {{consists of three}} main modules. The speech synthesis module enables speech synthesis from an arbitrary Slovenian text input, the speech recognition module performs speaker independent isolated word recognition and the dialogue module controls the different tasks of the HOMER system. The system runs under LINUX and requires a PENTINUM PC with minimum 32 MB of RAM and an additional standard SB 16 sound card. ...|$|E
40|$|The {{use of a}} <b>voice-driven</b> {{information}} retrieval system as first-line support in a help desk environment is proposed. It is suggested that the system should be implemented using two commercially available products: the Nuance speech recognition system, which creates text representations of spoken utterances, and the Deskartes knowledge management system, which {{is a type of}} serch engine that has the ability to learn from user feedback (in the form of question/answer couplings). In a <b>voice-driven</b> {{information retrieval}} system based on Nuance and Deskartes, the former will transform spoken questions into text representations, which will then be used by the latter as search queries. Evaluations are carried out in order to determine how a pilot system should be configured for optimal retrieval effectiveness and user satisfaction and how the Deskartes learning mechanism influences retrieval performance. A mapping from a set of prerecorded questions to a set of documents (answers) is used for training and evaluating the system. The results show that optimal performance is achieved when every searchable document is represented by at most five keywords and the vocabulary used in speech-to-text conversion is restricted to the indexing vocabulary of the set of documents together with the set of stop words utilized by Deskartes. It is also shown that an increase in the...|$|E
40|$|International audienceThis paper {{deals with}} use of MAUD as an oral {{interface}} for data entry and {{the description of the}} speech component of this system. The interface of MAUD has to combine <b>voice-driven</b> and keyboard dialogue in order to allow the user to use both keyboard and voice. The speech recognition system participated at the AUPELF-UREF evaluation of dictation machines. MAUD uses a vocabulary of 20 000 words. The mode of speech is continuous, and the language model was built with corpora of over than 48 million of words extracted from French newspapers...|$|E
40|$|This {{thesis is}} {{comprised}} {{of a collection of}} short stories, most of which are set in southern, urban milieus. The fictional characters contained within deal in their own unique ways with the crises they face. Most of these sources of conflict arise from domestic complications. Six of the eight stories are written in the first person; the collection is <b>voice-driven</b> and concerned with the idiosyncratic points of views of the focal characters, and in this way borrows from the tradition of Southern fiction, which is in many cases laced with dark humor...|$|E
40|$|The {{objective}} {{of this study was}} to determine the feasibility of using voice recognition technology to enable hands-free and eyes-free collection of data related to animal drug toxicology studies. Specifically, we developed and tested a prototype <b>voice-driven</b> data collection system for histopathology data using only voice input and computer-generated voice responses. The overall accuracy rate was 97 %. Additional work is needed to minimize training requirements and improve audible feedback. We conclude that this architecture could be considered a viable alternative for data collection in animal drug toxicology studies with reasonable recognition accuracy...|$|E
30|$|Speech signal {{processing}} technologies, which have made significant strides {{in the last}} few decades, now play various important roles in our daily lives. For example, speech communication technologies such as (mobile) telephones, video-conference systems, and hearing aids are widely available as tools that assist communication between humans. Speech recognition technology, which has recently left research laboratories and is increasingly coming into practical use, now enables a wide spectrum of innovative and exciting <b>voice-driven</b> applications. However, most of these applications consider a microphone located near the talker as a prerequisite for reliable performance, which prevents further proliferation.|$|E
40|$|Graduation date: 2009 This {{collection}} of poetry explores faith and isolation while questioning {{the ability of}} language to adequately express subjective experience. Gustie has provided a deeply contemplative, though rarely completely serious, romp through {{the lives of others}} who, ultimately, are reflections of the self. The three sections of the collection establish a near-mythic <b>voice-driven</b> narrative. His use of persona is established in the first section, is deconstructed in the second, {{and by the time we}} reach the third section, he has shown us that there may be, in fact, a place for the genuine in poetry...|$|E
40|$|The {{following}} collection {{represents the}} critical and creative work produced during my doctoral program in English. The dissertation consists of Part I, a critical preface, and Part II, {{a collection of}} seven short stories and two nonfiction essays. Part I, which contains the critical preface entitled "What to Say and How to Say It," examines the role of voice in discussions of contemporary literature. The critical preface presents a definition of voice and identifies examples of <b>voice-driven</b> writing in contemporary literature, particularly {{from the work of}} Mary Robison, Dorothy Allison, and Kathy Acker. In addition, the critical preface also discusses how the use of flavor, tone, and content contribute to voice, both in work of famous authors and in my own writing. In Part II of my dissertation, I present the creative portion of my work. Part II contains seven works of short fiction, titled "Among Waitresses," "The Lion Tamer," "Restoration Services," "Hospitality," "Blood Relation," "Managerial Timber," and "Velma A Cappella. " Each work develops a <b>voice-driven</b> narrative through the use of flavor, tone, and content. Also, two nonfiction essays, titled "Fentanyl and Happy Meals" and "Tracks," close out the collection. "Fentanyl and Happy Meals" describes the impact of methamphetamine addiction on family relationships, while "Tracks" focuses on the degradation of the natural world by human waste and other forms of pollution. In total, this collection demonstrates my approach to both scholarly and creative writing, and I am grateful for the University of North Texas for the opportunity to develop academically and achieve my goals...|$|E
40|$|This paper {{describes}} the frequency analysis of spoken Urdu numbers from ‘sifr ’ (zero) to ‘nau ’ (nine). Sound samples from multiple speakers were utilized to extract different features. Initial processing of data, i. e., normalizing and time-slicing was done {{using a combination}} of Simulink and MATLAB. Afterwards, the same tools were used for calculation of Fourier descriptions and correlations. The correlation allowed comparison of the same words spoken by the same and different speakers. The analysis presented in this paper is seen as the first step in creating an Urdu speech recognition system. Such a system can be potentially utilized in implementation of a <b>voice-driven</b> help setup at call centers of commercial organizations operating in Pakistan/India region...|$|E
30|$|Currently, modern {{voice control}} {{technology}} {{is available in}} many applications such as direct voice input (DVI) in aviation [1], information requests using Siri and speech-driven home automation. Command-and-control (C&C) appliances afford hands-free control, thus enhancing {{the independence of the}} physically incapacitated. Unfortunately, speech commands are sometimes misinterpreted when words overstep lexical boundaries and word sequences do not fit the preset grammars. Moreover, C&C appliances frequently fail to interpret dialectic or impaired speech, often encountered with physically challenged people. Consequently, people with non-standard speech are increasingly excluded from the growing market of <b>voice-driven</b> applications. The goal of this work is to investigate a vocal user interface (VUI) model which is able to learn words and grammars from end users, improving accessibility of C&C applications.|$|E
40|$|We {{present a}} novel {{interface}} for directing {{the actions of}} computer animated characters and camera movements. Our system takes spoken input in combination with mouse pointing to generate desired character animation based on motion capture data. The aim is to achieve a more natural animation interface by supporting the types of dialogue and pointing that might be used when one person is explaining a desired motion to another person. We compare our <b>voice-driven</b> system with a button-driven animation interface that has equivalent capabilities. An informal user study indicates that for the test scenarios, the voice-user interface (VUI) is faster than an equivalent graphical user interface (GUI). Potential applications include storyboarding for film or theatre, directing characters in video games, and scene reconstruction. 1...|$|E
40|$|We present VoiceDraw, a <b>voice-driven</b> drawing {{application}} {{for people with}} motor impairments that provides a way to generate free-form drawings without needing manual interaction. VoiceDraw was designed and built to investigate {{the potential of the}} human voice as a modality to bring fluid, continuous direct manipulation interaction to users who lack the use of their hands. VoiceDraw also allows us to study the issues surrounding the design of a user interface optimized for non-speech voice-based interaction. We describe the features of the VoiceDraw application, our design process, including our user-centered design sessions with a “voice painter, ” and offer lessons learned that could inform future voice-based design efforts. In particular, we offer insights for mapping human voice to continuous control...|$|E
40|$|Abstract- HOMER is a <b>voice-driven</b> text-to-speech sys-tem {{developed}} for {{blind or visually}} impaired persons for reading the Slovenian texts. Users can obtain texts from the special corpora organised on the computer network server at the information centre of the association of the Slovenian blind and visually impaired persons. The system consists of three main modules. The text-to-speech module enables speech synthesis from an arbi-trary Slovenian text input, the speech recognition mod-ule performs speaker independent isolated word recogni-tion and the dialogue module controls the different tasks of the HOMER system and obtains texts from the source text corpora. Presently, the system runs under LINUX and requires a PENTINUM/ 133 PC with minimum 32 MB of RAM and an additional standard 16 bit sound card. I...|$|E
40|$|This paper {{surveys the}} uses of non-speech voice as an {{interaction}} modality within sonic applications. Three main contexts of use have been identified: sound retrieval, sound synthesis and control, and sound design. An overview of different choices and techniques regarding the style of interaction, the selection of vocal features and their mapping to sound features or controls is here displayed. A comprehensive collection of examples instantiates the use of non-speech voice in actual tools for sonic interaction. It is pointed out that while voice-based techniques are already being used proficiently in sound retrieval and sound synthesis, their use in sound design is still at an exploratory phase. An example of creation of a <b>voice-driven</b> sound design tool is here illustrated...|$|E
40|$|SPEECON (Speech-Driven Interfaces for Consumer Devices) is {{a project}} which aims to develop <b>voice-driven</b> {{interfaces}} for consumer applications. Led by an industrial consortium, the project’s goal is to collect speech data for at least 20 languages and 600 speakers per language (mostly adults but children as well). Recorded in different environments which {{are expected to be}} representative for the future applications, the database corpus comprises both spontaneous and read speech, the latter including phonetically rich material, a large number of application commands and isolated items such as digits, names, etc. In order to safeguard consistency and high quality of the databases, all of them are subject to validation. This paper describes in detail the specifications of the databases as well as the validation procedure...|$|E
40|$|Six City is a 93 000 word <b>voice-driven</b> {{novel that}} {{traverses}} six countries as it follows its protagonist, a woman {{known only as}} S [...] -, after she is reported missing by her family. A lingerie-shop owner and politician 2 ̆ 7 s wife, S [...] - reinvents her identity from Barcelona to Morocco, through Mauritania, Senegal and Mali, and eventually into Sierra Leone. S [...] - is hotly pursued by a devoted 2 ̆ 2 Following, 2 ̆ 2 but when search efforts descend south into sub-Saharan Africa, the Following discovers that S [...] - has been found dead in the outskirts of Freetown. The result: a massive chase across multiple continents, tracing the steps of a runaway who [...] even in death [...] refuses to be found...|$|E
40|$|Dialogue {{material}} has been collected using bionic Wizard-of-Oz simulations of an automated <b>voice-driven</b> dialogue system; system turns, using rule-based speech synthesis, were systematically varied for both text and prosody. In a separate pilot experiment, subjects evaluated the system {{turns on a}} number of parameters. While they did not always clearly differentiate between the different conditions, they seemed to associate the "enhanced" prosody, preferred in an earlier laboratory experiment, with a more likeable personality. Keywords: prosody, synthesis, dialogue, evaluation, simulation 1. INTRODUCTION In the voice-operated dialogue system being developed in the SUNDIAL project, concentrating on a "flight enquiries" application, voice output for British English is provided by the Infovox text-to-speech synthesis system. The prosodic component has been adapted and expanded to optimise it for interactive dialogue as opposed to reading aloud. The selection of prosodic patterns is infor [...] ...|$|E
40|$|The {{purpose of}} this {{feasibility}} study is to bring an overview of state-of-the-art platforms for enabling Czech language for current ASR/TTS platform used in Research and Development Center for Mobile Applications in Prague. Other related issues, such as speech applications technology introduction and future implementation proposal, are also incorporated. Part I is primary intended for developers or such readers, that are interested in <b>voice-driven</b> applications deeply. Chapter 1 consists from a comprehensive introduction to VoiceXML. Chapter 2 discusses Media Resource Control Protocol, a key component for adding new ASR/TTS engines. Part II covers the feasibility study and proposes an approach, how the goal shall be achieved. Chapter 3 explains procedures and criterions of evaluation. Chapter 4 evaluates available speech engines. Chapter 5 proposes possible approaches of future implementation. Chapter 6 summarizes this study and recommends the future integration process...|$|E
40|$|Hands-free, <b>voice-driven</b> {{user input}} is gaining popularity, {{in part due}} to the {{increasing}} functionalities provided by intelligent digital assistances such as Siri, Cortana, and Google Now, and {{in part due to}} the proliferation of small devices that do not support more traditional, keyboard-based input. In this paper, we examine the gap in the mechanisms of speech recognition between human and machine. In particular, we ask the question, do the differences in how humans and machines understand spoken speech lead to exploitable vulnerabilities? We find, perhaps surprisingly, that these differences can be easily exploited by an adver-sary to produce sound which is intelligible as a command to a computer speech recognition system but is not easily understandable by humans. We discuss how a wide range of devices are vulnerable to such manipulation and de-scribe how an attacker might use them to defraud victims or install malware, among other attacks. ...|$|E
40|$|We {{conducted}} a 2. 5 week longitudinal study with five motor impaired (MI) and four non-impaired (NMI) participants, {{in which they}} learned to use the Vocal Joystick, a voice-based user interface control system. We found that the participants were able to learn the mappings between the vowel sounds and directions used by the Vocal Joystick, and showed marked improvement in their target acquisition performance. At {{the end of the}} ten session period, the NMI group reached the same level of performance as the previously measured “expert ” Vocal Joystick performance, and the MI group was able to reach 70 % of that level. Two of the MI participants were also able to approach the performance of their preferred device, a touchpad. We report {{on a number of issues}} that can inform the development of further enhancements in the realm of <b>voice-driven</b> computer control. Author Keywords Voice-based interface, speech recognition, input modality...|$|E
40|$|Abstract—Automatic speech {{recognition}} {{is a central}} and common component of <b>voice-driven</b> information processing systems in human language technology, including spoken language translation, spoken language understanding, voice search, spoken document retrieval, and so on. Interfacing {{speech recognition}} with its downstream text-based processing tasks of translation, understanding, and information retrieval creates both challenges and opportunities in optimal design of the combined, speech-enabled systems. We present an optimization-oriented statistical framework for the overall system design where the interactions between the sub-systems in tandem are fully incorporated and where design consistency is established between the optimization objectives and the end-to-end system performance metrics. Techniques for optimizing such objectives in both the decoding and learning phases of the speech-centric information processing system design are described, in which the uncertainty in speech recognition sub-system’s outputs is fully considered and marginalized. This paper {{provides an overview of}} the past and current work in this area. Future challenges and new opportunities are also discussed and analyzed...|$|E
40|$|In a {{previous}} work we presented {{a system for}} transcribing spoken rhythms into a symbolic score. Thereafter, the system was extended to process the vocal stream in real-time {{in order to allow}} a musician {{to use it as a}} <b>voice-driven</b> drum generator. Extensions to this work are the following. First we achieved a study of the system classification accuracy based on typical onomatopoeia used in western beat boxing, with the perspective of building a general supervised model for immediate use. Also, we want the user to be able to generate expressive rhythms, beyond the symbolic drum representation. Thus we considered a class-specific mapping of continuous vocal stream descriptors with either effects or synthesis parameters of the drum generator. The extraction of the symbolic drum stream is implemented in the BillaBoop VST Core plug-in. The class-specific mapping and the sound synthesis are carried out in Plogue Bidule 1 framework. All these components are integrated into a low-latency application that allows its use for live performances. 1...|$|E
40|$|Frames from a <b>voice-driven</b> animation, {{computed}} {{from a single}} baby picture and an adult model of facial control. Note the changes in upper facial expression. See figures 5, 6 and 7 for more examples of predicted mouth shapes. We introduce a method for predicting a control signal from another related signal, {{and apply it to}} voice puppetry: Generating full facial animation from expressive information in an audio track. The voice puppet learns a facial control model from computer vision of real facial behavior, automatically incorporating vocal and facial dynamics such as co-articulation. Animation is produced by using audio to drive the model, which induces a probability distribution over the manifold of possible facial motions. We present a lineartime closed-form solution for the most probable trajectory over this manifold. The output is a series of facial control parameters, suitable for driving many different kinds of animation ranging from video-realistic image warps to 3 D cartoon characters...|$|E
40|$|This paper {{describes}} {{the development of}} a voice data recording watch and the accompanying software and hardware needed to build an integrated solution for medical data capture and automatic codification. The developed system is used to capture on the stand-alone watch, either by voice recording or by form-filling, the relevant information on the healthcare service being provided. The captured information is stored locally and then downloaded to a PC, via a USB connection. In the PC, speech is translated first into text and then into medical codification, with minimum userintervention. The resulting information is stored into local or centralized patient databases, to be sent later to 3 rd-party SW for billing, reporting and statistics. The user controls the watch either via a touch-screen based interface or via a <b>voice-driven</b> interface for hands-free operation. Although this work is multidisciplinary, in this paper the emphasis is put on the description of the DSP implementation. 1...|$|E
40|$|Abstract: In <b>voice-driven</b> sound {{synthesis}} applications, phonetics convey musical information {{that might be}} related to the sound of an imitated musical instrument. Our initial hypothesis is that phonetics are user- and instrumentdependent, but they remain constant for a single subject and instrument. Hence, a user-adapted system is proposed, where mappings depend on how subjects performs musical articulations given a set of examples. The system will consist of, first, a voice imitation segmentation module that automatically determines noteto-note transitions. Second, a classifier determines the type of musical articulation for each transition from a set of phonetic features. For validating our hypothesis, we run an experiment where a number of subjects imitated real instrument recordings with the voice. Instrument recordings consisted of short phrases of sax and violin performed in three grades of musical articulation labeled as: staccato, normal, legato. The results of a supervised training classifier (user-dependent) are compared to a classifier based on heuristic rules (userindependent). Finally, with the previous results we improve the quality of a sample-concatenation synthesizer by selecting the most appropriate samples. ...|$|E
40|$|Abstract: A {{wide range}} of {{consumer}} electronics has been developed that {{have the capability to}} capture images such as digital imaging devices like digital cameras and camcorders. In order to assist a user in easily taking high-quality images, many digital image-capturing devices support various scene-modes that are accessible by a means of buttons, dials, or menus. So to overcome this disadvantage the proposed method Voice based Image transfer to USB using ARM 7 involves various aspects such as the hardware platform like LPC 2148 (ARM 7). Indeed, such a <b>voice-driven</b> scene-mode recommendation has several benefits. It reduces the number of actions required for a user to activate a scene-mode. The system is designed by using ARM 32 -bit Microcontroller which supports features of transferring the data to a removable drive by recognizing the voice. The prototype performs the tasks of opening an image, selecting a particular image and copying that image to a particular drive by recognizing a voice and the scene related image share opened and copied to USB with the recognition of voice using voice recognition kit...|$|E
40|$|GHOST TREE SOCIAL tells {{a coming}} out story of sorts. In terms of style, {{many of the}} poems are short, imagistic lyrics, though some are {{extended}} catalogues. Specific natural images—lakes, rivers, and snow—are often contrasted with cultural markers. The imagistic poems are thinking through the work of Sylvia Plath. The catalogue poems shift between diaristic, narrative, and critical modes, responding to the poetry of Elizabeth Bishop and the essays of Edouard Glissant. <b>Voice-driven</b> fragments disrupt the more traditional lyric poems. The fragments fall between formal lyrics like confetti from a gay club’s rafters; or the fragments hold the lyric poems in bondage. The lyric poem then re-signifies as form through resonances with the other discursive and poetic form of the fragment. Following critical writers such as Adrienne Rich and Audre Lorde, the re-signification of lyric form reflects {{the need for new}} signs for self and community organized queerly as opposed to more typical binary categories—man or woman, living or dead, rich or poor, white or black—where the first term is privileged and the second term often denigrated...|$|E
