13|34|Public
5000|$|... 2011 - EFILM {{develops}} post {{production workflow}} for Extremely Loud and Incredibly Close, the first U.S. Feature film to shoot using the ARRIRAW format on the Alexa. With an already long-established {{working relationship with}} ARRI and a <b>virtual</b> <b>pipeline</b> in place that could accommodate the project, EFILM was approached by the filmmakers to undertake this first-of-its-kind venture. It was veteran cinematographer Chris Menges' initial foray into digital cinematography and he wanted a very filmic look. Dailies were produced at Deluxe, New York and the look was set during the dailies process, per Menges's direction.|$|E
40|$|The OpenGL and OpenGL ES {{specifications}} {{describe a}} <b>virtual</b> <b>pipeline</b> in which triangles are processed in order: the vertices of a triangle are transformed, the triangle {{is set up}} and rasterized to produce fragments, the fragments are shaded and then written to the framebuffer. Once this has been done, the next triangle is processed...|$|E
40|$|A new {{distributed}} slot assignment {{protocol to}} minimize end-to-end delay for multi-hop service-MRSA (Multi-hop Relay Slot Assignment) is proposed in this study. This protocol adopts a new multi-hop relay reserve mechanism {{to establish a}} <b>virtual</b> <b>pipeline</b> connected the source and destination node before relaying the multi-hop service data packets. Modeling analysis and simulation {{results showed that the}} MRSA can still make the effective slot assignment and reduce the end-to-end delay when compared with traditional Time Division Multiple Access (TDMA) protocol even under heavy traffic load...|$|E
40|$|<b>Virtual</b> <b>Pipelining</b> allows {{designs of}} {{arbitrary}} size to ex-ecute on finite sized FPGA devices. It allows pipelined de-signs to be efficiently configured on a FPGA by overlapping the reconfiguration {{time of a}} pipeline stage with the execu-tion time of previous pipeline stages. This technique pro-duces performance improvement up to an order of 5 versus a non-pipelined execution of a design. We extend this prin-ciple for handling large designs that were previously too large to fit on an FPGA. This paper presents a framework for automatically synthesizing <b>virtual</b> <b>pipelines</b> on an Virtex FPGA. We also suggest criteria for extending our approach to non-Virtex FPGAs. 1...|$|R
50|$|ANG is {{also being}} tested for other methane storage applications, {{including}} CNG fueling station storage, <b>virtual</b> <b>pipelines,</b> and fugitive/ wellhead methane capture systems. Other significant opportunities for ANG include home heating and cooking applications in developing countries and in Southeast Asia.|$|R
40|$|To {{comply with}} natural gas demand growth {{patterns}} and Europe´s import dependency, the gas industry needs to organize an efficient upstream infrastructure. The best location of Gas Supply Units – GSUs and the alternative transportation mode – by phisical or <b>virtual</b> <b>pipelines,</b> {{are the key}} of a successful industry. In this work we study the optimal location of GSUs, as well as determining the most efficient allocation from gas loads to sources, selecting the best transportation mode, observing specific technical restrictions and minimizing system total costs. For the location of GSUs on system we use the P-median problem, for assigning gas demands nodes to source facilities we use the classical transportation problem. The developed model is an optimisation-based approach, based on a Lagrangean heuristic, using Lagrangean relaxation for P-median problems – Simple Lagrangean Heuristic. The solution of this heuristic can be improved by adding a local search procedure - the Lagrangean Reallocation Heuristic. These two heuristics, Simple Lagrangean and Lagrangean Reallocation, were tested on a realistic network - the primary Iberian natural gas network, organized with 65 nodes, connected by physical and <b>virtual</b> <b>pipelines.</b> Computational results are presented for both approaches, showing the location gas sources and allocation loads arrangement, system total costs and gas transportation mode...|$|R
40|$|A {{distributed}} Java platform {{has been}} designed and built for the simplified implementation of distributed Java applications. Its programmable nature means that code as well as data is distributed over a network. The platform is largely based on the Java Distributed Computation Library of Fritsche, Power, and Waldron. The generality of our system is demonstrated through the emulation of a MIMD (multiple instruction, multiple data) architecture. This is achieved by augmenting the server with a <b>virtual</b> <b>pipeline</b> processor. We explain {{the design of the}} system, its deployment over a university network, and its evaluation through a sample application...|$|E
40|$|The goal of {{this project}} {{is to develop a}} <b>Virtual</b> <b>Pipeline</b> System Testbed (VPST) for natural gas transmission. This study uses a fully {{implicit}} finite difference method to analyze transient, nonisothermal compressible gas flow through a gas pipeline system. The inertia term of the momentum equation is included in the analysis. The testbed simulate compressor stations, the pipe that connects these compressor stations, the supply sources, and the end-user demand markets. The compressor station is described by identifying the make, model, and number of engines, gas turbines, and compressors. System operators and engineers can analyze the impact of system changes on the dynamic deliverability of gas and on the environment...|$|E
40|$|In December 2015, {{leaders from}} Central and South Asia {{took part in}} the {{ground-breaking}} ceremony for the Turkmenistan-Afghanistan-Pakistan-India (TAPI) natural gas pipeline project. Sixteen months later, a confusing information flow continues to obfuscate external assessments of the project’s development: official rhetoric notwithstanding, there is no certainty on the details of project financing, while the pipeline route has yet to be determined. To illuminate this obscure implementation path, this article regards TAPI as a <b>virtual</b> <b>pipeline,</b> an infrastructure project that wields invaluable influence only when it is employed as a foreign policy tool or permeates domestic discourses of progress framed by the elites of the four consortium partners. The constituent elements of TAPI virtuality are discussed here through a dedicated focus on the process of energy policy-making of Turkmenistan – the sole supplier of gas for the pipeline project and the consortium’s key stakeholder...|$|E
40|$|A major {{determinant}} {{of the level}} of effective natural gas supply is the ease to feed customers, minimizing system total costs. The aim of this work is the study of the right number of Gas Supply Units – GSUs - and their optimal location in a gas network. This paper suggests a GSU location heuristic, based on Lagrangean relaxation techniques. The heuristic is tested on the Iberian natural gas network, a system modelized with 65 demand nodes, linked by physical and <b>virtual</b> <b>pipelines.</b> Lagrangean heuristic results along with the allocation of loads to gas sources are presented, using a 2015 forecast gas demand scenario...|$|R
50|$|Key CPU {{architectural}} innovations include index register, cache, <b>virtual</b> memory, instruction <b>pipelining,</b> superscalar, CISC, RISC, virtual machine, emulators, microprogram, and stack.|$|R
40|$|Efficient routing between nodes is {{the most}} {{important}} challenge in a Mobile Ad Hoc Network (MANET). A Connected Dominating Set (CDS) acts as a virtual backbone for routing in a MANET. Hence, the construction of CDS based on the need and its application plays a vital role in the applications of MANET. The PipeLined Strategic CDS (PLS-CDS) is constructed based on strategy, dynamic diameter and transmission range. The strategy used for selecting the starting node is, any source node in the network, which has its entire destination within a <b>virtual</b> <b>pipelined</b> coverage, instead of the node with maximum connectivity. The other nodes are then selected based on density and velocity. The proposed CDS also utilizes the energy of the nodes in the network in an optimized manner. Simulation results showed that the proposed algorithm is better in terms of size of the CDS and average hop per path length...|$|R
40|$|Pipeline {{morphing}} is {{a simple}} but effective technique for reconfiguring pipelined FPGA designs at run time. By overlapping computation and reconfiguration, the latency associated with emptying and refilling a pipeline can be avoided. We show how morphing {{can be applied to}} linear and mesh pipelines at both word-level and bit-level, and explain how this method can be implemented using Xilinx 6200 FPGAs. We also present an approach using morphing to map a large <b>virtual</b> <b>pipeline</b> onto a small physical pipeline, and the trade-offs involved are discussed. Introduction Pipeline architectures are commonly used in high-performance designs. This paper introduces morphing, a technique for enhancing the efficiency of reconfigurable pipelines at run time. We shall also describe the use of morphing in the emulation of large virtual pipelines by small physical pipelines, and explain how temporary storage can be used to improve performance. Implementing pipeline architectures using reconfigurable de [...] ...|$|E
40|$|Pipeline sensors generates {{significant}} amount of multivariate datasets during normal and leak situations. Therefore we have developed a data model to effectively manage such data and enhance the computational support needed for the effective data explorations. In this paper we discuss {{the development of an}} Augmented Reality (AR) - based scientific visualization system prototype that supports identification, localisation, and 3 D visualisation of oil pipeline leakages sensors datasets. A challenge of this approach is to reduce the data inefficiency powered by the disparate, repeated, inconsistent and missing attributes of most available sensors datasets. To handle this challenge, this paper aim to develop an AR-based scientific visualization interface which automatically identifies, localise and visualizes all necessary data relevant to a particularly selected region of interest (ROI) along the <b>virtual</b> <b>pipeline.</b> Necessary system architectural supports needed as well as the interface requirements for such visualizations are also discussed in this paper...|$|E
40|$|The Desire {{department}} of the province of Santa Cruz, Argentina, presents the greatest potential electrolytic Hydrogen Production Country, From Three primary sources of sustainable energy: wind, solar, biomass. There, the Hydrogen Plant of Pico Truncado has capacity central production of hydrogen 100 m 3 of H 2 / day, enough to supply 353 vehicles with hybrid fuel called HGNC, made by cutting 12 % V / V of hydrogen in CNG (in situ) at each station. Puerto Deseado, Fitz Roy, Caleta Olivia, Las Heras, Comodoro Rivadavia, Sarmiento and the Ancients: From the production cost, the cost of delivering hydrogen to the Southern Patagonian circuit comprised analyzed. Considering various local parameters are determined {{as a way of}} delivering more profitable <b>virtual</b> <b>pipeline,</b> with total cost of hydrogen estimated 6. 5 USD / kg H 2 and HGNC shipped in the station at 0. 50 USD / Nm 3...|$|E
40|$|The {{water mains}} burst in Seoul, Korea {{is one of}} the most {{frequently}} raised water management problems and the major reason can be found in the inefficiency in design and management of the network. The study developed a prototype system that allows the user to evaluate the level of deterioration and to design alternative water mains for simulation. The study suggests an evaluation scheme that models mains failure based on different variables such as pipe age, diameter, ground elevation, and pipe materials. Also, the user can interact with the system to create <b>virtual</b> <b>pipelines</b> that link the user-provided origin and destination points considering the efficiency in regional supply, distance, road network and construction cost. A shortest path algorithm called Dijkstra’s algorithm was used for the pipeline simulation. The data was constructed using the GIS based on the residential blocks which are defined by supply region, water demand and street network. The system was applied to a test region in Seoul, Korea. 1...|$|R
40|$|The {{best places}} to locate the Gas Supply Units (GSUs) on a natural gas systems and their optimal {{allocation}} to loads are the key factors to organize an efficient upstream gas infrastructure. The number of GSUs and their optimal location in a gas network is a decision problem that can be formulated as a linear programming problem. Our {{emphasis is on the}} formulation and use of a suitable location model, reflecting real-world operations and constraints of a natural gas system. This paper presents a heuristic model, based on lagrangean approach, developed for finding the optimal GSUs location on a natural gas network, minimizing expenses and maximizing throughput and security of supply. The location model is applied to the Iberian high pressure natural gas network, a system modelised with 65 demand nodes. These nodes are linked by physical and <b>virtual</b> <b>pipelines</b> – road trucks with gas in liquefied form. The location model result shows the {{best places to}} locate, with the optimal demand allocation and the most economical gas transport mode: by pipeline or by road truck...|$|R
40|$|One of {{the major}} {{challenges}} of in-silico <b>virtual</b> screening <b>pipelines</b> is dealing with increasing complexity of large scale data management along with efficiently fulfilling the high throughput computing demands. Despite the new workflow tools and technologies available {{in the area of}} computational chemistry, efforts in effective data management and efficient post-processing strategies are still ongoing. SCAI-VHTS fully automates virtual screening tasks on distributed computing resources to achieve maximum efficiency and to reduce the complexities involved in pre- and post-processing of large volumes of virtual screening data...|$|R
40|$|Abstract—As network {{bandwidth}} increases, designing an effective memory system for network processors becomes a significant challenge. The {{size of the}} routing tables, {{the complexity of the}} packet classification rules, and the amount of packet buffering required all continue to grow at a staggering rate. Simply relying on large, fast SRAMs alone {{is not likely to be}} scalable or cost-effective. Instead, trends point to the use of low-cost commodity DRAM devices as a means to deliver the worst-case memory performance that network data-plane algorithms demand. While DRAMs can deliver a great deal of throughput, the problem is that memory banking significantly complicates the worst-case analysis, and specialized algorithms are needed to ensure that specific types of access patterns are conflict-free. We introduce virtually pipelined memory, an architectural technique that efficiently supports high bandwidth, uniform latency memory accesses, and high-confidence throughput even under adversarial conditions. Virtual pipelining provides a simple-to-analyze programming model of a deep pipeline (deterministic latencies) with a completely different physical implementation (a memory system with banks and probabilistic mapping). This allows designers to effectively decouple the analysis of their algorithms and data structures from the analysis of the memory buses and banks. Unlike specialized hardware customized for a specific data-plane algorithm, our system makes no assumption about the memory access patterns. We present a mathematical argument for our system’s ability to provably provide bandwidth with high confidence and demonstrate its functionality and area overhead through a synthesizable design. We further show that, even though our scheme is general purpose to support new applications such as packet reassembly, it outperforms the state-of-the-art in specialized packet buffering architectures. Index Terms—Bank conflicts, DRAM, mean time to stall, memory, memory controller, MTS, network, packet buffering, packet reassembly, universal hashing, <b>virtual</b> <b>pipeline,</b> VPNM. I...|$|E
40|$|Today, {{business}} environment is turbulent and volatile, how effective {{the company can}} build up the partnership with their business partners {{is the key to}} be able to compete in the national and global market. The merits of lean and agile supply chain strategies have been much debated among practitioners and academics. While these strategies are often viewed as opposites, this research supports the view that they must not necessarily compete and can, in fact, be employed simultaneously through a so called "leagile" approach. Lean, agile, and leagile strategies are illustrated by modeling their respective applications at supermarket retail industry. Even though the emergence of agile paradigm had spurred a large stream of research by scholars, yet most of the research had been at the manufacturing level. Very few researches have gone beyond the manufacturing level to the larger supply chain level. And there are even fewer researches discussing about the combination of lean thinking and agile thinking in supply chain especially apply in supermarket retail industry. Based on the above statement, {{the purpose of this study}} is as follows: 1) To evaluate the definition and characteristics of Lean and Agile in supermarket retail industry. 2) To compare the difference of lean thinking and agile thinking in supermarket retail industry. 3) To explore the relationship of the lean thinking and the agile thinking in supermarket retail industry. 4) To prove the practicability of the findings in supermarket retail industry. Finally author gains some findings, list as below: Under virtual manufacturing factory model, 1. The floating decoupling point will be one of the key strategic variables in the future supermarket retail industry. 2. Of the virtual decoupling point pipeline schemes, league is the most straightforward to apply. 3. Lean thinking can be used at upstream with smoothed demand to minimize the stock to minimize waste, but at downstream, there is high fluctuation, agile thinking can be adopted to meet the demand of the market responsively and flexibly. 4. In the <b>virtual</b> <b>pipeline,</b> the closer to the end, the better adopt agile approach. Under virtual production line in warehouse model, the Pareto Principle approach can be adopted as a mixed model in supermarket warehouse environments...|$|E
40|$|What is a pipeline? For the {{purposes}} of this paper, a pipeline is a metaphor for a series of tasks or operations that run in sequence. In their most abstract form, pipelines are made up of pipes. These pipes can be tasks, processes, actions, etc. Each pipe has an input, a middle, and an output. The middle definition really depends on {{the context in which the}} pipeline exists. Figure 1. What is a pipe and what is a pipeline? For the moment, pretend the pipe is a real water pipe. This pipe might simply let water flow from one side to the other. Alternatively, the pipe could have a large funnel system at the input that actually collects fallen rain water, and on the output side there could be a bucket. Rain water would roll down the pipe and drip out of the other end into the bucket for collection. We would then define this pipe with a name and possibly a description; the name could be rain water pipe and our description a pipe that collects rainwater and drops it in a bucket. This is the most basic pipe [...] something with an input and an output that has something flowing through it. The middle always exists, but may not have a function. 1 Figure 2. Rain water pipe We could add a heated middle section to this rain water pipe, so as the rainwater rolls down the pipe it is gently heated. The addition of this feature makes it a different pipe, so we should rename it and change its description. With multiple pipes, it is possible to put them together in a preferred sequence to create new combinations. A series or sequence of pipes should also be named and described. Small pipes are made into small pipelines, which in turn can become part of larger and more complex pipelines. There are a few rules that make pipelines work: • Pipes must have an input and an output • Pipes can not loop on to themselves • Pipes should always be as simple as possible • The middle of a pipe can have multiple parts if necessary • Pipes can have conditions • The input, middle and output of a pipe can have parameters So what has all this got to do with the future of the web? If we change the context of our pipeline to a <b>virtual</b> <b>pipeline</b> (bearing in mind a pipeline is simply a sequence of tasks) we end up with a easily definable tasks that work like building blocks. Where did pipelines come from...|$|E
40|$|We {{introduce}} virtually-pipelined memory, {{an architectural}} technique that efficiently supports high-bandwidth, uniform latency memory accesses, and high-confidence throughput even under adversarial conditions. We apply this technique {{to the network}} processing domain where memory hierarchy design is an increasingly challenging problem as network bandwidth increases. <b>Virtual</b> <b>pipelining</b> provides a simple to analyze programing model of a deep pipeline (deterministic latencies) with a completely different physical implementation (a memory system with banks and probabilistic mapping). This allows designers to effectively decouple {{the analysis of the}}ir algorithms and data structures from the analysis of the memory buses and banks. Unlike specialized hardware customized for a specific data-plane algorithm, our system makes no assumption about the memory access patterns. In the domain of network processors this will be of growing importance as the size of the routing tables, the complexity of the packet classification rules, and the amount of packet buffering required, all continue to grow at a staggering rate. We present a mathematical argument for our system’s ability to provably provide bandwidth with high confidence and demonstrate its functionality and area overhead through a synthesizable design. We further show that, even though our scheme is general purpose to support new applications such as packet reassembly, it outperforms {{the state of the art}} in specialized packet buffering architectures. ...|$|R
40|$|The {{reliability}} of future processors {{is threatened by}} decreasing transistor robustness. Current architectures focus on delivering high performance at low cost; lifetime device reliability is a secondary concern. As the rate of permanent hardware faults increases, robustness will become a first class constraint for even low-cost systems. Current research into reliable architectures has focused on ad-hoc solutions to improve designs without altering their centralized control logic. Unfortunately, this centralized control presents a {{single point of failure}}, which limits long-term robustness. To address this issue, we introduce Viper, an architecture built from a redundant collection of fine-grained hardware components. Instructions are perceived as customers that require a sequence of services in order to properly execute. The hardware components vie to perform what services they can, dynamically forming <b>virtual</b> <b>pipelines</b> that avoid defective hardware. This is done using distributed control logic, which avoids a single point of failure by construction. Viper can tolerate a high number of permanent faults due to its inherent redundancy. As fault counts increase, its performance degrades more gracefully than traditional centralized-logic architectures. We estimate that fault rates higher than one permanent faults per 12 million transistors, on average, cause the throughput of a classic CMP design to fall below that of a Viper design of similar size. ...|$|R
40|$|Abstract. With the {{development}} of city, the scale of underground pipeline is always extending and {{to solve the problem}} of <b>virtual</b> underground <b>pipeline</b> generation is extremely urgent. In this paper, we will use the triangular slice to establish three-dimensional MultiPatch models. The three-dimensional vector will be used in this new modeling method of 3 D visualization of underground pipeline. The method can automatically deal with elbows or straight pipe modeling problems. And we find it suitable for rapid modeling and wide range of applications through our series of test...|$|R
40|$|This paper {{describes}} a <b>virtual</b> restoration <b>pipeline</b> for {{the broken glass}} plates in the fund of the Department of Archaeological, Philological and Historical Studies of Catania University. This archive includes rare images of the excavation activities of the Department in Sicily and Greece. Several of the plates are damaged and fractured and require virtual realignment and restoration. We adapted to this case a restoration algorithm based {{on the use of}} image processing techniques. The algorithm enhance the quality of scanned images, and subsequently performs a rigid registration and realignment of the fragments. A final application of inpainting techniques fills the gaps...|$|R
40|$|In {{order to}} reduce danger and cost in {{physical}} chemical process training and testing, this paper designed a distributed <b>virtual</b> reality-based <b>pipeline</b> simulation system which has abilities of chemical process training, monitoring, testing and replaying. After proposing a data-driven simulation framework, this paper presented a virtual reality modeling method for pipeline simulation and a process path calculation method. Then a virtual prototypes pick-up method for device operation and related-information display was further analyzed. With Bernoulli's equation, a mathematical model for constant flowing and instant flowing of fluid in pipeline system are constructed to estimate flowing speed, flux and pressure in real time. Using aforementioned methods, a pipeline simulation system was developed and it was proven to be helpful for chemical process training, design and optimization by practical use...|$|R
40|$|Virtual screening, {{the search}} for {{bioactive}} compounds via computational methods, provides {{a wide range of}} opportunities to speed up drug development and reduce the associated risks and costs. While virtual screening is already a standard practice in pharmaceutical companies, its applications in preclinical academic research still remain under-exploited, in spite of an increasing availability of dedicated free databases and software tools. In this survey, an overview of recent developments in this field is presented, focusing on free software and data repositories for screening as alternatives to their commercial counterparts, and outlining how available resources can be interlinked into a comprehensive <b>virtual</b> screening <b>pipeline</b> using typical academic computing facilities. Finally, to facilitate the set-up of corresponding pipelines, a downloadable software system is provided, using platform virtualization to integrate pre-installed screening tools and scripts for reproducible application across different operating systems...|$|R
40|$|Abstract—In {{order to}} reduce danger and cost in {{physical}} chemical process training and testing, this paper designed a distributed <b>virtual</b> reality-based <b>pipeline</b> simulation system which has abilities of chemical process training, monitoring, testing and replaying. After proposing a data-driven simulation framework, this paper presented a virtual reality modeling method for pipeline simulation and a process path calculation method. Then a virtual prototypes pick-up method for device operation and related-information display was further analyzed. With Bernoulli's equation, a mathematical model for constant flowing and instant flowing of fluid in pipeline system are constructed to estimate flowing speed, flux and pressure in real time. Using aforementioned methods, a pipeline simulation system was developed and it was proven to be helpful for chemical process training, design and optimization by practical use. Index Terms—Virtual reality, pipeline simulation, fluid model I...|$|R
40|$|In {{the last}} three decades, HEP {{experiments}} have faced the challenge of manipulating larger and larger masses of data from increasingly complex, heterogeneous detectors with millions and then {{tens of millions of}} electronic channels. LHC experiments abandoned the monolithic architectures of the nineties in favor of a distributed approach, leveraging the appearence of high speed switched networks developed for digital telecommunication and the internet, and the corresponding increase of memory bandwidth available in off-the-shelf consumer equipment. This led to a generation of experiments where custom electronics triggers, analysing coarser-granularity “fast” data, are confined to the first phase of selection, where predictable latency and real time processing for a modest initial rate reduction are “a necessary evil”. Ever more sophisticated algorithms are projected for use in HL- LHC upgrades, using tracker data in the low-level selection in high multiplicity environments, and requiring extremely complex data interconnects. These systems are quickly obsolete and inflexible but must nonetheless survive and be maintained across the extremely long life span of current detectors. New high-bandwidth bidirectional links could make high-speed low-power full readout at the crossing rate a possibility already in the next decade. At the same time, massively parallel and distributed analysis of unstructured data produced by loosely connected, “intelligent” sources has become ubiquitous in commercial applications, while the mass of persistent data produced by e. g. the LHC experiments has made multiple pass, systematic, end-to-end offline processing increasingly burdensome. A possible evolution of DAQ and trigger architectures could lead to detectors with extremely deep asynchronous or even <b>virtual</b> <b>pipelines,</b> where data streams from the various detector channels are analysed and indexed in situ quasi-real-time using intelligent, pattern-driven data organization, and the final selection is operated as a distributed “search for interesting event parts”. A holistic approach is required to study the potential impact of these different developments on the design of detector readout, trigger and data acquisition systems in the next decades...|$|R
40|$|This paper {{discusses}} {{the effectiveness of}} various speculative techniques to minimize the latency of <b>virtual</b> channel router <b>pipelines.</b> The proposals are compared {{to each other as}} well as to a 4 -stage (20 FO 4) canonical example router. One technique reduced the router to a single stage (35 FO 4), but was not flexible enough to accommodate adaptive routing algorithms. Another implementation uses two pipeline cycles (FO 4 not reported) but works with both adaptive and oblivious routing algorithms. In both cases substantial latency gains are observed with the use of speculation. 1...|$|R
40|$|WebCiao is {{a system}} for {{visualizing}} and tracking the structures of websites by creating, differencing, and analyzing archived website databases. The architecture of WebCiao allows users to create customized website analysis tools by combining a set of query and analysis operators on a <b>virtual</b> database <b>pipeline.</b> Each <b>virtual</b> database sent on the pipe {{can be converted to}} directed graphs, database views, or HTML reports. Within a graph view, operators can be fired from any graph node to study a selected neighborhood. WebCiao helps creators of large websites to monitor the dynamics of structural changes closely. It also helps web surfers to quickly identify new products and services from a website. An on-line demo, Website News, based on the WebCiao technology, has helped sharpen our focus with its daily analysis of new web contents from the internet and telecommunications industries. 1. Introduction The complexity and ever-changing nature of major websites are presenting problems to both [...] ...|$|R
40|$|Traditionally, {{automatic}} classification and metadata extraction {{have been performed}} in isolation, usually on unformatted text. SCORE Enhancement Engine (SEE) is a component of a Semantic Web technology called the Semantic Content Organization and Retrieval Engine (SCORE). SEE takes the next natural steps by supporting heterogeneous content (not only unformatted text), as well as following up {{automatic classification}} with extraction of contextually relevant, domain-specific (i. e., semantic) metadata. Extraction of semantic metadata not only includes identification of relevant entities but also relationships {{within the context of}} relevant ontology. This paper describes SEE 2 ̆ 7 s architecture, which provides a common API for heterogeneous document processing, with discrete, reusable and highly configurable modular components. This results in exceptional flexibility, extensibility and performance. Referred to as SEE modules (SEEMs), which are divided along functional lines, these processors perform one of the following roles: restriction (determine the segments of the input text to operate upon); enhancement (discover textual features of semantic interest); filtering (augment, remove or supplement the features recognized); or outputting (generate reports, annotate the original, update databases, or other actions). Each SEEM manages its configuration options and is arranged serially in <b>virtual</b> <b>pipelines</b> to perform designated semantic tasks. These configurations can be saved and reloaded on a per-document basis. This allows a single SEE installation to act logically as any number of Semantic Applications, and to compose these Semantic Applications as needed to perform even more complex semantic tasks. SEE leverages SCORE 2 ̆ 7 s unique approach of creating and using large knowledge base in semantic processing. It enables SCORE to provide flexible handling of highly heterogeneous content (including raw text, HTML, XML and documents of various formats); reliable automatic classification of documents; accurate extraction of semantic, domain-specific metadata; and extensive management of the enhancement processes including various reporting and semantic annotation mechanisms. This results in SCORE 2 ̆ 7 s advanced capability in heterogeneous content integration at a higher semantic level, rather than syntactical and structural level approaches based on XML and RDF, by supporting and exploiting domain specific ontologies. This work also presents an approach to automatic semantic annotation, a key scalability challenge faced in realizing the Semantic Web...|$|R
40|$|The feasibility, cost, and {{air quality}} impacts of using {{electrical}} grids to shift water use from drought-stricken regions to areas with more water availability were examined. Power plant cooling represents {{a large portion}} of freshwater withdrawals in the United States, and shifting where electricity generation occurs can allow the grid to act as a <b>virtual</b> water <b>pipeline,</b> increasing water availability in regions with drought by reducing water consumption and withdrawals for power generation. During a 2006 drought, shifting electricity generation out of the most impacted areas of South Texas (~ 10 % of base case generation) {{to other parts of the}} grid would have been feasible using transmission and power generation available at the time, and some areas would experience changes in air quality. Although expensive, drought-based electricity dispatch is a potential parallel strategy that can be faster to implement than other infrastructure changes, such as air cooling or water pipelines. National Science Foundation (U. S.). Office of Emerging Frontiers in Research and Innovation (Grant 0835414) United States. Dept. of Energ...|$|R
40|$|In today’s SOC, {{the number}} of {{processing}} cores is increasing with growth of VLSI technology. The on chip communication among multiple cores using NOC based architecture is effective than conventional bus based architecture, Since NOC has many advantages than bus based architecture mainly in terms of scalability (increase in number of nodes) and flexibility. Hence in this paper, Mesh based NOC architecture with packet switching is adopted for cryptography without <b>virtual</b> channel and <b>pipelining</b> techniques. The deterministic X-Y routing algorithm is used for routing a packet within the NOC. This paper presents a 10 - 20 % less area consumption NOC with AES as processing element, over previous work...|$|R
40|$|To {{improve the}} {{identification}} sensitivity of local damages in pipelines, we propose an added virtual mass method thatprevents adding real masses to the pipeline. First, we develop {{a method of}} adding <b>virtual</b> masses to <b>pipelines</b> based on the virtual distortion method (VDM). Second, a frequency response to the added mass is constructed using the excitation and acceleration responses. The quantity of mass and the corresponding selected natural frequency with high sensitivity are both determined by the analyzing {{the sensitivity of the}} relationship between mass and natural frequency. Finally, the degree of damage can be accurately identified by adding virtual masses on the substructure of the pipeline combined with sensitivity and frequency. Using numerical simulations and experiments, we verify the feasibility of the added virtual mass method for the identification of damages to pipeline structures...|$|R
40|$|Using {{molecular}} {{similarity to}} discover bioactive small molecules with novel chemical scaffolds can be computationally demanding. We describe Ultra-fast Shape Recognition with Atom Types (UFSRAT), an efficient algorithm that considers both the 3 D distribution (shape) and electrostatics of atoms to score and retrieve molecules {{capable of making}} similar interactions {{to those of the}} supplied query. Computational optimization and pre-calculation of molecular descriptors enables a query molecule to be run against a database containing 3. 8 million molecules and results returned in under 10 seconds on modest hardware. UFSRAT has been used in pipelines to identify bioactive molecules for two clinically relevant drug targets; FK 506 -Binding Protein 12 and 11 β-hydroxysteroid dehydrogenase type 1. In the case of FK 506 -Binding Protein 12, UFSRAT was used as {{the first step in a}} structure-based <b>virtual</b> screening <b>pipeline,</b> yielding many actives, of which the most active shows a KD, app of 281 µM and contains a substructure present in the query compound. Success was also achieved running solely the UFSRAT technique to identify new actives for 11 β-hydroxysteroid dehydrogenase type 1, for which the most active displays an IC 50 of 67 nM in a cell based assay and contains a substructure radically different to the query. This demonstrates the valuable ability of the UFSRAT algorithm to perform scaffold hops. A web-based implementation of the algorithm is freely available at [URL]...|$|R
