7|40|Public
40|$|Iterative {{alignment}} is {{one method}} for feature-based matching {{of an image}} and {{a model for the}} purpose of object recognition. The method alternately hypothesizes feature pairings and estimates a <b>viewpoint</b> <b>transformation</b> from those pairings; at each stage a refined transformation estimate is used to suggest additional pairings. This paper extends iterative alignment in the domain of 2 D similarity transformations so that it represents the uncertainty in the position of each model and image feature, and that of the transformation estimate. A model describes probabilistically the significance, position, and intrinsic attributes of each feature, plus topological relations among features. A measure of the match between a model and an image integrates all four of these, and leads to an efficient matching procedure called probabilistic alignment. That procedure supports both recognition and a learning procedure for acquiring models from training images. By explicitly representing uncertainty, on [...] ...|$|E
40|$|We {{describe}} how {{to model the}} appearance of a 3 -D object using multiple views, learn such a model from training images, and use the model for object recognition. The model uses probability distributions to describe the range of possible variation in the object's appearance. These distributions are organized on two levels. Large variations are handled by partitioning training images into clusters corresponding to distinctly different views of the object. Within each cluster, smaller variations are represented by distributions characterizing uncertainty in the presence, position, and measurements of various discrete features of appearance. Many types of features are used, ranging in abstraction from edge segments to perceptual groupings and regions. A matching procedure uses the feature uncertainty information to guide the search for a match between model and image. Hypothesized feature pairings are used to estimate a <b>viewpoint</b> <b>transformation</b> taking account of feature uncertai [...] ...|$|E
40|$|Abstract. In {{this paper}} we {{introduce}} a new problem which we call ob-ject co-detection. Given a set of images with objects observed from two or multiple images, the goal of co-detection is to detect the objects, es-tablish the identity of individual object instance, as well as estimate the <b>viewpoint</b> <b>transformation</b> of corresponding object instances. In designing a co-detector, we follow the intuition that an object has consistent ap-pearance when observed from the same or different viewpoints. By mod-eling an object using state-of-the-art part-based representations such as [1, 2], we measure appearance consistency between objects by comparing part appearance and geometry across images. This allows to effectively account for object self-occlusions and viewpoint transformations. Exten-sive experimental evaluation indicates that our co-detector obtains more accurate detection results than if objects were to be detected from each image individually. Moreover, we demonstrate the relevance of our co-detection scheme to other recognition problems such as single instanc...|$|E
40|$|We {{propose a}} {{framework}} for fast view alignment using adaptive affine tracking. We {{address the issue of}} modelling both shape and texture information in eigenspace for view alignment. We present an effective bootstrapping process based on colour segmentation and selective attention. We recover affine parameters with dynamic updates to the eigenspace using most recent history and perform predictions in parameter space. Experimental results are given to illustrate our approach. 1 Introduction A view-based representation assumes that a piecewise linear vector space exists in which each view is represented by a vector [1]. For object recognition in dynamic scenes using view-based representation, frame to frame view alignment is essential. This requires establishing image correspondences in successive frames of a moving object which may undergo both affine and <b>viewpoint</b> <b>transformations</b> [2]. However, to obtain consistent dense image correspondence is both problematic and expensive since chang [...] ...|$|R
40|$|Abstract. In this work, {{we focus}} on the problem of {{tracking}} objects un-der significant viewpoint variations, which poses a big challenge to tradi-tional object tracking methods. We propose a novel method to track an object and estimate its continuous pose and part locations under severe viewpoint change. In order to handle the change in topological appear-ance introduced by <b>viewpoint</b> <b>transformations,</b> we represent objects with 3 D aspect parts and model the relationship between viewpoint and 3 D aspect parts in a part-based particle filtering framework. Moreover, we show that instance-level online-learned part appearance can be incorpo-rated into our model, which makes it more robust in difficult scenarios with occlusions. Experiments are conducted on a new dataset of chal-lenging YouTube videos and a subset of the KITTI dataset [14] that include significant viewpoint variations, as well as a standard sequence for car tracking. We demonstrate that our method is able to track the 3 D aspect parts and the viewpoint of objects accurately despite significant changes in viewpoint...|$|R
40|$|We {{propose a}} novel {{probabilistic}} framework for learning visual models of 3 D object categories by combining appear-ance information and geometric constraints. Objects are {{represented as a}} coherent ensemble of parts that are con-sistent under 3 D <b>viewpoint</b> <b>transformations.</b> Each part {{is a collection of}} salient image features. A generative frame-work is used for learning a model that captures the relative position of parts within each of the discretized viewpoints. Contrary to most of the existing mixture of viewpoints mod-els, our model establishes explicit correspondences of parts across different viewpoints of the object class. Given a new image, detection and classification are achieved by deter-mining the position and viewpoint of the model that maxi-mize recognition scores of the candidate objects. Our ap-proach is among the first to propose a generative proba-bilistic framework for 3 D object categorization. We test our algorithm on the detection task and the viewpoint classifi-cation task by using “car ” category from both the Savarese et al. 2007 and PASCAL VOC 2006 datasets. We show promising results in both the detection and viewpoint clas-sification tasks on these two challenging datasets. 1...|$|R
30|$|Figure  4 a {{shows the}} {{registration}} results for infrared and visible image with the proposed method under {{the condition of}} the same scale and viewpoint. The images in Fig.  4 b have certain <b>viewpoint</b> <b>transformation.</b> Due to the feature point positioning of salient region center based on entropy priority strategy, the information of SIFT feature descriptor is dense in neighborhood center region for feature point; however, the information description is sparse in salient region center’s peripheral region, so the proposed algorithm is robust to the viewpoint transform in some way. In Fig.  4 c, there is 20 °of rotation between registration images. The structural feature descriptors possess with rotation invariance and most feature points can achieve the correct matching because of adding the main direction when constructing descriptors and selecting feature point’s neighborhood according to the main direction. Figure  4 d shows the experimental results for scale invariance. It can be seen that the proposed algorithm has some robust to scale transformation. Figure  4 e, f respectively shows the registration results for infrared and visible light image during day and night. It can be seen that this algorithm also has certain robustness for illumination changes.|$|E
40|$|Abstract—We {{propose a}} general {{architecture}} for action (mimicking) and program (gesture) level visual imitation. Action-level imitation involves two modules. The <b>viewpoint</b> <b>Transformation</b> (VPT) performs a “rotation ” to align the demonstrator’s body {{to that of}} the learner. The Visuo-Motor Map (VMM) maps this visual information to motor data. For program-level (gesture) imitation, there is an additional module that allows the system to recognize and generate its own interpretation of observed gestures to produce similar gestures/goals at a later stage. Besides the holistic approach to the problem, our approach differs from traditional work in i) the use of motor information for gesture recognition; ii) usage of context (e. g., object affordances) to focus the attention of the recognition system and reduce ambiguities, and iii) use iconic image representations for the hand, as opposed to fitting kinematic models to the video sequence. This approach is motivated by the finding of visuomotor neurons in the F 5 area of the macaque brain that suggest that gesture recognition/imitation is performed in motor terms (mirror) and rely on the use of object affordances (canonical) to handle ambiguous actions. Our results show that this approach can outperform more conventional (e. g., pure visual) methods. Index Terms—Anthropomorphic robots, imitation, learning, visuomotor coordination...|$|E
40|$|The aim of {{the current}} study was to address the long-standing {{question}} of whether three- dimensional information plays a role in face recognition, as well as to test the hypothesis that the advantage of the intermediate view derives from better structural information about the face provided by this view. We examined these issues by comparing recognition performance across a change in viewpoint with or without stereo information. Subjects' eye-movements were also recorded during both 2 D and 3 D sessions. Our data analysis supported conclusions from the previous studies that a 22. 5 ° intermediate view leads to better face recognition performance compared to the full frontal view. The results revealed that participants were more accurate in the 3 D condition, while ‘region of interest’ analysis of gaze data showed that rich volumetric properties provided by certain facial features were attended more in the 3 D condition compared to the 2 D condition. Taken together, these two findings support the conclusion that face recognition across <b>viewpoint</b> <b>transformation</b> is facilitated with the addition of stereoscopic depth cues. Contrary to our predictions, we did not find a statistically significant interaction effect between depth information and angle of view of the face. However, results of a follow-up statistical analysis suggest the effect could become significant with the addition of more data...|$|E
40|$|One of {{the strongest}} cues for {{retrieval}} of content information from images is shape. However, due to {{the wide range of}} transformations that an object might undergo, this is also the most difficult one to handle. It seems that shape retrieval {{is one of the major}} barriers nowadays to image databases being commonly used. We present an approach for shape retrieval from pictorial databases which is based on invariant features of the image. In particular we use a combination of semi-local multivalued invariant signatures and global features. Spatial relations and global properties are used to eliminate nonrelevant images before similarity is computed. The advantages of the proposed approach are its ability to handle images distorted by different <b>viewpoint</b> <b>transformations,</b> its ability to retrieve images even in situations in which part of the shape is missing (i. e., in case of occlusion or sketch-based queries), and its ability to support efficient indexing. We have implemented our approach in a heterogeneous database having a SQL-like user interface augmented with sketch-based queries. The system is built on top of a commercial database system and can be activated from the Web. We present experimental results demonstrating the effectiveness of the proposed approach...|$|R
40|$|Abstract: This work {{focuses on}} the {{automatic}} monitoring of the natural environment using images of animals. An unsupervised 3 D colour texture segmentation and recognition algorithm is developed and applied in the recognition of natural patterns (monkeys) in outdoor image scenes (Amazon Forest). We present an unsupervised segmentation algorithm, which uses 3 D Graph-theoretical clustering for colour images. This approach is based on image chromaticity and intensity. Subsequently each cluster of the segmented image is represented by six colour texture angle indices [1] [2] and the Frobenius norm is applied to measure the distance between candidate and prototype colour images. The robustness of this algorithm to <b>viewpoint</b> <b>transformations</b> (rigid, scaling, perspective [...] .), change of illumination (intensity, spectral power distribution [...] .), non-rigid change of shape and background and the suitability to operate in a co-operative integrated system are considered very important characteristics. We demonstrate the segmentation approach using colour images of natural scenes from the Amazon Forest and we provide experimental results that compare the recognition algorithm with competing methods using colour invariants. The effect of the segmentation {{on the performance of}} the recognition algorithm is also considered...|$|R
40|$|Integrability {{criterion}} for the Egorov hydrodynamic type systems is presented. The general solution by the generalized hodograph method is found. Examples are given. A description of three orthogonal curvilinear coordinate nets is discussed from the <b>viewpoint</b> of reciprocal <b>transformations.</b> In honour of Sergey Tsare...|$|R
40|$|Object {{recognition}} from images is a longstanding and challenging problem in computer vision. The main challenge {{is that the}} appearance of objects in images is affected {{by a number of}} factors, such as illumination, scale, camera viewpoint, intra-class variability, occlusion, truncation, and so on. How to handle all these factors in object recognition is still an open problem. In this dissertation, I present my efforts in building 3 D object representations for object recognition. Compared to 2 D appearance based object representations, 3 D object representations can capture the 3 D nature of objects and better handle viewpoint variation, occlusion and truncation in object recognition. I introduce three new 3 D object representations: the 3 D aspect part representation, the 3 D aspectlet representation and the 3 D voxel pattern representation. These representations are built to handle different challenging factors in object recognition. The 3 D aspect part representation is able to capture the appearance change of object categories due to <b>viewpoint</b> <b>transformation.</b> The 3 D aspectlet representation and the 3 D voxel pattern representation are designed to handle occlusions between objects in addition to viewpoint change. Based on these representations, we propose new object recognition methods and conduct experiments on benchmark datasets to verify the advantages of our methods. Furthermore, we introduce, PASCAL 3 D+, a new large scale dataset for 3 D object recognition by aligning objects in images with 3 D CAD models. We also propose two novel methods to tackle object co-detection and multiview object tracking using our 3 D aspect part representation, and a novel Convolutional Neural Network-based approach for object detection using our 3 D voxel pattern representation. In order to track multiple objects in videos, we introduce a new online multi-object tracking framework based on Markov Decision Processes. Lastly, I conclude the dissertation and discuss future steps for 3 D object recognition...|$|E
40|$|Many {{visualizations}} use smoothly animated {{transitions to}} help the user interact with information structures. These transitions are intended to preserve perceptual constancy during <b>viewpoint</b> <b>transformations.</b> However, animated transitions also have costs – they increase the transition time, {{and they can be}} complicated to implement – and {{it is not clear whether}} the benefits of smooth transitions outweigh the costs. In order to quantify these benefits, we carried out two experiments that explore the effects of smooth transitions. In the first study, subjects were asked to determine whether graph nodes were connected, and navigated the graph either with or without smooth scene transitions. In the second study, participants were asked to identify the overall structure of a tree after navigating the tree through a viewport that either did or did not use smooth transitions for view changes. The results of both experiments show that smooth transitions can have dramatic benefits for user performance – for example, participants in smooth transition conditions made half the errors of the discretemovement conditions. In addition, short transitions were found to be as effective as long ones, suggesting that some of the costs of animations can be avoided. These studies give empirical evidence on the benefits of smooth transitions, and provide guidelines about when designers should use them in visualization systems...|$|R
40|$|AbstractEarlier {{studies showed}} that the {{disparity}} with respect to other visible points could not explain stereoacuity performance, nor could various spatial derivatives of disparity [Glennerster, A., McKee, S. P., & Birch, M. D. (2002). Evidence of surface-based processing of binocular disparity. Current Biology, 12 : 825 – 828; Petrov, Y., & Glennerster, A. (2004). The role of the local reference in stereoscopic detection of depth relief. Vision Research, 44 : 367 – 376. ] Two possible cues remain: (i) local changes in disparity gradient or (ii) disparity with respect to an interpolated line drawn through the reference points. Here, we aimed to distinguish between these two cues. Subjects judged, in a two AFC paradigm, whether a target dot {{was in front of}} a plane defined by three reference dots or, in other experiments, in front of a line defined by two reference dots. We tested different slants of the reference line or plane and different locations of the target relative to the reference points. For slanted reference lines or plane, stereoacuity changed little as the target position was varied. For judgments relative to a frontoparallel reference line, stereoacuity did vary with target position, but less than would be predicted by disparity gradient change. This provides evidence that disparity with respect to the reference plane is an important cue. We discuss the potential advantages of this measure in generating a representation of surface relief that is invariant to <b>viewpoint</b> <b>transformations...</b>|$|R
40|$|Earlier {{studies showed}} that the {{disparity}} with respect to other visible points could not explain stereoacuity performance, nor could various spatial derivatives of disparity [Glennerster, A., McKee, S. P., & Birch, M. D. (2002). Evidence of surface-based processing of binocular disparity. Current Biology, 12 : 825 - 828; Petrov, Y., & Glennerster, A. (2004). The role of the local reference in stereoscopic detection of depth relief. Vision Research, 44 : 367 - 376. ] Two possible cues remain: (i) local changes in disparity gradient or (ii) disparity with respect to an interpolated line drawn through the reference points. Here, we aimed to distinguish between these two cues. Subjects judged [...] in a two AFC paradigm, whether a target dot {{was in front of}} a plane defined by three reference dots or, in other experiments, in front of a line defined by two reference dots. We tested different slants of the reference line or plane and different locations of the target relative to the reference points. For slanted reference lines or plane, stereoacuity changed little as the target position was varied. For judgments relative to a frontoparallel reference line, stereoacuity did vary with target position, but less than would be predicted by disparity gradient change. This provides evidence that disparity with respect to the reference plane is an important cue. We discuss the potential advantages of this measure in generating a representation of surface relief that is invariant to <b>viewpoint</b> <b>transformations.</b> (c) 2006 Elsevier Ltd. All rights reserved...|$|R
5000|$|In physics, general {{covariant}} transformations are symmetries of [...] gravitation {{theory on}} a world manifold [...] They are gauge transformations whose parameter functions are vector fields on [...] From the physical <b>viewpoint,</b> general covariant <b>transformations</b> are treated as particular (holonomic) reference frame transformations in general relativity. In mathematics, general covariant transformations are defined as particular automorphisms of so-called natural fiber bundles.|$|R
5000|$|In 1986, {{he started}} to paint landscapes from {{literature}} like the [...] "Magic Mountain (after Thomas Mann)" [...] and portraits of writers and philosophers as William S. Burroughs, Virginia Woolf, Ludwig Wittgenstein, and more. His large scale triptych [...] "Frost" [...] is [...] "a material image in harsh black and white which depicts a literary landscape of snow and ice in different <b>viewpoints</b> ... a picturesque <b>transformation</b> of Thomas Bernhards 1963 novel".|$|R
40|$|Two {{important}} trends can {{be identified}} in parallel computing. First of all, the scale of parallel computing platforms is rapidly increasing. Secondly, the complexity and variety of current software systems requires to consider the parallelization of application modules beyond algorithms. These two trends {{have led to a}} complexity that is not scalable and tractable anymore for manual processing, and therefore automated support is required to design and implement parallel applications. In this context, we present a model-driven transformation chain for supporting the automation of the lifecycle of parallel computing applications. The model-driven transformation chain adopts metamodels that are derived from architectural <b>viewpoints.</b> The <b>transformation</b> chain is defined as a logical sequence consisting of model-to-model transformations. We present the tool support that implements the metamodels and transformations...|$|R
40|$|International audienceThis paper first {{underlines the}} main advantages, use and {{limitations}} of the electron backscatter diffraction technique from the <b>viewpoint</b> of phase <b>transformations.</b> To get {{a deeper understanding of}} physical mechanisms involved in phase transformations, several evolutions are now in progress to get an insight into both three-dimensional and real-time information. Two of them, in particular, improvement of data collection versus improvement of data processing are discussed {{in the second part of}} this paper...|$|R
30|$|The vector {{description}} {{dimension of}} each SIFT feature is 128. The SIFT feature descriptor {{is a local}} feature of image which maintains invariance to rotation transformation, scale zoom, and brightness change. It remains stable to <b>viewpoint</b> change, affine <b>transformation</b> and noise to some extent. However, {{the size of the}} SIFT feature descriptor is too large, so the extraction of SIFT feature is time consuming. Besides, there exists redundant among each dimension information for SIFT feature, and the correctness of registration is decreased if the redundancy is not eliminated.|$|R
5000|$|They {{collaborated with}} {{choreographer}} and dancer Carolyn Carlson, {{in the creation}} of her work Double Vision (2006). The work is an hour-long solo dance, to ambient music composed by Nicolas de Zorzi. In the piece, Carlson wears a huge skirt of parachute silk that connects her to the stage. It becomes a screen for Electronic Shadows light designs. A gigantic mirror above the stage provides a second <b>viewpoint</b> for the <b>transformations</b> happening onstage. The dancers environment {{has been described as a}} [...] "high-tech playground" [...] filled with light and color.|$|R
40|$|Poly(carbosilane) or PCS, {–CH 2 –SiH(CH 3) –}n, {{is used as}} a Si-bearing pre-cursor in {{combination}} with a coal tar pitch to study thermally induced transfor-mations toward SiC-modified carbon composites. Following mixing of the components in the molten pitch at 160 ºC, the mixture is heated under argon atmosphere at 500 ºC yielding a solid carbonizate that is further subjected to separate pyrolysis experiments at 1300 ºC or 1650 ºC. At temperatures up to 500 ºC, the PCS reacts with suitable pitch components as well as undergoing decomposition reactions. At higher temperatures, clusters of prevailingly nano-crystalline b-SiC are confirmed after the 1650 ºC pyrolysis step with indications that the formation of the compound starts at 1300 ºC. 29 Si MAS NMR, XRD, FT-IR, XPS, and elemental analysis are used to characterize each pyrolysis step, especially, from the <b>viewpoint</b> of <b>transformation</b> of silicon species to silicon carbide in the carbon matrix evolved from the pitch. KEY WORDS: Poly(carbosilane); nanocrystalline silicon carbide; SiC; car-bonizate; carbon composites; carbon materials; 29 Si MAS NMR...|$|R
3000|$|For this example, Abass [22] {{gives the}} optimal {{solution}} as (x^*,y^*)=(0, 6) {{with the possibility}} degree level λ= 0.3 by adopting the midpoints and half-widths of the interval coefficients to formulate both interval objective functions and applying {{the concept of the}} possibility degree of interval number to treat interval inequality constraints. As far as the final result is concerned, the result obtained by our approach is the same as that obtained in [22]. From the <b>viewpoint</b> of equivalent <b>transformation</b> of interval inequality constraint, our approach can ensure that the formulated stochastic constraint at the obtained optimal solution holds with probability of at least 95 [...]...|$|R
40|$|This paper {{describes}} {{the main features}} of a view-based model of object recognition. The model tries to capture general properties to be expected in a biological architecture for object recognition. The basic module is a regularization network in which each of the hidden units is broadly tuned to a specic view of the object to be recognized. The network output, which may be largely view independent, is first {{described in terms of}} some simple simulations. The following renements and details of the basic module are then discussed: (1) some of the units may represent only components of views of the object the optimal stimulus for the unit, its ", is eectively a complex feature; (2) the units' properties are consistent with the usual description of cortical neurons as tuned to multidimensional optimal stimuli; (3) in learning to recognize new objects, preexisting centers may be used and modied, but also new centers may be created incrementally so as to provide maximal invariance; (4) modules are part of a hierarchical structure: the output of a network may be used as one of the inputs to another, in this way synthesizing increasingly complex features and templates; (5) in several recognition tasks, in particular at the basic level, a single center using view-invariant features may be sufficient. Modules of this type can deal with recognition of specic objects, for instance a specic face under various transformations such as those due to viewpoint and illumination, provided that a sufficient number of example views of the specic object are available. An architecture for 3 D object recognition, however, must cope to some extent even when only a single model view is given. The main contribution of this paper is an outline of a recognition architecture that deals with objects of a nice class undergoing a broad spectrum of transformations due to illumination, pose, expression and so on by exploiting prototypical examples. A nice class of objects is a set of objects with sufficiently similar transformation properties under specic <b>transformations,</b> such as <b>viewpoint</b> <b>transformations.</b> For nice object classes, we discuss two possibilities: (a) class-specic transformations are to be applied to a single model image to generate additional virtual example views, thus allowing some degree of generalization beyond what a single model view could otherwise provide; (b) class specic, view-invariant features are learned from examples of the class and used with the novel model image, without an explicit generation of virtual examples...|$|R
40|$|The paper {{investigates the}} {{development}} of the notion of rationality in choice and decision theory from the <b>viewpoint</b> of the <b>transformation</b> undertaken by neoclassical economics during the 20 th century — the so-called formalist revolution. The main point is that the reduction of the economic agent to a consistency restriction, carried out by Samuelson’s revealed preference theory and (via von Neumann and Morgenstern) Savage’s expected utility theory, behind the façade of improving the empirical accountability of economics, eventually acted as a catalyst for that transformation. Thus, no real difference seems to exist on these grounds between Samuelson’s, von Neumann and Morgenstern’s or Savage’s operationalist accounts of choice and decision theory and a purely formalistic approach like Debreu’s one...|$|R
50|$|In {{traditional}} geometry, affine geometry {{is considered}} to be a study between Euclidean geometry and projective geometry. On the one hand, affine geometry is Euclidean geometry with congruence left out; on the other hand, affine geometry may be obtained from projective geometry by the designation of a particular line or plane to represent the points at infinity. In affine geometry, there is no metric structure but the parallel postulate does hold. Affine geometry provides the basis for Euclidean structure when perpendicular lines are defined, or the basis for Minkowski geometry through the notion of hyperbolic orthogonality. In this <b>viewpoint,</b> an affine <b>transformation</b> geometry is a group of projective transformations that do not permute finite points with points at infinity.|$|R
40|$|This report {{describes}} {{the main features}} of a view-based model of object recognition. The model does not attempt to account for specific cortical structures; it tries to capture general properties to be expected in a biological architecture for object recognition. The basic module is a regularization network (RBF-like; see Poggio and Girosi, 1989; Poggio, 1990) in which each of the hidden units is broadly tuned to a specific view of the object to be recognized. The network output, which may be largely view independent, is first {{described in terms of}} some simple simulations. The following refinements and details of the basic module are then discussed: (1) some of the units may represent only components of views of the object—the optimal stimulus for the unit its “center,” is effectively a complex feature; (2) the units' properties are consistent with the usual description of cortical neurons as tuned to multidimensional optimal stimuli and may be realized in terms of plausible biophysical mechanisms; (3) in learning to recognize new objects, preexisting centers may be used and modified, but also new centers may be created incrementally so as to provide maximal view invariance; (4) modules are part of a hierarchical structure—the output of a network may be used as one of the inputs to another, in this way synthesizing increasingly complex features and templates; (5) in several recognition tasks, in particular at the basic level, a single center using view-invariant features may be sufficient. Modules of this type can deal with recognition of specific objects, for instance, a specific face under various transformations such as those due to viewpoint and illumination, provided that a sufficient number of example views of the specific object are available. An architecture for 3 D object recognition, however, must cope- to some extent—even when only a single model view is given. The main contribution of this report is an outline of a recognition architecture that deals with objects of a nice class undergoing a broad spectrum of transformations—due to illumination, pose, expression, and so on- by exploiting prototypical examples. A nice class of objects is a set of objects with sufficiently similar transformation properties under specific <b>transformations,</b> such as <b>viewpoint</b> <b>transformations.</b> For nice object classes, we discuss two possibilities: (1) class-specific transformations are to be applied to a single model image to generate additional virtual example views, thus allowing some degree of generalization beyond what a single model view could otherwise provide; (2) class-specific, view-invariant features are learned from examples of the class and used with the novel model image, without an explicit generation of virtual examples...|$|R
40|$|It {{has been}} long {{recognized}} that a key obstacle to achieving human-level object recognition performance {{is the problem}} of invariance. The human visual system excels at factoring out the image transformations that distort object appearance under natural conditions. Models with a cortex-inspired architecture such as HMAX as well as nonbiological convolutional neural networks are invariant to translation (and in some cases scaling) by virtue of their wiring. The transformations to which this approach has been applied so far are _generic_; a single example image of any object contains all the information needed to synthesize a new image of the transformed object. In contrast, <b>viewpoint</b> and illumination <b>transformations</b> depend on the object&#x 27;s 3 D structure and material properties. These are normally consistent within, but not between, classes. |$|R
40|$|Abstract: We {{study the}} {{linearization}} {{of a class}} of Liénard type nonlinear second-order ordinary differential equations from the generalized Sundman <b>transformation</b> <b>viewpoint.</b> The linearizing generalized Sundman transformation for the class of equations is constructed. The transformation is used to map the underlying class of equations into a linear second-order ordinary differential equation which {{is not in the}} Laguerre form. The general solution of this class of equations is obtained by integrating the linearized equation and applying the generalized Sundman transformation. Moreover, we apply a Riccati transformation to a general linear third-order variable coefficients ordinary differential equation to extend the underlying class of equations and we also derive the conditions of linearizability of this new class of nonlinear second-order ordinary differential equations using the generalized Sundman transformation method and obtain its general solution...|$|R
40|$|Abstract. Understanding the {{architectures}} {{of complex}} system, e. g. enterprises, is greatly facilitated by using graphical views thereof. These views {{result from the}} application of an underlying viewpoint to a comprehensive architectural description. The viewpoint thereby describes, which architectural concepts should be visualized in which way. The creation of views that consistently represent the enterprise architecture (EA) from a specific viewpoint, is a challenge of ongoing interest in EA management. In this paper, we present a technique {{that can be used}} to create views consistent to arbitrary architecture viewpoints and show, how this technique is realized in a prototypic tool. Central constituents of the tool are a model providing the graphical primitives for describing visualizations (called visualization model) and a model-to-model transformation reifying an architectural viewpoint. Key words: model <b>transformation,</b> <b>viewpoints,</b> views, EA management...|$|R
40|$|Unified Modeling Language (UML) {{provides}} various diagram types {{for describing}} a system from different perspectives or abstraction levels. Hence, various UML {{models of the}} same system are dependent and strongly overlapping. This paper discusses various general approaches and <b>viewpoints</b> of model <b>transformations</b> in UML. The possible source and target diagram types are analyzed and categories are given for different transformations. It is argued that such transformations should be {{defined in terms of}} the UML metamodel, rather than on the level of the actual diagrams. A detailed example of a transformation operation from sequence diagrams into class diagrams is presented to illustrate such operations. It is concluded that the transformation operations can automate a substantial part in both forward and reverse engineering. These operations can be used, for example, for model checking, merging, slicing, and synthesis...|$|R
40|$|Background: Interacting {{with other}} people {{involves}} spatial awareness of one’s own body and the other’s body and viewpoint. In the past, social cognition has focused largely on belief reasoning, which is abstracted away from spatial and bodily representations, {{while there is a}} strong tradition of work on spatial and object representation which does not consider social interactions. These two domains have flourished independently. A small but growing body of research examines how awareness of space and body relates to the ability to interpret and interact with others. This also builds on the growing awareness that many cognitive processes are embodied, which could be of relevance for the integration of the social and spatial domains: Online mental transformations of spatial representations have been shown to rely on simulated body movements and various aspects of social interaction have been related to the simulation of a conspecific’s behaviour within the observer’s bodily repertoire. Both dimensions of embodied transformations or mappings seem to serve the purpose of establishing alignment between the observer and a target. In spatial cognition research the target is spatially defined as a particular viewpoint or frame of reference (FOR), yet, in social interaction research another viewpoint is occupied by another’s mind, which crucially requires perspective taking in the sense of considering what another person experiences from a different viewpoint. Perspective taking has been studied in different ways within developmental psychology, cognitive psychology, psycholinguistics, neuropsychology and cognitive neuroscience over the last few decades, yet, integrative approaches for channelling all information into a unified account of perspective taking and <b>viewpoint</b> <b>transformations</b> have not been presented so far. Aims: This Research Topic aims to bring together the social and the spatial, and to highlight findings and methods which can unify research across areas. In particular, the topic aims to advance our current theories and set the stage for future developments of the field by clarifying and linking theoretical concepts across disciplines. Scope: The focus of this Research Topic is on the SPATIAL and the SOCIAL, and we anticipate that all submissions will touch on both aspects and will explicitly attempt to bridge conceptual gaps. Social questions could include questions of how people judge another person’s viewpoint or spatial capacities, or how they imagine themselves from different points of view. Spatial questions could include consideration of different physical configurations of the body and the arrangement of different viewpoints, including mental rotation of objects or viewpoints that have social relevance. Questions could also relate to how individual differences (in personality, sex, development, culture, species etc.) influence or determine social and spatial perspective judgements. Many different methods can be used to explore perspective taking, including mental chronometry, behavioural tasks, EEG/MEG and fMRI, child development, neuropsychological patients, virtual reality and more. Bringing together results and approaches from these different domains is a key aim of this Research Topic. We welcome submissions of experimental papers, reviews and theory papers which cover these topics...|$|R
40|$|This paper {{presents}} an interactive modeling system that constructs 3 D models from {{a collection of}} panoramic image mosaics. A panoramic mosaic consists {{of a set of}} images taken around the same <b>viewpoint,</b> and a <b>transformation</b> matrix associated with each input image. Our system first recovers the camera pose for each mosaic from known line directions and points, and then constructs the 3 D model using all available geometrical constraints. In particular, we carefully partition constraints into "soft" and "hard" linear constraints so that the modeling process can be formulated as a linearly-constrained least-squares problem, which can be solved e#ciently using QR factorization. Wire frame and texture-mapped 3 D models from single and multiple panoramas are presented. 1 Introduction A great deal of e#ort has been expended on 3 D scene reconstruction from image sequences (with calibrated or uncalibrated cameras) using computer vision techniques. Unfortunately, the results from most automatic [...] ...|$|R
40|$|Abductive logic {{programming}} (ALP) and disjunctive {{logic programming}} (DLP) are two di#erent extensions of logic programming. This paper investigates {{the relationship between}} ALP and DLP from the program <b>transformation</b> <b>viewpoint.</b> It is shown that the belief set semantics of an abductive program is expressed by the answer set semantics and the possible model semantics of a disjunctive program. In converse, the possible model semantics of a disjunctive program is equivalently expressed by the belief set semantics of an abductive program, while such a transformation is generally impossible for the answer set semantics. Moreover, it is shown that abductive disjunctive programs are always reducible to disjunctive programs both under the answer set semantics and the possible model semantics. These transformations are verified from the complexity viewpoint. The results of this paper turn out that ALP and DLP are just di#erent {{ways of looking at}} the same problem if we choose an appropriate seman [...] ...|$|R
40|$|The paper {{describes}} {{an approach to}} measuring convergence of an algorithm to its result {{in terms of an}} entropy-like function of partitions of its inputs of a given length. The goal {{is to look at the}} algorithmic data processing from the <b>viewpoint</b> of information <b>transformation,</b> with a hope to better understand the work of algorithm, and maybe its complexity. The entropy is a measure of uncertainty, it does not correspond to our intuitive understanding of information. However, it is what we have in this area. In order to realize this approach we introduce a measure on the inputs of a given length based on the Principle of Maximal Uncertainty: all results should be equiprobable to the algorithm at the beginning. An algorithm is viewed as a set of events, each event is an application of a command. The commands are very basic. To measure the convergence we introduce a measure that is called entropic weight of events of the algorithm. The approach is illustrated by two examples...|$|R
