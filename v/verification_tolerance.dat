0|54|Public
40|$|The topics {{addressed}} are {{history of}} reusable external insulation (RSI), {{and examples of}} shuttle RSI development challenges, which includes: (1) manufacturing of raw materials, such as fibers, coating components; (2) design of tile planform size, strain isolation, and gap heating; (3) insulation, such as bonding, bond <b>verification,</b> <b>tolerances,</b> and quality control; and (4) operation, such as durability, and water proofing...|$|R
5000|$|... #Subtitle level 2: Fault <b>tolerance</b> <b>verification</b> and {{validation}} ...|$|R
40|$|The {{project has}} been focused on the {{investigation}} of process variability and <b>tolerance</b> <b>verification</b> using coordinate metrology in LEGO System A/S. There are some causes that influence the stability of the process. Therefore the aim of the project is to identify these causes and reduce them, by creating a standard procedure that can be applied for every elemen...|$|R
40|$|The {{tolerancing}} step has a {{great importance}} in the design process. It characterises {{the relationship between the}} different sectors of the product life cycle: Design, Manufacturing and Control. We can distinguish several methods to assist the tolerancing process in the design. Based on arithmetic and statistical method, this paper presents a new approach of analysis and <b>verification</b> of <b>tolerances.</b> The chosen approach is based on the Worst Case Method as an arithmetic method and Monte Carlo method as a statistical method. In this paper, we compare these methods and we present our main approach, which is validated using an example of 1 D tolerancing...|$|R
40|$|An {{investigation}} is described, aimed at estimating effects of several factors on measurement uncertainty within an industrial environment. The case study concerns mainly <b>tolerance</b> <b>verification</b> on a {{coordinate measuring machine}} (CMM) of a rather complex part. Obtained results highlight the importance of probe qualification in the case at hand, and show how substantial savings in cycle time may be obtaine...|$|R
40|$|Collision {{detection}} and distance computation {{are important for}} a number of engineering applications including dynamic simulation, <b>tolerance</b> <b>verification,</b> object manipulation, motion planning and control. Numerous algorithms and techniques have been proposed. (See a recent survey [LG 98].) In order to meet the stringent requirement of haptic rendering, new algorithms and specialized system implementation need to be developed to substain KHz haptic updat...|$|R
40|$|The last {{generation}} of CAD software systems allows {{the creation of}} models that consider the whole product lifecycle. Along the product development phase important activities {{are related to the}} geometrical tolerances prescription and their control planning. However, the Geometrical and Dimensional Tolerances (GD&T) representation, analysis and verification processes are still rarely taken into account in CAD systems. Such a bottleneck implies a “over the wall” communication between design and quality control departments. On the other hand, the <b>tolerances</b> <b>verification</b> process needs an high level of automation in order to extend the number of inspected products and to shorten the time from control strategy definition to the measurement process. Our research work is focused on the development of an approach for the automatic inspection planning, simulation and optimisation of geometrical tolerances. The approach is based on the integration of three technologies: the augmented CAD models, the 3 D optical digitizing systems and the articulated robot systems. In this paper the methodology is presented both for specific geometrical <b>tolerances</b> <b>verification</b> and for the global shape control...|$|R
40|$|This paper {{presents}} a complete model including method and result for <b>tolerance</b> <b>verification</b> of bent sheet metal parts. The method uses {{a mathematical model}} {{to take into account}} aspects influencing the dimensional accuracy of linear and angular dimensions of bent parts. The model was implemented using a set of experimental data to estimate the dimensional accuracy for dimensions of interest of a specific part. Comparing the prediction data with the measured results allowed validation of the model. status: publishe...|$|R
40|$|We {{perform a}} case study on the interrelations between problem {{specifications}} in ideal environments and in faulty ones. The problem considered is that of consensus and the failure model used is crash. The goal {{of this research is}} to uncover the influences that specific failure models have on problem specifications so that fault-tolerance specifications can be systematically derived. Keywords: specification, <b>verification,</b> fault <b>tolerance,</b> failure models, consensus problem. 1. Introduction A system is usually defined as a "thing" that interacts with its environment in a discrete fashion across a well-defined boundary (called an interface) [12]. Specifications for systems identify the intended behavior at the interface. If a system S exhibits only the kind of behavior permitted by some specification M, then we say that "S is correct regarding M ". In ideal and fault-free environments we call M the ideal problem specification. There are different notions of correctness when faulttole [...] ...|$|R
40|$|We {{present a}} formal model of {{asynchronous}} communication between two digital hardware devices. The model {{takes the form}} of a function in the Boyer-Moore logic. The function transforms the signal stream generated by one processor into that consumed by an independently clocked processor, given the phases and rates of the two clocks and the communications delay. The model can be used quantitatively to derive concrete performance bounds on communications at ISO protocol level 1 (physical level). We use the model to show that an 18 -bit/cell biphase mark protocol reliably sends messages of arbitrary length between two processors provided the ratio of the clock rates is within 5 % of unity. Keywords: hardware <b>verification,</b> fault <b>tolerance,</b> protocol <b>verification,</b> clock synchronization, Manchester format, automatic theorem proving, Boyer-Moore logic, ISO protocol level 1, performance modeling. 1. Introduction In this paper we will (a) formalize the lowest-level communication between two indepe [...] ...|$|R
40|$|Boolean operations, Minkowski sum evaluation, {{configuration}} space computation, {{and motion}} planning are fundamental problems in solid modeling and robotics. Their applications include computer-aided design, numerically-controlled machining, <b>tolerance</b> <b>verification,</b> packing, assembly planning, and dynamic simulation. Prior algorithms for solving these {{problems can be}} classified into exact and approximate approaches. The exact approaches are difficult to implement and are prone to robustness problems. Current approximate approaches may not solve these problems accurately. Our work aims to bridge this gap between exact and approximate approaches. We present a sampling-based approach to solve these geometric problems. Our approac...|$|R
40|$|AbstractComputed {{tomography}} (CT) {{is becoming}} an important technology for industrial applications, enabling fast and accurate control of manufactured parts. In only a few minutes, a complete 3 D model of a part may be obtained, allowing measurements of external and internal features. This paper presents results of <b>tolerance</b> <b>verification</b> of a plastic housing for an insulin pen manufactured by Novo Nordisk A/S. Calculation of measurement uncertainties was {{taken into account in}} decision making regarding the specified tolerance limits. Variables in terms of CT systems, data sets, and evaluation software are considered in this study...|$|R
40|$|Mechanical {{product quality}} is {{strongly}} influenced by the respect of Geometrical Tolerances (GT). On the other hand, competitiveness forces companies to improve their productivity making the <b>tolerance</b> <b>verification</b> process faster and faster and more flexible. Component control by 3 D full field optical digitizing systems and specific CAD-based (Computer Aided Design) inspection software tools are important steps forward for {{the achievement of the}} above-mentioned goals. However, the adoption of these solutions in industry is minimal. This may be due both to technological factors, i. e. poor systems usability, and organizing factors, i. e. clear separation between design department and quality control department. In this context, our research aims at developing a new easy to use CAD-based tool for simulating, driving and optimizing the GT inspection process. Once a component has been digitized, the developed software system automatically realizes the tolerances virtual control. Hence, the designer can prescribe tolerances, pilot the measurement system and verify the component conformity. The implemented tool is based on Full of Information (FoI) CAD models, which contain tolerance data, linked to a knowledge database, where measurement strategies and verification rules are stored. A computation engine calculates the measurement paths and performs the <b>tolerances</b> <b>verification.</b> The prototypal system has been tested on different real cases. Experimental results showed high performances in terms of timesaving and robustness...|$|R
40|$|The {{need for}} {{increased}} product variety and improved aesthetics require the manufacturing enterprise to reduce {{time to market}} and to increase use of free-form surfaces {{in the form of}} the product. These changes lead to problems in the traditional approach for specification and <b>verification</b> of <b>tolerances</b> especially for a free form surfaces. In the case of freeform surfaces, the desired performance of a product depends on its geometry and is often controlled by intrinsic parameters such as curvature. Design intent therefore requires control on variations in these parameters. Ideally therefore, tolerances have to be applied on these parameters to prescribe allowable variations in the geometry of free-form surfaces. Since only the geometry of the product is controlled in manufacturing, tolerance specification has to ensure that the tolerances specified on the part geometry will ensure that the resulting value of the parameter of interest is within the limits prescribed by the designer. Relationship between allowable range in design parameters and that in geometry is not linear. Tolerance specification therefore becomes a trial and error process requiring considerable expertise and time. This thesis provides designers with a tool to automatically derive the corresponding tolerances to be specified to the manufacturing process to realize the final shape, such that the parameters that are used to control shape of the surface are within the prescribed variations. Automation in acquiring inspection data has brought dramatic changes in procedure for <b>tolerance</b> <b>verification</b> too. Optical scanners and similar non-contact devices provide large amount of points on the surface of the part quite rapidly. The unstructured point data are then processed to determine if the part complies with the given tolerance specifications. For freeform surfaces, current methods of verification uses minimum distance criterion between the nominal surface and unstructured point data. This ignores the correspondence between the points in the two data sets and may result in the rejection of good parts and acceptance of bad parts. There are other unresolved such as the singularity at corners of polyhedral shapes and handling datum. A new approach based on the Medial Axis Transform (MAT) has been proposed. It has been shown that reasoning on the MAT of the nominal model and the measured point set respectively enables the identification of corresponding points in the two sets. <b>Verification</b> of the <b>tolerance</b> allocated is therefore free from the problem mentioned above. MAT exhibits dimensional reduction and hence reduces verification time. It also eliminates surface fitting for detected feature. Results of implementation are provided for <b>tolerance</b> specification and <b>verification</b> using MAT...|$|R
40|$|International audienceSurface {{roughness}} of tiny micro machined features is {{not easy}} to verify. The statistical variation of the surface itself can be the limiting factor that hampers <b>tolerance</b> <b>verification.</b> In this paper we have studied this effect and we also test the performance of 10 different surface profilers over a very well specified surface area. For this area 6 profilers yielded the same result within a standard deviation window of ± 6 %. For other areas, on top of narrow bars and in narrow and deep channels, a much larger spread in the Round Robin results was found...|$|R
40|$|In {{advanced}} hybrid {{electric vehicle}} development, performance and dependability are essential considerations in the design process. High efficiency is crucial in a successful design and fault tolerance is necessary to provide limp-home capability under faulted operating conditions. The reduction of electromagnetic interference is mandatory to reduce interference in communications and control systems, especially in military applications. This thesis is a performance analysis of several generator and active rectifier configurations for use in military {{hybrid electric vehicle}} applications. Simulation results are obtained using Matlab Simulink with experimental hardware testing for <b>verification.</b> Fault <b>tolerance</b> is explored in generator design {{through the use of}} multi-phase generator configurations in both wye-connected and independent phase configurations. Passive rectifier investigations are included as a baseline for active rectifier performance comparison. Simple voltage-controlled active rectifier results are obtained with initial three- and six-phase generator configurations. Current-controlled rectifier models are then simulated with more advanced generator designs. In all simulations, performance is evaluated through harmonic analysis of rectifier input currents and output dc bus voltage, as well as input power factor measurements...|$|R
40|$|Abstract: We {{present a}} novel and fast {{algorithm}} to compute penetration depth (PD) between two polyhedral models. Given two overlapping polyhedra, it computes the minimal translation distance {{to separate them}} {{using a combination of}} object-space and image-space techniques. The algorithm computes pairwise Minkowski sums of decomposed convex pieces, performs closest-point query using rasterization hardware and refines the estimated PD by object-space walking. It uses bounding volume hierarchies, model simplification, object-space and image-space culling algorithms to further accelerate the computation and refines the estimated PD in a hierarchical manner. We highlight its performance on complex models and demonstrate its application to dynamic simulation and <b>tolerance</b> <b>verification...</b>|$|R
40|$|This paper {{presents}} a methodology for estimating the verification cost of geometric tolerances at the design stage, which {{is useful for}} optimizing the tolerance specifications of mechanical products. The <b>tolerance</b> <b>verification</b> cost {{may be seen as}} the sum of the measurement and uncertainty costs. The measurement cost depends on the adopted device and on the measurement time. The uncertainty cost is related to type A (declaring a conforming part non-conforming) and type B (declaring a non-conforming part conforming) errors: these errors are strongly influenced by the measurement procedure, which depends on the measurement device and determines the measurement time. The methodology is illustrated and validated using simulation and industrial case studies...|$|R
40|$|<b>Tolerance</b> <b>verification</b> {{permits to}} check the product {{conformity}} and to verify assumptions made by the designer. For conformity assessment, the uncertainty associated with {{the values of the}} measurands must be known. In fact, to evaluate form characteristics of large aircraft structure workpieces, sampling is required, so a measurement error is present: exact estimation of form characteristics would require complete knowledge of the surface. To minimise this measurement error, this paper presents a Krigingbased procedure to identify the minimum of measured points {{to check the}} conformity with a given confidence level. The proposed method is validated on a simple example of orientation tolerance and then performed to inspect the form defect on three large aircraft workpieces...|$|R
40|$|The paper gives {{a survey}} of the {{upcoming}} use of X-ray computed tomography (CT) for dimensional quality control purposes: i. e. for traceable measurement of dimensions of technical (mechanical) components and for <b>tolerance</b> <b>verification</b> of such components. It describes the basic principles of CT metrology, putting emphasis on issues as accuracy, traceability to the unit of length (the meter) and measurement uncertainty. It provides a state of the art (anno 2011) and application examples, showing the aptitude of CT metrology to: (i) check internal dimensions that cannot be measured using traditional coordinate measuring machines and (ii) combine dimensional quality control with material quality control in one single quality inspection run. status: publishe...|$|R
40|$|International audienceThe CIRP Seminar on Computer Aided Tolerancing (CAT) has evolvedrapidly {{over the}} last 25 years. Since R. D. Weill {{suggested}} to meet every two years for atwo days Working Seminar on this subject, 12 conferences have been held and almost 500 papers have been published since 1989. Over the last 30 years, tolerancing hasbeen a permanent and important research issue in {{the long history of}} CIRP with broadapplications in industry. The development of Computer-Aided Technologies andcoordinate metrology have boosted the research on tolerancing and the evolution of thestandards. Therefore, it becomes fundamental to analyze the progress of the researcharea, define the identity of the different research topics and unify the efforts of the CIRPCAT community. This paper aims to provide an up-to-date review of the different Computer AidedTolerancing CIRP Seminars and Conferences based on a comprehensive scientometricanalysis. The records are collected and gathered from the proceedings papers, and aself-developed database is used here to archive, search and query all the data effectively. Titles, keywords and abstracts of the publications provide considerable information toextract relevant features. Moreover, quantitative data analysis techniques are applied todetermine the growth of literature and authorship, country and research institutions,paper’s citations and keyword occurence. In addition, the papers are qualitativelyclassified considering their research area among tolerance specification, tolerancemodeling, tolerance simulation or <b>tolerance</b> <b>verification</b> and metrology. The findings of this analysis offer further insight into the state and development of theComputer Aided Tolerancing CIRP Seminars and Conferences. Furthermore, theconferences mainly attracted the interest of academic research institutions being thetolerance simulation the prevailing area of the publications, although since 2007 <b>tolerance</b> <b>verification</b> and metrology is becoming more popular. Finally, it should bepointed out that there is a huge diversity among the keywords provided by the paperauthors, which may hinder rigorous and systematic analysis of the proceedings papers...|$|R
40|$|We {{present an}} {{accelerated}} proximity query algorithm between moving convex polyhedra. The algorithm combines Voronoi-based feature tracking with a multi-level-of-detail representation, {{in order to}} adapt to the variation in levels of coherence and speed up the computation. It provides a progressive refinement framework for collision detection and distance queries. We have implemented our algorithm and have observed significant performance improvements in our experiments, especially on scenarios where the coherence is low. 1 Introduction Proximity queries, i. e. distance 1 computations and the closely related collision detection problems, are ubiquitous in robotics, design automation, manufacturing, assembly and virtual prototyping. The set of tasks include motion planning, sensor-based manipulation, assembly and disassembly, dynamic simulation, maintainability study, simulation-based design, <b>tolerance</b> <b>verification,</b> and ergonomics analysis. Proximity queries have been extensively stud [...] ...|$|R
40|$|Tolerances are {{assigned}} to a mechanical engineering design either {{on the basis of}} functional and/or manufacturing requirements (toleranced dimensions, geometrical tolerances) or {{on the basis of the}} general categories-fine, medium, coarse-of the international standards or the designer's knowledge and experience (untoleranced dimensions). Conventional dimensions of the currently applicable dimensioning rules and implicit dimensions, including those attributed to geometrical tolerances, thus create four groups of tolerances which may or may not be compatible. In addition, any tolerance compromise, however tedious and difficult, not achieved systematically may well lead to accuracies which cannot be produced by the available machine tools. In the paper, a systematic approach to the above problems is presented. A methodology is demonstrated for the <b>verification</b> of the <b>tolerance</b> compatibility and for the assignment of compatible, producible and cost optimum tolerances. © 1993 Springer-Verlag...|$|R
40|$|In {{this paper}} we target the {{verification}} of fault tolerant aspects of distributed applications {{written in the}} Erlang programming language. Erlang programmers mostly work with ready-made language components. Our approach to <b>verification</b> of fault <b>tolerance</b> is to verify systems built using a central component of most Erlang software, a generic server component with fault tolerance handling. To verify such Erlang programs we automatically translate them into processes of the µCRL process algebra, generate their state spaces, and use a model checker {{to determine whether they}} satisfy correctness properties specified in the µ-calculus. The key observation of this paper is that, due to the usage of these higher-level design patterns, the state space generated from a Erlang program, even with failures occurring, is relatively small, and can be generated automatically. ...|$|R
40|$|To {{ensure the}} gear precision, {{industries}} need a coherent model to express, to analyse and to check geometrical specifications. Most gear tolerance representations are directly {{driven by the}} convenience of dimensional metrology and not by the convenience of the set of activities of the tolerancing process. Therefore, to ensure the coherence of all tolerancing process activities, there is a necessity to develop a complete gear tolerance model which should: represent standard tolerance practices; be integrated in the Computer-Aided systems of design, manufacturing and metrology; be controlled by CMM; and support automated tolerance analysis. The proposed model extends capabilities of a vectorial dimensioning & tolerancing model {{in order to satisfy}} the four requirements. This model is based on GeoSpelling [1]. Its coherence is illustrated by two applications: gear tolerance analysis and gear <b>tolerance</b> <b>verification</b> by CMM...|$|R
40|$|The need {{to perform}} fast and {{accurate}} proximity queries arises frequently in physically-based modeling, simulation, animation, real-time interaction within a virtual environment, and game dynamics. The set of proximity queries include intersection detection, <b>tolerance</b> <b>verification,</b> exact and approximate minimum distance computation, and (disjoint) contact determination. Specialized data structures and algorithms {{have often been}} designed to perform each type of query separately. We present a unified approach to perform any of these queries seamlessly for general, rigid polyhedral objects with boundary representations which are orientable 2 -manifolds. The proposed method involves a hierarchical data structure built upon a surface decomposition of the models. Furthermore, the incremental query algorithm takes advantage of coherence between successive frames. It {{has been applied to}} complex benchmarks and compares very favorably with earlier algorithms and systems. 1...|$|R
40|$|International audienceTo {{ensure the}} gear precision, {{industries}} need a coherent model to express, to analyse and to check geometrical specifications. Most gear tolerance representations are directly {{driven by the}} convenience of dimensional metrology and not by the convenience of the set of activities of the tolerancing process. Therefore, to ensure the coherence of all tolerancing process activities, there is a necessity to develop a complete gear tolerance model which should: represent standard tolerance practices; be integrated in the Computer-Aided systems of design, manufacturing and metrology; be controlled by CMM; and support automated tolerance analysis. The proposed model extends capabilities of a vectorial dimensioning & tolerancing model {{in order to satisfy}} the four requirements. This model is based on GeoSpelling [1]. Its coherence is illustrated by two applications: gear tolerance analysis and gear <b>tolerance</b> <b>verification</b> by CMM...|$|R
40|$|In this paper, we {{survey the}} state of the art in {{collision}} detection between general geometric models. The set of models include polygonal objects, spline or algebraic surfaces, CSG models, and deformable bodies. We present a number of techniques and systems available for contact determination. We also describe several N-body algorithms to reduce the number of pairwise intersection tests. 1 Introduction The goal of collision detection (also known as interference detection or contact determination) is to automatically report a geometric contact when it is about to occur or has actually occurred. The geometric models may be polygonal objects, splines, or algebraic surfaces. The problem is encountered in computer-aided design and machining (CAD/CAM), robotics and automation, manufacturing, computer graphics, animation and computer simulated environments. Collision detection enables simulationbased design, <b>tolerance</b> <b>verification,</b> engineering analysis, assembly and dis-assembly, motion pla [...] ...|$|R
40|$|Laser Fault Injection (LFI) {{testing has}} been {{demonstrated}} to be a useful tool in the prediction of single event upset rates in microcircuits. In addition LFI {{has contributed to the}} basic understanding of the mechanisms that cause single event upsets. However, very little research has been performed on the viability of LFI as a tool for verifying fault tolerant designs incorporated in ASICs, FPGAs, microprocessors and embedded systems. Current fault tolerant design verification techniques such as simulation and test have several significant limitations that prevent the complete verification of a fault tolerant design. However, LFI possesses spatial, temporal and financial advantages related to its use, which are very beneficial. This thesis presents results of the fault <b>tolerance</b> <b>verification</b> tests that were performed using laser fault injection on a four-bit fault tolerant filter that was implemented in a commercial FPGA...|$|R
40|$|The paper {{describes}} {{the development of}} a handwritten signature verification system incorporating pen pressure of signature path, time duration of the signing procedure, velocity profile of signature and position of signature shape. The handwritten signals have been captured and digitized using a tablet. The main features of the proposed signature verification system are the dynamically update of handwritten signature, retries capability in <b>verification,</b> application of <b>tolerance</b> bands and threshold values, development of user friendly Graphic User Interface, application of Common Time Axes and verification of signatures using a class of a multilayer feed-forward neural network. A novel algorithm has been applied that provides the ability to produce consistent and high accuracy verification result and maintain the speed of verification. The system has yielded 1. 33 % of False Reject Rate and 0 % False Acceptation Rate with the verification using random forgery signatures...|$|R
40|$|We {{describe}} {{a system for}} interactive proximity queries on massive models that are composed of {{tens of millions of}} geometric primitives. The set of queries include collision detection, distance computation and <b>tolerance</b> <b>verification.</b> These are essential for interaction with massive models. We address issues related to interactive data access and processing in a large geometric database, which may not fit into main memory of typical desktop workstations or computers. We present a new algorithm using overlap graphs for localizing the "regions of interest" within a massive model, thereby reducing runtime memory requirements. The overlap graph is computed offline, pre-processed using graph partitioning algorithms, and modified on the fly as needed. At run time, we traverse localized sub-graphs to check the corresponding geometry for proximity tests and pre-fetch geometry and auxiliary data structures. To perform interactive proximity queries, we use bounding-volume hierarchies and take ad [...] ...|$|R
40|$|Abstract. The paper {{describes}} {{the development of}} a handwritten signature verification system incorporating pen pressure of signature path, time duration of the signing procedure, velocity profile of signature and position of signature shape. The handwritten signals have been captured and digitized using a tablet. The main features of the proposed signature verification system are the dynamically update of handwritten signature, retries capability in <b>verification,</b> application of <b>tolerance</b> bands and threshold values, development of user friendly Graphic User Interface, application of Common Time Axes and verification of signatures using a class of a multilayer feed-forward neural network. A novel algorithm has been applied that provides the ability to produce consistent and high accuracy verification result and maintain the speed of verification. The system has yielded 1. 33 % of False Reject Rate and 0 % False Acceptation Rate with the verification using random forgery signatures...|$|R
40|$|In {{the course}} of an ARIANE 5 {{evolution}} program, the booster cases made of 48 CrMoNiV 4 10 high strength, low alloy steel will be modified such that 6 of the shear bolt connections will be replaced by EB-welds as illustrated in Fig. 1. The introduction of welds has been chosen to achieve the following goals: Mass reduction Cost reduction Increase of reliability Simplification of thermal insulation application The concept of the damage <b>tolerance</b> <b>verification</b> which is in progress for justification of the improved booster cases is presented. By a very detailed local approach based on the characterization of the properties and the mechanical behaviour of the different zones of a weld seam, the overall damage tolerance of the weld shall be derived. The achieved prediction model has been verified by different component tests and will finally be qualified by a full scale qualification test...|$|R
40|$|The {{importance}} of correct prescription and verification in Geometric Dimensioning and Tolerancing is widely accepted and recognized, particularly if complex or interrelated tolerances are assigned, as usually occurs when using position tolerances {{and the associated}} modifiers. In position <b>tolerancing</b> <b>verification,</b> the adoption of visual techniques, usually recognized as “paper gaging”, in order to investigate part acceptance is very appealing because of their easiness of use, the explicit visualization of deviations and moreover suggestions {{on the extent of}} the hypothetical re-work. The current literature on paper gaging suggests a simplified approach which disregards form and orientation errors. Aim of the present paper is to address an extension of this technique that takes into account also the actual orientation deviations of the considered features. This methodology has been also implemented in a computer program that joins together the clearness of the visual approach and the precision of the computational techniques...|$|R
40|$|Micro {{manufacturing}} {{is important}} for new product applications. As such, due to its size, micro <b>tolerance</b> <b>verification</b> is a challenging task. Optical metrology instruments provide solutions for this task due to their flexibility in accessing the part surface. In addition, it can capture big data in shorter time compared to the tactile (contact) instruments as well as eliminating the risk of damaging the part surface. A fundamental aspect in geometric measurement is traceability, which strictly relates to measurement uncertainty. Only by quantifying uncertainty, measurement results are reliable and comparable each other. Moreover, uncertainty estimation in geometric metrology is not easy since it is “taskspecific” with respect to what is currently measured. In this paper, the importance of measurement uncertainty and its available estimation methods are briefly presented. The implementation of a simulation method to estimate the task specific measurement uncertainty is presented and discussed with a case study...|$|R
40|$|Graduation date: 2007 In {{advanced}} hybrid {{electric vehicle}} development, performance and dependability are essential considerations in the design process. High efficiency is crucial in a successful design and fault tolerance is necessary to provide limp-home capability under faulted operating conditions. The reduction of electromagnetic interference is mandatory to reduce interference in communications and control systems, especially in military applications. This thesis is a performance analysis of several generator and active rectifier configurations for use in military {{hybrid electric vehicle}} applications. Simulation results are obtained using Matlab Simulink with experimental hardware testing for <b>verification.</b> Fault <b>tolerance</b> is explored in generator design {{through the use of}} multi-phase generator configurations in both wye-connected and independent phase configurations. Passive rectifier investigations are included as a baseline for active rectifier performance comparison. Simple voltage-controlled active rectifier results are obtained with initial three- and six-phase generator configurations. Current-controlled rectifier models are then simulated with more advanced generator designs. In all simulations, performance is evaluated through harmonic analysis of rectifier input currents and output dc bus voltage, as well as input power factor measurements. Scaled hardware testing is performed for verification of simulation results. Multiple generator configurations are represented through utilization of a three-phase, fully programmable source with various transformer configurations at the output to achieve six-phase capability. Several active rectifier configurations are obtained through the use of configurable Powerex H-Bridge IGBT assemblies with control provided using an Opal-RT hardware-in-loop rapid prototyping system...|$|R
