3|46|Public
40|$|We {{consider}} the main and essential legislative and applicative issues of metrological assurance of thread joints in the Russian Federation and international practice. Basic {{limitations of the}} existing measuring methods of the main thread gage parameter - the pitch diameter of thread - are represented. We give {{the description of the}} first echelon state working calibration standard for linear unit, placed in service in D. I. Mendeleyev Institute for Metrology (VNIIM), including the general field of applications and metrological characteristics according to the state <b>verification</b> <b>schedule.</b> The possibilities of etalon application in the field of measurements of thread gages parameters are shown. Technical solutions for reducing the components of uncertainty of measurements of the pitch diameter using the etalon are described: temperature stability; reducing of external vibration effects on measuring procedure; developing of the special software for minimizing human factor and increasing automation level of the measuring procedure. The solutions described have enabled to achieve expanded uncertainty value about 0. 7 - 1. 0 microns. As a result, we also propose further ways for development and improvement of the system of metrology assurance in the field of thread joints...|$|E
40|$|In the verifiable {{database}} (VDB) model, a computationally weak client (database owner) delegates his {{database management}} to a database service provider on the cloud, {{which is considered}} untrusted third party, while users can query the data and verify the integrity of query results. Since the process can be computationally costly and has a limited support for sophisticated query types such as aggregated queries, we propose in this research a framework that helps {{bridge the gap between}} security and practicality. The proposed framework remodels the verifiable database problem using Stackelberg security game. In the new model, the database owner creates and uploads to the database service provider the database and its authentication structure (AS). Next, the game is played between the defender (verifier), who is a trusted party to the database owner and runs scheduled randomized verifications using Stackelberg mixed strategy, and the database service provider. The idea is to randomize the <b>verification</b> <b>schedule</b> in an optimized way that grants the optimal payoff for the verifier while making it extremely hard for the database service provider or any attacker to figure out which part of the database is being verified next. We have implemented and compared the proposed model performance with a uniform randomization model. Simulation results show that the proposed model outperforms the uniform randomization model. Furthermore, we have evaluated the efficiency of the proposed model against different cost metrics...|$|E
40|$|Introduction. The paper gives {{information}} {{on the development of}} a certified reference material (CRM) for composition of high-purity copper (Cu CRM UNIIM). The CRM is included as the transfer standard into the State primary standard of the mass (molar) fraction and mass (molar) concentration of the component in liquid and solid substances and materials based on coulometry GET 176 - 2013. Materials and methods. The CRM represents pieces of oxygen-free copper wire rod, brand KMB, produced according to GOST R 53803 - 2010, weighing from 0. 5 to 1 g. The CRM is packed in plastic vials with the capacity of 30 or 50 cm 3. The certified characteristic of the CRM is copper mass fraction in copper wire rod, expressed in percentages. The certified value for copper mass fraction was established by the primary method of controlled-potential coulometry using the State primary standard GET 176 - 2013. Results. The permitted interval of the certified value for copper mass fraction in the CRM is from 99, 950 % to 100, 000 %. The relative expanded uncertainty (k= 2) of the certified value for copper mass fraction does not exceed 0, 030 %; the relative standard uncertainty due to inhomogeneity does not exceed 0. 010 %; the relative standard uncertainty due to instability does not exceed 0. 010 %. The shelf life of the developed CRM is 10 years provided that standard storage conditions are ensured. Discussion and conclusions. The developed CRM is included into the State register of type approved RMs under the number GSO 10800 - 2016. The CRM of high-purity copper (Cu CRM UNIIM) as a transfer standard is intended for reproduction, storage and transfer of the copper mass fraction unit to other reference materials and chemical reagents by the method of comparison using a comparator and by conducting direct measurements. This CRM may also be used: – for verification of measuring instruments (MIs) according to the state <b>verification</b> <b>schedule</b> described in GOST R 8. 735. 0 - 2014, – for calibration, graduation of MIs; control of metrological characteristics when conducting MI tests, including for the purposes of type approval; – for validation of measurement procedures, accuracy control of measurement procedures in the process of their implementation.  </p...|$|E
40|$|Automated Theorem Provers (ATPs) and SAT/SMT solvers are [...] . {{fast and}} efficient; [...] . applied in {{different}} contexts: program <b>verification,</b> <b>scheduling,</b> test case generation, etc. Interactive Theorem Provers (ITPs) have been [...] . enriched with dependent types, (co) inductive types, type classes and provide rich programming environments...|$|R
40|$|Please {{note that}} this {{introduction}} {{is very similar to}} the one given in deliverable 2. 3. a, it serves for selfcontained reading to establish the context. A main emphasis of AMETIST is the automatic analysis of timed systems (such as those of the case studies) with the aims of <b>verification,</b> <b>scheduling</b> and control synthesis. The chain o...|$|R
40|$|A main {{emphasis}} of Ametist is the automatic analysis of timed systems (such {{as those of}} the case studies) with the aims of <b>verification,</b> <b>scheduling</b> and control synthesis. The chain of such automated analysis goes from modeling in a suitable modeling formalism, specification of the analysis goal, via the automatic coding to a more abstract search or constraint satisfaction problem...|$|R
40|$|Delay {{variation}} due to crosstalk {{has made}} timing analysis a hard problem. In sequential circuits with transparent latches, crosstalk makes the clock <b>schedule</b> <b>verification</b> even harder. In this paper, we {{point out a}} false negative problem in current clock <b>schedule</b> <b>verification</b> techniques and propose a new approach based on switching windows. In this approach, coupling delay calculations are naturally combined with latch iterations. A novel algorithm is given for clock <b>schedule</b> <b>verification</b> {{in the presence of}} crosstalk and primary experiments show promising results. 1...|$|R
40|$|A typical {{hardware}} development flow {{starts the}} verification process concurrently with RTL, {{but the overall}} schedule becomes limited by the effort required to complete all the necessary verification tasks. Being the limiting factor, <b>verification</b> <b>schedules</b> become unpredictable, often resulting in slippage of the tapeout dates. This paper looks at ways to restructure the flow to complete {{a significant part of}} this effort during the architectural phase of the project, {{prior to the start of}} RTL. This front-loading of the schedule allows a smaller verification team to complete the process with a tighter schedule...|$|R
40|$|Pacific Northwest National Laboratory (PNNL) is {{assisting}} the U. S. Department of Energy (DOE) Distributed Energy (DE) Program by developing advanced control algorithms {{that would lead}} to development of tools to enhance performance and reliability, and reduce emissions of distributed energy technologies, including combined heat and power technologies. This report documents phase 2 of the program, providing a detailed functional specification for algorithms for performance monitoring and commissioning <b>verification,</b> <b>scheduled</b> for development in FY 2006. The report identifies the systems for which algorithms will be developed, the specific functions of each algorithm, metrics which the algorithms will output, and inputs required by each algorithm...|$|R
40|$|SystemC is a {{widespread}} language for developing SoC designs. Unfortunately, most SystemC simulators {{are based on a}} strictly sequential scheduler that heavily limits their performance, impacting <b>verification</b> <b>schedules</b> and time-to-market of new designs. Parallelizing SystemC simulation entails a complete re-design of the simulator kernel for the specific target parallel architectures. This paper proposes an automatic methodology to generate a parallel SystemC simulator kernel, exploiting the massive parallelism of GP-GPU architectures. Our solution leverages static scheduling to reduce synchronization overheads. The generated simulator code targets both CUDA and OpenCL libraries, to boost scalability and provide support for multiple GP-GPU architectures. Finally, the paper compares the performance of our solution on CUDA vs. OpenCL platforms, with the goal of investigating advantages and drawbacks that the two thread management libraries offer to concurrent SystemC simulation...|$|R
40|$|SystemC is a {{widespread}} language for HW/SW system simulation and design exploration, and thus a key development platform in embedded system design. However, the growing complexity of SoC designs is {{having an impact on}} simulation performance, leading to limited SoC exploration potential, which in turns affects development and <b>verification</b> <b>schedules</b> and time-to-market for new designs. Previous efforts have attempted to parallelize SystemC simulation, targeting both multiprocessors and GPUs. However, for practical designs, those approaches fall far short of satisfactory performance. This paper proposes SAGA, a novel simulation approach that fully exploits the intrinsic parallelism of RTL SystemC descriptions, targeting GPU platforms. By limiting synchronization events with ad-hoc static scheduling and separate independent dataflows, we shows that we can simulate complex SystemC descriptions up to 16 times faster than traditional simulator...|$|R
40|$|In {{the field}} of {{real-time}} systems, accurate estimates of the worst-case execution time of programs are required for real-time modelling and <b>verification,</b> <b>scheduling</b> analysis, feasibility analysis, and dimensioning of the system hardware. At present, such estimates are produced by measuring program runs on inputs which are believed to produce long execution times. Unfortunately, such measurements provide no guarantee for finding the worst execution time, thereby producing unsafe systems. To obtain safe estimates (estimates which do not underestimate the worst-case time), {{we have to use}} formal static analysis. Because the market demands high performance, low cost products, we must use optimizing compilers when developing software for real-time systems. An optimizing compiler improves the performance of a program and reduces the size of the code, both important factors to save costs while maintaining performance. An optimizing compiler makes static analysis of a program more difficult by [...] ...|$|R
40|$|We {{present an}} {{algorithm}} to check feasibility and find solutions {{on a set}} of timing constraints of repetitive nature. Our model permits to specify a real-time system executing an infinite number of cycles, where at each cycle a set of events must occur. The occurrence times of events inside the same cycle, and between consecutive cycles is guarded by a set of simple linear constraints. Given such a model, we show how to check feasibility (does the system has an infinite execution where all constraints are met ?) and how to build an execution. A prototype implementation demonstrates the efficiency of our technique even for systems of large size. Keywords. Linear-constraint solving, Constraint propagation, Real-time systems, <b>Verification,</b> <b>Scheduling.</b> 1 Introduction Real-time systems are often designed to execute in cycles, which are to be repeated (in principle) an infinite number of times. A cycle represents a time interval in which a set of events occur. Such an event could corre [...] ...|$|R
40|$|The {{purpose of}} this Corrective Action Plan is to {{demonstrate}} the OW planned and/or completed actions to implement ISMS as well as prepare for the RPP ISMS Phase II <b>Verification</b> <b>scheduled</b> for August, 1999. This Plan collates implied or explicit ORP actions identified in several key ISMS documents and aligns those actions and responsibilities perceived necessary to appropriately disposition all ISM Phase II preparation activities specific to the ORP. The objective will be to complete or disposition the corrective actions prior to the commencement of the ISMS Phase II Verification. Improvement products/tasks not slated for completion prior to the RPP Phase II verification will be incorporated as corrective actions into the Strategic System Execution Plan (SSEP) Gap Analysis. Many of the business and management systems that were reviewed in the ISMS Phase I verification are being modified to support the ORP transition and are being assessed through the SSEP. The actions and processes identified in the SSEP will support {{the development of the}} ORP and continued ISMS implementation as committed to be complete by end of FY- 2000...|$|R
50|$|Sehat (Urdu: ﺻﺤﺖ) is {{an online}} {{pharmacy}} in Pakistan providing retail B2C online medicinal sales in Pakistan. sehat.com.pk {{is the latest}} venture under the Fazal Din Group, {{one of the oldest}} healthcare conglomerates in Pakistan (est 1948). This e-commerce portal supplies the majority of its medicines directly from the manufacturer and provides nationwide delivery with multiple payment methods (including Credit Card Payment), prescription <b>verification,</b> order <b>scheduling,</b> and store credit features, becoming the first online pharmacy of its kind to do so in Pakistan.|$|R
40|$|In {{this paper}} we augment DLT (Divisible Load Theory) with {{incentives}} {{such that it}} is beneficial for processors to report their true processing capacity and compute their assignments at full capacity. We propose a strategyproof mechanism with <b>verification</b> for <b>scheduling</b> divisible loads in linear networks with boundary load origination. The mechanism provides incentives to processors for reporting deviants. The deviants are penalized which abates their willingness to deviate in the first place. We prove that the mechanism is strategyproof and satisfies the voluntary participation condition. 1...|$|R
50|$|The Distributed Real-time Embedded Analysis Method (DREAM) is a platform-independent {{open-source}} {{tool for}} the verification and analysis of distributed real-time and embedded (DRE) systems which focuses on the practical application of formal verification and timing analysis to real-time middleware. DREAM supports formal <b>verification</b> of <b>scheduling</b> based on task timed automata using the Uppaal model checker and the Verimag IF toolset {{as well as the}} random testing of real-time components using a discrete event simulator. DREAM is developed at the Center for Embedded Computer Systems at the University of California, Irvine, in cooperation with researchers from Vanderbilt University.|$|R
50|$|The term {{scheduling}} {{analysis in}} real-time computing includes {{the analysis and}} testing of the scheduler system and the algorithms used in real-time applications. In computer science, real-time scheduling Analysis is the evaluation, testing and <b>verification</b> of the <b>scheduling</b> system and the algorithms used in real-time operations. For critical operations, a real-time system must be tested and verified for performance. In computer science, testing and verification {{is also known as}} model checking.|$|R
40|$|To support {{planning}} of massive transportations under time-critical conditions, in particular, evacuation {{of people from}} a disasteraffected area, we have developed a software module for automated generation of transportation schedules and a suite of visual analytics tools that enable the <b>verification</b> of a <b>schedule</b> by a human expert. We combine computational, visual, and interactive techniques to help the user to deal with large and complex data involving geographical space, time, and heterogeneous objects...|$|R
40|$|Abstract. In {{order to}} answer the complex service {{requirements}} of the user, composite web services have to be constructed correctly and effectively. Various approaches and formalism {{have been used for}} web service composition and integration. The semantic modeling of composite services is necessary for automatic discovery, integration and execution. For this purpose, ontology languages and ontologies have been defined. OWL-S is a OWL-based ontology of services, in which composite processes can be modeled. For reasoning and verification on the composite services, logic-based formalisms have an important role. Concurrent Constraint Transaction Logic is a formalism that provides means for modeling, <b>verification</b> and <b>scheduling</b> of composite web services. In this work, we describe how OWL-S and CCTR can be used together for modeling a complex service and constraints, and make reasoning and verification on this model under the given set of constraints. ...|$|R
40|$|In {{this paper}} we generalize {{the concept of}} {{verification}} introduced by Nisan and Ronen [STOC 1999]. We assume to have selfish agents with general valuation functions and we study mechanisms with verification for optimization problems with these selfish agents. We provide a technique for designing truthful mechanisms with verification that optimally solve the underlying optimization problem. Our technique {{can be applied to}} a rich class of problems that includes, as special cases, utilitarian problems and many others considered in literature for so called one-parameter agents (e. g., the makespan studied by Archer and Tardos [STOC 2001]). Our technique extends the one recently presented by Auletta et al as it works for any finite multi-dimensional valuation domain. No method was known to deal with any domain. As special case we give a different proof (w. r. t. to the one given by Nisan and Ronen) of the existence of exact truthful mechanisms with <b>verification</b> for <b>Scheduling</b> Unrelated Selfish Machines. Furthermore, our technique also applies to the case of compound agents (i. e., agents declaring more than a value). No solution was known for designing mechanisms (with verification) for problems involving such general kind of agents. As an application we provide the first optimal truthful mechanism with <b>verification</b> for <b>Scheduling</b> Unrelated Selfish Compound Machines in which every agent controls more than one (unrelated) machine. This mechanism does not run in polynomial time. We then turn our attention to efficient approximating truthful mechanisms and provide a technique that transforms any approximation algorithm into a mechanism with verification with no significant loss of approximation ratio. This technique works for smooth problems involving compound one-parameter agents. We apply this technique to Scheduling Related Compound Machines problem (i. e, agents control more related machines). If the number of machines is constant then our solution runs in polynomial-time. Finally, we give some considerations on the construction of mechanisms (with verification) for infinite domains...|$|R
50|$|It is {{not always}} {{possible}} to meet the required deadline; hence further <b>verification</b> of the <b>scheduling</b> algorithm must be conducted. Two different models can be implemented using a dynamic scheduling algorithm; a task deadline can be assigned according to the task priority (earliest deadline) or a completion time for each task is assigned by subtracting the processing time from the deadline (least laxity). Deadlines and the required task execution time must be understood in advance to ensure the effective use of the processing elements execution times.|$|R
40|$|In the Universal Traffic Management Systems (UTMS) {{now being}} studied and {{developed}} in Japan, the infrared beacon (referred to as IR beacon from now on) {{is used for}} realizing the Dynamic Route Guidance System (DRGS) in real world. In DRGS, the IR beacon receives an optional destination information a driver specified, and immediately sends back to the vehicle the dynamic route guidance information including the most suitable route to the required destination. This paper describes methods of data calculation process incorporated with the IR beacon for realizing DRGS, and also evaluated results {{on the performance of}} IR beacons developed for an DRGS <b>verification</b> test <b>scheduled</b> to be carried out this fall. PREFACE UTMS project, as shown in Figure 1, is now studied and developed in Japan under an initiative of the National Police Agency (NPA). One of UTMS sub-systems, AMIS is a system that provides drivers with such information as traffic jam, accidents, road works etc. via IR beacons to car navigation units. It has {{played a major role in}} operating the Vehicle Information & Communication System (VICS), which started it...|$|R
40|$|This Calibration/Verification Plan –Cal/Ver-Plan- {{sets the}} time {{schedule}} {{and is the}} master steering document for all calibration and verification activities in the TerraSAR-X Project during the Commissioning Phase. It is to be co-ordinated with the Flight Operations Plan from MOS [RD - 5]. This document defines the terms calibration, characterization, verification and validation for the TerraSAR-X project Comissioning Phase. It describes the complete document structure for the TerraSAR-X <b>verification</b> and the <b>scheduling</b> for all calibration and verification activities in the Comissioning Phase. The calibration and in-flight instrument characterization is documented in detail in the TerraSAR-X In-Orbit Calibration Plan [RD - 4]. The IOCS Calibration Section receives its high level tasks and the high level schedule from this document. ...|$|R
40|$|We propose Alternating-time Dynamic Logic (ADL) asa multi-agent {{variant of}} Dynamic Logic in which atomic {{programs}} {{are replaced by}} coalitions. In ADL, the Dynamic Logic operators are parametrised with regular expressions over coalitions and tests. Such regular expressions describe the dynamic structure of a coalition. This means that, when moving from Dynamic Logic to ADL, the focus shifts away from describing what is executed and when, toward describing who is acting and when. While Dynamic Logic provides for reasoning about complex programs, ADL facilitates reasoning about coalitions with an inner dynamic structure, socalled coordinated coalitions. The semantics for such coalitions involves partial strategies {{and a variety of}} ways to combine them. Different combinations of partial strategies give rise to different semantics for ADL. In this paper, we mainly focus on one version of the semantics but we provide a discussion on other semantic variants of ADL together with possible syntactic extensions. We see ADL to be suitable for the specification and the <b>verification</b> of <b>scheduling</b> and planning systems, and we therefore present a model checking algorithm for ADL and investigate its computational complexity. Categories and Subject Descriptor...|$|R
40|$|The {{purpose of}} this {{research}} is to know the implementation of SOP (Standard Operational Procedures) for birth certificate services at the Population and Civil Registration Board of Bengkulu City. The method of the data analysis was used is descriptive and qualitative analysis. The study shows that: (1) the preparation phase of the implementation of SOP services at the Population and Civil Registration Board has been executed well, where socialization SOP has been done properly, the SOP has been well implemented and equipment of the Ministry has been drafted and personals, has been shared with the right personnel, so as to facilitate the process of service to the community and (2) the implementation of SOP implementation the issuance of a birth certificate in the Population and Civil Registration Board of Bengkulu City from the stage of receiving and examining the file the petition for a birth certificate, file <b>verification</b> application <b>scheduled,</b> and collect files petition, enter data and print a birth certificate, check, legacy and sign, and submit the quote birth certificate to the community have been implemented in accordance with the stages of service assigned. Every employee involved in the birth certificate service has been implemented in accordance with the stages of service assigned...|$|R
40|$|This work {{addresses}} the modeling, <b>verification,</b> planning and <b>scheduling</b> problems of non-cycle discrete systems {{with emphasis on}} Flexible Manufacturing Systems. We introduce a special type of Petri nets, the Conflict-Free nets with Input and Output transitions (CFIO nets) that provide a much needed platform on which all these problems can be tackled in a unified manner. It is shown that CFIO nets are live, reversible, if consistent, and can be kept bounded under certain conditions. We develop reduction rules which facilitate the detection of the above properties. We then {{take advantage of the}} qualitative properties of CFIO nets and use those models to develop a linear programming formulation of the production planning problem. Finally we use the CFIO nets, along with the production planning results, to develop the production schedule of the Flexible Manufacturing system...|$|R
40|$|Abstract—General-purpose many-core network {{processors}} {{have been}} widely used in network packet processing due to its high programmability and parallel processing ability. The design of general-purpose many-core network processors involves a lot of key technologies, including packet scheduling, inter-core communication, co-processing, etc. The verification of these technologies is essential before applied to the system. However, there are some of limitations in the current software-based verification platform, such as low simulation speed and fidelity. A FPGA-based verification platform for general-purpose many-core network processors (VeriNP) is proposed in this paper. The VeriNP supports verification of network processor for at least 16 real RISC CPU cores, and the frequent of cores can run at 100 MHz or higher. Based on the VeriNP platform, two packet scheduling algorithms are studied and verified. Keywords—general-purpose many-core; network processor; <b>verification</b> platform; packet <b>scheduling</b> algorithm; FPGA I...|$|R
40|$|Abstract. SMT {{stands for}} Satisfiability Modulo Theories. An SMT solver decides the satisfiability of propositionally complex {{formulas}} in theories such as arithmetic and uninterpreted functions with equality. SMT solving has numerous applications in automated theorem proving, in {{hardware and software}} <b>verification,</b> and in <b>scheduling</b> and planning problems. This paper describes Yices, an efficient SMT solver developed at SRI International. Yices supports a rich combination of first-order theories that occur frequently in software and hardware modeling: arithmetic, uninterpreted functions, bit vectors, arrays, recursive datatypes, and more. Beyond pure SMT solving, Yices can solve weighted MAX-SMT problems, compute unsatisfiable cores, and construct models. Yices is the main decision procedure used by the SAL model checking environment, and it is being integrated to the PVS theorem prover. As a MAX-SMT solver, Yices is {{the main component of}} the probabilistic consistency engine used in SRI’s CALO system. ...|$|R
40|$|In this paper, {{existing}} {{state machine}} and dataflow models of computation are revisited. A common representation called SCF is presented {{that enables the}} representation of several dataflow models using a mixture of functional programming and state machines. In particular, models like cyclostatic dataflow [3, 8], Synchronous Data Flow [14, 15], marked graphs [7] and communicating state machines [11, 18, 1] as well Petri nets {{turn out to be}} special subclasses of the SCF model. The model can be extended with information on the timing of computations and timing constraints. In addition, as abstraction and refinement are defined, the new model supports a hierarchical approach to problems like <b>scheduling,</b> <b>verification</b> and implementation. ETH Zurich, Computer Engineering and Networks Laboratory, CH- 8092 Zurich, email: fthiele, teich, naedele, strehlg@tik. ee. ethz. ch and Technische Universitaet Braunschweig, Institut fuer Datenverarbeitungsanlagen, D- 38106 Braunschweig, email: ziegenb [...] ...|$|R
40|$|Recent {{progress}} in the timing analysis of digital circuits has involved the use of maximum and minimum timing constraints. We put forward the theory of min-max functions, a non-linear generalization of "max-plus" algebra, as the correct framework in which to study problems with mixed constraints. We state several new results which enable us to (a) refine and extend the work of Burns on timing metrics for asynchronous circuits and (b) explain, {{on the basis of}} general theory, the observations of Szymanski and Shenoy on the <b>verification</b> of clock <b>schedules</b> for synchronous circuits. Visiting Scholar at the Department of Computer Science from Hewlett-Packard's Stanford Science Centre 1 Introduction In the last few years considerable {{progress has been made in}} the timing analysis of both asynchronous and synchronous digital circuits. Burns, in his thesis, [3], introduced a timing metric for asynchronous circuits arising from Martin's synthesis method, [7], and developed efficient methods [...] ...|$|R
40|$|Abstract: This work {{presents}} an Adaptively Secure Broadcast Mechanism (ASBM) based on threats analytics and case based reasoning. It defines the security intelligence of a broadcast system comprehensively {{with a novel}} concept of collective intelligence. The algorithmic mechanism is analyzed {{from the perspectives of}} security intelligence, communication complexity, computational intelligence and business intelligence. The computational intelligence is associated with the complexity of broadcast <b>scheduling,</b> <b>verification</b> of security intelligence of broadcasting system, key management strategies and payment function computation. The cost of communication depends on number of agents and subgroups in the broadcasting group and complexity of data. The business intelligence depends on payment function and quality of data stream. ASBM recommends a set of intelligent model checking moves for the verification of security intelligence of a broadcasting system. The primary objective of ASBM is {{to improve the quality of}} broadcast through fundamental rethinking and radical redesign of a reliable communication schema...|$|R
40|$|In {{this paper}} we {{address the problem of}} synthesizing control {{programs}} for a distributed system controlling a batch production plant. We present a timed automata model of a production plant. This model aims at faithfully reflecting the level of abstraction required for synthesizing control programs, therefore it quickly becomes too detailed and complicated for automatic synthesis. To solve this problem we present a general way of adding guidance to a model by augmenting it with additional guidance variables and decorate transitions with extra guards. Applying this technique have made synthesis of control programs feasible for a plant producing as many as 35 batches. In comparison, we could only handle plants producing two batches without using guides. The synthesized control programs were executed in the plant. By doing so we found and corrected some errors in the model. Keywords: real-time <b>verification,</b> guided model-checking, <b>scheduling,</b> program synthesis, distributed systems. 1 Intro [...] ...|$|R
40|$|This paper {{looks at}} the {{independent}} verification and validation (IV&V) of NASA's Space Shuttle Day of Launch I-Load Update (DoLILU) project. IV&V is defined. The system's development life cycle is explained. Data collection and analysis are described. DoLILU Issue Tracking Reports (DITRs) authored by IV&V personnel are analyzed to determine the effectiveness of IV&V in finding errors before the code, testing, and integration phase of the software development life cycle. The study's findings are reported along with {{the limitations of the}} study and planned future research. 1 Keywords: IV&V, software, <b>verification,</b> validation, cost, <b>schedule,</b> quality II. Introduction Management in government, private industries, and academia have been concerned about software (s) development that is feasible, within cost estimates, and on schedule with a high quality product that fulfills the requirements. These concerns are well founded. A study of 8380 application development efforts performed by th [...] ...|$|R
40|$|There is a {{high demand}} for {{correctness}} for safety critical systems, often requiring the use of formal <b>verification.</b> Simple, well-understood <b>scheduling</b> strategies ease <b>verification</b> but are often very inefficient. In contrast, efficient concurrent schedulers are often complex and hard to reason about. This paper will show how the TLA logic {{can be used to}} verify schedulers of concurrent components. TLA allows us to prove that one program preserves the behaviour of another program, in particular that an efficient scheduling strategy preserves the behaviour of a simpler one. For an arbitrary program we can use the simpler scheduler for correctness verification, knowing that the properties also hold in the more efficient one, which we then implement. This approach is illustrated with the Hume programming language, which is based on concurrent rich automata. We show an efficient extension to the Hume scheduler, and prove that this extension preserves the behaviour of the standard Hume scheduler...|$|R
40|$|This paper {{describes}} {{an approach to}} using simulation and visualization of discrete event oriented simulation models for multi-criteria scheduling optimization with genetic algorithms. A simulation model was used for fitness function computation of genetic algorithm results, {{as well as for}} visual representation of process behavior of a chosen schedule following genetic algorithm optimization. In this way, with the help of simulation and visualization, an additional <b>verification</b> of a <b>schedule’s</b> suitability was conducted on optimization results. The described methodology provides the planner with a quick and efficient scheduling method and enables him/her to experiment and decide which of the suitable solutions will become the production plan. The scheduling system is composed of a business information system- a database, a discrete event simulation model and a scheduling algorithm. The purpose of the integrated system is to help operative management personnel with production scheduling and planning. By comparing various scheduling methods, we established that the system utilizing genetic algorithms and simulation yielded from 5 % to 15 % better scheduling within a shorter time compared to manual scheduling. 1...|$|R
