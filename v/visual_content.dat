1547|585|Public
5|$|Marilena from P7 also {{attracted}} the critics' attention {{due to its}} use of explicit verbal and <b>visual</b> <b>content.</b> Cristian Nemescu said {{that this kind of}} scenes were a necessarily element for the realism of the movie, declaring himself a partisan of explicit content.|$|E
5|$|The {{video was}} {{directed}} by British directing duo James Frost & Alex of The Artists Company. It was shot at 50 frames per second, twice the regular speed. At the shoot, Chris Martin had to sing the song at double speed so that the audio and <b>visual</b> <b>content</b> would be in sync, a common yet difficult practice of music videos. The final product is slowed to 25 frames per second, giving the slow motion effect of the video. The transition of the video from night to day was achieved during the telecine process. During the transfer from film to videotape, an operator manually adjusted the amount of lighting for blue in the beginning, red in the middle, and yellow {{at the end of}} the video.|$|E
25|$|The {{study of}} {{computer}} graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating <b>visual</b> <b>content.</b> Although the term often refers to three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing.|$|E
5000|$|Informatics and <b>Visual</b> <b>Contents</b> (joint study {{programme}} with Dongseo University, South Korea) ...|$|R
40|$|Image {{classification}} is {{an enthusiastic}} research field where {{large amount of}} image data is classified into various classes based on their <b>visual</b> <b>contents.</b> Researchers have presented various low-level features-based techniques for classifying images into different categories. However, efficient and effective classification and retrieval is still a challenging problem due to complex nature of <b>visual</b> <b>contents.</b> In addition, the traditional information retrieval techniques are vulnerable to security risks, making it easy for attackers to retrieve personal <b>visual</b> <b>contents</b> such as patients records and law enforcement agencies databases. Therefore, we propose a novel ontology-based framework using image steganography for secure image classification and information retrieval. The proposed framework uses domain-specific ontology for mapping the low-level image features to high-level concepts of ontologies which consequently results in efficient classification. Furthermore, the proposed method utilizes image steganography for hiding the image semantics as a secret message inside them, making the information retrieval process secure from third parties. The proposed framework minimizes the computational complexity of traditional techniques, increasing its suitability for secure and real-time <b>visual</b> <b>contents</b> retrieval from personalized image databases. Experimental results confirm the efficiency, effectiveness, and security of the proposed framework as compared with other state-of-the-art systems. Comment: A short paper of 11 pages for secure <b>visual</b> <b>contents</b> retrieval. The original version can be accessed at this link: [URL]...|$|R
50|$|Guilford's {{original}} {{model was}} composed of 120 components (when the behavioral component is included) because he had not separated Figural Content into separate Auditory and <b>Visual</b> <b>contents,</b> nor had he separated Memory into Memory Recording and Memory Retention. When he separated Figural into Auditory and <b>Visual</b> <b>contents,</b> his model increased to 5 x 5 x 6 = 150 categories. When Guilford separated the memory functions, his model finally increased to 180 factors.|$|R
25|$|Open source {{software}} tools deploy machine learning, statistics, and {{natural language processing}} techniques to automate sentiment analysis on large collections of texts, including web pages, online news, internet discussion groups, online reviews, web blogs, and social media. Knowledge-based systems, on the other hand, make use of publicly available resources, to extract the semantic and affective information associated with natural language concepts. Sentiment analysis can also be performed on <b>visual</b> <b>content,</b> i.e., images and videos. One of the first approach in this direction is SentiBank utilizing an adjective noun pair representation of <b>visual</b> <b>content.</b> In addition, {{the vast majority of}} sentiment classification approaches rely on the bag-of-words model, which disregards context, grammar and even word order. Approaches that analyses the sentiment based on how words compose the meaning of longer phrases have shown better result, but they incur an additional annotation overhead.|$|E
25|$|The {{front-end}} (or client) {{component of}} a commercial, modern MMORPG features 3D graphics. As with other modern 3D games, the front-end requires expertise with implementing 3D engines, real-time shader techniques and physics simulation. The actual <b>visual</b> <b>content</b> (areas, creatures, characters, weapons, spaceships and so forth) is developed by artists who typically begin with two-dimensional concept art, and later convert these concepts into animated 3D scenes, models and texture maps.|$|E
25|$|Red Bank Road {{is at the}} Northern end of {{the world-famous}} Blackpool Illuminations. The area at Bispham Cliffs {{contains}} the famous tableaux displays, {{where there is a}} pathway for holidaymakers and locals to view the tableaux close up. The first animated tableaux were erected in 1932 running along the cliffs from North Shore to Bispham, and the Illuminations were extended to its current length running from Starr Gate to Red Bank Road at Bispham. Some of the tableaux have sound and <b>visual</b> <b>content</b> that can only be viewed and heard by walking by them.|$|E
30|$|Eventually, {{we include}} in the Appendix {{additional}} {{information that can be}} useful to perform a deeper analysis on the available <b>visual</b> <b>contents.</b>|$|R
40|$|Abstract — In Content-Based Image Retrieval (CBIR) systems, the <b>visual</b> <b>contents</b> of {{the images}} in the {{database}} are took out and represented by multi-dimensional characteristic vectors. A well known CBIR system that retrieves images by unsupervised method known as cluster based image retrieval system. For enhancing the performance and retrieval rate of CBIR system, we fuse the <b>visual</b> <b>contents</b> of an image. Recently, we developed two cluster-based CBIR systems by fusing the scores of two <b>visual</b> <b>contents</b> of an image. In this paper, we analyzed {{the performance of the}} two recommended CBIR systems at different levels of precision using images of varying sizes and resolutions. We also compared the performance of the recommended systems with that of the other two existing CBIR systems namely UFM and CLUE. Experimentally, we find that the recommended systems outperform the other two existing systems and one recommended system also comparatively performed better in every resolution of image...|$|R
40|$|Certain simple {{thoughts}} about pictures {{suggest that the}} contents of pictures are closely bound to vision. But how far can the striking features of depiction be accounted for merely {{in terms of the}} especially <b>visual</b> <b>contents</b> which belong to pictures, without considering, for example, any issues concerning the nature of the visual experiences with which pictures provide us? This article addresses that question by providing an account of the distinctively <b>visual</b> <b>contents</b> belonging to pictures, and by using that account to explain many notable general facts about depiction. Some implications of the resulting framework for the main stream of current theorizing about pictorial representation are also discusse...|$|R
25|$|Straight {{delay is}} used in sound {{reinforcement}} systems, a straight delay is used {{to compensate for the}} passage of sound through the air. Unlike audio delay effects devices, straight delay is not mixed back in with the original signal. The delayed signal alone is sent to loudspeakers so that the speakers distant from the stage, as in a large outdoor rock festival, will reinforce the stage sound at the same time or slightly later than the acoustic sound from the stage. The delayed signal uses approximately 1 millisecond of straight delay per foot of air or 3 milliseconds per meter, depending on the air temperature's effect on the speed of sound. Because of the Haas effect, this technique allows audio engineers to use additional speaker systems placed away from the stage and still give the illusion that all sound originates from the stage. The purpose is to deliver sufficient sound volume {{to the back of the}} venue without resorting to excessive sound volumes near the front. Straight delay is also used in audio to video synchronization to align sound with visual media (e.g., on TV or web broadcasting), if the visual source is delayed. Visual media can become delayed by a number of mechanisms or reasons, in which case the associated audio should be delayed to match the <b>visual</b> <b>content.</b>|$|E
500|$|She {{claims to}} rise before 6am, plays tennis and has {{her hair and}} makeup done, then gets to Vogues offices two hours later. She always arrives at fashion shows well before their {{scheduled}} start. [...] "I use the waiting time to make phone calls and notes; I get {{some of my best}} ideas at the shows," [...] she says. According to the BBC documentary series Boss Woman, she rarely stays at parties for more than 20minutes at a time and goes to bed by 10:15 every night. She exerts a great deal of control over the magazine's <b>visual</b> <b>content.</b> Since her first days as editor, she has required that photographers not begin until she has approved Polaroids of the setup and clothing. Afterwards, they must submit all their work to the magazine, not just their personal choices.|$|E
500|$|This {{painting}} exemplifies Lichtenstein's {{use of the}} background/foreground {{shift and}} ironic colloquialisms in critical commands. Although most of Lichtenstein's war imagery depicts American war themes, this depicts [...] "a scarred German submarine captain at a battle station". The manner of depiction with the commander's face pressed against the periscope reflects fusions of industrial art of the 1920s and 1930s. [...] The ironic aspect of this in 1963 is {{in part due to}} its temporal displacement referring back to World War II during the much later period of the Cold War. The styling of the balloon content, especially that of the large font characters, is complemented by or complementary to the other traditional <b>visual</b> <b>content</b> of the painting. Lichtenstein's alterations heightened the sense of urgency in the image, however, they also offset that menace by forming a detached work. A November 1963 Art Magazine review stated that {{this was one of the}} [...] "broad and powerful paintings" [...] of the 1963 exhibition at Castelli's Gallery.|$|E
30|$|Automatic image {{annotation}} is {{a challenging}} work of tasks related to understanding {{what we see}} in a visual scene due to the well-known semantic gap [1]. Given an input image, the goal of image annotation is to assign meaningful tags to the image aiming at summarizing its <b>visual</b> <b>contents.</b> Such methods {{are becoming more and more}} important given the growing collections of both private and publicly available images. However, challenges for these methods often lie in three aspects: the inter-tag similarity problem that different tags may have similar <b>visual</b> <b>contents,</b> the tag locality problem that most tags are only related to their corresponding semantic regions, and the intra-tag diversity problem that the relevant regions for each tag at different images can be different.|$|R
50|$|Computer {{graphics}} is {{the study}} of digital <b>visual</b> <b>contents,</b> and involves synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.|$|R
40|$|Spectral and {{polarization}} sensitivities of blowfly R 1 - 6 photoreceptor {{cells were}} measured by intracellular recordings in cells which differed in <b>visual</b> pigment <b>content.</b> The spectral sensitivity in the visible wavelength range can be quantitatively explained from the absorption spectrum of blowfly visual pigment when the waveguide {{properties of the}} rhabdomere are taken into account. The peak wavelengths of absorption and sensitivity spectrum are 490 and 487 nm respectively. At low <b>visual</b> pigment <b>content</b> a sensitizing pigment can enhance the relative U. V. sensitivity from 0. 3 to 2. 0. In flies with a high <b>visual</b> pigment <b>content</b> selfscreening substantially broadens the visible band of the spectral sensitivity and lowers the relative U. V. sensitivity. The gain of the electrical response appears to be independent of the <b>visual</b> pigment <b>content.</b> ...|$|R
500|$|Elements of {{the music}} video for [...] "We Found Love" [...] have been {{compared}} to the works of Britney Spears, Eminem and Madonna. The scene in which Rihanna and O'Shaughnessy blow marijuana exhalations into each other's mouths is stylistically reminiscent of the cover artwork on English musician Tricky's 2001 album Blowback. Rihanna's video was compared to the video for Spears' song [...] "Everytime" [...] (2004). Critics noted that Rihanna lay in a bathtub and submerged her head under the water, which {{can be interpreted as}} a failed suicide attempt. In [...] "Everytime", Spears is shown as a successful singer constantly bothered by the media and paparazzi about her turbulent relationship with her boyfriend. In [...] "Everytime"s video, Spears drowns in a bathtub {{as a result of a}} concussion. Rihanna's video also bears resemblances to that for Eminem and Rihanna's [...] "Love the Way You Lie" [...] (2010); both contain lyrical and <b>visual</b> <b>content</b> about a doomed relationship and scenes of graphic violence and substance abuse. Like [...] "We Found Love", [...] "Love the Way You Lie" [...] displays scenes of a couple in various stages of undress and intimacy. [...] "We Found Love"s video includes stylistic references to Madonna's video for [...] "Ray of Light" [...] (1998). James Montgomery of MTV News noted that [...] "Ray of Light"s video features [...] "a whole lot of high-speed, time-lapse shots taken in cities around the world", the same technique used for the drug scenes in [...] "We Found Love".|$|E
2500|$|... the {{sub-field}} {{of computer}} science which studies methods for digitally synthesizing and manipulating <b>visual</b> <b>content,</b> see study of computer graphics ...|$|E
2500|$|The imagery {{was also}} {{controversial}} {{due to its}} <b>visual</b> <b>content,</b> which featured the bust of an armless, yet lifelike, mannequin. According to [...] The Telegraph's culture critic Lucy Jones, the image depicted violence against women. In her article, which has been since removed from the Telegraph’s website, Jones applauded the censorship of Musilek’s photograph, arguing that “aking down a poster that could plant a seed in one person’s head that violence is acceptable is commendable.” ...|$|E
5000|$|First, the <b>visual</b> field <b>content</b> {{is unknown}} before {{perception}} occurred ...|$|R
5000|$|The Awards for <b>Visual</b> Digital <b>Content</b> Creation Tools {{and their}} Impact: ...|$|R
30|$|In this paper, {{we focus}} on video {{programs}} that are intended to disseminate information and knowledge such as news, documentaries, seminars, etc, and present an audiovisual summarization system that summarizes the audio and <b>visual</b> <b>contents</b> of the given video separately, and then integrating the two summaries with a partial alignment. The audio summary is created by selecting spoken sentences that best present the main content of the audio speech while the visual summary is created by eliminating duplicates/redundancies and preserving visually rich contents in the image stream. The alignment operation aims to synchronize each spoken sentence in the audio summary with its corresponding speaker′s face and to preserve the rich <b>content</b> in the <b>visual</b> summary. A Bipartite Graph-based audiovisual alignment algorithm is developed to efficiently find the best alignment solution that satisfies these alignment requirements. With the proposed system, we strive to produce a video summary that: (1) provides a natural <b>visual</b> and audio <b>content</b> overview, and (2) maximizes the coverage for both audio and <b>visual</b> <b>contents</b> of the original video without having to sacrifice either of them.|$|R
2500|$|Databases of <b>visual</b> <b>content</b> and promotional {{documentation}} {{became available}} on DVD formats and online through personal websites and through large databases, {{such as the}} [...] "Prelinger Archives" [...] on Archive.org. Many VJs began releasing digital video loop sets on various websites under Public Domain or Creative Commons licensing for other VJs to use in their mixes, such as Tom Bassford's [...] "Design of Signage" [...] collection (2006), Analog Recycling's [...] "79 VJ Loops" [...] (2006), VJzoo's [...] "Vintage Fairlight Clips" [...] (2007) and Mo Selle's [...] "57 V.2" [...] (2007).|$|E
2500|$|Many tools {{have been}} {{developed}} to visualize data. Computer generated imagery can be categorized into several different types: two dimensional (2D), three dimensional (3D), and animated graphics. As technology has improved, 3D computer graphics have become more common, but 2D computer graphics are still widely used. Computer graphics has emerged as a sub-field of computer science which studies methods for digitally synthesizing and manipulating <b>visual</b> <b>content.</b> Over the past decade, other specialized fields {{have been developed}} like information visualization, and scientific visualization more concerned with [...] "the visualization of three dimensional phenomena (architectural, meteorological, medical, biological, etc.), where the emphasis is on realistic renderings of volumes, surfaces, illumination sources, and so forth, perhaps with a dynamic (time) component".|$|E
2500|$|Pinterest Analytics is {{much like}} Google Analytics. It is a created service that generates {{comprehensive}} statistics on a specific website's traffic, commonly used by marketers. Pins, pinners, repins, and repinners are some aspects of user data that Pinterest Analytics provides. It also collects data that depicts the percentage of change within a specific time, {{to determine if a}} product is more popular on a specific day during the week, or slowly becoming unpopular. This data helps marketing agencies alter their strategies to gain more popularity, often changing the <b>visual</b> <b>content</b> to appeal to the Pinterest community. The [...] "Most Clicked" [...] tab in Pinterest Analytics demonstrates products that are more likely to sell. According to a study by Converto, in April 2012, Pinterest drove more social media-originated e-commerce sales than Facebook or Twitter.|$|E
50|$|Initially {{there was}} a {{broadcast}} in Malayalam for just 70 minutes daily from 1830 hrs to 1910 hrs. The first ever news bulletin Malayalam (വാർത്തകൾ) was aired live at 1900 hrs on 2 January 1985. This live bulletin was produced by Mr.T.Chamiyar and presented by Mr.G.R.Kannan. The text and <b>visual</b> <b>contents</b> were edited by Mr.Baiju Chandran, Mr.A.Anwar and Mr.P.K.Mohanan.|$|R
40|$|We {{introduce}} a video search system using multi-modal and clustering analysis. In this system, users can make queries by keywords or by <b>visual</b> <b>contents</b> such as frames or rectangle regions within a frame, the retrieved results are then organized into clusters and only representative frames of these clusters are shown to users. Experiments on TRECVID datasets showed promising results. Categories and Subject Descriptor...|$|R
50|$|Trinca was {{launched}} by Doncel, an editor owned by Young Front Frente de Juventudes an organization from Franquist Regime for young people. Although this origin, editors kept {{an open mind}} about <b>visual</b> <b>contents,</b> and support innovation in graphics, and illustration. The founder of the magazine was Esteban Maroto and its publisher was Alma de Dragón. The magazine was published on a weekly basis.|$|R
2500|$|Weinstein co-manages {{the sister}} site, Insane Films, which {{features}} personal video blogs of Madge's alter ego, Richard Bluestein, {{as well as}} features by underground filmmakers and miscellaneous <b>visual</b> <b>content.</b> Insane Films {{is one of the}} earliest videoblogs ever created, originating sometime in 2000. However, Madge Weinstein's fame grew largely from her indie podcast Yeast Radio, which began in late November 2004 [...] Weinstein gained notoriety for her viral video, Cooking With Madge: Lesbians On Acid, which featured a disheveled Madge and her unstable, ambiguously gendered friends attempting to cook [...] "les beans" [...] while high on LSD. A pioneer of the podcasting medium, Adam Curry, helped promote the show in its early life. In late April 2005, Weinstein created Yeast2, a free-form experimental channel in which [...] "anyone (could) contribute to the strange, ugly, pretty macabra [...] and surreal." [...] Yeast2 closed in 2008.|$|E
2500|$|Richard Strauss's operatic {{adaptation}} of the play also features the Dance of the Seven Veils. The dance remains unnamed except in the acting notes, but Salome's sexual fascination with John seems to motivate the request—though Herod is portrayed as pleased. The music for the dance comes from near {{the climax of the}} opera. The <b>visual</b> <b>content</b> of that scene (about seven minutes long with standard tempi) has varied greatly depending on the aesthetic notions of the stage director, choreographer, and soprano, and on the choreographic skills and body shape of that singer. Strauss himself stipulated that the dance should be [...] "thoroughly decent, as if it were being done on a prayer mat." [...] Nevertheless, many productions made the dance explicitly erotic. In a 1907 production in New York [...] the dancer [...] "spared the audience nothing in active and suggestive detail", {{to such an extent that}} some ladies in the audience [...] "covered their eyes with their programs." ...|$|E
2500|$|Montreal based {{multimedia}} company Moment Factory {{joined the}} tour, {{this being the}} third collaboration with Madonna, following the Super Bowl XLVI halftime show and The MDNA Tour. They worked closely with King and Madonna to develop new backdrops and content, designing and producing the videos for the show. The three large screens were used to give the theatrical effects and <b>visual</b> <b>content</b> on a large scale. The opening video was shot with boxer Mike Tyson, who is a guest vocalist on the Rebel Heart song [...] "Iconic". He clarified that the video might be received negatively since the scenes filmed involved him as a savage character, naked in a cage, held as a hostage. A clip of the video was previewed by the singer before the tour started, showing her pushing against the cage, embracing a shirtless man and a troupe of soldiers walking with insignia. The singer's official website unveiled a contest where fans submitted their online fan-arts related to her, and they were displayed as a live digital gallery, during the performance of [...] "Rebel Heart".|$|E
30|$|To {{tackle the}} tag {{locality}} problem, one may employ local image features instead of holistic image features {{to describe the}} <b>visual</b> <b>contents</b> of a certain tag. The work in [7] considered each image as a bag of multiple segmented regions and predicted the tag of each region by a multiclass bag classifier. This method, however, heavily depends on the segmentation performance, which is very sensitive to the image noise. Recently, implicit image representations attract much attention on describing local regions. To reveal the tag locality, Bao et al. [8] introduced hidden concepts for decomposing holistic image representation into tag representations. Mesnil et al. [9] learned implicit representations for both the objects and their parts. Although these representations cannot explicitly describe the regions of a certain tag, they implicitly capture the tag’s local <b>visual</b> <b>contents</b> by learning from large amount of annotated images. Thus, implicit image representation is nontrivial for tackling the tag locality problem in large-scale datasets.|$|R
40|$|A popular {{approach}} to semantic image understanding is to manually tag images with keywords and then learn a mapping from vi- sual features to keywords. Manually tagging images is a subjective pro- cess {{and the same}} or very similar <b>visual</b> <b>contents</b> are often tagged with different keywords. Furthermore, not all tags have the same descriptive power for <b>visual</b> <b>contents</b> and large vocabulary available from natural language {{could result in a}} very diverse set of keywords. In this paper, we propose an unsupervised visual theme discovery framework as a better (more compact, efficient and effective) alternative to semantic represen- tation of <b>visual</b> <b>contents.</b> We first show that tag based annotation lacks consistency and compactness for describing visually similar contents. We then learn the visual similarity between tags based on the visual features of the images containing the tags. At the same time, we use a natural language processing technique (word embedding) to measure the seman- tic similarity between tags. Finally, we cluster tags into visual themes based on their visual similarity and semantic similarity measures using a spectral clustering algorithm. We conduct user studies to evaluate the effectiveness and rationality of the visual themes discovered by our unsu- pervised algorithm and obtains promising result. We then design three common computer vision tasks, example based image search, keyword based image search and image labelling to explore potential applica- tion of our visual themes discovery framework. In experiments, visual themes significantly outperforms tags on semantic image understand- ing and achieve state-of-art performance in all three tasks. This again demonstrate the effectiveness and versatility of proposed framework...|$|R
50|$|Storytelling {{and native}} advertisingWith {{companies}} growing into their roster of social media accounts, {{the flood of}} branded content has intensified the contest for viewership {{at the same time}} that attention spans have diminished. The challenge for brands in 2014 is to relate authentically to consumers, which means a heavier reliance on eye-catching <b>visuals,</b> <b>content</b> that informs and entertains the viewer, and behind-the-scenes accounts that humanize the brand.|$|R
