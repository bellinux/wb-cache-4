0|14|Public
30|$|It is {{well known}} that stereo images {{acquired}} by frame camera(s) have epipolar geometry, that is, there exist epipolar lines and the epipolar pairs on each line throughout the space. However, it has been shown [2] that the epipolar geometry of linear pushbroom camera images are not lines but hyperbola-like non-linear curves because of the non-coplanarity of line of <b>sight</b> <b>vectors.</b> Therefore, an epipolar curve for the entire image can only be approximated incrementally in linear segments, and epipolar pairs exist only locally.|$|R
40|$|Desert ants, Cataglyphis fortis, {{associate}} nestward-directed vector memories (local <b>vectors)</b> {{with the}} <b>sight</b> of landmarks along a familiar route. This view-based navigational strategy works in {{parallel to the}} self-centred path integration system. In the present study we ask at what temporal stage during a foraging journey does the ant acquire nestward-directed local vector information from feeder-associated landmarks: during its outbound run to a feeding site or during its homebound run to the nest. Tests performed after two reversed-image training paradigms revealed that the ants associated such vectors exclusively with landmarks present during their homebound runs...|$|R
5000|$|Greg Raleigh, V. K. Jones, and Michael Pollack founded Clarity Wireless in 1996. The company built a {{prototype}} MIMO-OFDM fixed wireless link running 100 Mbit/s in 20 MHz of spectrum in the 5.8 GHz band, and demonstrated error-free operation over six miles with one watt of transmit power. Cisco Systems acquired Clarity Wireless in 1998 for its non-line of <b>sight,</b> <b>vector</b> OFDM (VOFDM) technology. [...] The Broadband Wireless Industry Forum (BWIF) {{was created in}} 1999 to develop a VOFDM standard.Arogyaswami Paulraj founded Iospan Wireless in late 1998 to develop MIMO-OFDM products. Iospan was acquired by Intel in 2003. Neither Clarity Wireless nor Iospan Wireless shipped MIMO-OFDM products before being acquired.|$|R
40|$|In {{this paper}} {{we have shown}} how the {{direction}} cosine method of stripmap-mode IFSAR maybe modified {{for use in the}} spotlight-mode case. Spotlight-mode IFSAR geometry dictates a common aperture phase center, velocity vector, and baseline vector for every pixel in an image. Angle with respect to the velocity vector is the same for every pixel in a given column and can be computed from the column index, the Doppler of the motion compensation point and the Doppler column sample spacing used in image formation. With these modifications, the direction cosines and length of the line of <b>sight</b> <b>vector</b> to every scatterer in the scene may be computed directly from the raw radar measurements of range, Doppler, and interferometric phase...|$|R
40|$|The {{objective}} {{of this paper is}} to develop a robust and efficient approach for relative navigation and at-titude estimation of spacecraft flying in formation. The approach developed here uses information from a new optical sensor that provides a line of <b>sight</b> <b>vector</b> from the master spacecraft to the secondary satel-lite. The overall system provides a novel, reliable, and autonomous relative navigation and attitude determi-nation system, employing relatively simple electronic circuits with modest digital signal processing require-ments and is fully independent of any external systems. State estimation is achieved through an optimal ob-server design, which is analyzed using a Lyapunov and contraction mapping approach. Simulation results in-dicate that the combined sensor/estimator approach provides accurate relative position and attitude esti-mates...|$|R
50|$|Once the ground, orbital and {{topographic}} contributions {{have been}} removed the interferogram contains the deformation signal, along with any remaining noise (see Difficulties below). The signal measured in the interferogram represents the change in phase caused by an increase or decrease in distance from the ground pixel to the satellite, therefore only the component of the ground motion parallel to the satellite line of <b>sight</b> <b>vector</b> will cause a phase difference to be observed. For sensors like ERS with a small incidence angle this measures vertical motion well, but is insensitive to horizontal motion perpendicular {{to the line of}} sight (approximately north-south). It also means that vertical motion and components of horizontal motion parallel to the plane of the line of sight (approximately east-west) cannot be separately resolved.|$|R
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references (leaves 133 - 136). Issued also on microfiche from Lange Micrographics. The objective of this thesis is to develop an efficient and robust approach for relative navigation and attitude estimation of spacecraft flying in formation. The approach developed here uses information from a new optical sensor that provides a line of <b>sight</b> <b>vector</b> from the master spacecraft to the secondary satellite. The overall system provides a novel, reliable, and autonomous relative navigation and attitude determination system, employing relatively simple electronic circuits with modest digital signal processing requirements and is fully independent of any external systems. State estimation is achieved using a Lyapunov and contraction mapping approach...|$|R
40|$|Abstract—Method for 3 D {{rendering}} {{based on}} intersection image display which allows representation of internal structure is proposed. The proposed method is essentially {{different from the}} conventional volume rendering based on solid model which allows representation of just surface of the 3 D objects. By using afterimage, internal structure can be displayed through exchanging the intersection images with internal structure for the proposed method. Through experiments with CT scan images, the proposed method is validated. Also one of other applicable areas of the proposed for design of 3 D pattern of Large Scale Integrated Circuit: LSI is introduced. Layered patterns of LSI can be displayed and switched by using human eyes only. It is confirmed that the time required for displaying layer pattern and switching the pattern to the other layer by using human eyes only is much faster than that using hands and fingers. Keywords— 3 D object display; volume rendering; solid model; afterimage; line of <b>sight</b> <b>vector</b> I...|$|R
30|$|Computational stereo is in {{the fields}} of {{computer}} vision and photogrammetry. In the computational stereo and surface reconstruction paradigms, {{it is very important to}} achieve appropriate epipolar constraints during the camera-modeling step of the stereo image processing. It has been shown that the epipolar geometry of linear pushbroom imagery has a hyperbola-like shape because of the non-coplanarity of the line of <b>sight</b> <b>vectors.</b> Several studies have been conducted to generate resampled epipolar image pairs from linear pushbroom satellites images; however, the currently prevailing methods are limited by their pixel scales, skewed axis angles, or disproportionality between x-parallax disparities and height. In this paper, a practical and unified piecewise epipolar resampling method is proposed to generate stereo image pairs with zero y-parallax, a square pixel scale, and proportionality between x-parallax disparity and height. Furthermore, four criteria are suggested for performance evaluations of the prevailing methods, and experimental results of the method are presented based on the suggested criteria. The proposed method is shown to be equal to or an improvement upon the prevailing methods.|$|R
40|$|High resolution, Earth imaging {{missions}} place stringent {{requirements on}} the spacecraft Attitude Control System (ACS) {{in terms of}} pointing accuracy, attitude knowledge, and orbit knowledge. For OrbView- 4, a body scanner, the spacecraft must also be agile to maximize the number of targets that can be imaged each orbit. These requirements dictate a robust, high performance design that allows rapid target acquisition and high line scan rates for mapping while maintaining precise knowledge {{and control of the}} camera line of <b>sight</b> <b>vector.</b> The OrbView- 4 ACS combines state of the art components with robust algorithm design to meet these objectives. Equipment selection, attitude determination, attitude control, and GPS based onboard orbit determination are discussed as the elements of an ACS design for an advanced Earth imaging spacecraft. An overview of the OrbView- 4 mission is presented along with a discussion of the requirements imposed on the ACS. The system architecture is described in terms of its hardware and software elements. The design of the attitude determination, orbit determination and attitude control algorithms are discussed, and the results of a system performance analysis are summarized...|$|R
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references (leaves 184 - 187). Issued also on microfiche from Lange Micrographics. The major technological obstacle to be overcome for practical and reliable autonomous probe-and-drogue aerial refueling is obtaining accurate relative position and attitude measurements during the docking phase. An integrated controller-sensor-navigation system for this task must be robust and possess good disturbance rejection properties. Previous attempts to solve this problem have used video servoing with pattern recognition algorithms and the differential Global Positioning System. This thesis seeks to determine the feasibility of autonomous aerial refueling by developing a robust docking controller and integrating it with the relative position and attitude measurements from a novel Vision-based Navigation (VisNav) sensor. VisNav accurately determines the line of <b>sight</b> <b>vector</b> between a positioning sensing diode and a target configured with multiple light emitting diode beacons. A study is conducted to determine the best number and placement of the beacons on the drogue and the best location to mount the sensor on an Unmanned Air Vehicle (UAV). Optimal Nonzero Set Point and optimal Command Generator Tracker controllers are developed and used to simulate six degree-of-freedom docking maneuvers using dynamical system models of a UAV and a refueling drogue. Test cases for stationary and moving drogues in atmospheric turbulence are evaluated in terms of docking position errors, control effort, control rate, and quadratic cost. Simulation results demonstrate that a Proportional Integral Filter Command Generator Tracker controller, coupled with the VisNav sensor and navigation system, provides a viable candidate solution to the autonomous aerial refueling problem. The beacon lights can be placed in the location of lights currently on the drogue, and the sensor can be placed {{at the base of the}} refueling probe on the UAV...|$|R
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references (leaves 116 - 119). Issued also on microfiche from Lange Micrographics. A novel split field of view star tracker is being developed for the EO- 3 GIFTS mission (2004). The camera is designed to be autonomously self-calibrating, and capable of a rapid/reliable solution of the lost in space problem as well as recursive attitude estimation. An efficient Kalman filter algorithm for the spacecraft attitude estimation problem has been developed. This algorithm makes use of three axis gyros for the rate data and star line of <b>sight</b> <b>vector</b> measurements derived from the split field of view star camera to estimate the spacecraft attitude and the gyro bias. An accidental gyro failure (e. g. failure of four of the six rate gyros on the Earth Radiation Budget Satellite) or intentional omission of gyros (e. g. in Small Explorer (SAMPEX)) due to their high cost can necessitate "gyroless" attitude estimation. Two different sequential algorithms have been developed for the spacecraft body angular rates estimation in the absence of gyro rate data for a star camera mission. Star camera calibration plays an important role for the case of the spacecraft attitude estimation problem. Two efficient Kalman filter algorithms for the autonomous on-orbit calibration of the star camera are developed. In the first algorithm, we estimate the attitude, principal point offset and the focal length with the help of two Kalman filters working in tandem. Occasionally, we find that the correlation of attitude and principal point offset can make this algorithm somewhat unstable. A novel algorithm has been developed to estimate the principal point offset and focal length by using measurements independent of attitude. The relative merits of the two algorithms are then studied for estimating the focal plane distortion and the attitude of a simulated spacecraft motion. Simulation results indicate that both algorithms produce precise attitude estimates by determining principal point offset, focal length and gyro bias or spacecraft angular velocity in case of gyro failure. However, reliability and robustness characteristics favor the second algorithm. An autonomous on-orbit calibration will not only make our calibration coefficients more robust to environmental disturbances but will also help to bring down the overall cost of the mission...|$|R
40|$|The use of {{autonomously}} underwater vehicles (AUVs) has a {{great potential}} in scientific mission involving underwater exploration. However a major drawback with todays AUV missions is the launch and recovery process which are usually performed manually from a manned supply ship. These manned ships have a huge daily operation cost, and because AUVs can have operation times up to 70 hours these missions become extremely costly. Since the combination of an AUV together with a manned mothership is very costly the use of AUVs are very restricted. A solution here is to replace the manned mothership with an unmanned vehicle such as a unmanned surface vehicle (USV). This will {{reduce the cost of}} AUV mission drastically and therefore increase the use of AUVs on scientific missions. This motivates the need for an AUV-USV docking method {{which is one of the}} two docking scenarios treated in this master thesis. Another docking method treated here is the possibility to dock a USV together with a manned mothership without human interference. A docking method that removes the human intervention will make the USV completely unmanned, since USVs today are manually docked together with a mothership or driven back to shore by a remote control. To achieve an understanding of the field, a summary of the most relevant findings in todays literature are given. This includes the possibility to autonomously dock together an AUV with another vehicle or installation, and other related fields such as spacecraft docking and aerial refuelling. The main findings involving AUV docking, ranges from a simple fuzzy logic procedure to more advanced methods involving trajectory planning and potential field guidance. Since no extensive previous work exist on general USV docking, a short introduction is given to the most related fields, such as spacecraft docking and aerial refuelling. During air refuelling two methods are summarised which includes racetrack pattern or waypoint paths, where the receiver aircraft has two different ways of rendezvousing with the tanker, namely point parallel- or route-rendezvous. In both docking scenarios treated here, rendezvous guidance is developed since the vehicles are assumed underactuated. The docking procedure is divided into two stages, a homing stage and a docking stage. In the homing stage only rough guidance is needed which is not the case during docking stage where requirements are much tighter on positioning to avoid collisions. In the AUV to USV homing stage the USV does all the work, but during docking stage the AUV has full responsibility, since the USV only traverses along a straight path. The USV's path is here orientated against the wind direction to minimise the sideslip effect caused due to weather disturbances. Once the USV has converged to a straight path the AUV proceeds to docking from behind the USV to finalise docking. For the USV to mothership docking scenario, the USV has the full responsibility during the whole docking procedure. Here the USV is underactuated, and therefore the mothership will be in motion and only has to avoid sudden manoeuvres. In the homing stage the USV will manoeuvre towards a point given on the line of <b>sight</b> <b>vector</b> between the two vehicles. Once the USV reaches this point it will steer along a circle around the mothership to avoid collisions and to position itself in clear sight of the docking point. With clear sight achieved the USV will use its forward motion to converge sideways towards the docking point, such that docking can be completed. Finally, simulations are carried out to verify the behaviour of the developed guidance laws. During these simulations two 3 DOFs underactuated USV models are being used, where both vehicles only has controllability over surge speed and yaw rate. In both docking scenarios the whole docking procedure is analysed including homing and docking stage. The simulation results shows a proper docking with a satisfying approach in both scenarios. Also the mothership's velocity is examined to understand the USV's sideway approach towards the mothership. </p...|$|R
40|$|We {{are happy}} to (finally) release v. 0. 7 of the Pupil Project! (Release notes for new {{features}} introduced in v 0. 7 and v 0. 7. 3. These notes serve as the offical 0. 7 release notes.) Contributors We want to thank everybody that has contributed! Be it in code, bug reports or feature request and feedback! Pupil {{would not be possible}} without you! New features Pupil Detection Ported the existing detector to C++ and improved its performance. The detector CPU load has be cut in half! v 0. 7 introduces our new 3 D model based pupil detector. We call it 3 D detector and renamed the old detector 2 D detector. 3 D Pupil Detector The old pupil detector and gaze mapper hits the limits of regression based calibration: It assumes the headset is "screwed to the head". When the headset moves the gaze mapping shits. This is usually referred to as drift. Additionally, a model-less search for the ellipse yields poor results when the pupil is partially obstructed by reflections or eyelashes and eyelids. With the use of a 3 D model of the eye and a pinhole model of the camera based on Swirski’s work in [“A fully-automatic, temporal approach to single camera, glint-free 3 D eye model fitting” PETMEI 2013] we model the eyeball as a sphere and the pupil as a disk on that sphere. The sphere size used is based on an average human eyeball diameter is 24 mm. The state of the model is the position of the sphere in eye camera space and two rotation vectors that describe the location of the pupil on the sphere. Using temporal constraints and competing eye models we can detect and compensate for slippage events when 2 D pupil evidence is strong. In case of weak 2 D evidence we can use constraints from existing models to robustly fit pupils with considerably less evidence than before. Using the 3 d model we can now compute some exciting and very useful properties from the eye video footage: 3 D pupil normal: The normal vector of the pupil in relation to the 3 D location of the eye camera. 3 D pupil diameter: Using the 3 D model we correct the pupil diameter for perspective scale and transform it to millimeter (mm) space using the assumption that the eye ball size has a diameter of 24 mm. 3 D sphere location: The location of the eyeball in relation to the eye camera. Augmented 2 D detector confidence: Using a strong 3 D model we have a better handle on false positives. This new 3 D pupil detector still has areas that we will continue to work on, but we feel that it already outperforms the 2 D detector and recommend giving it a try! 3 D Gaze Mapper & Calibration With a 3 D location of the eye and 3 D vectors of gaze we don’t have to rely on a generic polynomial or similar for gaze mapping. Instead we use a geometric gaze mapping approach: We model the world camera as a pinhole camera with distortion, and project pupil line of <b>sight</b> <b>vectors</b> onto the world image. For this we need to know the rotation translation of world and eye camera. This rigid transformation is obtained in a 5 point calibration routine. Under the hood we use a custom bundle adjustment routine. Since we are now detecting and modeling the location of the eyeball we are taking movement of the headset into account. This means that the 3 D model in combination with the geometric gaze mapper can compensate for slippage. You can take off the headset and put it back on without having to re-calibrate! We are still working on this feature, and you can expect this to become better over time. For robust 3 D pupil detection and fast slippage compensation we found that detecting the eye(s) at 90 Hz or greater is very helpful. Restructured Pupil Server & Updated pupil-helper scripts Pupil server now sends JSON data, that conforms to a standard format structured topic. Every subject in the events like pupil and gaze data is broadcast as a separate topic. Motivated by @zenithlight’s pull request: [URL] See updated pupil (and much easier to use) helper scripts: [URL] Introducing Plugin Notifications This scheme allows plugins to notify their in-app environment and (using pupil sync) other programs and machine. This simplifies developing more intertwined behaviour while keeping plugins isolated. Plugin notifications that carry the flag record are saved during recording and can be retrieved. Pupil Sync This is not a new feature but we added some cool new stuff! Added proper network time synchronization for multiple source and multi user synchronous recordings. Check out the updated pupil sync examples: [URL] Plugin notifications that carry the flag network propagate are shared with all other Pupil Sync nodes. Full Support for Binocular Operation Start and stop left and right eye during runtime. 2 D and 3 D gaze mappers support binocular mapping. Better Visual Feedback in Pupil Capture Pupil and Gaze confidence is represented as transparency in the gaze dot in the world window and the pupil dot in the eye window(s). More opaque dot = higher confidence. Changes to the Recording Format We have depreciated the pupil_positions. npy and gaze_postions. npy files and now record all events and notifications that happen during recording into the pupil_data file. pupil_data is a pickled python dictionary. Exporting Data We have restructured export logic in Pupil Player. Now you can press e or click the UI button e and all open plugins that have export capability will export. All exports are separated from your raw data and contained in the exports directory. The exports directory will live within your recording folder. (ref Rafael’s issue on separating raw data from exports and close) We have added an exporter in Pupil Player that will give you access to all raw data that was acquired during the recording. Bugfixes and Small improvements GLFW MacOS fixes for render issues and crashes Player GUI design is more consistent Various pyglui fixes and improvements Audio modes are settable Camera default values for 120 fps capture Fix bugs in libuvc and pyuvc that appeared when using Logitech C 525 /B 525 cameras. Fixed bug in Pupil Player that prevented exports when world. wav file was present. Licence Change: On 2016 - 12 - 15 we transitioned to a new license. Pupil is now licensed under LGPLv 3 We hope you find these new features useful and look forward to feedback! Best, The Pupil Dev Tea...|$|R

