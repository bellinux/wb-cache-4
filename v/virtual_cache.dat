29|56|Public
25|$|Virtual: For <b>virtual</b> <b>cache</b> types, {{as there}} is no {{physical}} container.|$|E
25|$|Virtual caches are {{coordinates}} for a location, {{which has}} some other described object. Validation for finding a <b>virtual</b> <b>cache</b> generally requires one to email the cache hider with {{information such as}} a date or a name on a plaque, or to post a picture of oneself at the site with GPS receiver in hand. New virtual caches are no longer allowed by Groundspeak, but they remain supported by other sites.|$|E
50|$|Virtual: For <b>virtual</b> <b>cache</b> types, {{as there}} is no {{physical}} container.|$|E
25|$|Earthcaches are <b>virtual</b> <b>caches</b> {{that are}} {{organized}} by the Geological Society of America. The cacher usually has to perform a task which teaches him/her an educational lesson about the earth science of the cache area.|$|R
40|$|In our {{quest to}} bring down the power {{consumption}} in low-power chip-multiprocessors, we have found that TLB and snoop accesses account for about 40 % of the energy wasted by all L 1 data-cache accesses. We have investigated the prospects of using <b>virtual</b> <b>caches</b> {{to bring down}} the number of TLB accesses. A key observation is that while the energy wasted in the TLBs are cut, the energy associated with snoop accesses becomes higher. We then contribute with two techniques {{to reduce the number of}} snoop accesses and their energy cost. <b>Virtual</b> <b>caches</b> together with the proposed techniques are shown to reduce the energy wasted in the L 1 caches and the TLBs by about 30 %...|$|R
25|$|Webcam <b>caches</b> are <b>virtual</b> <b>caches</b> whose {{coordinates}} have {{a public}} webcam. The finder is often required to capture their image from the webcam for verification of the find. New webcam caches are no longer allowed by Groundspeak, but they remain supported by other sites.|$|R
5000|$|... #Caption: A Geocacher {{finding a}} <b>Virtual</b> <b>Cache</b> at McMurdo Station, Antarctica ...|$|E
50|$|Virtual caches are {{coordinates}} for a location, {{which has}} some other described object. Validation for finding a <b>virtual</b> <b>cache</b> generally requires one to email the cache hider with {{information such as}} a date or a name on a plaque, or to post a picture of oneself at the site with GPS receiver in hand. New virtual caches are no longer allowed by Groundspeak, but they remain supported by other sites.|$|E
40|$|Abstract. Recursive data {{structures}} (lists, trees, graphs, etc.) {{are used}} throughout scientific and commercial software. The common {{approach is to}} allocate storage to the individual nodes of such structures dynamically, maintaining the logical connection between them via pointers. Once such a data structure goes through a sequence of updates (inserts and deletes), it may get scattered all over memory yielding poor spatial locality, which in turn introduces many cache misses. In this paper we present the new concept of <b>Virtual</b> <b>Cache</b> Lines (VCLs). Basically, the mechanism keeps groups of consecutive nodes in close proximity–forming <b>virtual</b> <b>cache</b> lines–while allowing the groups to be stored arbitrarily far away from each other. <b>Virtual</b> <b>cache</b> lines increase the spatial locality of the given data structure resulting in better locality of references. Furthermore, since the spatial locality is improved, software prefetching becomes much more attractive. Indeed, we also present a software prefetching algorithm {{that can be used}} when dealing with VCLs, resulting in even higher data cache performance. Our results show that the average performance of linked list operations–like scan, insert, and delete–can be improved by more than 200 % even in architectures that do not support prefetching. Moreover, when using prefetching, one can gain additional 100 % improvement. We believe that given a program that manipulates certain recursive data structures, compilers will be able to generate VCL-based code. Until this vision becomes true, VCLs can be used to build more efficient user libraries, operating-systems, and applications programs. ...|$|E
40|$|The StrongARM SA- 1100 is a {{high-speed}} low-power processor aimed at embedded and portable applications. Its architecture features <b>virtual</b> <b>caches</b> and TLBs {{which are not}} tagged by an address-space identifier. Consequently, context switches on that processor are potentially very expensive, as they may require complete flushes of TLBs and caches...|$|R
40|$|A <b>virtual</b> address <b>cache</b> memory, whose {{operation}} is controlled explicitly by software, is presented. Ad-hoc hardware mechanisms, including new machine instructions {{and a new}} operand addressing mode, reduce the complexity of cache management logic in favour of {{the capacity of the}} cache, and solve the major problem of <b>virtual</b> address <b>cache</b> organization: two or more virtual addresses mapping into the same real address...|$|R
40|$|Caches {{mitigate}} the long memory latency that limits {{the performance of}} modern processors. However, caches can be quite inefficient. On average, a cache block in a 2 MB L 2 cache is dead 59 % of the time, i. e., {{it will not be}} referenced again before it is evicted. Increasing cache efficiency can improve performance by reducing miss rate, or alternately, improve power and energy by allowing a smaller cache with the same miss rate. This paper proposes using predicted dead blocks to hold blocks evicted from other sets. When these evicted blocks are referenced again, the access can be satisfied from the other set, avoiding a costly access to main memory. The pool of predicted dead blocks {{can be thought of as}} a <b>virtual</b> victim <b>cache.</b> A <b>virtual</b> victim <b>cache</b> in a 16 -way set associative 2 MB L 2 cache reduces misses by 11. 7 %, yields an average speedup of 12. 5 % and improves cache efficiency by 15 % on average, where cache efficiency is defined as the average time during which cache blocks contain live information. This <b>virtual</b> victim <b>cache</b> yields a lower average miss rate than a fully-associative LRU cache of the same capacity. The <b>virtual</b> victim <b>cache</b> significantly reduces cache misses in multi-threaded workloads. For a 2 MB cache accessed simultaneously by four threads, the <b>virtual</b> victim <b>cache</b> reduces misses by 12. 9 % and increases cache efficiency by 16 % on average Alternately, a 1. 7 MB <b>virtual</b> victim <b>cache</b> achieves about the same performance as a larger 2 MB L 2 cache, reducing the number of SRAM cells required by 16 %, thus maintaining performance while reducing power and area. 1...|$|R
40|$|Because {{the spatial}} {{locality}} of numerical codes is significant, {{the potential for}} performance improvements is important. However, large cache lines cannot be used in current on-chip caches because of the important pollution they breed. In this paper, we propose a hardware design, called the Virtual Line Scheme, that allows the utilization of large <b>virtual</b> <b>cache</b> lines on when fetching data from memory for better exploitation of spatial locality, while the actual physical cache line is smaller than currently found cache lines for better exploitation of temporal locality. Simulations show that a 17 % to 64 % reduction of the average memory access time can be obtained for a 20 -cycle memory latency. It is also shown how simple software informations {{can be used to}} significantly decrease memory traffic, a flaw associated with the utilization of both large physical or large <b>virtual</b> <b>cache</b> lines. Keywords: cache architecture, spatial locality, temporal locality, numerical codes. 1 Introduction [...] ...|$|E
40|$|This paper {{proposes a}} {{cost-effective}} {{solution to the}} <b>virtual</b> <b>cache</b> synonym problem. In the proposed solution, a minimal hardware addition guarantees correct handling of the synonym problem whereas a simple modification to the virtual-to-physical address mapping in the operating system optimizes the performance. The key to the proposed solution is a small physically-indexed cache called a U-cache. The U-cache maintains the reverse translation information of cache blocks that belong to unaligned virtual pages, where unaligned means that the lower bits of the virtual page number {{that are used to}} index the <b>virtual</b> <b>cache</b> do not match those of the corresponding physical page number. The biggest advantage of the U-cache approach is that it leaves room for software optimization in the form of mapping alignment. Performance evaluation based on memory reference traces from a real system shows that the U-cache, with only a few entries, performs almost as well as (in some cases outperforms) a full [...] ...|$|E
40|$|If one is {{interested}} solely in processor speed, one must use virtually indexed caches. The traditional purported weakness of virtual caches is {{their inability to}} support shared memory. Many implementations of shared memory {{are at odds with}} virtual caches—ASID aliasing and virtual-address aliasing (techniques used to provide shared memory) can cause false cache misses and/or give rise to data inconsistencies in a <b>virtual</b> <b>cache,</b> but are necessary features of many virtual memory implementations. By appropriately using a segmented architecture one can solve these problems. In this tech report we describe a virtual memory system developed for a segmented microarchitecture and present the following benefits derived from such an organization: (a) the need to flush virtual caches can be eliminated, (b) <b>virtual</b> <b>cache</b> consistency management can be eliminated, (c) page table space requirements can be cut in half by eliminating the need to replicate page table entries for shared pages, and (d) the virtual memory system can be made less complex because it does not {{have to deal with the}} virtual-cache synonym problem...|$|E
25|$|Terracaching.com {{seeks to}} provide {{high-quality}} caches made {{so by the}} difficulty of the hide or from the quality of the location. Membership is managed through a sponsorship system, and each cache is under continual peer review from other members. Terracaching.com embraces <b>virtual</b> <b>caches</b> alongside traditional or multi-stage caches and includes many locationless caches among the thousands of caches in its database. It is increasingly attracting members who like the point system. In Europe, TerraCaching is supported by Terracaching.eu. This site is translated in different European languages, has an extended FAQ and extra supporting tools for TerraCaching. TerraCaching strongly discourages caches that are listed on other sites (so-called double-listing).|$|R
50|$|The I-box fetches and decodes VAX instructions. It also {{contains}} the 2 KB direct-mapped <b>virtual</b> instruction <b>cache</b> (VIC) and the 512-entry by 4-bit branch history table. The I-box aimed to fetch eight bytes of instruction {{data from the}} VIC during every cycle.|$|R
40|$|AbstractÐWe {{present a}} {{feasibility}} study for performing virtual address translation without specialized translation hardware. Removing address translation hardware and instead managing address translation in software {{has the potential}} to make the processor design simpler, smaller, and more energy-efficient at little or no cost in performance. The {{purpose of this study is}} to describe the design and quantify its performance impact. Trace-driven simulations show that software-managed address translation is just as efficient as hardware-managed address translation. Moreover, mechanisms to support such features as shared memory, superpages, fine-grained protection, and sparse address spaces can be defined completely in software, allowing for more flexibility than in hardware-defined mechanisms. Index TermsÐVirtual memory, virtual address translation, <b>virtual</b> <b>caches,</b> memory management, software-managed address translation, translation lookaside buffers. æ...|$|R
40|$|Conventional memory {{systems are}} {{organized}} as a rigid hierarchy, with multiple levels of progressively larger and slower memories. Hierarchy allows a simple, fixed design to benefit {{a wide range}} of applications, because working sets settle at the smallest (and fastest) level they fit in. However, rigid hierarchies also cause significant overheads, because each level adds latency and energy even when it does not capture the working set. In emerging systems with heterogeneous memory technologies such as stacked DRAM, these overheads often limit performance and efficiency. We propose Jenga, a reconfigurable cache hierarchy that avoids these pathologies and approaches the performance of a hierarchy optimized for each application. Jenga monitors application behavior and dynamically builds <b>virtual</b> <b>cache</b> hierarchies out of heterogeneous, distributed cache banks. Jenga uses simple hardware support and a novel software runtime to configure <b>virtual</b> <b>cache</b> hierarchies. On a 36 -core CMP with a 1 GB stacked-DRAM cache, Jenga outperforms a combination of state-of-the-art techniques by 10 % on average and by up to 36 %, and does so while saving energy, improving system-wide energy-delay product by 29 % on average and by up to 96 %...|$|E
40|$|Most modern cores {{perform a}} highly-associative {{translation}} look aside buffer (TLB) lookup on every memory access. These designs often hide the TLB lookup latency by overlapping it with L 1 cache access, but this overlap does not hide the power dissipated by TLB lookups. It can even exacerbate the power dissipation by requiring higher associativity L 1 cache. With today's concern for power dissipation, designs could instead adopt a virtual L 1 cache, wherein TLB access power is dissipated only after L 1 cache misses. Unfortunately, virtual caches have compatibility issues, such as supporting writeable synonyms and x 86 ’s physical page table walker. This work proposes an Opportunistic <b>Virtual</b> <b>Cache</b> (OVC) that exposes virtual caching as a dynamic optimization by allowing some memory blocks to be cached with virtual addresses {{and others with}} physical addresses. OVC relies on small OS changes to signal which pages can use virtual caching (e. g., no writeable synonyms), but defaults to physical caching for compatibility. We show OVC's promise with analysis that finds <b>virtual</b> <b>cache</b> problems exist, but are dynamically rare. We change 240 lines in Linux 2. 6. 28 to enable OVC. On experiments with Parsec and commercial workloads, the resulting system saves 94 - 99 % of TLB lookup energy and nearly 23 % of L 1 cache dynamic lookup energy. ...|$|E
40|$|Virtual caches have {{potentially}} lower access latency {{and energy}} consumption than physical caches due not to consulting the TLB prior to every cache access. However, {{they have not}} been popular in commercial designs. The crux of the problem is the possibility of synonyms. This paper makes several empirical observations about the temporal characteristics of synonyms, especially in caches of sizes that are typical of L 1 caches. By leveraging these observations, we propose a practical design of an L 1 <b>virtual</b> <b>cache</b> that (1) dynamically decides a unique virtual page for all the synonymous virtual pages that map to the same physical page and (2) uses this unique page to place and look up data in the virtual caches. Accesses with this unique page proceed without any intervention. Accesses to other synonymous pages are dynamically detected, and remapping to the corresponding unique virtual page is performed to correctly look up the data in the cache. Such remapping operations are rare, due to the temporal properties of synonyms. This new <b>Virtual</b> <b>Cache</b> with Dynamic Synonym Remapping (VC-DSR) can handle them in an energy and latency efficient manner, which achieves most of the benefits of virtual caches without software involvement. Experimental results based on real world applications show over 96 % dynamic energy savings for TLB lookups without significant impact on performance compared to the system with ideal virtual caches (less than 0. 4 % slowdown) ...|$|E
40|$|Data {{movement}} {{is a growing}} problem in modern chip-multiprocessors (CMPs). Processors spend {{the majority of their}} time, energy, and area moving data, not processing it. For example, a single main memory access takes hundreds of cycles and costs the energy of a thousand floating-point operations. Data movement consumes more than half the energy in current processors, and CMPs devote more than half their area to on-chip caches. Moreover, these costs are increasing as CMPs scale to larger core counts. Processors rely on the on-chip caches to limit data movement, but CMP cache design is challenging. For efficiency reasons, most cache capacity is shared among cores and distributed in banks throughout the chip. Distribution makes cores sensitive to data placement, since some cache banks can be accessed at lower latency and lower energy than others. Yet because applications require sufficient capacity to fit their working sets, {{it is not enough to}} just use the closest cache banks. Meanwhile, cores compete for scarce capacity, and the resulting interference, left unchecked, produces many unnecessary cache misses. This thesis presents novel architectural techniques that navigate these complex tradeoffs and reduce data movement. First, <b>virtual</b> <b>caches</b> spatially partition the shared cache banks to fit applications' working sets near where they are used. <b>Virtual</b> <b>caches</b> expose the distributed banks to software, and let the operating system schedule threads and their working sets to minimize data movement. Second, analytical replacement policies make better use of scarce cache capacity, reducing expensive main memory accesses: Talus eliminates performance cliffs by guaranteeing convex performance, and EVA uses planning theory to derive the optimal replacement metric under uncertainty. These policies improve performance and make qualitative contributions: Talus is cheap to predict, and so lets cache partitioning techniques (including <b>virtual</b> <b>caches)</b> work with high-performance cache replacement; and EVA shows that the conventional approach to practical cache replacement is sub-optimal. Designing CMP caches is difficult because architects face many options with many interacting factors. Unlike most prior caching work that employs best-effort heuristics, we reason about the tradeoffs through analytical models. This analytical approach lets us achieve the performance and efficiency of application-specific designs across a broad range of applications, while further providing a coherent theoretical framework to reason about data movement. Compared to a 64 -core CMP with a conventional cache design, these techniques improve end-to-end performance by up to 76 % and an average of 46 %, save 36 % of system energy and reduce cache area by 10 %, while adding small area, energy, and runtime overheads. by Nathan Beckmann. Thesis: Ph. D., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2015. Cataloged from PDF version of thesis. Includes bibliographical references (pages 167 - 176) ...|$|R
50|$|Key CPU {{architectural}} innovations include index register, <b>cache,</b> <b>virtual</b> memory, instruction pipelining, superscalar, CISC, RISC, virtual machine, emulators, microprogram, and stack.|$|R
40|$|A <b>virtual</b> arms <b>cache</b> at your fingertips. HackNotes Linux and Unix Security Portable Reference is a {{valuable}} reference for busy administrators and consultants who value the condensed and practical insight to understanding the threats they face and how to practically utilize tools to test the security of their environments. ...|$|R
30|$|Both q-LRU and k-LRU {{differ from}} LRU for the {{insertion}} policy, but are identical to LRU for eviction policy. In q-LRU, upon {{arrival of a}} request, a new content item is inserted into the cache with probability q. In k-LRU, the cache is managed by a <b>virtual</b> <b>cache.</b> The requests have to traverse a chain of k[*]−[*] 1 virtual caches, which store only content item hashes and perform meta-cache operations before the content item is stored in the physical cache indexed by k. When a cache request arrives, the request enters cache i if its hash is already stored in cache i- 1 (or if i[*]=[*] 1).|$|E
40|$|The St Augustine Amphitheater Marker, St Augustine, Fl. This marker {{was placed}} by the St Johns Historical Commission. It reads as : 2 ̆ 2 This 2, 000 seat theater {{is built on}} the site of the royal Spanish quarry, where the native shellstone called coquina was dug and ferried across Mantanzas Bay for the {{construction}} of Castillo de San Marcos and many other early structures in colonial St. Augustine. Here on June 27, 1965, the Paul Green symphonic drama CROSS AND SWORD was first presented to commemorate the 400 th anniversary of the founding of St. Augustine. There is also an active cache and a <b>virtual</b> <b>cache</b> near this site. 2 ̆ 2 [URL]...|$|E
40|$|Recent {{years have}} seen a {{tremendous}} growth of interests in streaming continuous media such as video data over the Internet. This would create an enormous increase in the demand on various server and networking resources. To minimize service delays and to reduce loads placed on these resources, we propose an Overlay Caching Scheme (OCS) for overlay networks. OCS utilizes <b>virtual</b> <b>cache</b> structures to coordinate distributed overlay caching nodes along the delivery path between the server and the clients. OCS establishes and adapts these structures dynamically according to clients' locations and request patterns. Compared with existing video caching techniques, OCS offers better performances in terms of average service delays, server load, and network load in most cases in our study...|$|E
50|$|There {{are other}} heap {{variants}} which are efficient in computers using <b>virtual</b> memory or <b>caches,</b> such as cache-oblivious algorithms, k-heaps, and van Emde Boas layouts.|$|R
40|$|The StrongARM SA- 1100 is a {{high-speed}} low-power processor aimed at embedded and portable applications. Its architecture features <b>virtual</b> <b>caches</b> and TLBs {{which are not}} tagged by an address-space identier. Consequently, context switches on that processor are potentially very expensive, as they may require complete ushes of TLBs and caches. This report presents the design of an address-space management technique for the StrongARM which minimises TLB and cache ushes and thus context switching costs. The basic idea is to implement the top-level of the (hardware-walked) page-table as a cache for page directory entries for dierent address spaces. This allows switching address spaces with minimal overhead {{as long as the}} working sets do not overlap. For small (32 MB) address spaces further improvements are possible by making use of the StrongARM's re-mapping facility. Our technique is discussed {{in the context of the}} L 4 microkernel in which it will be implemented. Permission to make digital [...] ...|$|R
40|$|New {{approaches}} are considered for performance enhancement of discrete-event simulation software. Instead {{of taking a}} purely algorithmic analysis view, we supplement algorithmic considerations with focus on system factors such as compiler/interpreter efficiency, hybrid interpreted/compiled code, <b>virtual</b> and <b>cache</b> memory issues, and so on. The work here consists of {{a case study of}} the SimPy language, in which we achieve significant speedups by addressing these factors. ...|$|R
40|$|Abstract. An {{address decoder}} {{is a small}} {{hardware}} unit that uses an address to index and place the data into memory units including cache memories. In current CPU cache designs {{there is a single}} decoder unit which serves to place data into the cache. In this paper we describe a technique to reduce contention on CPU’s caches through the use of multiple address decoders. We argue that by using multiple decoding techniques better data placement can be achieved and the CPU cache can be better utilized. We present an overview of an instrumentation tool developed to collect fine-grained data traces and a technique for virtually splitting caches using separate address decoders. Our results demonstrate the feasibility and the impact of <b>virtual</b> <b>cache</b> splitting...|$|E
40|$|Traditional web caching systems {{based on}} {{client-server}} model suffer from various limitations like single point of failure, congestion, higher document retrieval time, limited cache space, etc. In this project, based on existing research on peer-to-peer communications, we develop an efficient peer-to-peer cache distribution system, {{in which all}} the peers share their respective web caches, joining the individual web caches to create a huge <b>virtual</b> <b>cache</b> space. We implement an efficient algorithm for managing and searching in the combined cache. We also develop a peer-to-peer communication and messaging protocol to enable interaction and communication among the peers. Finally we implement consistency control to prevent dispersion of old, unoriginal web objects in peers’ caches. Our experimental results demonstrate that the prototyped system has a high performance...|$|E
40|$|A {{distributed}} system consists of, possibly heterogeneous, computing nodes connected by communication network {{that do not}} share memory or clock. One of the main benefits of {{distributed system}}s is resource sharing which speeds up computation, enhances data availability and reliability. However resources must be discovered and allocated {{before they can be}} shared. Virtual caching is a new caching scheme which allows a host node to grant authority of caching pages in some fraction of its own cache to nearby nodes. However the virtual caching protocol doesn't mentions how a client node obtains <b>virtual</b> <b>cache</b> from remote host. To address this problem we formulate a resource discovery and allocation problem. We are focusing our attention on how to locate resources-surplus donor nodes and to determine how much of the request for resources of deficien...|$|E
40|$|Coherent shared {{virtual memory}} (cSVM) is highly coveted for {{heterogeneous}} architectures {{as it will}} simplify programming across different cores and manycore accelerators. In this context, <b>virtual</b> L 1 <b>caches</b> {{can be used to}} great advantage, e. g., saving energy consumption by eliminating address translation for hits. Unfortunately, multicore virtual-cache coherence is complex and costly because it requires reverse translation for any coherence request directed towards a virtual L 1. The reason is the ambiguity of the virtual address due to the possibility of synonyms. In this paper, we take a radically different approach than all prior work which is focused on reverse translation. We examine the problem {{from the perspective of the}} coherence protocol. We show that if a coherence protocol adheres to certain conditions, it operates effortlessly with <b>virtual</b> <b>caches,</b> without requiring reverse translations even in the presence of synonyms. We show that these conditions hold in a new class of simple and efficient request-response protocols that use both selfinvalidation and self-downgrade. This results in a new solution for virtual-cache coherence, significantly less complex and more efficient than prior proposals. We study design choices for TLB placement under our proposal and compare them against those under a directory-MESI protocol. Our approach allows for choices that are particularly effective as for example combining all per-core TLBs in a single logical TLB in front of the last level cache. Significant area, energy, and performance benefits ensue as a result of simplifying the entire multicore memory organization...|$|R
40|$|This paper {{proposes a}} novel {{hardware-based}} anti-aliasing technique that improves {{not only the}} miss ratio but also the storage utilisation of the direct-mapped <b>virtual</b> <b>caches.</b> The key to the proposed scheme is the incorporation of a secondary mapping function to select the set. This secondary mapping function is applied only when the cache access based on the primary set selection function, which is usually a bit selection function from the virtual address, is a miss. The secondary mapping function we use is a bit selection function from the physical address. Therefore, in our scheme, a given memory block can be placed in two different sets in the cache, one based on the virtual address and the other based on the physical address. One benefit of such remapping {{is that it can}} eliminate many misses due to conflicts among frequently used blocks that happen to be mapped to the same set by the primary mapping function. Another important benefit of the above mentioned remapping is that it reduces the so-called anti-aliasing misses that result from the accesses to the blocks previously evicted from the cache for anti-aliasing purposes. A quantitative evaluation based on trace-driven simulations using ATU...|$|R
50|$|The short-pipeline, in-order, three-issue cores {{implement}} a MIPS-inspired VLIW instruction set. Each core has a register file and three functional units: two integer arithmetic logic units and a load-store unit. Each of the cores ("tile") {{has its own}} L1 and L2 caches plus an overall <b>virtual</b> L3 <b>cache</b> which is an aggregate of all the L2 caches. A core is able to run a full operating system on its own or multiple cores {{can be used to}} run a symmetrical multi-processing operating system.|$|R
