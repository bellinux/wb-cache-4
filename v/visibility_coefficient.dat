4|14|Public
40|$|Keywords:automotive; visibility; {{collision}} {{safety protection}} system; safe distance Abstract. This paper introduces {{the principle of}} automotive active collision safety protection system. Establish the speed relationship that our car with the car of before and after model for calculating the safe distance. Introducing the <b>visibility,</b> <b>coefficient</b> of road adhesion and other factors. The model of calculating the safe distance closer to the actual driving environment, the model of calculating the safe overcome the existing safety distance model in some special environment the safety distance small or large defects...|$|E
40|$|A novel {{approach}} {{towards a}} simplified, though still reliable human vision model {{based on the}} spatial masking properties of the human visual system (HVS) is presented. The model contains two basic characteristics of the HVS, namely texture masking and luminance masking. These masking effects are implemented as simple spatial filtering followed by a weighting function, and are efficiently combined into a single <b>visibility</b> <b>coefficient.</b> This HVS model is applied to a blockiness metric by using its output to scale the block-edge strength. To validate the proposed model, its performance in the blockiness metric is determined by comparing it to the same blockiness metric having different HVS-based models embedded. The {{results show that the}} proposed model is indeed simple, without compromising its accuracy. © Springer-Verlag Berlin Heidelberg 2007...|$|E
40|$|In {{most of the}} cases, logging {{companies}} in Central Africa are now legally obliged to integrate a game management plan in their global management plans. The number of fauna inventories has increased rapidly. Fauna inventories {{make it possible to}} evaluate the relative abundance of animal species and their distribution, in order to define particularly valuable areas that should obtain a protected area status. However, logging companies often do not ensure a reliable coverage of their concessions. Moreover, inventory methods are not always reliable, which leads to a lack of precision and low representativeness of the results. It is absolutely necessary to use a pragmatic low cost methodology for fauna inventories, which considers the particular context of logging companies and which is able to guarantee a quite precise estimation of fauna densities. Line transect sampling methods are currently considered as the best to monitor large and medium-size game species living within rain forests. Two methods make it possible to evaluate the abundance of fauna. The so-called distance sampling method is used to estimate absolute density figures. Indicators of animal presence (observation of individuals for small primates, dung piles for duikers and elephants or nests within the case of gorillas) within sampling units well knowing that not all the animals will be counted. Distances between the indicators of animal presence and the transect line are measured. The distribution of the observations using distance classes makes it possible to draw a detection curve and to calculate an absolute animal density. The particularly critical stage in this method appears to be the moment when dung piles and nest densities have to be converted into animal density figures. Indeed, conversion factors such as production rates and degradation time of dung and nests are not exactly known and can vary a lot. This means {{that it is impossible to}} evaluate animal densities in a precise way. The Index of Kilometric Abundance method calculates the number of inventoried observations over a certain distance without measuring the distance between the transect and indicators of animal presence. This method is easier to apply, but neglects the decreasing detection probability of objects in relation with the distance from the transect. It is possible to balance the Index of Kilometric Abundance obtained using a <b>visibility</b> <b>coefficient.</b> Some authors have shown that the IKA method is a reliable tool to monitor relative abundance of animal populations in rain forests. It allows to collect maximum amounts of data in a cheap and easy way. Considering the fact that the main goal of fauna inventories within forest concessions is to define the most valuable zones for animal species in order to choose the areas that should be protected, it is not necessary to estimate an absolute density. It is even not justified to do this, as it costs a lot to open new transects. Moreover, opening new transects should be avoided as much as possible, as they attract hunters. Fauna inventories make it possible to define the most interesting areas for fauna conservation. While defining these areas in order to protect them, the following parameters should be considered: global fauna densities; densities of threatened or vulnerable species; the importance of human activities (hunting in particular). It is necessary to standardize these parameters in order to give them the same weight. For each inventory plot, values for animal abundance and densities of threatened species have to be added up. Afterwards, the value for hunting has to be subtracted from the sum of the first values. The index obtained is called “conservation potential”. Using software such as ArcView and its extension Spatial Analyst makes it possible to extrapolate the results obtained and to apply them to the whole forest concession. The botanical interest of the inventoried plots should also be considered through different parameters (species richness, diversity, vulnerable species, chorology) in order to produce a global index. Fauna inventories do not stand alone, but have to be considered as a basic tool to set up a sustainable game management. They have to be used to develop actions to preserve animal species, such as the creation of a protected area. They can also make it possible to monitor animal populations in order to evaluate the effectiveness of developed actions dedicated to reduce pressure on fauna. Peer reviewe...|$|E
40|$|Abstract — This paper {{presents}} a novel basis function, called spherical piecewise constant basis function (SPCBF), for precomputed radiance transfer. SPCBFs have several desirable properties: rotatability, ability to represent all-frequency signals, {{and support for}} efficient multiple product. By smartly partitioning the illumination sphere into a set of subregions, and associating each subregion with an SPCBF valued 1 inside the region and 0 elsewhere, we precompute the light coefficients using the resulting SPCBFs. Efficient rotation of the light representation in SPCBFs is achieved by rotating the domain of SPCBFs. During run-time rendering, we approximate the BRDF and <b>visibility</b> <b>coefficients</b> using the set of SPCBFs for light, possibly rotated, through fast lookup of summed-area-table (SAT) and visibility distance table (VDT), respectively. SPCBFs enable new effects such as object rotation in all-frequency rendering of dynamic scenes and onthe-fly BRDF editing under rotating environment lighting. With graphics hardware acceleration, our method achieves real-time frame rates...|$|R
40|$|In this paper, {{we propose}} a new method that can {{consistently}} orient all normals of any mesh (if at all possible), while ensuring that most polygons are seen with their front-faces from most viewpoints. Our algorithm combines the proximity-based {{with a new}} visibility-based approach. Thus, it virtually eliminates the problems of proximitybased approaches, while avoiding the limitations of previous solid-based approaches. Our new method builds a connectivity graph of the patches of the model, which encodes the “proximity ” of neighboring patches. In addition, it augments this graph with two <b>visibility</b> <b>coefficients</b> for each patch. Based on this graph, a global consistent orientation of all patches is quickly found by a greedy optimization. We have tested our new method with a large suite of models, many of which from the automotive industry. The results show that almost all models can be oriented consistently and sensibly using our new algorithm. 1...|$|R
40|$|This paper {{presents}} a novel basis function, called spherical piecewise constant basis function (SPCBF), for precomputed radiance transfer. SPCBFs have several desirable properties: rotatability, ability to represent all-frequency signals, {{and support for}} efficient multiple product. By smartly partitioning the illumination sphere into a set of subregions and associating each subregion with an SPCBF valued 1 inside the region and 0 elsewhere, we precompute the light coefficients using the resulting SPCBFs. Efficient rotation of the light representation in SPCBFs is achieved by rotating the domain of SPCBFs. During runtime rendering, we approximate the BRDF and <b>visibility</b> <b>coefficients</b> using the set of SPCBFs for light, possibly rotated, through fast lookup of summed-area table (SAT) and visibility distance table (VDT), respectively. SPCBFs enable new effects such as object rotation in all-frequency rendering of dynamic scenes and on-the-fly BRDF editing under rotating environment lighting. With graphics hardware acceleration, our method achieves real-time frame rates...|$|R
40|$|Abstract—This paper {{presents}} a novel basis function, called spherical piecewise constant basis function (SPCBF), for precomputed radiance transfer. SPCBFs have several desirable properties: rotatability, ability to represent all-frequency signals, {{and support for}} efficient multiple product. By smartly partitioning the illumination sphere into a set of subregions and associating each subregion with an SPCBF valued 1 inside the region and 0 elsewhere, we precompute the light coefficients using the resulting SPCBFs. Efficient rotation of the light representation in SPCBFs is achieved by rotating the domain of SPCBFs. During runtime rendering, we approximate the BRDF and <b>visibility</b> <b>coefficients</b> using the set of SPCBFs for light, possibly rotated, through fast lookup of summed-area table (SAT) and visibility distance table (VDT), respectively. SPCBFs enable new effects such as object rotation in all-frequency rendering of dynamic scenes and on-the-fly BRDF editing under rotating environment lighting. With graphics hardware acceleration, our method achieves real-time frame rates. Index Terms—Real-time rendering, precomputed radiance transfer, spherical piecewise constant basis functions. Ç...|$|R
40|$|We {{present a}} {{practical}} system to visualize large datasets interactively on commodity PCs. Interactive visualization has applications in many areas, including computeraided design, engineering, entertainment, and training. Traditionally, visualization of large datasets has required expensive high-end graphics workstations. Recently, with the exponential trend of higher performance and lower cost of PC graphics cards, inexpensive PCs are becoming an attractive alternative to high-end machines. But a barrier in exploiting this potential {{is the small}} memory size of typical PCs. Our system uses new out-of-core techniques to visualize datasets much larger than main memory. In a preprocessing phase, we build a hierarchical decomposition of the dataset using an octree, precompute <b>coefficients</b> used for <b>visibility</b> determination, and create levels of detail. At runtime, we use multiple threads to overlap visibility computation, cache management, and rasterization. The structure of the octree and the <b>visibility</b> <b>coefficients</b> are kept in main memory. The contents of the octree nodes are loaded on demand from disk into a cache. To find the visible set, we use a fast approximate algorithm followed by a hardware-assisted conservative algorithm. T...|$|R
40|$|The redshifted 21 cm {{signal of}} neutral {{hydrogen}} from the epoch of reionization (EoR) is extremely weak and its first detection is therefore {{expected to be}} statistical with first-generation low-frequency radio interferometers. In this letter we propose a method to extract the angular power spectrum of EoR from the <b>visibility</b> correlation <b>coefficients</b> p_ij(u,v), instead of the visibilities V_ij(u,v) measured directly by radio interferometers in conventional algorithm. The <b>visibility</b> correlation <b>coefficients</b> are defined as p_ij(u,v) =V_ij(u,v) /√(|V_ii||V_jj|) by introducing the auto-correlation terms V_ii and V_jj such that the angular power spectrum C_ℓ can be obtained through C_ℓ= 4 pi^ 2 T_ 0 ^ 2, independently of the primary beams of antennas. This also removes partially the influence of receiver gains in the measurement of C_ℓ because the amplitudes of the gains {{cancel each other out}} in the statistical average operation of. We use the average system temperature T_ 0 as a calibrator of C_ℓ, which is dominated by the Milky Way and extragalactic sources in our interested frequency range below 200 MHz. Finally we demonstrate the feasibility of the novel method using the simulated sky maps as targets and the 21 CentiMeter Array (21 CMA) as interferometer. Comment: 12 pages, 3 figures. Minor corrections [an error in eq. (3) is fixed] and references added, conclusions remain unchanged. Accepted for publication in ApJ...|$|R
40|$|We {{study by}} {{comparison}} {{the structure of}} singlet type states in Q-bit space {{in the light of}} quantum and classical paradigms. It is shown that only the classical paradigm implies a variation in the <b>visibility</b> of correlation <b>coefficients,</b> that has been observed in fact in experiments. We conclude that Q-bit space in not a appropriate venue for an EPR test of quantum completeness. Comment: 3 pages RevTe...|$|R
40|$|Baseline {{dependent}} averaging (BDA) {{can be used}} {{to reduce}} the volume of visibility data significantly. Most current BDA schemes perform (weighted) averaging over a certain time interval. This quickly causes decorrelation due to time averaging. We propose to reduce this decorrelation by representing the <b>visibilities</b> by polynomial <b>coefficients.</b> The high compression made feasible by this approach may cause fast-changing calibration parameters to become undersampled. We propose the Compress-Expand-Compress (CEC) method to mitigate this. All compression and expansion methods proposed herein are very simple and cause negligible computation overhead. We demonstrate the effectiveness of our scheme in a simulation emulating a highdynamic range imaging problem...|$|R
40|$|Context: Most {{upcoming}} CMB polarization experiments {{will use}} direct imaging {{to search for}} the primordial gravitational waves through the B-modes. Bolometric interferometry is an appealing alternative to direct imaging that combines the advantages of interferometry in terms of systematic effects handling and those of bolometric detectors in terms of sensitivity. Aims: We calculate the signal from a bolometric interferometer in order to investigate its sensitivity to the Stokes parameters paying particular attention to the choice of the phase-shifting scheme applied to the input channels in order to modulate the signal. Methods: The signal is expressed as a linear combination of the Stokes parameter <b>visibilities</b> whose <b>coefficients</b> are functions of the phase-shifts. Results: We show that the signal to noise ratio on the reconstructed visibilities can be maximized provided the fact that the phase-shifting scheme is chosen in a particular way called coherent summation of equivalent baselines. As a result, a bolometric interferometer is competitive with an imager having the same number of horns, but only if the coherent summation of equivalent baselines is performed. We confirm our calculations using a Monte-Carlo simulation. We also discuss the impact of the uncertainties on the relative calibration between bolometers and propose a way to avoid this systematic effect. Comment: 8 pages, 6 figures, submitted to A&...|$|R
40|$|Abstract Detection and {{mitigation}} {{of radio}} frequency interference (RFI) is the first and also the key step for data processing in radio observations, especially for ongoing low frequency radio experiments towards the detection of the cosmic dawn and epoch of reionization (EoR). In this paper we demonstrate the technique and efficiency of RFI identification and mitigation for the 21 Centimeter Array (21 CMA), a radio interferomter dedicated to the statistical measurement of EoR. For terrestrial, man-made RFI, we con-centrate mainly on statistical approach by identifying and then excising the non-Gaussian signatures, {{in the sense that}} the extremely weak cosmic signal is actually buried under thermal and therefore Gaussian noise. We also introduce the so-called <b>visibility</b> correla-tion <b>coefficient</b> instead of conventional visibility, which allows a further suppression of rapidly time-varying RFI. Finally, we briefly discuss removals of the sky RFI, the leakage of sidelobes from off-field strong radio sources with a time-invariant power and feature-less spectrum. It turns out that state-of-the-art technique should allow us to detect and mitigate RFI to a satisfactory level in present low frequency interferomter observations such as 21 CMA, and the accuracy and efficiency can be greatly improved with the em-ployment of low-cost, fast-speed computing facilities in data acquisition and processing. Key words: dark ages, reionization, first stars- instrumentation: interferometers- meth-ods: data analysis: observational- techniques: interferometric...|$|R
40|$|Field {{measurements}} of aerosols {{were conducted on}} Xiaoyangshan Island in the East China Sea in May 2006 to investigate the impact of anthropogenic air pollutants to the coastal environment. MM 5 /CMAQ was used to model {{the development of the}} episode and identify the sources of major ionic species during the episode. The results showed that the major ionic species in TSP were SO 42 -, NO 3 - and NH 4 +, which accounted for about 76 % of the total water soluble ions (TWSI). The mean concentrations of SO 42 -, NO 3 - and NH 4 + were 20. 9, 10. 4 and 5. 1 £gg m- 3, respectively, and the peak values on a foggy day were 69. 87, 38. 48 and 22. 75 £gg m- 3, respectively. The most abundant ionic species was SO 42 -. The increase of SO 42 - concentration was found to be significantly correlated with the decrease of the atmospheric <b>visibility</b> (correlation <b>coefficient</b> = 0. 82). Process analysis implied that advection, diffusion and cloud chemical processes contributed to this pollution episode. Backward trajectory analysis revealed that the airflow on the foggy and most polluted day during the sampling campaign was westerly, i. e., from the continent. Both the measured data and modeling results showed that the coastal atmosphere was seriously affected by terrestrial pollutants in the continental outflow...|$|R
40|$|This paper {{presents}} a low-cost {{system for the}} measurement of atmospheric visibility. The measurement setup is composed by a consumer digital camera which is controlled by a computer. The camera acquires photos of the landscape that include natural dark objects. Then the computer calculates the atmospheric visibility based on the apparent contrast, against the background, of these dark objects through the Lambert–Beer law. Two different approaches are proposed so {{that the system is}} able to measure the atmospheric visibility both under normal and low visibility conditions. The use of the three color channels of the camera allows the measurement of the extinction coefficient at different wavelengths along with the Angström exponent which is an important parameter in the classification of atmosphere aerosols. Measurements performed with the developed system are presented and include the atmospheric <b>visibility,</b> the extinction <b>coefficient</b> and the Angström exponent. The results presented correspond to the measurements performed along a week that included a desert dust event. This event dramatically reduced the atmospheric visibility due to the desert dust particles. Angström exponent measurements were performed with another instrument for comparison. Finally, an uncertainty analysis of the measured atmospheric visibility is presented...|$|R
40|$|The {{measurement}} of visibility has many different applications from meteorological uses to air quality measurements. In fact, {{the amount of}} particles and molecules in the atmosphere can be estimated from visibility measurements, through the extinction coefficient, ext b. Also, visibility is an important parameter, for example, in airborne traffic control. It {{is well known that}} the extinction coefficient can be obtained from the contrast C between two black objects positioned at different but known distances, 1 x and 2 x from the observer (Horvath, 1996). From the extinction <b>coefficient,</b> <b>visibility</b> can be inferred using either the Koschmieder formula Visibility = 3. 9 bext (Koschmieder, 1925) or its modified version 3. 0 ext Visibility = b (Middleton, 1968). Our approach to obtain visibility is based on digital photography made at three different channels (red, green and blue) of an adequate landscape as shown in the picture. In this case, the objects chosen were a tree group located at 1 x = 6 km from the observation point and a mountain background which contains the same type of trees sited at a distance of 2 x = 24 km. The objects are not black, but are sufficiently dark for our purposes, in a similar fashion as the one used by Horvath et al. (1996). The contrast between the two objects and the background sky is measured by the camera for the three colors, and the extinction coefficient is obtained using the Lambert Beer Law, e. g. () () 0 exp ext C x = C −b x, where 0 C is a calibrating constant. By using two different objects relative to the same (identical) background, the calibrating constant can be eliminated. Hence this method can be used with any background. For the case shown in the picture the extinction <b>coefficient</b> and the <b>visibility</b> are shown in Table 1. A human observer could estimate the visibility of 25 km which corresponds to the value of the green channel. At the conference, further first results as a daytime diurnal cycle and other cases under a variety of different visibility conditions will be presented. It will be shown that the extinction <b>coefficient</b> and atmospheric <b>visibility</b> can be retrieved by means of a digital camera. Hence this method has the advantage of being very low-cost, robust, easy to set-up and it works for both clear and cloudy skies. Using this method, measurements up to 60 km visibility were obtained, which enlarge the capabilities of the weather services in Portugal, where these high visibility values are currently not reported...|$|R

