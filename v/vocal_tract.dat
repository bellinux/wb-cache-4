2386|40|Public
5|$|The lip {{position}} and pressure, shaping of the <b>vocal</b> <b>tract,</b> choice of reed and mouthpiece, amount of air pressure created, and evenness of the airflow account {{for most of}} the clarinetist's ability to control the tone of a clarinet. A highly skilled clarinetist will provide the ideal lip and air pressure for each frequency (note) being produced. They will have an embouchure which places an even pressure across the reed by carefully controlling their lip muscles. The airflow will also be carefully controlled by using the strong stomach muscles (as opposed to the weaker and erratic chest muscles) and they will use the diaphragm to oppose the stomach muscles to achieve a tone softer than a forte rather than weakening the stomach muscle tension to lower air pressure. Their <b>vocal</b> <b>tract</b> will be shaped to resonate at frequencies associated with the tone being produced.|$|E
5|$|The pulmonic {{consonant}} table, {{which includes}} most consonants, is arranged in rows that designate manner of articulation, meaning how the consonant is produced, and columns that designate place of articulation, meaning {{where in the}} <b>vocal</b> <b>tract</b> the consonant is produced. The main chart includes only consonants with a single place of articulation.|$|E
5|$|Since {{the length}} of closure is {{defined in terms of}} time between the closure of the <b>vocal</b> <b>tract</b> after the {{preceding}} vowel, and the release before the following vowel, stops at the beginning or end of a word do not have a fortis-lenis contrast. Orthographically in Wagiman, word-initial stops are written using the voiced Roman letters (b, d and g), {{but at the end of}} a word, voiceless letters (p, t and k) are used instead.|$|E
25|$|Infants <b>vocal</b> <b>tracts</b> are smaller, and {{initially}} also shaped differently from adults’ <b>vocal</b> <b>tracts.</b> The infant’s tongue fills the entire mouth, thus reducing {{the range of}} movement. As the facial skeleton grows, the range for movement increases, which probably contributes to the increased variety of sounds infants start to produce. Development of muscles and sensory receptors also gives infants more control over sound production.|$|R
40|$|As {{children}} age, their <b>vocal</b> <b>tracts</b> grow. This {{growth is}} nonlinear, {{and results in}} disproportionate growth of the pharynx (Vorperian et al., 1997) This growth forces children to modify the articulatory settings used to produce different vowels. •Acoustic simulations of the maximum vowel spaces of children at different stages (Boë and Maeda, 1997) of development are shown in Figure 1. This type of output {{has been used to}} synthesize a range of vocal productions for different vocal-tract growth stages. •Perception studies have examined the labels that adults attribute to productions of hypothetical <b>vocal</b> <b>tracts</b> at different stages of development (e. g., Ménard et al., 2004, 2009; McGowan, 2006). Studies by Ménard and colleagues have shown that even the youngest <b>vocal</b> <b>tracts</b> are capable of generating tokens that are identified as the point vowels /i/, /Q/, /A/, and /u/. As <b>vocal</b> <b>tracts</b> grow, more back and high vowels are identified. •These studies were conducted with English and French listeners. The {{purpose of this study was}} to extent this finding to two more-symmetric vowel systems, those of Korean and Greek, shown in Figures 2 through 4. This experiment is part of a larger collaborative project on the dynamics of production-perception relationships during speech-sound acquisition across languages. The research questions this investigation addresses are: F 1 (Hz...|$|R
5000|$|In a 2016 study, {{a team of}} {{biologists}} {{from several}} universities concluded that macaques possess <b>vocal</b> <b>tracts</b> physically capable of speech, [...] "but lack a speech-ready brain to control it".|$|R
5|$|The {{speed of}} sound in helium is nearly three times the {{speed of sound}} in air. Because the {{fundamental}} frequency of a gas-filled cavity is proportional to {{the speed of sound}} in the gas, when helium is inhaled there is a corresponding increase in the resonant frequencies of the <b>vocal</b> <b>tract.</b> The fundamental frequency (sometimes called pitch) does not change, since this is produced by direct vibration of the vocal folds, which is unchanged. However, the higher resonant frequencies cause a change in timbre, resulting in a reedy, duck-like vocal quality. The opposite effect, lowering resonant frequencies, can be obtained by inhaling a dense gas such as sulfur hexafluoride or xenon.|$|E
5|$|The vocal {{production}} system {{is controlled by}} the cranial nerve nucleus in the brain, and supplied by the recurrent laryngeal nerve and the superior laryngeal nerve, branches of the vagus nerve. The <b>vocal</b> <b>tract</b> is supplied by the hypoglossal nerve and facial nerves. Electrical stimulation of the periaqueductal gray (PEG) region of the mammalian midbrain elicit vocalizations. The ability to learn new vocalizations is only exemplified in humans, seals, cetaceans, and possibly bats; in humans, this {{is the result of a}} direct connection between the motor cortex, which controls movement, and the motor neurons in the spinal cord.|$|E
5|$|As in {{all other}} tetrapods, mammals have a larynx that can quickly open and close to produce sounds, and a supralaryngeal <b>vocal</b> <b>tract</b> which filters this sound. The lungs and {{surrounding}} musculature provide the air stream and pressure required to phonate. The larynx controls the pitch and volume of sound, but the strength the lungs exert to exhale also contributes to volume. More primitive mammals, such as the echidna, can only hiss, as sound is achieved solely through exhaling through a partially closed larynx. Other mammals phonate using vocal folds, {{as opposed to the}} vocal cords seen in birds and reptiles. The movement or tenseness of the vocal folds can result in many sounds such as purring and screaming. Mammals can change the position of the larynx, allowing them to breathe through the nose while swallowing through the mouth, and to form both oral and nasal sounds; nasal sounds, such as a dog whine, are generally soft sounds, and oral sounds, such as a dog bark, are generally loud.|$|E
3000|$|The mouth {{movement}} can {{be derived}} from the coarticulation property of the <b>vocal</b> <b>tracts.</b> Key-frame-based rendering interpolates the frames between key frames. For example, [25] defined the basic visemes as the key frames and the transition in the animation is based on morphing visemes. A viseme is the basic mouth image corresponding to the speech unit [...] "phoneme", for example, the phonemes [...] "m'', [...] "b'', [...] "p'' correspond to the closure viseme. However, this approach {{does not take into account}} the coarticulation models [24, 26]. As preceding and succeeding visemes affect the <b>vocal</b> <b>tracts,</b> the transition between two visemes also gets affected by other neighbor visemes.|$|R
50|$|Infants <b>vocal</b> <b>tracts</b> are smaller, and {{initially}} also shaped differently from adults’ <b>vocal</b> <b>tracts.</b> The infant’s tongue fills the entire mouth, thus reducing {{the range of}} movement. As the facial skeleton grows, the range for movement increases, which probably contributes to the increased variety of sounds infants start to produce. Development of muscles and sensory receptors also gives infants more control over sound production.The limited movement possible by the infant jaw and mouth might {{be responsible for the}} typical consonant-vowel (CV) alternation in babbling and it has even been suggested that the predominance of CV syllables in the languages of the world might evolutionarily have been caused by this limited range of movements of the human vocal organs.|$|R
50|$|An otonality {{corresponds}} to an arithmetic series of frequencies, or lengths of a vibrating string. Brass instruments naturally produce otonalities, and indeed otonalities {{are inherent in}} the harmonics of a single fundamental tone. Tuvan Khoomei singers produce otonalities with their <b>vocal</b> <b>tracts.</b>|$|R
25|$|A motor {{plan is a}} {{high level}} motor {{description}} for the production and articulation of a speech items (see motor goals, motor skills, articulatory phonetics, articulatory phonology). In our neurocomputational model ACT a motor plan is quantified as a <b>vocal</b> <b>tract</b> action score. <b>Vocal</b> <b>tract</b> action scores quantitatively {{determine the number of}} <b>vocal</b> <b>tract</b> actions (also called articulatory gestures), which need to be activated in order to produce a speech item, their degree of realization and duration, and the temporal organization of all <b>vocal</b> <b>tract</b> actions building up a speech item (for a detailed description of <b>vocal</b> <b>tract</b> actions scores see e.g. Kröger & Birkholz 2007). The detailed realization of each <b>vocal</b> <b>tract</b> action (articulatory gesture) depends on the temporal organization of all <b>vocal</b> <b>tract</b> actions building up a speech item and especially on their temporal overlap. Thus the detailed realization of each <b>vocal</b> <b>tract</b> action within an speech item is specified below the motor plan level in our neurocomputational model ACT (see Kröger et al. 2011).|$|E
25|$|Sufficiently strong {{resonances}} of the <b>vocal</b> <b>tract</b> can strongly {{influence the}} timbre of the instrument.|$|E
25|$|The {{differences}} between the <b>vocal</b> <b>tract</b> of infants and adults {{can be seen in}} figure 3 (infants) and figure 4 (adults) below.|$|E
40|$|Problems of the {{numerical}} {{solution of the}} supraglottal space are dealt with. At first there is introduced a numerical solution {{of the problem of}} the semi-closed cylindrical acoustic space oscillation in the interaction with an elastic thin plate. The results are compared with the FEM solution. The next part deals with methods of excitation both healthy and defective supraglottal spaces for vowels a, i, and with recognition of responses and frequency response functions by means of numerical modelling. Alternative calculations of the <b>vocal</b> <b>tracts</b> and the supraglottal space are presented in the next part. Experimental measurements of the <b>vocal</b> <b>tracts</b> for different vowels are confronted with results from the previous parts of the thesis. Available from STL Prague, CZ / NTK - National Technical LibrarySIGLECZCzech Republi...|$|R
40|$|The {{developed}} two-dimensional {{finite element}} model of the flow-induced self-oscillation of the human vocal folds is used for solving fluid-structure-acoustic interaction occurring during phonation. The aim is to nalyze the influence of stiffness of superficial lamina propria on vocal folds vibration considering <b>vocal</b> <b>tracts</b> shaped for the vowels [i:] and [u:]...|$|R
50|$|Uses for machine lipreading {{could include}} {{automated}} lipreading of video-only records, automated lipreading of speakers with damaged <b>vocal</b> <b>tracts,</b> and speech processing in face-to-face video (i.e. from videophone data). Automated lipreading may help in processing noisy or unfamiliar speech. Automated lipreading {{may contribute to}} biometric person identification, replacing password-based identification.|$|R
25|$|At some frequencies, whose values {{depend on}} the {{position}} of the player's tongue, resonances of the <b>vocal</b> <b>tract</b> inhibit the oscillatory flow of air into the instrument.|$|E
25|$|During the 1970s, it {{was widely}} {{believed}} that the Neanderthals lacked modern speech capacities. It was claimed that they possessed a hyoid bone so {{high up in the}} <b>vocal</b> <b>tract</b> as to preclude the possibility of producing certain vowel sounds.|$|E
25|$|The {{developmental}} {{changes in}} infants’ vocalizations {{over the first}} year of life are influenced by physical developments during that time. Physical growth of the <b>vocal</b> <b>tract,</b> brain development, and development of neurological structures responsible for vocalization are factors for the development of infants’ vocal productions.|$|E
5000|$|Similarly, the phonetician JC Catford {{presented}} {{readers with}} [...] "a series of simple introspective experiments {{be carried out}} inside their own <b>vocal</b> <b>tracts,</b> their own throats and mouths. By actually making sounds (very often silently) and attending to the muscular sensations that accompany their production one can discover how they are produced ..." ...|$|R
40|$|The larynges and <b>vocal</b> <b>tracts</b> of four {{species of}} Artiodactyla were {{investigated}} {{in combination with}} acoustical analyses of their respective calls. Different evolutionary specializations of laryngeal characters may lead to similar effects on sound production. Both the elongation of the vocal folds and the increase of the oscillating masses lower the fundamental frequency. The influence of an enlarged volume of the laryngeal vestibulum on sound production remains unclear. 1...|$|R
40|$|The {{experimental}} two-microphone {{transfer function}} method (TMTF) {{is adapted to}} the numerical framework to compute the radiation and input impedances of three-dimensional <b>vocal</b> <b>tracts</b> of elliptical cross section. In its simplest version, the TMTF method only requires measuring the acoustic pressure at two points in an impedance duct and the postprocessing of the corresponding transfer function. However, some considerations are {{to be taken into}} account when using the TMTF method in the numerical context, which constitute the main objective of this paper. In particular, the importance of including absorption at the impedance duct walls to avoid lengthy numerical simulations is discussed and analytical complex axial wave numbers for elliptical ducts are derived for this purpose. It is also shown how the plane wave restriction of the TMTF method can be circumvented to some extent by appropriate location of the virtual microphones, thus extending the method frequency range of validity. Virtual microphone spacing is also discussed on the basis of the so called singularity factor. Numerical examples include the computation of the radiation impedance of vowels /a/, /i/ and /u/ and the input impedance of vowel /a/, for simplified <b>vocal</b> <b>tracts</b> of circular and elliptical cross sections...|$|R
25|$|Speech {{repetition}} is {{the saying}} by one individual of the spoken vocalizations made by another individual. This requires the ability {{in the person}} making the copy to map the sensory input they hear from the other person's vocal pronunciation into a similar motor output with their own <b>vocal</b> <b>tract.</b>|$|E
25|$|Spoken {{words are}} {{sequences}} of motor movements organized around <b>vocal</b> <b>tract</b> gesture motor targets. Vocalization {{due to this}} is copied {{in terms of the}} motor goals that organize it rather than the exact movements with which it is produced. These vocal motor goals are auditory. According to James Abbs 'For speech motor actions, the individual articulatory movements would not appear to be controlled with regard to three- dimensional spatial targets, but rather with regard to their contribution to complex <b>vocal</b> <b>tract</b> goals such as resonance properties (e.g., shape, degree of constriction) and or aerodynamically significant variables'. Speech sounds also have duplicable higher-order characteristics such as rates and shape of modulations and rates and shape of frequency shifts. Such complex auditory goals (which often link—though not always—to internal vocal gestures) are detectable from the speech sound which they create.|$|E
25|$|Other {{controllers}} include theremin, lightbeam controllers, touch buttons (touche d’intensité) on the ondes Martenot, {{and various}} types of foot pedals. Envelope following systems, the most sophisticated being the vocoder, are controlled by the power or amplitude of input audio signal. A musician uses the talk box to manipulate sound using the <b>vocal</b> <b>tract,</b> though it is rarely categorized as a synthesizer.|$|E
50|$|Kanzi cannot speak vocally in {{a manner}} that is comprehensible to most humans, as bonobos have {{different}} <b>vocal</b> <b>tracts</b> from humans, which makes them incapable of reproducing most of the vocal sounds humans make. At the same time, it was noticed that every time Kanzi communicated with humans with specially designed graphic symbols, he also produced some vocalization. It was later found out that Kanzi was actually producing the articulate equivalent of the symbols he was indicating, although in a very high pitch and with distortions.|$|R
40|$|Abstract — We {{address the}} problem of {{audiovisual}} speech inversion, namely recovering the <b>vocal</b> <b>tract’s</b> geometry from auditory and visual speech cues. We approach the problem in a statistical framework, combining ideas from multistream Hidden Markov Models and canonical correlation analysis, and demonstrate effective estimation of the trajectories followed by certain points of interest in the speech production system. Our experiments show that exploiting both audio and visual modalities clearly improves performance relative to either audio-only or visual-only estimation. We report experiments on the QSMT database which contains audio, video, and electromagnetic articulography data recorded in parallel. I...|$|R
40|$|The {{effect of}} {{tonsillectomy}} on production of Czech vowels is numerically and experimentally examined. Experimental results show statistically significant shift down to lower frequencies of 3 rd formant frequency for vowels /a/, /e/ and /o/. The finite element (FE) {{models of the}} acoustic spaces corresponding to the human <b>vocal</b> <b>tracts</b> and acoustic space around the human head are used in numerical simulations of phonation. Computed results show that tonsillectomy causes frequency shifts of some formant frequencies mostly down to lower frequencies. The frequency shifts of the formants are significantly dependent on position and size of th tonsils. Experimental and computational results are consistent...|$|R
25|$|The {{activation}} pattern {{within the}} motor map determines the movement pattern of all model articulators (lips, tongue, velum, glottis) for a speech item. In {{order not to}} overload the model, no detailed modeling of the neuromuscular system is done. The Maeda articulatory speech synthesizer is used in order to generate articulator movements, which allows the generation of a time-varying <b>vocal</b> <b>tract</b> form and the generation of the acoustic speech signal for each particular speech item.|$|E
25|$|For speech production, the ACT model {{starts with}} the {{activation}} of a phonemic representation of a speech item (phonemic map). In {{the case of a}} frequent syllable, a co-activation occurs {{at the level of the}} phonetic map, leading to a further co-activation of the intended sensory state at the level of the sensory state maps and to a co-activation of a motor plan state at the level of the motor plan map. In the case of an infrequent syllable, an attempt for a motor plan is generated by the motor planning module for that speech item by activating motor plans for phonetic similar speech items via the phonetic map (see Kröger et al. 2011). The motor plan or <b>vocal</b> <b>tract</b> action score comprises temporally overlapping <b>vocal</b> <b>tract</b> actions, which are programmed and subsequently executed by the motor programming, execution, and control module. This module gets real-time somatosensory feedback information for controlling the correct execution of the (intended) motor plan. Motor programing leads to activation pattern at the level lof the primary motor map and subsequently activates neuromuscular processing. Motoneuron activation patterns generate muscle forces and subsequently movement patterns of all model articulators (lips, tongue, velum, glottis). The Birkholz 3D articulatory synthesizer is used in order to generate the acoustic speech signal.|$|E
25|$|Research {{has shown}} that the larynx, the pharynx and the oral cavity are the main {{resonators}} of vocal sound, with the nasal cavity only coming into play in nasal consonants, or nasal vowels, such as those found in French. This main resonating space, from above the vocal folds to the lips is known as the <b>vocal</b> <b>tract.</b> Many voice users experience sensations in the sinuses that may be misconstrued as resonance. However, these sensations are caused by sympathetic vibrations, and are a result, rather than a cause, of efficient vocal resonance.|$|E
40|$|The study {{provides}} information on experimental research on a complete 1 : 1 scaled model of human phonation. The model includes human lungs, the trachea, the laryngeal part with artificial vocal folds and the <b>vocal</b> <b>tracts</b> designed for different vowels. The measurement set up enables modelling the time signals not easily measured in humans during phonation as for example fluctuations of the subglottic, laryngeal and oral pressures measured simultaneously with the glottis opening and the glottis area registered by a high-speed camera. The simulation of phonation is performed in the ranges of the airflow rate and the subglottic pressure typical for a normal humans' physiology...|$|R
40|$|The goal of {{this work}} is to recover articulatory {{information}} from the speech signal by acoustic-to-articulatory inversion. One of the main difficulties with inversion is {{that the problem is}} under-constrained and inversion methods generally offer no guarantee on the phonetical realism of the inverse solutions. A way to adress this issue is to use additional constraints: in this work, contraints on phonetically relevant caracteristics of the <b>vocal</b> <b>tracts.</b> Inversion experiments were conducted on two different speakers (male and female), using different implementations of phonetical constraints. The articulatory trajectories were compared with those obtained without using these constraints and to articulatory trajectories measured from X-ray data...|$|R
25|$|The {{resulting}} acoustic {{structure of}} concrete speech productions {{depends on the}} physical and psychological properties of individual speakers. Men, women, and children generally produce voices having different pitch. Because speakers have <b>vocal</b> <b>tracts</b> of different sizes (due to sex and age especially) the resonant frequencies (formants), which are important for recognition of speech sounds, will vary in their absolute values across individuals (see Figure 3 for an illustration of this). Research shows that infants {{at the age of}} 7.5 months cannot recognize information presented by speakers of different genders; however by the age of 10.5 months, they can detect the similarities. Dialect and foreign accent can also cause variation, as can the social characteristics of the speaker and listener.|$|R
