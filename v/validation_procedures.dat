586|1259|Public
5|$|Also in 1988, {{a chapter}} titled Modelling Human Exposure to Altered Pressure Environments, by T.R. Hennessy was {{published}} in Environmental Ergonomics, discussing the shortcomings of several decompression models and the associated experimental <b>validation</b> <b>procedures.</b>|$|E
25|$|No {{enrichment}} {{and fuel}} element fabrication. Since 100% of natural thorium {{can be used}} as a fuel, and the fuel is in the form of a molten salt instead of solid fuel rods, expensive fuel enrichment and solid fuel rods' <b>validation</b> <b>procedures</b> and fabricating processes are not needed. This greatly decreases LFTR fuel costs. Even if the LFTR is started up on enriched uranium, it only needs this enrichment once just to get started. After startup, no further enrichment is required.|$|E
5000|$|It is {{important}} that a knowledge engineer incorporates <b>validation</b> <b>procedures</b> into their systems within the program code. After the knowledge-based system is constructed, it can be maintained by the domain expert [...]|$|E
3000|$|... {{parameter}} of the kernel with a leave-one-out cross-validation procedure. According to {{the results}} of cross <b>validation</b> <b>procedure</b> [...]...|$|R
40|$|A {{critical}} {{discussion of}} the in-house <b>validation</b> <b>procedure</b> presents the benefits of an application of a capillary electrophoresis (CE) method in aqueous process samples consisting of various types of carbohydrates. This study emphasized the complexity of CE via <b>validation</b> <b>procedure,</b> {{in the case of}} heterogeneous processes. An in-house <b>validation</b> <b>procedure</b> of a capillary electrophoretic method aiming at analysis of aqueous process samples with heterogeneous matrices was evaluated. The validation parameters were discussed through an example case of a CE method, developed for the determination of saccharose, glucose and polydatin, applied in calibration solutions and process samples. The validation data was used in evaluation of uncertainty components. The results from the in-house <b>validation</b> <b>procedure</b> showed that the most critical parameter in the determination of uncertainty was repeatability. Selectivity and reproducibility are also critical, particularly in the case of analyzing heterogeneous samples with changing composition. Especially in process analytical applications the evaluation of uncertainty factors was concluded to be essential, as in addition to process conditions the sample composition itself caused variation...|$|R
40|$|In this paper, an {{experimental}} <b>validation</b> <b>procedure</b> {{is applied to}} an improved one-dimensional model of fuel additive assisted regeneration of a diesel particulate filter. Full-scale tests on an engine bench of the regeneration behaviour of a diesel filter fitted to a modern diesel engine run on catalyst-doped fuel are employed for this purpose. The main objectives of the <b>validation</b> <b>procedure</b> concern {{the ability of the}} model to predict the effects of exhaust mass flowrate, initial soot loading mass, volatile organic fraction of the soot and additive concentration in the fuel. The results of the <b>validation</b> <b>procedure</b> are intended to demonstrate the scope and extent of applicability of models of this type to real-world design and optimization studies with diesel filters...|$|R
50|$|The {{foundation}} {{should be}} laid strong and firm. primary, upper primary {{and middle school}} should provide the space for children to explore and develop rational thinking that they would imbibe in them and have sufficient knowledge of concepts, language, knowledge, investigation and <b>validation</b> <b>procedures.</b>|$|E
50|$|Although not a {{shortcoming}} of data deduplication, {{there have}} been data breaches when insufficient security and access <b>validation</b> <b>procedures</b> are used with large repositories of deduplicated data. In some systems, as typical with cloud storage, an attacker can retrieve data owned by others by knowing or guessing the hash value of the desired data.|$|E
50|$|While the labeler code is {{assigned}} by the FDA, both the product and package segments are {{assigned by the}} labeler. While in the past labelers may {{have had the opportunity}} to reassign old product codes no longer used to new products, according to the new FDA <b>validation</b> <b>procedures,</b> once an NDC code {{is assigned}} to one product (defined by key properties including active ingredients, strength, and dosage form) it may not be later reassigned to a different product.|$|E
40|$|RPKI Validation Reconsidered draft-huston-rpki-validation- 00. txt This {{document}} {{reviews the}} certificate <b>validation</b> <b>procedure</b> specified in RFC 6487 and highlights aspects of operational management of certificates in the RPKI {{in response to}} the movement of resources across registries, and the associated actions of Certification Authorities to maintain certification of resources during this movement. The document describes an alternative <b>validation</b> <b>procedure</b> that reduces the operational impact of certificate management during resource movement. Status of this Memo This Internet-Draft is submitted in full conformance with th...|$|R
30|$|Futures Maps are {{generated}} for actors and their decision making. The <b>validation</b> <b>procedure</b> with Futures Maps serves {{in this way}} decision making and action focus of the standard 1.4.|$|R
40|$|Park et al. 1 In the {{application}} of microscopic simulation models, the importance of model calibration and validation cannot be overemphasized. A recent study proposed a systematic approach of conducting simulation model calibration and <b>validation</b> <b>procedure</b> {{on the basis of}} experimental design and optimization, and applied it to an isolated intersection using a VISSIM simulation model. This study further evaluates the previously developed simulation model calibration and <b>validation</b> <b>procedure</b> using an urban arterial network consisted of 12 coordinated actuated signalized intersections. Both VISSIM and CORSIM simulation models were used. Travel time was used for calibration measure, while maximum queue length was used for validation measure. The study results showed that the calibrated and validated simulation models were able to adequately represent field conditions while default parameter based models were not. As such the previously developed simulation model calibration and <b>validation</b> <b>procedure</b> was proven to be effective for an arterial network under both VISSIM and CORSIM simulation models. Park et al. ...|$|R
50|$|No {{enrichment}} {{and fuel}} element fabrication. Since 100% of natural thorium {{can be used}} as a fuel, and the fuel is in the form of a molten salt instead of solid fuel rods, expensive fuel enrichment and solid fuel rods' <b>validation</b> <b>procedures</b> and fabricating processes are not needed. This greatly decreases LFTR fuel costs. Even if the LFTR is started up on enriched uranium, it only needs this enrichment once just to get started. After startup, no further enrichment is required.|$|E
50|$|Compared with {{conventional}} bioreactor systems, the single-use solution has some advantages. Application of single-use technologies reduces cleaning and sterilization demands. Some estimates show cost savings {{of more than}} 60% with single use systems compared to fixed asset stainless steel bioreactors. In pharmaceutical production, complex qualification and <b>validation</b> <b>procedures</b> can be made easier, and will finally lead to significant cost reductions. The application of single-use bioreactors {{reduces the risk of}} cross contamination and enhances the biological and process safety. Single-use applications are especially suitable for any kind of biopharmaceutical product.|$|E
50|$|Mutual Recognition (MR) {{refers to}} those {{activities}} relates with {{the signing of}} a document between foreign Customs Administration that allows an exchange of information aiming to improve supply chain security. The signed document, or MR, indicates that security requirements or standards of the foreign partnership program, {{as well as its}} <b>validation</b> <b>procedures</b> are similar. The essential concept of a Mutual Recognition Agreement (MRA) is that C-TPAT and the foreign program are compatible in both theory and practice so that one program will recognize the validation findings of the other program.|$|E
30|$|We also {{explored}} {{alternative approaches}} to derive an empirical classification of graduate jobs (see Green and Henseke 2014 for details). These fared only slightly {{worse in the}} <b>validation</b> <b>procedure</b> than SOC(HE)_GH.|$|R
50|$|In {{addition}} to the Architects Registration Board, the RIBA provides accreditation to architecture schools in the UK under a course <b>validation</b> <b>procedure.</b> It also provides validation to international courses without input from the ARB.|$|R
40|$|In this paper, {{we present}} and apply a {{computer-assisted}} method to study steady states of a triangular cross-diffusion system. Our approach consist in an a posteriori <b>validation</b> <b>procedure,</b> {{that is based}} on using a fxed point argument around a numerically computed solution, {{in the spirit of the}} Newton-Kantorovich theorem. It allows us to prove the existence of various non homogeneous steady states for different parameter values. In some situations, we get as many as 13 coexisting steady states. We also apply the a posteriori <b>validation</b> <b>procedure</b> to study the linear stability of the obtained steady states, proving that many of them are in fact unstable...|$|R
50|$|Validation is {{intended}} to ensure a product, service, or system (or portion thereof, or set thereof) results in a product, service, or system (or portion thereof, or set thereof) that meets the operational needs of the user. For a new development flow or verification flow, <b>validation</b> <b>procedures</b> may involve modeling either flow and using simulations to predict faults or gaps {{that might lead to}} invalid or incomplete verification or development of a product, service, or system (or portion thereof, or set thereof). A set of validation requirements (as defined by the user), specifications, and regulations may then be used as a basis for qualifying a development flow or verification flow for a product, service, or system (or portion thereof, or set thereof). Additional <b>validation</b> <b>procedures</b> also include those that are designed specifically to ensure that modifications made to an existing qualified development flow or verification flow will have the effect of producing a product, service, or system (or portion thereof, or set thereof) that meets the initial design requirements, specifications, and regulations; these validations help to keep the flow qualified. It is a process of establishing evidence that provides a high degree of assurance that a product, service, or system accomplishes its intended requirements. This often involves acceptance of fitness for purpose with end users and other product stakeholders. This is often an external process.|$|E
50|$|The {{system was}} mainly used for {{key-to-disk}} operations {{to replace the}} previously popular IBM card punches and more advanced key-to-tape systems manufactured for example by Mohawk Data Sciences (MDS) or Singer. In addition to the basic key-to-disk function, the proprietary operating system, called XLOS, supported indexed file operations for on-line transaction processing even with data journaling. The system was programmed in two different ways. The data entry was either described in several tables that specified {{the format of the}} input record with optional automatic data <b>validation</b> <b>procedures</b> or the indexed file operations were programmed in a special COBOL dialect with IDX and SEQ file support.|$|E
50|$|The huge volume, {{diversity}} (space and ground, {{visible and}} near infrared, morphometry, photometry and spectroscopy) {{and the high}} level of precision of measurements needed demand considerable care and effort in the data processing making this a critical part of the mission. ESA, the national agencies and the Euclid Consortium are spending considerable resources to set up top level teams of researchers and engineers in algorithm development, software development, testing and <b>validation</b> <b>procedures,</b> data archiving and data distribution infrastructures. In total, 9 Science Data Centres spread over countries of the Euclid Consortium will process more than 10 petabytes of raw input images over 10 years to deliver by 2028 a public data base of the Euclid mission to the whole scientific community.|$|E
40|$|This paper employs {{previously}} developed modeling, validation, and stimulation {{tools to}} address, {{for the first}} time, the realistic macroscopic simulation of a real large-scale motorway network. More specifically, the macroscopic simulator METANET, involving a second-order traffic flow model as well as network-relevant extensions, is utilized. A rigorous quantitative <b>validation</b> <b>procedure</b> is applied to individual network links, and subsequently a heuristic qualitative <b>validation</b> <b>procedure</b> is employed at a network level. The large-scale motorway network around Amsterdam, The Netherlands, is considered in this investigation. The main goal of the paper is to describe the application approach and procedures and to demonstrate the accuracy and usefulness of macroscopic modeling tools for large-scale motorway networks...|$|R
30|$|In {{order to}} address the {{scientific}} reliability of the developed biomass productivity models, a <b>validation</b> <b>procedure</b> was applied using an independent dataset. A remote sensing–derived productivity indicator was used to validate all three land use–specific soil productivity models independently.|$|R
40|$|A new mutual {{information}} based algorithm is introduced for term selection in spatio-temporal models. A generalised cross <b>validation</b> <b>procedure</b> is also introduced for model length determination and examples based on cellular automata, coupled map lattice and partial differential equations are described...|$|R
50|$|The U.S. Food and Drug Administration (FDA) has strict {{regulation}} {{about the}} cleaning validation. For example, FDA requires firms to have written general procedures on how cleaning processes will be validated. Also, FDA expects the general <b>validation</b> <b>procedures</b> to address {{who is responsible}} for performing and approving the validation study, the acceptance criteria, and when revalidation will be required. FDA also require firms to conduct the validation studies in accordance with the protocols and to document the results of studies.The valuation of cleaning validation is also regulated strictly, which usually mainly covers the aspects of equipment design,cleaning process written, analytical methods and sampling. Each of these processes has their related strict rules and requirements. Regarding to the establishment of limits, FDA does not intend to set acceptance specifications or methods for determining whether a cleaning process is validated. But some limits that have been mentioned by industry include analytical detection levels such as 10 PPM, biological activity levels such as 1/1000 of the normal therapeutic dose and organoleptic levels.|$|E
50|$|A boundary-layer radar wind {{profiler}} can be configured {{to compute}} averaged wind profiles for periods {{ranging from a}} few minutes to an hour. Boundary-layer radar wind profilers are often configured to sample in more than one mode. For example, in a “low mode,” the pulse of energy transmitted by the profiler may be 60 m in length. The pulse length determines the depth of the column of air being sampled and thus the vertical resolution of the data. In a “high mode,” the pulse length is increased, usually to 100 m or greater. The longer pulse length means that more energy is being transmitted for each sample, which improves the signal-to-noise ratio (SNR) of the data. Using a longer pulse length increases the depth of the sample volume and thus decreases the vertical resolution in the data. The greater energy output of the high mode increases the maximum altitude to which the radar wind profiler can sample, but at the expense of coarser vertical resolution and an increase in thealtitude at which the first winds are measured. When radar wind profilers are operated in multiple modes, the data are often combined into a single overlapping data set to simplify postprocessing and data <b>validation</b> <b>procedures.</b>|$|E
50|$|Introduction {{of organic}} matter into water systems occurs not only from living organisms and from {{decaying}} matter in source water, but also from purification and distribution system materials. A relationship may exist between endotoxins, microbial growth, {{and the development of}} biofilms on pipeline walls and biofilm growth within pharmaceutical distribution systems. A correlation is believed to exist between TOC concentrations and the levels of endotoxins and microbes. Sustaining low TOC levels helps to control levels of endotoxins and microbes and thereby the development of biofilm growth. The United States Pharmacopoeia (USP), European Pharmacopoeia (EP) and Japanese Pharmacopoeia (JP) recognize TOC as a required test for purified water and water for injection (WFI). For this reason, TOC has found acceptance as a process control attribute in the biotechnology industry to monitor the performance of unit operations comprising purification and distribution systems. As many of these biotechnology operations include the preparation of medicines, the U.S. Food and Drug Administration (FDA) enacts numerous regulations to protect the health of the public and ensure the product quality is maintained. To make sure there is no cross-contamination between product runs of different drugs, various cleaning procedures are performed. TOC concentration levels are used to track the success of these cleaning <b>validation</b> <b>procedures</b> especially clean-in-place (CIP).|$|E
40|$|The MASLab {{project was}} {{launched}} {{with the goal}} of developing a full mathematical model for several different aircraft. Since the model was to be used for the validation of a Flight Management System, it had to include modeling of the autopilot suites for all the modeled aircraft. This paper details the implementation and <b>validation</b> <b>procedure</b> employed to accomplish this part of the modeling exercise. Starting from a description of {{the context in which the}} model operates and of the architecture of the model itself, the autopilot suites are described both from a theoretical (mathematical) and a practical (implementation) point of view. The <b>validation</b> <b>procedure</b> is then outlined, and a selection of test results presented...|$|R
40|$|This {{document}} {{relates to}} the <b>validation</b> <b>procedure</b> for the Politecnico di Torino “Progetto Raddoppio”. In Italy, following the introduction of European regulations and current national legislation, validation management of the “construction process” is acquiring strategic value and aims to reduce problems encountered at works completion stage. In most cases at present, validation is via formal documentation control by the purchaser, whereas in other cases such as this, validation is performed by an external authority and involves systematic project and contractual document monitoring. The final validation management report should cover the entire procedure, from preliminary to executive project design. In the case study, the <b>validation</b> <b>procedure</b> was applied only in the executive design stage, and therefore the problems encountered as indicated under “observations” mainly relate to typical aspects concerning this project stage, e. g. technological difficulties and those relating to works management and construction site organisation (work safety management and environmental impact management). This document aims to clarify observations recorded in the <b>validation</b> <b>procedure,</b> explaining how these influenced the technological choices and works management aspects in the executive stage, including safety management, regarding both operators and environmental impact...|$|R
30|$|Considering the {{definition}} of mean error and standard deviation, the condition (4) corresponds to an admissible relative error on the measured braking distances of about 6  %– 6.5  %, which is thus larger than the one adopted for the TTBS 01 <b>validation</b> <b>procedure.</b>|$|R
40|$|The use {{of social}} <b>validation</b> <b>procedures</b> has become {{widespread}} in recent years. Although most researchers have used social <b>validation</b> <b>procedures</b> to select target behaviors and {{to evaluate whether}} the changes produced by a treatment program should be considered socially useful, {{little attention has been}} focused upon using the social validation process to determine the optimal levels for target behaviors. This paper suggests several ways in which social <b>validation</b> <b>procedures</b> can be employed in order to select when and how much to change target behaviors...|$|E
40|$|Abstract. This paper {{provides}} {{a discussion on}} <b>validation</b> <b>procedures</b> associated with heuristic solution approaches used in forest planning, initiated by Bettinger et al. (2008) (Bettinger, Sessions and Boston, 2008. A review of the status and use of <b>validation</b> <b>procedures</b> for heuristics used in forest planning. MCFNS 1 (1) : 26 – 37). Three issues are addressed...|$|E
40|$|This {{document}} {{establishes the}} guidelines for method selection and the procedures for verification of standard method performance, {{as well as the}} validation of non-standard methods. ORA laboratories verify standard method performance and validate nonstandard methods introduced into the laboratory. A. Directors: ensures implementation of method verification and <b>validation</b> <b>procedures.</b> B. Supervisors: implements method verification and <b>validation</b> <b>procedures</b> in respective division. C. Staff: adheres to written protocol for method performance verification, validation or modification...|$|E
40|$|This article {{describes}} <b>validation</b> <b>procedure</b> {{of the birds}} throw into gas-turbine engine (GTE) simulation model and the preceding verification of its main components. The provided data are including predictive and experimental data, as an additional documentary proof of the simulation model fidelity, repeatability and sensitivity...|$|R
40|$|Domain {{adaptation}} {{deals with}} adapting classifiers trained {{on data from}} a source distribution, to work effectively on data from a target distribution. In this paper, we introduce the Nonlinear Embedding Transform (NET) for unsupervised domain adaptation. The NET reduces cross-domain disparity through nonlinear domain alignment. It also embeds the domain-aligned data such that similar data points are clustered together. This results in enhanced classification. To determine the parameters in the NET model (and in other unsupervised domain adaptation models), we introduce a <b>validation</b> <b>procedure</b> by sampling source data points that are similar in distribution to the target data. We test the NET and the <b>validation</b> <b>procedure</b> using popular image datasets and compare the classification results across competitive procedures for unsupervised domain adaptation. Comment: AAAI Workshops 201...|$|R
30|$|The <b>validation</b> <b>procedure</b> for activating, deactivating, and {{releasing}} SPS is explained in section 9.2 of the 3 GPP standard [19]. In the SPS scheduling of VoLTE packets, the scheduler is switched off during silence periods. The proposed scheme gives high priority to SPS scheduling over default dynamic scheduling.|$|R
