16|132|Public
5000|$|Index {{keys are}} stored in {{normalized}} form which can be, in many cases, denormalized to the original column <b>value.</b> <b>Normalization</b> is not always reversible. For example, Text and Long Text column types cannot be denormalized [...] In addition, index keys may be truncated when column data is very long. In cases where columns cannot be retrieved directly from secondary indexes, the record can always be accessed to retrieve the necessary data.|$|E
40|$|This paper {{points out}} an {{inconsistency}} between SQL/XML {{on one side}} and XQuery and other XML-related specifications on the other side with respect to whether values for an XML attribute should be normalized as the current rules of XMLAttributes prescribe. This inconsistency is fixed by aligning SQL/XML with the other XML-related specification and removing the attribute <b>value</b> <b>normalization.</b> TC changes are needed but currently not included...|$|E
40|$|International audienceIn a {{previous}} paper [1], {{extensions of the}} 2 -level stochastic speech understanding system have been proposed. Firstly the 3 -level system is obtained through {{the introduction of a}} stochastic concept <b>value</b> <b>normalization</b> module. Then the 2 + 1 -level system is obtained as a degraded 3 -level system where the conceptual decoding and <b>value</b> <b>normalization</b> steps are decoupled, thus allowing to greatly reduce the model complexity and improve its trainability. In this paper, a multi-level spoken language understanding system is presented. This stochastic module is for the first time based on dynamic Bayesian networks. Factored language models with a generalized parallel backoff procedure are used as edge implementation to provide efficiently smoothed conditional probability estimates. This framework allows a great flexibility in terms of probability representation facilitating the development of the stochastic levels of the system. The proposed approaches, 3 -level and 2 + 1 -level, are evaluated on the French MEDIA task (tourist information and hotel booking). The MEDIA 10 k-utterance training corpus is segmentally annotated, allowing a direct training of the various levels of the conceptual models. The best DBN-based system obtains performance comparable to those of the MEDIA' 05 evaluation campaign best system [2]...|$|E
30|$|In this study, we further {{investigate}} the thyroid transcriptional response to 131 I and 211 At exposure by using normalized intensity values {{from three different}} experiments where separate analyses of each experiment have been published elsewhere [20 – 22]. All normalized intensity <b>values</b> (<b>normalization</b> was performed according to experiment) from these three experiments were imported together into Nexus Expression 3.0 (BioDiscovery; El Segundo, CA) for filtering, linear modelling and determination of differentially expressed genes.|$|R
3000|$|Step 1 Determine {{the fuzzy}} weights W_ij using {{modified}} FCM algorithm given in Eqs. (27)–(29) Initialize the fuzzy weights V_j [...] as fuzzy numbers randomly Calculate the initial <b>values</b> of <b>normalization</b> factor by Eq. (13) [...]...|$|R
30|$|The {{results of}} our CNF {{determination}} are presented in Fig.  5. In general, the <b>values</b> of <b>normalization</b> determined using the planar method 2 (with scatter correction) and all tomographic scans were very similar. However, when scatter correction was not applied (method 1), the CNF values were overestimated.|$|R
40|$|PT. Coats Rejo Indonesia is peripatetic {{company in}} the field of textile with {{producing}} yarn of Filament. PT. Coats Rejo Indonesia is company of big enough textile where in it there are some department of which support, departmental the example HRD, Purchasing, Quality Assurance, and etc. Then from one of this department specially part of Quality Assurance there are internal issue determine criterion of Quality Assurance staff. In the effort finishing the the problems hence trouble-shooting alternative by identifying and decision making analysis concerning planning of criterion of Quality Assurance staff by using method of Fuzzy Multi-Attribute Decision Making (FMADM) to know the each wight at alternative, as reference in decision making. From analysis taken is chosen criterion pursuant to priority rangking in the plan criterion of staff Quality Assurance at PT. Coats Rejo Indonesia use Fuzzy Multi Attribute Decision Making (FMADM) is criterion 1 (Skill). Seen from highest value which obtained with integral total method of value before and after its <b>value</b> <b>normalization</b> is : Before normalization = 0 : 0, 583 = 0, 5 : 0, 749 = 1 : 0, 916), Later;Then after normalization = 0 : 0, 142 = 0, 5 : 0, 135 = 1 : 0, 128). While for the index of performance before and after its <b>value</b> <b>normalization</b> is : before normalization : t = 0, 65 and after normalization t = 0, 76...|$|E
40|$|A sizable body of {{evidence}} {{has shown that the}} brain computes several types of value-related signals to guide decision making, such as stimulus values, outcome values, and prediction errors. A critical question for understanding decision-making mechanisms is whether these value signals are computed using an absolute or a normalized code. Under an absolute code, the neural response used to represent the value of a given stimulus does not depend on what other values might have been encountered. By contrast, under a normalized code, the neural response associated with a given value depends on its relative position in the distribution of values. This review provides a simple framework for thinking about <b>value</b> <b>normalization,</b> and uses it to evaluate the existing experimental evidence...|$|E
40|$|Although {{the impact}} of {{dopamine}} on reward learning is well documented, its influence on other aspects of behavior remains {{the subject of much}} ongoing work. Dopaminergic drugs are known to increase risk-taking behavior, but the underlying mechanisms for this effect are not clear. We probed dopamine’s role by examining the effect of its precursor L-DOPA on the choices of healthy human participants in an experimental paradigm that allowed particular components of risk to be distinguished. We show that choice behavior depended on a baseline (ie, value-independent) gambling propensity, a gambling preference scaling with the amount/variance, and a <b>value</b> <b>normalization</b> factor. Boosting dopamine levels specifically increased just the value-independent baseline gambling propensity, leaving the other components unaffected. Our results indicate that the influence of dopamine on choice behavior involves a specific modulation of the attractiveness of risky options—a finding with implications for understanding a range of reward-related psychopathologies including addiction...|$|E
40|$|This thesis {{discusses}} preprocessing {{methods in}} knowledge discovery. There are four methods of discretization, {{filling in the}} missing <b>values</b> and <b>normalization</b> is investigated. There is closer view on building the application which discretizate numeric atribute in database. All methods were successfully tested {{and the results were}} added...|$|R
40|$|Microarrays are {{a widely}} used {{research}} tool in gene expression analysis. A large variety of preprocessing methods for raw intensity measures {{is available to}} establish gene expression <b>values.</b> <b>Normalization</b> is the key stage in preprocessing methods, since it removes systematic variations in microarray data. Then, the subsequent analyses may be highly dependent on normalization strategy employed. Our research focuses on detecting rhythmic signals in measured circadian gene expressions. We have observed that rhythmicity detection depends not only upon the rhythmicity detection algorithm but also upon the normalization strategy employed. We analyze the effects of well-known normalization strategies in literature within three different rhythmicity detection algorithms; JTK, RAIN and our recently proposal ORI, a novel statistical methodology based on Order Restricted Inference. The results obtained are compared using artificial microarray data and publicly available circadian data bases...|$|R
3000|$|... and the {{distributed}} arrows {{illustrate the}} snapshot of E. Here, the electrical field takes the normalized <b>value,</b> with the <b>normalization</b> obtained by dividing with |E [...]...|$|R
40|$|In {{this paper}} we {{introduce}} the topic input validation, analyze its {{great importance to}} Web applications and suggest a new comprehensive approach to input validation. The approach has been developed {{as a result of}} an evaluation of current input validation approaches that showed that no sufficient solution to common input validation requirements is available at present. The paper describes important requirements for input validation frameworks, especially in the Web context, and introduces main concepts of this approach. The approach is based on the declarative, rule based definition of validation logic and the automatic translation of validation rules into server side and client side code. It supports conditional, composite and complex inter-field validation scenarios. It considers topics such as <b>value</b> <b>normalization,</b> inter-field dependencies and validation actions and integrates these aspects into a consistent validator based system. Our evaluation shows the benefits of this new approach and highlights its advantages compared to other popular and promising approaches such as PowerForms or Topes...|$|E
40|$|African trypanosomiases are {{parasitic}} diseases transmitted by tse-tse flies, considered {{as the main}} sanitary obstacle to animal production development in sub-Saharan Africa. However, if trypanosomiases have dramatic consequences on zebu (Bos indicus) populations, they have a weaker impact on the western African taurine (Bos taurus), which {{is known to be}} naturally tolerant to trypanosome infection. Mechanisms governing this trypanotolerant trait are still poorly understood, but today, recent postgenomic biotechnologies, such as the SAGE technique (serial analysis of gene expression) allow us to explore the full transcriptome. Twelve SAGE libraries were constructed from two trypanotolerant animals (N'Dama and Baoule) and one susceptible species of cattle (the Sudanese zebu) during an experimental Trypanosoma congolense infection; 43, 458 different tags were obtained at several particular points during the infection (before infection, at the maximum of parasitemia, the maximum of anemia, {{and at the end of}} the experiment after <b>value</b> <b>normalization).</b> Bioinformaties analyses highlighted some interesting gene variations with respect to the trypanotolerance status of the animal...|$|E
30|$|Soil half-lives {{derived from}} time series data from {{laboratory}} simulation and field studies show large variability between different tests. Extrapolation of such results to conditions {{other than those}} used in the test (e.g., average environmental conditions) is a critical step that would require detailed knowledge {{about the effect of}} sorption and consideration of the large natural variability of biomass activity and biological degradation potential for the compounds of interest. Normalization for temperature and soil moisture is a common procedure in characterizing P and is required to extrapolate degradation rates to different scenarios. For the most part, normalization reduces the variability of field degradation rates measured under extreme conditions of temperature and moisture content. In order to compare field degradation rates with a single trigger <b>value,</b> <b>normalization</b> could aid arrival at a more conclusive decision. If normalization is not applied, differences in P in the field could be due to the time of application e.g., spring versus autumn. Normalization is usually not required for assessment of P in water and sediment because half-lives are mostly measured under laboratory standard conditions, e.g., room temperature.|$|E
40|$|High-throughput {{microarray}} {{technologies are}} a widely used research tool in gene expression analysis. A large variety of preprocessing methods for raw intensity measures {{is available to}} establish gene expression <b>values.</b> <b>Normalization</b> is the key stage in preprocessing methods, since it removes systematic variations in microarray data. Then, {{the choice of the}} normalization strategy can make a substantial impact to the final results. Additionally, we have observed that the identification of rhythmic circadian genes depends not only on the normalization strategy but also on the rhythmicity detection algorithm employed. We analyze three different rhythmicity detection algorithms. On the one hand, JTK and RAIN which are widely extended among biologists. On the other hand, ORIOS, a novel statistical methodology which heavily relies on Order Restricted Inference and that we propose to detect rhythmic signal for Oscillatory Systems. Results on the determination of circadian rhythms are compared using artificial microarray data and publicly available circadian data bases...|$|R
5000|$|... where 257.7 the <b>normalization</b> <b>value,</b> n is {{the number}} of carbon-carbon bonds, and d are bond lengths in {{angstrom}} (dopt is optimized (1.388 Å) and di is the experimental or computed).|$|R
30|$|In {{order to}} avoid {{rigidity}} problem, significance ratings, weight assign <b>values,</b> and <b>normalization</b> factor as given in Table  1 were used to compute both extended {{as well as the}} proposed simplified indices. Extended indices: WQIobj and WQIsub (considering all 17 parameters), were calculated using Eq. (3), while WQIobj was calculated with k[*]=[*] 1 in all the cases, because all the sites along Damodar river appeared clear during the time of sampling.|$|R
40|$|The {{aim of the}} article: the {{peculiarities of}} the modern methods of {{treatment}} efficiency objective assessment in considerable contingent of patients with intraperitoneal bleeding of a genital origin are investigated. Material et metods: The original data of intra- and postoperative medical interventions efficiency are given and analyzed in 78 patients who were operated laparoscopically owing to intraperitoneal bleedings up to 500 ml of blood. Results. Circulating blood volume in these patients was restituted by blood Autohaemoreinfusion together with donors' blood and blood-substituting compounds injection. Red blood cells and hemoglobin content together with heart rate variability indexes were taken as the markers of the patients' organism functional condition and rehabilitation efficacy estimation. One month after miniinvazive treatment in women with acute intraperitoneal blood loss above 500 ml one could see certain sympathetic activation with reciprocal parasympathetic activity suppression together with pertinent activation degree <b>value</b> <b>normalization.</b> Conclusions. The conclusion was made that HRV data analysis allows to make an objective estimation of women with acute genital intraperitoneal bleeding (above 500 ml) treatment efficacy. The last {{is very important in}} the prognostic aspect because there are few objective methods of afteroperational state analysis in patients with bleeding of different origin...|$|E
40|$|OBJECTIVE: To {{measure the}} {{cross-sectional}} area (CSA) of the median nerve by ultrasonography (US) {{before and after}} surgery in subjects with carpal tunnel syndrome (CTS), and to verify whether the normalization of presurgical parameters can be predicted by presurgical CSA values. PATIENTS AND METHODS: Sixty-seven consecutive cases, mean age 60. 5 years, underwent surgical decompression. Before surgery, clinical and electrophysiological severity and self-assessment of symptoms (using the Boston questionnaire, BQ) were recorded. CSAs were measured proximal to the carpal tunnel inlet (CSA-I), at mid-tunnel (CSA-M), and at the tunnel outlet (CSA-O). Follow-ups were performed 1 and 6 months after surgery. Logistic regressions were performed with normalization of CSA, clinical and electrophysiological parameters as independent variables, and presurgical findings as dependent variables. RESULTS: Before and after surgery there were correlations between CSA-I and clinical and electrophysiological severity scales. After 1 and 6 months, the clinical, electrophysiological, and BQ findings improved. CSA-I reduced at the 1 -month follow-up and CSA-O increased {{between the first and}} second follow-up. Presurgical values of CSA-I could predict the normalization of its postsurgical <b>value,</b> <b>normalization</b> of the clinical severity scale, BQ, and full patient satisfaction postsurgery. CONCLUSIONS: CSA-I is the most sensitive US measurement before surgery. The presurgical value of CSA-I is a predictor of postsurgical normalization of clinical parameters and of its own value...|$|E
40|$|BACKGROUND Excessive alcohol {{consumption}} starts {{to change the}} structure of transferrin into thecarbohydrate-deficient transferrin (CDT). CDT is the only specific laboratory marker foralcohol dependency. However no one at all is sensitive enough. Method In our study we wanted to confirm the alcohol dependency by means of questionnairesand laboratory markers, particularly CDT. RESULTS Investigated were 68 (92. 6 % males, 7. 4 % females) general practice healthy patientsand blood donors and 186 (89. 2 % males, 10. 8 % females) inpatient alcoholics. A bloodsample was taken once from every healthy subject and three times from every alcoholic: onadmission to hospital, after 12 days and again after 42 days. In alcoholics we found statistically significantly elevated CDT, glutamate dehidrogenaze(GLDH), aspartate-aminotransferaze (AST), alanine-aminotransferaze (ALT), gamaglutamyltransferaze (GGT) and mean corpuscular volume (MCV) and decreased ureavalues. CDT was the most reliable marker with high specificity (91. 2 %) and sensitivity(81. 4 %). The area under ROC-curve was exceptional with 99. 9 %. The kinetics of CDT, AST, ALT, GGT, MCV and creatinine normalization after 14 dayswere also statistically significant as well as kinetics of CDT, AST, GGT, MCV and creatininenormalization after 42 days. We estimated the course of CDT <b>value</b> <b>normalization</b> as themost applicable. The most important diagnostic marker combination was composed by CDT, MCV and AST. GGT had lower importance as expected. CONCLUSIONS In our population CDT was the most specific marker of all, attaining high sensitivity. But itwas not reliable enough as the sole marker or as the screening one for alcohol dependency. Its main applicability is in diagnostics of alcoholism, control of alcohol abstention andforensic cases but solely {{in combination with other}} laboratory markers of alcoholis...|$|E
30|$|Equation (3) {{actually}} {{determines the}} normalized {{distance of a}} rating x_ij from the associated target <b>value</b> t_j. The <b>normalization</b> technique can also be employed in traditional MULTIMOORA approach in which beneficial and non-beneficial attributes only exist.|$|R
40|$|Description Statistical {{tools for}} ChIP-seq data analysis. The package {{includes}} the statistical method described in Kaufmann et al. (2009) PLoS Biology: 7 (4) :e 1000090. Briefly, Taking the average DNA fragment size subjected to sequencing into account, the software calculates genomic single-nucleotide read-enrichment <b>values.</b> After <b>normalization,</b> sample and control are compared using a test {{based on the}} Poisson distribution. Test statistic thresholds to control the false discovery rate are obtained through random permutation...|$|R
40|$|Abstract Shotgun proteomic {{data are}} {{affected}} {{by a variety of}} known and unknown systematic biases as well as high proportions of missing <b>values.</b> Typically, <b>normalization</b> is performed in an attempt to remove systematic biases from the data before statistical inference, sometimes followed by missing value imputation to obtain a complete matrix of intensities. Here we discuss several approaches to normalization and dealing with missing values, some initially developed for microarray data and some developed specifically for mass spectrometry-based data. </p...|$|R
30|$|The results {{produced}} by many MVA methods depend strongly {{on the data}} matrix preprocessing. Data preprocessing includes normalization, mean centering, scaling and transformation. The goal of data preprocessing is to remove variance from the matrix that is not due to chemical differences between the samples. This could include variation due to the instrumentation, differences in the absolute intensity of peaks within a spectrum, differences due to topography or other factors. The assumptions made when preprocessing a data matrix have been addressed [33]. Normalization is done by dividing each variable (peak) in the matrix by a scalar <b>value.</b> <b>Normalization</b> is done to remove variance in the data that is due to differences such as sample charging, instrumental conditions or topography. It should be noted however that normalization can accentuate noise in ToF-SIMS images due to the low count rates often found within the data. Therefore, care should be taken when using normalization of ToF-SIMS images. Mean centering subtracts the mean of each variable (peak) from the data set. This allows the data to be compared across a common mean of zero. Scaling refers to dividing each variable by a constant, and transformation refers to transforming the data with a function such as the logarithm or square root. There are many different ways to scale and transform a data set. It is recommended that one selects a scaling and transformation method based on the experimental uncertainty of each peak. ToF-SIMS data often follows a Poisson distribution. Keenan and Kotula [39, 44, 45] proposed a scaling method that accounts for Poisson noise which {{has been found to}} work well for ToF-SIMS data. If Poisson scaling is not available in the software package being used, other good alternatives include using a square root transform of the data or scaling by the square root of the mean of the data. Regardless of the preprocessing methods used, one should understand the assumptions made [33] when applying a given method and choose a method based on what they are trying to learn from the data and not what gives the best looking results.|$|E
40|$|Life Cycle Assessment (LCA) is an {{emerging}} supporting tool {{designed to help}} practitioner in systematically assessing the environmental performance of selected product's life cycle. A product's life cycle includes the extraction of raw materials, production, and usage, and ends with waste treatment or disposal. Life cycle impact assessment (LCIA) {{as a part of}} LCA is a method used to derive the environmental burdens from selected product's stages. LCIA is structured in classification, characterization, normalization and weighting. Presently most of the LCIA practices use European database to establish the characterization, normalization and weighting value. However, using these values for local LCA practice {{might not be able to}} reflect the actual Malaysian's environmental scenario. The aim of this study is to create a Malaysian version of normalization and weighting value using the pollution database within Malaysia. This research work used the LCIA top-down approach of the Eco-indicator 99 impact assessment methodology as a guide for the formulation of normalization and weighting <b>value.</b> <b>Normalization</b> values were formulated based on Malaysia's pollutant emission rate while the weighting values were formulated using the results of questionnaire survey that was distributed among local LCA practitioner and experts. The results from the studies showed that the normalization values for Malaysia in average are four times higher compared to European value while the weighting values for Malaysia are almost similar to the reference European value. In this study, an attempt was made to formulate Malaysian normalization and weighting values of the LCIA step using the Eco-indicator methodology. By adopting these values, the LCA study conducted will be more accurate and meaningful in evaluating Malaysian environmental impact. As a recommendation, this study can be further expanded by using different type of software and different type of method in formulating the normalization and weighting values. Besides that, a better collection with more recent data should be use for estimation of the normalization values. While in the weighting values estimation, a bigger size and different social background of panel should be used in the questionnaire survey approach to indicate different perception of average Malaysian citizen on environmental concerns...|$|E
40|$|Indocyanine green (ICG) is a {{fluorescent}} dye {{used as an}} indicator in medicine and surgery. The maximum absorption wavelength of ICG is at 785 nm, while the maximum emission is around 820 nm. ICG is nontoxic and is rapidly excreted into the bile. Near infrared (NIR) fluorescence imaging or spectroscopy offer new settings for seeing the blood vessels, and also in oncological applications for finding sentinel lymph nodes (SLN) to investigate if the cancer has spread from the tumor to the lymphatic system. Given the aforementioned applications, {{the aim of this}} thesis was to develop a hardware control and a user interface in LabVIEW, and to evaluate the software, as well as the instrumentation using phantom measurements. The system consisted of a spectrometer, a laser (785 ± 5 nm) for ICG excitation, optical filters, and a fiber optical probe containing five fibers for light excitation, and one for light collection. The basic LabVIEW program designed for the spectrometer was used, and additional features were added such as the recording functions, online measurements, opening of the recorded files, saving comments, and a loop was created for the laser control. Optical phantoms were prepared to model tissue for measurements using 20 % intralipid that gave μs = 298 mm−¹ at the excitation wavelength. Agar 1 % w/v and ICG were added to the phantoms using different fluorophore concentrations of 2 μg/mL, 10 μg/mL, 20 μg/mL, 25 μg/mL, and 40 μg/mL. The objective was to perform controlled measurements of steady state ICG fluorescence, the dynamics of photobleaching at different concentrations, and to find the optimal ICG concentration for obtaining the maximum fluorescence intensity. The light to excite ICG fluorescence emission was provided by using a laser output power of 10. 4 mW and 200 ms of integration time in the spectrometer for optimal measurements. Measurements using the different gel phantoms showed maximum fluorescence ICG concentration to be between 16 μg/mL and 20 μg/mL. Moreover, photobleaching measurements showed to be ICG concentration-dependent, where those concentrations higher than the optimal one incrementally photobleached with time after being exposed to light. Higher concentrations presented an incremental photobleaching where they first reached a maximum peak and then the intensity decayed with time. Additionally, laser reflection at 782 nm showed that the reflection increased with time ranging from 130 % – 460 % as the ICG photobleached to 50 % of its initial <b>value.</b> <b>Normalization</b> of ICG by the laser reflection signal was investigated to compensate for the intensity variations due to the measurement parameters including the distance from the light source to the target, and the angle of inclination of the probe. The lowest ICG concentration detectable by the system was 0. 05 μg/mL. In conclusion, a LabVIEW hardware control and user interface was developed for controlling the spectrometer and the laser. Several measurements were made using the different phantoms, where the optimal concentration of ICG was estimated. It was shown that ICG fluorescence intensity and photobleaching behavior were dependent on the concentration. The results gave suggestions for future experimental design. NIR...|$|E
3000|$|Competitive {{propagation}} can {{be effective}} only if the competing metrics are well balanced. Otherwise, one would dominate the other. Exponential distance definition in (5) and (6) normalizes plain and gradient-sticky distances. A proper balance between them is achieved using appropriate <b>values</b> of the <b>normalization</b> factors h [...]...|$|R
40|$|The {{regional}} {{variability of}} some geochemical parameters in the Gulf of Trieste is considered {{in terms of}} their relationship with Al, used as a normalization factor. Baselines calculated from these relationships are used to determine a simple enrichment factor for each element, defined as the ratio between the actual and predicted baseline <b>value.</b> The <b>normalization</b> procedure permits a new non dimensional reference baseline to be obtained that could help to assess the size of possible anomalies and to provide information on the diffusion and dispersion patterns of pollutants inside the monitored area...|$|R
40|$|The maximum {{likelihood}} method offers a standard way {{to estimate the}} three parameters of a generalized extreme value (GEV) distribution. Combined with the block maxima method, it is often used in practice to assess the extreme <b>value</b> index and <b>normalization</b> constants of a distribution satisfying a first order extreme value condition, assuming implicitely that the block maxima are exactly GEV distributed. This is unsatisfactory since the GEV distribution is a good approximation of the block maxima distribution only for blocks of large size. The {{purpose of this paper}} is to provide a theoretical basis for this methodology. Under a first order extreme value condition only, we prove the existence and consistency of the {{maximum likelihood}} estimators for the extreme <b>value</b> index and <b>normalization</b> constants within the framework of the block maxima method. Comment: 18...|$|R
5000|$|... here [...] is {{the set of}} EOFs {{on which}} the {{reconstruction}} is based. The <b>values</b> of the <b>normalization</b> factor , {{as well as of}} the lower and upper bound of summation [...] and , differ between the central part of the time series and the vicinity of its endpoints (Ghil et al., 2002).|$|R
40|$|A good leaders or Government is {{the basic}} need to develop country. In India, who is largest {{democratic}} {{country in the world}} people are not fully involved in the selection process of Leaders. On an average there are 60 - 65 % voting is done. For this purpose we have build the Data Warehouse that containing all the information related to election to increase awareness of voting. Using this we can findinteresting patterns that are extracted and represented using Visual Data mining to arrange awareness program. The approach is divided into 5 phases: I) Data Preprocessing: II) Data Warehouse Creation:III) Task-Relevant Data Extraction; IV) Data Mining and V) Visualization. Generally, data collected from various cities of a state may be noisy, inconsistent and requires cleaning hence the database is preprocessed for missing <b>values,</b> <b>normalization</b> etc. in first phase. In Data Warehouse Creation phase, warehouse with vote as measure in fact and voter Gender, voter age, voter education, candidate, Religion, time, session, word as dimension. Word dimension has 4 -level concept hierarchy of country, state, city, area/word id. Time dimension has concept hierarchy with year, Quarter, month, and date. Similarly, candidate has 2 level and Voter age, voter education, Religion, and session have level 1 concept hierarchy. In Last phase, results of data mining phase are represented using Visual Data mining techniques...|$|R
40|$|The authors {{present a}} review of their {{experience}} with posterior fossa tumours in adults {{during a period of}} 20 years. Symptoms, physical findings, tumour histology, operative treatment and results are reviewed in 109 cases. The <b>value</b> of preoperative <b>normalization</b> of intracranial pressure by insertion of a ventriculo-subcutaneous drainage is discussed. status: publishe...|$|R
30|$|Here, α is the {{weighted}} coefficient, P_Nout^* and R_total^* are min-max <b>normalization</b> <b>values.</b> Finding the optimal solution analytically in closed form {{does not seem}} feasible, numerical solutions are possible with very low effort. Nevertheless, the pareto solutions of the optimization problem can be obtained using the above equation and numerical experiments based on the multi objective optimization model.|$|R
30|$|In the {{quantitative}} assessment of rCBF using PET, CT, and magnetic resonance imaging [MRI], it is recognized that the global variation in CBF may {{have a significant impact}} on the regional variation [15 – 18]. We, thus, repeated the comparison of grey matter <b>values</b> after <b>normalization</b> to the volume-weighted average of the four white matter ROIs. The two-tailed significance level for the paired t tests was at α = 0.05, but a Bonferroni correction was applied for multiple non-independent comparisons. Thus, the thresholds were p < 0.0036 (14 comparisons) and p < 0.005 (10 comparisons) for the quantified and white matter normalized analyses, respectively.|$|R
40|$|ObjectiveThe role of matrix metalloproteinases {{and their}} tissue {{endogenous}} inhibitors {{has been documented}} in abdominal aortic aneurysms, but few articles have investigated their role after thoracic aortic aneurysm treatment. Our report investigates matrix metalloproteinases and tissue endogenous inhibitor- 1 plasmatic changes in patients who have undergone endovascular aneurysm repair for descending thoracic aortic aneurysms and assesses their clinical significance. MethodsThirty-two patients with thoracic aortic aneurysms who underwent endovascular aneurysm repair were compared with 25 healthy volunteers. Plasma matrix metalloproteinase- 3 /matrix metalloproteinase- 9 and tissue endogenous inhibitor- 1 values were determined by an enzyme-linked immunosorbent assay method at a predetermined time interval. ResultsThe preoperative levels of matrix metalloproteinases in the endovascular aneurysm repair group were 3 -fold and 2 -fold higher {{than those in the}} control group (P <. 001 and. 02, respectively). Matrix metalloproteinase values normalized after endovascular aneurysm repair, whereas patients experiencing endoleaks had higher matrix metalloproteinase values and matrix metalloproteinase- 9 /tissue endogenous inhibitor- 1 ratio compared with the control group (P =. 003, <. 001, and =. 02, respectively, at 1 -month follow-up). These values normalized with the resolution of the endoleak. ConclusionsPlasma matrix metalloproteinase values are increased in patients with thoracic aortic aneurysms, along with reduced tissue endogenous inhibitor- 1 expression. Successful endovascular aneurysm repair results in <b>values</b> <b>normalization,</b> whereas high levels persist in patients with endoleaks. The enzyme-linked immunosorbent assay test is a simple and reliable technique that is useful to assess the efficacy of endovascular aneurysm repair and to detect endoleaks...|$|R
