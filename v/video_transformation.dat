7|62|Public
50|$|Another {{advantage}} is that depth maps are created {{in the course of}} 2D-to-stereo 3D conversion using almost any approach to this <b>video</b> <b>transformation.</b> That is why there is the respective 2D-plus-Depth representation of a converted stereo footage in almost all cases. Considering the lack of 3D content shot in stereo and the number of converted 3D films, this is a big benefit.|$|E
40|$|While camcorders {{have become}} a {{commodity}} home appliance, few watch the recorded videos or share them with friends and relatives due {{to the difficulty of}} turning the raw footage into a compelling video story. Previous works on Automated Video Editing (AVE) developed an automatic solution for video content selection and video-music matching. However, to automatically apply <b>video</b> <b>transformation</b> effects, such as fast/slow motion, thresholding, binarization and watercolor, has not been solved or even addressed. In this paper, we proposed several automatic video effect and content matching schemes, which facilitate generating more compelling and interesting AVE results. 1...|$|E
40|$|Online video lectures are {{becoming}} commonplace in higher education. One {{problem is that}} the instructor might block what he has written on the board. In our project, we reconstruct the online lecture video so that the information on the whiteboard is always available to the viewers. The background subtraction, object tracking, and object replacement technique have been used to reconstruct the video. The complete access of the whiteboard information is obtained by replacing the lecturer/object with whiteboard information from the future frames or the past. The object is detected by using background subtraction method and is tracked by object tracking technique. The algorithm can be implemented for different board conditions and video resolutions. Keywords- background subtraction, object tracking, whiteboard reconstruction, object replacement, online lectures, and <b>video</b> <b>transformation</b> 1...|$|E
5000|$|Blender {{features}} a fully functional, production ready Non-Linear video editor called Video Sequence Editor or VSE for short. Blender's VSE has many features including effects like Gaussian Blur, color grading, Fade and Wipe transitions, and other <b>video</b> <b>transformations.</b> However, {{there is no}} multi-core support for rendering video with VSE.|$|R
40|$|We {{describe}} {{our system}} for Content-Based Copy Detection (CBCD) task submitted to TRECVID 2010. Our system is multi-modal and integrates {{the results of}} global visual features, local visual features and audio features to produce the final run results. Each of these features is designed {{to take care of}} different aspects of the <b>video</b> <b>transformations.</b> We submitted two runs each for BALANCED as well as NOFA profile: • KDDILabs-SRI. m. balanced. ...|$|R
40|$|Abstract — In {{this paper}} a low-complexity and highlyintegrated IP Core for image/video transformations is presented. It is {{possible}} to perform quantized 8 × 8 DCT and quantized 8 × 8 / 4 × 4 integer transforms on the presented reconfigurable architecture using only shift and add operations. The XVID experimental and FPGA synthesis {{results show that the}} proposed architecture not only achieves multiplierless <b>video</b> <b>transformations</b> efficiently, but also retains good transformation quality. It is worth noticing that the proposed IP Core is very suitable for low-complexity and multi-purpose Video CODECs in SoC designs...|$|R
40|$|The fixed-length {{sliding window}} {{approach}} {{is widely used}} in video search applications. This approach computes the similarity between a given query and each windowed sequence, and determines whether their similarity ex-ceeds a predefined threshold. However, using the fixed window length and the predefined threshold is inflexible for detecting video copies {{that might have been}} derived by applying a variety of video transformations. To ad-dress this problem, we present a novel matching scheme, which estimates an appropriate window length and threshold for each matching by inferring the context of the query and the current window. Moreover, to make the matching more efficient, we adopt a coarse-to-fine strat-egy which utilizes the compact min-hashing signature for fast filtering and robust spatio-temporal analysis for de-tailed verification. Substantial experiments show that the proposed video matching framework yields the robust accuracy and efficient computation for handling every type of <b>video</b> <b>transformation...</b>|$|E
40|$|Video copy {{detection}} is {{an important}} task with many applications, especially since detecting copies is an alternative to watermarking. In this paper we describe a simple, but efficient approach that is easy to parallelize, works well, and has low storage requirements. We represent each video frame by a count {{of the number of}} SURF interest points in each of 4 by 4 quadrants, a total of 16 bytes per frame. This representation is tolerant of the typical transformations that exist in video, but is still computationally efficient and compact. The approach was tested on the TRECVID copy detection task, for which approximately 15 different groups submitted a solution. Performance was among the best for localization, and was approximately equal to the median with regards to the false positive/negative rate. However, performance varies significantly with the <b>video</b> <b>transformation.</b> We believe that the change in gamma, and decrease in video quality transformations are the most common in practice. For these two transformations our method works well. 1...|$|E
40|$|The {{increase}} in the number of video copies, both legal and illegal, has become a major problem in the multimedia and Internet era. In this paper, we propose a novel method for detecting various video copies in a video sequence. To achieve fast and robust detection, the method fully integrates several components, namely the min-hashing signature to compactly represent a video sequence, the spatio-temporal matching scheme to accurately evaluate the video similarity compiled from the spatial and temporal aspects, and some speed-up techniques to expedite both min-hashing indexing and spatio-temporal matching. The results of experiments demonstrate that, compared to several baseline methods with different feature descriptors and matching schemes, the proposed method that combines both global and local feature descriptors yields the best performance when encountering a variety of <b>video</b> <b>transformation.</b> The method is very fast, requiring approximately 0. 06 seconds to search for copies of a thirty-second video clip in a six-hour video sequence...|$|E
40|$|Passive content {{fingerprinting}} {{is widely}} used for video content identification and monitoring. However, many challenges remain unsolved especially for partial-copies detection. The main challenge {{is to find the}} right balance between the computational cost of fingerprint extraction and fingerprint dimension, without compromising detection performance against various attacks (robustness). Fast video detection performance is desirable in several modern applications, for instance, in those where video detection involves the use of large video databases or in applications requiring real-time video detection of partial copies, a process whose difficulty increases when <b>videos</b> suffer severe <b>transformations.</b> In this context, conventional fingerprinting methods are not fully suitable to cope with the attacks and transformations mentioned before, either because the robustness of these methods is not enough or because their execution time is very high, where the time bottleneck is commonly found in the fingerprint extraction and matching operations. Motivated by these issues, in this work we propose a content fingerprinting method based on the extraction of a set of independent binary global and local fingerprints. Although these features are robust against common <b>video</b> <b>transformations,</b> their combination is more discriminant against severe <b>video</b> <b>transformations</b> such as signal processing attacks, geometric transformations and temporal and spatial desynchronization. Additionally, we use an efficient multilevel filtering system accelerating the processes of fingerprint extraction and matching. This multilevel filtering system helps to rapidly identify potential similar video copies upon which the fingerprint process is carried out only, thus saving computational time. We tested with datasets of real copied videos, and the results show how our method outperforms state-of-the-art methods regarding detection scores. Furthermore, the granularity of our method makes it suitable for partial-copy detection; that is, by processing only short segments of 1 second length...|$|R
40|$|II This thesis’s {{topic is}} Content-Based Video Copy Detection. After {{bringing}} out {{the definition of the}} task and its mass application, we introduce two kinds of existing mainstream algorithms from the view of features used. Then, an algorithm based on ordinal measure and statistics histogram is proposed and tested on the database MUSCLE-VCD- 2007 [1] provided by CIVR. The result shows it represents relatively low time complexity but fails to detect certain kinds of <b>video</b> <b>transformations.</b> So we propose an image-sequence matching algorithm based on SURF[19]. Its performance and complexity are very satisfying. Especially, this algorithm represents high local accuracy, which can be further improved by a successive step...|$|R
5000|$|Many fanvids are {{concerned}} with shipping, the love stories that a fan sees within her favorite films or TV shows. Existing romantic scenes may be further romanticized through <b>video</b> <b>transformations</b> or song choice. The author may also argue for a romantic pairing that does not occur in the source text through juxtaposing relevant scenes or even splicing in additional material. Supporters or [...] "shippers" [...] of on-screen couples may also manipulate clips to retroactively change scenes to fit a within-vid reality that incorporates their pairing.This led to the inevitable creation of a whole new vocabulary currently in use amongst vidders which includes terms such as shipwars and OTPs.|$|R
40|$|This paper briefly summarises {{our results}} on gaze {{guidance}} such as {{to complement the}} demonstrations that we plan to present at the workshop. Our goal is to integrate gaze into visual communication systems by measuring and guiding eye movements. Our strategy is to predict a set of about ten salient locations and then change the probability {{for one of these}} candidates to be attended: for one candidate the probability is increased, for the others it is decreased. To increase saliency, in our current implementation, we show a natural-scene movie and overlay red dots very briefly such that they are hardly perceived consciously. To decrease the probability, for example, we locally reduce the temporal frequency content of the movie. We here present preliminary results, which show that the three steps of our above strategy are feasible. The long-term goal is to find the optimal real-time <b>video</b> <b>transformation</b> that minimises the difference between the actual and the desired eye movements without being obtrusive. Applications are in the area of vision-based communication, augmented vision, and learning...|$|E
50|$|A <b>video</b> {{depicting}} SingPost's <b>transformation</b> {{story was}} recently published by Flyin Peach on 19 December 2016.|$|R
40|$|International audienceThis paper {{introduces}} a video copy detection system which efficiently matches individual frames and then verifies their spatio-temporal consistency. The approach for matching frames {{relies on a}} recent local feature indexing method, {{which is at the}} same time robust to significant <b>video</b> <b>transformations</b> and efficient in terms of memory usage and computation time. We match either keyframes or uniformly sampled frames. To further improve the results, a verification step robustly estimates a spatio-temporal model between the query video and the potentially corresponding video segments. Experimental results evaluate the different parameters of our system and measure the trade-off between accuracy and efficiency. We show that our system obtains excellent results for the TRECVID 2008 copy detection task...|$|R
5000|$|Jonathan Rogness, University of Minnesota, is {{well known}} for his {{beautiful}} mathematical visualizations, including an award-winning <b>video,</b> Möbius <b>Transformations</b> Revealed, which went viral online and has been viewed by nearly two million people. At MathPath he has taught cryptology and the Shape of Space.|$|R
40|$|In this paper, a novel {{method is}} {{presented}} to detect video copies for a given video query. These copies and the query have identical or near-duplicate content, which might differ in their spatiotemporal structures slightly. To address both the efficient and effective issues, we conduct the bag-of-words model for video feature representation, and apply a coarse-to-fine matching scheme to analyze the video spatiotemporal structure. The proposed method can deal with various kinds of <b>video</b> <b>transformations,</b> such as cropping, zooming, speed change, and subsequence insertion/deletion, which are not well addressed in existing methods. Besides, two indexing methods are employed {{to speed up the}} matching process. Experimental results show that the proposed method can behave in an efficient and effective manner. 1...|$|R
40|$|We present several {{theoretical}} contributions {{which allow}} Lie groups {{to be fit}} to high dimensional datasets. Transformation operators are represented in their eigen-basis, reducing the computational complexity of parameter estimation to that of training a linear transformation model. A transformation specific "blurring" operator is introduced that allows inference to escape local minima via a smoothing of the transformation space. A penalty on traversed manifold distance is added which encourages the discovery of sparse, minimal distance, transformations between states. Both learning and inference are demonstrated using these methods for the full set of affine transformations on natural image patches. Transformation operators are then trained on natural video sequences. It is shown that the learned <b>video</b> <b>transformations</b> provide a better description of inter-frame differences than the standard motion model based on rigid translation...|$|R
40|$|Content-Based Copy Detection (CBCD) {{of digital}} videos is an {{important}} research field that aims at the identification of modified copies of an original clip, e. g., on the Internet. In this application, the video content is uniquely identified by the content itself, by extracting some compact features that are robust to a certain set of <b>video</b> <b>transformations.</b> Given the huge amount of data present in online video databases, the computational complexity of the feature extraction and comparison {{is a very important}} issue. In this paper, a landmark based multi-dimensional scaling technique is proposed to speed up the detection procedure which is based on exhaustive search and the MPEG- 7 Dominant Color Descriptor. The method is evaluated under the MPEG Video Signature Core Experiment conditions, and simulation results show impressive time savings at the cost of a slightly reduced detection performance...|$|R
40|$|Abstract. Every Möbius {{transformation}} can {{be constructed}} by stereographic projection of the complex plane onto a sphere, followed by a rigid motion of the sphere and projection back onto the plane, illustrated in the <b>video</b> Möbius <b>Transformations</b> Revealed. In this article we show that, for a given Möbius transformation and sphere, this representation is unique. Acknowledgements: I would like to sincerely thank Dr. Jonathan Rogness for his generou...|$|R
40|$|Video {{identification}} or copy detection is {{a challenging}} problem and {{is becoming increasingly}} important with {{the growing popularity of}} online video services. The problem addressed in this paper is the identification of a given video clip in a given set of videos. For a given query video, the system returns all the instance of the video in the data set. This identification system uses video signatures based on video tomography. A robust and low complexity video signature is designed. The signatures are generated for video shots and not individual frames. This results in a compact signature of 64 bytes per video segment / shot. The signatures are matched using simple Euclidean distance metric. Both signature generation and matching have a very low complexity. The system was evaluated using the dataset from video copy detection evaluations organized at the CIVR 2007. The results show that the proposed signatures outperform the results reported in the CIVR competition. The results also show that the signature is robust to common <b>video</b> <b>transformations...</b>|$|R
2500|$|The Wholeness of Life: Part I (1978). Contains an {{abridgement}} {{of discussions}} between Krishnamurti, physicist David Bohm, and psychiatrist David Shainberg, held in 1976. Available on <b>video</b> as The <b>Transformation</b> of Man, see Audio and video resources below.|$|R
40|$|Abstract—We {{propose a}} video copy {{detection}} system which is sufficiently robust to detect transformed versions of original videos with ability to pinpoint location of copied segments. Precisely {{due to the}} good stability and discriminative capability, SIFT feature is used for visual content description. However SIFT based feature matching has high computational cost due to large number of keypoints and high dimensionality of its feature vector. Thus to minimize computational complexity, video content is divided into number of segments depending on homogeneity of video content. SIFT features are extracted from keyframes of video segments. Then SIFT features are quantized to generate clusters. Further binary SIFT are generated for every cluster for optimized matching. To perform video segment matching with SIFT descriptors, firstly visual words matching is done at cluster level then at feature level, similarity measure is employed. In order to find video copy detection, an optimal sequential pattern matching is done which will ensure effectiveness and efficiency of the proposed video copy detection system. Our video copy detection system gives improved results in terms of accuracy and {{has been able to}} detect most of the dataset query <b>video</b> <b>transformations</b> in time-efficient manner...|$|R
40|$|According to a {{study by}} the International Data Corporation (IDC), the digital {{universe}} is doubling in size every two years to rich 44 trillion gigabytes by 2020. A large part of this big universe consists of audio and videos (e. g. music, TV shows and films), which are distributed over the Internet in an effortless way. This has increased the need for powerful tools to handle this data in terms of identification, filtering and retrieval. In this context, multimedia copy detection, which consists of identifying duplicate (or near duplicate) multimedia content, has become an emerging and active research area due to its broad applications. Multimedia copy detection can be used in a wide variety of applications such as broadcast monitoring, music identification, copyright control, law enforcement investigation and music library organization. Content-Based Copy Detection (CBCD) has been recently introduced as a solution to the problem of multimedia copy detection. This approach extracts fingerprints from a candidate copy and then compares them against fingerprints of the original content. However, audio and video signals are subjected to various kinds of transformations that make robust fingerprint extraction challenging. Thus, fingerprints should be robust to a variety of audio and <b>video</b> <b>transformations</b> and also discriminate against imposter fingerprints. In addition, the search of a candidate copy against a large dataset of fingerprints should be very fast. In this thesis, we propose an efficient multimedia copy detection system that is highly robust to a variety of audio and <b>video</b> <b>transformations.</b> We first describe a new audio feature extraction schema that allows the generation of three kinds of audio fingerprints. We then address the problem of video copy detection and we describe two video fingerprint extraction algorithms. In addition, we propose a fusion technique that combines the results achieved separately from the audio and the video parts to tackle the problem of audio+video copy detection. In the last part of this thesis, we address the problem of fingerprint retrieval, and we propose two solutions to improve the speed of the search algorithm. In the first solution we propose to parallelize the similarity search algorithm by using a Graphics Processing Unit (GPU), whereas the second solution is based on a clustering technique. We evaluate the proposed systems on the TRECVID 2009 and 2010 datasets, and we evaluate our approaches in terms of detection performance, localization accuracy and run time. In addition, we demonstrate the effectiveness of our methods by comparing them to several state-of-the-art audio and video copy detection systems...|$|R
40|$|Abstract—Content-based video copy {{detection}} is very impor-tant for {{copyright protection}} {{in view of}} the growing popularity of video sharing websites, which deals with not only whether a copy occurs in a query video stream but also where the copy is located and where the copy is originated from. While a lot of work has addressed the problem with good performance, less effort has been made to consider the copy detection problem {{in the case of a}} continuous query stream, for which precise temporal localization and some complex <b>video</b> <b>transformations</b> like frame insertion and video editing need to be handled. We attempt to attack the problem by presenting a frame fusion based copy detection approach, which converts video copy detection to frame similarity search and frame fusion under a temporal consistency assumption. Our work focuses mainly on the frame fusion stage due to its critical role in copy detection performance. The proposed frame fusion scheme is based on a Viterbi-like algorithm, comprising an online back-tracking strategy with three relaxed constraints. The experimental results show that the proposed approach achieves high localization accuracy in both the query stream and the reference database even when a query video stream undergoes some complex transformations, while achieving comparable performance compared with state-of-the-art copy detection methods. Index Terms—Frame fusion, HMM, video copy detection, viterbi algorithm...|$|R
50|$|The {{morphing}} function {{can transform}} photos {{from one person}} to another. To do so, the persons must be photographed in similar postures and their shapes need to be cut out {{with the help of the}} software. After doing so, points of support are set on noticeable spots (eyes, nose, mouth, etc.). Finally, the software calculates the intermediate images and creates a <b>video</b> of the <b>transformation.</b>|$|R
30|$|To {{describe}} the inverse perspective projection, we first need {{to define the}} forward perspective projection that describes the image capturing process. Much of this section follows the standard model for the projective camera as described by Hartley and Zissermann [7]. This model describes a transformation from the 3 D world axes to the 2 D image axes of the captured <b>video</b> frame. This <b>transformation</b> consists of two steps.|$|R
5000|$|The {{music video}} for [...] "Fantasy" [...] was {{directed}} by Ryan Littman and Berman Fenelus and filmed at the Tribeca Cinemas in New York in April, 2010. [...] The concept of the <b>video</b> was the <b>transformation</b> and elevation of Ali as the 'Queen of Clubs'. The theme of the video was used to promote Ali's upcoming greatest hits collection,Queen of Clubs Trilogy: The Best of Nadia Ali Remixed.|$|R
5000|$|R has 9 unknowns which {{correspond}} to the rotational angle with each axis and T has 3unknowns (Tx,Ty,Tz) which account for translation in X,Y and Z directions respectively.This motion(3-D) in time when captured by a camera(2-D) corresponds to change ofpixels in the subsequent frames of the <b>video</b> sequence. This <b>transformation</b> {{is also known as}} 2-D rigid body motion or the 2-D Euclidean transformation. It can be written as: ...|$|R
40|$|Abstract. UAV Video {{is rapidly}} {{emerging}} as a widely used source of imagery for many applications in recent years. This paper presents our research on the mosaic of UAV video {{for the purpose of}} harbor surveillance. First, one new framework on estimating <b>video</b> frame <b>transformation</b> with Optical flow is presented in this paper. For this new framework, fewer number of Gaussian pyramid is created for implementing for the multiresolution approach and thus, more details for optical flow computation is well kept; Second, we make a discussion on using Fourier-Mellin Transformation in image frequency domain to estimate initial motion parameter of adjacent video frames and with those initial motion parameters, small displacements for optical flow computation can be achieved; The experimental results demonstrated that the mosaic image generated from aerial video shows satisfied visual quality and its surveillance application for fast response to time-critical event, e. g., flood, is descried...|$|R
40|$|In this work, {{we focus}} on a {{challenging}} task: synthesizing multiple imaginary videos given a single image. Major problems come from high dimensionality of pixel space and the ambiguity of potential motions. To overcome those problems, we propose a new framework that produce imaginary <b>videos</b> by <b>transformation</b> generation. The generated transformations are applied to the original image in a novel volumetric merge network to reconstruct frames in imaginary video. Through sampling different latent variables, our method can output different imaginary video samples. The framework is trained in an adversarial way with unsupervised learning. For evaluation, we propose a new assessment metric $RIQA$. In experiments, we test on 3 datasets varying from synthetic data to natural scene. Our framework achieves promising performance in image quality assessment. The visual inspection indicates that it can successfully generate diverse five-frame videos in acceptable perceptual quality. Comment: 9 pages, 10 figure...|$|R
40|$|Face-off is an {{interesting}} case of style transfer where the facial expressions and attributes of one person could be fully transformed to another face. We {{are interested in the}} unsupervised training process which only requires two sequences of unaligned video frames from each person and learns what shared attributes to extract automatically. In this project, we explored various improvements for adversarial training (i. e. CycleGAN[Zhu et al., 2017]) to capture details in facial expressions and head poses and thus generate <b>transformation</b> <b>videos</b> of higher consistency and stability. Comment: Github repo: [URL]...|$|R
50|$|Together with a {{colleague}} (Jonathan Rogness), Arnold produced a <b>video</b> explaining Möbius <b>transformations</b> which won an honorable mention in a contest sponsored by Science magazine and the National Science Foundation in 2007.The video was watched over a million times at YouTube.Other honors include the award of a Guggenheim Fellowship in 2008 and election as a foreign member of the Norwegian Academy of Science and Letters in 2009. In 2012 he became {{a fellow of the}} American Mathematical Society. In 2009, he was named a fellow of the Society for Industrial and Applied Mathematics.|$|R
40|$|We {{reflect on}} two methods that explore {{personal}} experience of natural places within a human-centred framework for design. The methods {{attend to the}} meanings people make of their experiences in natural places and emotional and intellectual sense {{of being in a}} place. We use Egocentric Point-of-View <b>video</b> to explore <b>transformations</b> of immediate sensory transactions in natural places and Nature Probes to elicit affective qualities of experiences. We apply McCarthy and Wright’s dialogical approach (2005) to uncover relationships between place and self and discuss insights emerging from our methods with respect to belonging to a community through its natural landscape...|$|R
40|$|This thesis {{deals with}} modern methods of a lossy still image and <b>video</b> compression. Wavelet <b>transformation</b> and SPIHT {{algorithm}} also belong to these methods. In {{second half of}} this thesis, a videocodec is implemented based on acquired knowledge. This codec uses Daubechies wavelets to analyse an image. Afterwards there is a modified SPIHT algorithm applied on gained coefficients. A lot of effort was put in order to optimize this computation. It is possible to use the created codec in Video for Windows, DirectShow and FFmpeg multimedia frameworks. At {{the end of this}} thesis, commonly used codecs are compared with newly created one...|$|R
40|$|We {{present a}} fuzzy color histogram-based shot-boundary {{detection}} algorithm specialized for content based copy detection applications. The proposed method aims to detect both cuts and gradual transitions (fade, dissolve) effectively in <b>videos</b> where heavy <b>transformations</b> (such as cam-cording, insertions of patterns, strong re-encoding) occur. Along with the color histogram generated with the fuzzy linking method on L*a*b* color space, the system extracts a mask for still regions {{and the window}} of picture-in-picture transformation for each detected shot, which will be useful in a content-based copy detection system. Experimental results show that our method effectively detects shot boundaries and reduces false alarms {{as compared to the}} state-of-the-art shot-boundary detection algorithms...|$|R
