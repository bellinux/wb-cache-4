4|7|Public
50|$|The 2008 {{fiscal year}} saw the {{creation}} of the Sandy Springs Traffic Management Center (TMC). The TMC was constructed and began to operate in less than six months. Construction began in February 2008, five cameras viewed traffic along Roswell Road by the end of June. Special features of the TMC include a webpage that allows the public access to real-time traffic conditions and <b>voice-activated</b> <b>controls.</b> By June 2009, 16 traffic cameras are now available and can be viewed online at the city's website.|$|E
5000|$|Hands-free {{legislation}} across North America {{has provided}} {{increased interest in}} iLane and similar in-car telematics systems offering voice control. Ford SYNC and GM OnStar and MyLink are examples of factory-installed infotainment systems also designed to mitigate distracted driving with <b>voice-activated</b> <b>controls.</b> One key difference is that iLane is an after-market solution and can go with the driver from car to car. The hardware that provides the iLane platform can be powered by any car’s cigarette lighter and is compatible with nearly any Bluetooth audio system and smartphone using Blackberry Operating System 4.1 or higher.|$|E
40|$|A {{helicopter}} simulation {{was conducted}} to examine the interaction between <b>voice-activated</b> <b>controls</b> and altitude-hold automation. A single-pilot scout/attack helicopter with conventional controls was simulated with full out-the-window computer-generated imagery. Helicopter pilots performed simulated missions which included hover, cruise and ground attack phases. For half of the flights, <b>voice-activated</b> <b>controls</b> were used instead of manual controls, for two flight tasks (weapon selection and data-burst transmission). Hover performance during data-burst hover improved when voice-activation was used. However, subjective ratings indicated higher workload levels with voice-activation. An altitude-hold automation manipulation was crossed with the voice-activation/manual manipulation. Altitude-hold automation provided two significant benefits: (1) pilots were able to hover with less lateral movement; and (2) pilots reported less workload. However, there was a performance cost associated with the automation. Pilots were significantly slower in responding to unexpected events during hover...|$|E
5000|$|Safko managed two Apple Computer stores {{during his}} time at BWIP. He {{subsequently}} invented SoftVoice, the world’s first <b>voice-activated,</b> environmental <b>control</b> computer system for the physically disabled, in 1985, and has subsequently created further assistive-technology software. [...] Safko subsequently founded a second company, Safko International, Inc.|$|R
5000|$|In {{the default}} mode the device {{continuously}} listens to all speech, monitoring for the wake word to be spoken, which is primarily {{set up as}} [...] "Alexa" [...] (derived from Alexa Internet, the Amazon-owned Internet indexing company). The device also comes with a manually and <b>voice-activated</b> remote <b>control</b> {{which can be used}} in lieu of the 'wake word'. Echo's microphones can be manually disabled by pressing a mute button to turn off the audio processing circuit.|$|R
50|$|The {{following}} {{features were}} {{standard in the}} Q45: all-leather interior, power sunroof, 8-way power front seats, remote keyless entry, rain-detecting wipers and side curtain airbags. Standard electronics include: CD, 8 Bose speakers, trip computer, RearView Monitor parking camera system, and a voice-activated navigation system. The 2002 Q45 was the first vehicle to ever offer <b>voice-activated</b> navigation <b>controls</b> and a reverse parking camera outside Japan, following the JDM 1991 Toyota Soarer. The car seated five and had a base price of US$56,200.|$|R
40|$|Given the {{proliferation}} of ‘intelligent’ and ‘socially-aware’ digital assistants embodying everyday mobile technology – and the undeniable logic that utilising <b>voice-activated</b> <b>controls</b> and interfaces in cars reduces the visual and manual distraction of interacting with in-vehicle devices – it appears inevitable that next generation vehicles will be embodied by digital assistants and utilise spoken language {{as a method of}} interaction. From a design perspective, defining the language and interaction style that a digital driving assistant should adopt is contingent on the role that they play within the social fabric and context in which they are situated. We therefore conducted a qualitative, Wizard-of-Oz study to explore how drivers might interact linguistically with a natural language digital driving assistant. Twenty-five participants drove for 10 min in a medium-fidelity driving simulator while interacting with a state-of-the-art, high-functioning, conversational digital driving assistant. All exchanges were transcribed and analysed using recognised linguistic techniques, such as discourse and conversation analysis, normally reserved for interpersonal investigation. Language usage patterns demonstrate that interactions with the digital assistant were fundamentally social in nature, with participants affording the assistant equal social status and high-level cognitive processing capability. For example, participants were polite, actively controlled turn-taking during the conversation, and used back-channelling, fillers and hesitation, as they might in human communication. Furthermore, participants expected the digital assistant to understand and process complex requests mitigated with hedging words and expressions, and peppered with vague language and deictic references requiring shared contextual information and mutual understanding. Findings are presented in six themes which emerged during the analysis – formulating responses; turn-taking; back-channelling, fillers and hesitation; vague language; mitigating requests and politeness and praise. The results can be used to inform the design of future in-vehicle natural language systems, in particular to help manage the tension between designing for an engaging dialogue (important for technology acceptance) and designing for an effective dialogue (important to minimise distraction in a driving context) ...|$|E
50|$|Matthews' {{involvement}} {{in trying to}} mesh human voices to technology was {{many years in the}} making. A fellow friend and pilot perished in a mid-air collision, which Matthews believed was caused when he momentarily took his eyes off of his plane's controls to adjust his radio frequency. After he was discharged from the military, Matthews went to work for IBM to help develop <b>voice-activated</b> cockpit <b>controls</b> which would help lessen similar types of catastrophic errors in the future. After IBM, Matthews went to work for Texas Instruments in 1966.|$|R
50|$|The {{complete}} package concept would end and digital receivers would return but with lower power ratings, and matching cassette recorders were far {{smaller than the}} previous models, offering less complexities and timing devices, but quality was starting to slip, {{as compared to the}} build and bulk of the previous earlier models. Eventually the Optonica brand was eliminated but would emerge once again in the late 1980s as a line of high-end stereo television receivers, VCR, CD, portable cassette and CD radios, <b>voice-activated</b> remote <b>controls,</b> and surround sound receivers for television and audio listening. These products were available until the early 1990s, and that marked the end of the Optonica high-end line of audio and video products.|$|R
30|$|In {{order to}} allow drivers to {{maintain}} {{their eyes on the}} forward roadway, nearly every vehicle sold in the US and Europe can now be optionally equipped with an in-vehicle information system (IVIS). Using voice commands, drivers can access functions as varied as voice dialing, music selection, GPS destination entry, and even climate <b>control.</b> <b>Voice-activated</b> features {{would seem to be a}} natural evolution in vehicle safety that requires little justification. However, a large and growing body of literature cautions that auditory/vocal tasks may have unintended consequences that adversely affect traffic safety (e.g., Bergen, Medeiros-Ward, Wheeler, Drews, & Strayer, 2013).|$|R
40|$|Facial {{expression}} is a visible {{manifestation of the}} affective state, cognitive activity, intention, personality and psychopathology of a person; it not only expresses our expressions, but also provides important communicative cues during social interaction. Expression recognition can be embedded into a face recognition system to improve its robustness. In a real-time face recognition system where a series of images of an individual are captured, facial expression recognition (FER) module picks the one which is most similar to a neutral expression for recognition, because normally a face recognition system is trained using neutral expression images. In the case where only one image is available, the estimated expression can be used either to decide which classifier to choose or to add some kind of compensation. In a human-computer interaction (HCI), {{expression is}} an input of great potential in terms of communicative cues. This {{is especially true in}} <b>voice-activated</b> <b>control</b> systems. This implies an FER module can markedly improve the performance of such systems. Customer&# 039;s facial expressions can also be collected by service providers as implicit user feedback to improve their service. Compared with a conventional questionnaire-based method, this should be more reliable and furthermore, has virtually no cost. The main challenge for FER system is to attain the highest possible classification rate for the recognition of six expressions (Anger, Disgust, Fear, Happy, Sad and Surprise). The other challenges are the illumination variation, rotation and noise. In this thesis, several innovative methods based on image processing and pattern recognition theory have been devised and implemented. The main contributions of algorithms and advanced modelling techniques are summarized as follows. 1) A new feature extraction approach called HLAC-like (higher-order local autocorrelation-like) features has been presented to detect and to extract facial features from face images. 2) An innovative design is introduced with the ability to detect cases using face feature extraction method based on orthogonal moments for images with noise and/or rotation. Using this technique, the expression from face images with high levels of noise and even rotation has been recognized properly. 3) A facial expression recognition system is designed based on the combination region. In this system, a method called hybrid face regions (HFR) according to the combined part of an image is presented. Using this method, the features are extracted from the components of the face (eyes, nose and mouth) and then the expression is identified based on these features. 4) A novel classification methodology has been proposed based on structural similarity algorithm in facial expression recognition scenarios. 5) A new methodology for expression recognition is presented using colour facial images based on multi-linear image analysis. In this scenario, the colour images are unfolded to two dimensional (2 -D) matrix based on multi-linear algebra and then classified based on multi-linear discriminant analysis (LDA) classifier. Furthermore, the colour effect on facial images of various resolutions is studied for FER system. The addressed issues are challenging problems and are substantial for developing a facial expression recognition system...|$|R

