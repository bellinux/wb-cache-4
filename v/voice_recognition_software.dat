65|2176|Public
25|$|Pop-on or pop-up or block: A caption {{appears on}} any of the 14 screen rows as a {{complete}} sentence, which can be followed by additional captions. This method is used when captions come from an intermediary file (such as the Scenarist or EBU STL file formats) for pre-taped television and film programming, commonly produced at captioning facilities. This method of captioning can be aided by digital scripts or <b>voice</b> <b>recognition</b> <b>software,</b> and if used for live events, would require a video delay to avoid a large delay in the captions' appearance on-screen, which occurs with Teletext-encoded live subtitles.|$|E
50|$|Perio Charting: <b>voice</b> <b>recognition</b> <b>software</b> compatible.|$|E
50|$|While {{working in}} Europe, Eisinger helped expose frauds at Lernout & Hauspie, a Belgian company specializing in <b>voice</b> <b>recognition</b> <b>software,</b> and Élan, an Irish {{pharmaceutical}} company.|$|E
40|$|When {{psychological}} {{experiments are}} run using computers, most reactions are {{input from the}} keyboard. However, {{there are a lot}} of experiments whose reactions are obtained by <b>voice.</b> Speech <b>recognition</b> <b>software</b> was applied to psychological experiments，and methods of measuring reactions by voices were shown. 　コンピュータで心理学実験を行う場合，反応の多くがキーボードからの入力となる。しかし，声による反応が重要な要素である実験も多い。ここでは，心理学実験に音声認識ソフトウェアを応用し，声による反応をとる方法が示された...|$|R
40|$|This paper {{presents}} ongoing {{research on}} human-computer interaction in virtual environments using <b>voice</b> <b>recognition</b> systems. The paper starts describing <b>voice</b> <b>recognition</b> technologies (<b>software</b> and hardware based). The Virtual Reality Laboratory of the University of Colima has begun an initial {{development of a}} desktop virtual environment where a virtual molecule was manipulated and analyzed through voice commands. A hardware-based <b>voice</b> <b>recognition</b> system was used. Initial tests showed that the neural net, integrated in the <b>voice</b> <b>recognition</b> system, was fast and accurate enough {{to interact with the}} virtual environment...|$|R
40|$|The {{original}} article is available as an open access file on the Springer website {{in the following}} link: [URL] paper presents the <b>voice</b> emotion <b>recognition</b> part of the FILTWAM framework for real-time emotion recognition in affective e-learning settings. FILTWAM (Framework for Improving Learning Through Webcams And Microphones) intends to offer timely and appropriate online feedback based upon learner’s vocal intonations and facial expressions in order to foster their learning. Whereas the facial emotion recognition part has been successfully tested in a previous study, the here presented study describes the development and testing of FILTWAM's vocal emotion <b>recognition</b> <b>software</b> artefact. The main goal {{of this study was}} to show the valid use of computer microphone data for real-time and adequate interpretation of vocal intonations into extracted emotional states. The software that was developed was tested in a study with twelve participants. All participants individually received the same computer-based tasks in which they were requested eighty times to mimic specific vocal expressions (960 occurrences in total). Each individual session was recorded on video. For the validation of the <b>voice</b> emotion <b>recognition</b> <b>software</b> artefact, two experts annotated and rated participants' recorded behaviours. Expert findings were then compared with the <b>software</b> <b>recognition</b> results and showed an overall accuracy of Kappa of 0. 743. The overall accuracy of the <b>voice</b> emotion <b>recognition</b> <b>software</b> artefact is 67 % based on the requested emotions and the recognized emotions. Our FILTWAM-software allows to continually and unobtrusively observing learners’ behaviours and transforms these behaviours into emotional states. This paves the way for unobtrusive and real-time capturing of learners' emotional states for enhancing adaptive e-learning approaches. The Netherlands Laboratory for Lifelong Learning (NELLL) of the Open University Netherland...|$|R
5000|$|Pro Audio 16 Basic: Stripped down {{version of}} Pro Audio Spectrum 16, without SCSI interface, the bundled <b>voice</b> <b>recognition</b> <b>software</b> and microphone; {{based on the}} MVD101 chipset.|$|E
50|$|Dentrix {{products}} include practice management {{software for}} dental offices, imaging software, patient education software, computer-based training software, <b>voice</b> <b>recognition</b> <b>software,</b> {{and other products}} designed to enhance the dental office experience.|$|E
50|$|Use of <b>voice</b> <b>recognition</b> <b>software,</b> in {{conjunction}} with a digital audio recorder and a personal computer running word-processing software has proven to be positive for restoring damaged short-term-memory capacity, in stroke and craniotomy individuals.|$|E
5000|$|Speech RecognitionLingvoSoft uses Lingvobit speech {{recognition}} technology {{in most of}} its applications. This technology allows applications to [...] "understand" [...] voice commands {{and to provide a}} translation based on this input. Following initial <b>voice</b> <b>recognition</b> training, the <b>software</b> is able to choose a translation that matches from a set number of phrases. This technology is also used for voice navigation in many of the company's applications.|$|R
40|$|<b>Voice</b> <b>recognition</b> (VR) <b>software</b> has {{increased}} in accuracy {{and ease of use}} over the last decade. While VR has been seen as carrying the potential to significantly ease the transcription process, only recently has it gained enough accuracy and ease of use to become a valid option to manually typed transcription of qualitative data. However, the use of VR transcription in Mixed Methods research has largely remained unexplored. This article aims to illustrate how VR software is useful when transcribing open-ended questionnaires and interviews in mixed methods research. A significant amount of time was saved yet valuable insights of emerging themes were gained at an early stage of the data processing...|$|R
40|$|Hardware and <b>software</b> of a <b>voice</b> <b>recognition</b> {{system are}} constructed. It's {{hardware}} {{consists of a}} <b>voice</b> <b>recognition</b> board and a host computer FM- 7. The former is constructed using the set of LSI's for processing words spoken by specific speakers, and {{a small number of}} standard IC's. The set of LSI's are MC 4760,μPD 7761 D and μPD 7762 G made by NEC. According to the 12 commands reserved by the processing unit μPD 7762 G for <b>voice</b> <b>recognition,</b> the system <b>software</b> is programmed with the FBASIC. According to the number of registered standard patterns and/or the repetitive number of speeches per word or syllable, the exprimental results of identification accuracy are shown. When the number of standard patterns and the repetitive number are two respectively, the average accuracy of identification is above 94 %, which seems to be a comparatively satisfactory result...|$|R
50|$|INETCO {{was founded}} in 1984 by Angus Telfer, an {{engineer}} who worked at several telecom companies, including Bell-Northern Research, and Bell Telephone Laboratories, designing and developing <b>voice</b> <b>recognition</b> <b>software,</b> protocol analysis, network monitoring, telecommunications billing, and quality of service technologies.|$|E
50|$|The game is also {{compatible}} with the PlayStation 4 and Xbox One's <b>voice</b> <b>recognition</b> <b>software,</b> allowing players {{to talk to their}} race engineers during the race and ask for race information, weather updates, and tyre status, and even request a change of tyres or wing.|$|E
50|$|These {{innovations}} provide benefits {{available by}} existing products but {{result in a}} new set of actions for the consumer. <b>Voice</b> <b>recognition</b> <b>software</b> is one example of this type of innovation. Consumers create documents or emails, for example, by dictating (instead of typing) to a computer.|$|E
40|$|This paper {{presents}} {{exploration of}} speech enable operating systems, software, and applications. It {{begins with a}} description of how such systems work, and the level of accuracy that can be expected. It explains the applications of speech recognition technology in different areas education, medical, mobile computing, railway reservation, dictation, and web browsing. A brief comparison of the operating systems supported for <b>voice,</b> speech <b>recognition</b> <b>software</b> or tool. It gives the brief introduction about the potential of voice/speech <b>recognition</b> <b>software.</b> It explains the feature of different speech enable Operating system and speech <b>recognition</b> <b>software.</b> Windows speech <b>recognition</b> have many innovative features for Windows operating system and efficiently assist the computer to control, dictate, navigate, selecting the words, sending emails and correcting the words or sentences. It also explains the benefits and issue related to speech technology. In last era speech recognition technology grew tremendously. There are large number of companies who are working in these area and developing software {{for the people who are}} not able to control the system through keyboard or mouse such as physically impaired and senior citizens. This paper gives a brief introduction of speech enabled OS and speech <b>recognition</b> <b>software.</b> Comment: 9 pages, Proc. of the International Conference on System Modeling and Advancement in Research Trends (SMART), Teerthankar Mahaveer University, Moradabad, UP, Indi...|$|R
40|$|This {{paper has}} been already {{published}} on the Springer website and is available online in the following link: [URL] paper introduces {{the integration of the}} face emotion recognition part and the <b>voice</b> emotion <b>recognition</b> part of our FILTWAM framework that uses webcams and microphones. This framework enables real-time multimodal emotion recognition of learners during game-based learning for triggering feedback towards improved learning. The main goal {{of this study is to}} validate the integration of webcam and microphone data for a real-time and adequate interpretation of facial and vocal expressions into emotional states where the software modules are calibrated with end users. This integration aims to improve timely and relevant feedback, which is expected to increase learners’ awareness of their own behavior. Twelve test persons received the same computer-based tasks in which they were requested to mimic specific facial and vocal expressions. Each test person mimicked 80 emotions, which led to a dataset of 960 emotions. All sessions were recorded on video. An overall accuracy of Kappa value based on the requested emotions, expert opinions, and the recognized emotions is 0. 61, of the face emotion <b>recognition</b> <b>software</b> is 0. 76, and of the <b>voice</b> emotion <b>recognition</b> <b>software</b> is 0. 58. A multimodal fusion between the software modules can increase the accuracy to 78 %. In contrast with existing software our software modules allow real-time, continuously and unobtrusively monitoring of learners’ face expressions and voice intonations and convert these into emotional states. This inclusion of learner's emotional states paves the way for more effective, efficient and enjoyable game-based learning. The Netherlands Laboratory for Lifelong Learning (NELLL) of the Open University Netherland...|$|R
40|$|Software {{development}} environments {{have changed}} little since their origins as low-level text editors. Programmers with repetitive strain injuries and other motor disabilities can find these environments {{difficult or impossible}} to use due to their emphasis on typing. Our research adapts <b>voice</b> <b>recognition</b> to the <b>software</b> development process, both to mitigate this difficulty and to provide insight into natural forms of high-level interaction. Our contribution is to use program analysis to interpret speech as code, thereby enabling {{the creation of a}} program editor that supports voicebased programming. We have created Spoken Java, a variant of Java which is easier to verbalize than its traditional typewritten form, and an associated spoken command language to manipulate code. We are conducting user studies to understand the cognitive effects of spoken programming, as well as to inform the design of the language and editor. ...|$|R
50|$|Additionally, {{both the}} TIPC and TIPPC were {{equipped}} with <b>voice</b> <b>recognition</b> <b>software,</b> allowing {{the user to}} speak basic commands to the computers. Both computers also featured an Ethernet card, a new device developed in 1983 by 3COM. The TIPC and the TIPPC were superseded by IBM Personal Computers and compatible machine.|$|E
5000|$|... iFlytek (...) , styled as iFLYTEK, is a Chinese {{information}} technology company established in 1999. It creates <b>voice</b> <b>recognition</b> <b>software</b> and 10+ voice-based internet/mobile products covering education, communication, music, intelligent toys industries. 15% of their shares will be bought by China Mobile. It {{is listed in}} the Shenzhen Stock Exchange with market capitalization at 25 billion RMB.|$|E
50|$|Captions {{are created}} by a {{communications}} assistant using a computer with <b>voice</b> <b>recognition</b> <b>software.</b> The communications assistant listens to and revoices the hearing party's side of the conversation into the microphone of a headset. A voice recognition program creates the captions and they are sent out to the captioned telephone where they are read by the user.|$|E
50|$|VDW {{technology}} has also undergone an evolution as more competitors {{have entered the}} market. The first solutions were based on dedicated and rugged voice appliances, mobile computing devices that ran the speech <b>recognition</b> <b>software</b> and that communicated with a server over a wireless network. These special purpose voice appliances use a speech recognition engine that was specially designed for the warehouse and provided by the appliance manufacturer. Since the early 2000s, more voice suppliers have entered the market, providing <b>voice</b> <b>recognition</b> systems for standard mobile computing devices {{that had been used}} previously for barcode scanning applications in the warehouse. These standard mobile computers from companies like Motorola, Intermec and LXE also support non-proprietary <b>recognition</b> <b>software.</b> The uncoupling of the hardware and speech <b>recognition</b> <b>software</b> has resulted in lower priced voice-directed warehousing solutions and {{an increase in the number}} of software providers. These two factors contributed to a rapid rise in adoption of VDW that continues today.|$|R
5000|$|Phillips {{returned}} to MIT as a visiting scientist and co-founded Vlingo in 2006 with former SpeechWorks colleague John Nguyen. An intelligent software assistant, Vlingo is a speech-to-text application integrated with user-facing apps for iPhone, Android, BlackBerry, and other smartphones. Vlingo software allowed users to text and navigate smartphones via <b>voice</b> <b>recognition.</b> The first cell phone speech <b>recognition</b> <b>software</b> that successfully interpreted user input and learned over time, the software {{would later be}} adapted into the popular personal assistant software Siri.|$|R
25|$|On 25 September 2013, Nintendo {{announced}} it had purchased a 28% {{stake in a}} Panasonic spin-off company called PUX Corporation. The company specializes in face and <b>voice</b> <b>recognition</b> technology, with which Nintendo intends to improve the usability of future game systems. Nintendo has also worked with this company {{in the past to}} create character <b>recognition</b> <b>software</b> for a Nintendo DS touchscreen. After announcing a 30% dive in profits for the April to December 2013 period, president Satoru Iwata announced he would take a 50% pay-cut, with other executives seeing reductions by 20%–30%.|$|R
5000|$|... "A soaPOPera for Laptops/ Mac Minis", 1997-2005(with Peter Sinclair) Four {{computers}} {{talk to one}} another, using {{synthetic voice}} and <b>voice</b> <b>recognition</b> <b>software.</b> During performances the computers are mounted on robot vehicles. They {{interact with each other}} and the human performers. When a guitar is played the computers sing along. In installation format the computers gossip with each other.|$|E
50|$|In 2009 {{his wife}} died of breast cancer. She had {{originally}} been diagnosed when she was eight weeks pregnant with their only child {{and by the end}} lost the ability to write. He made a font for his wife to use via <b>voice</b> <b>recognition</b> <b>software</b> based on her handwriting. This font, Maree, was finally completed and launched on MyFonts.com on 20 June 2013.|$|E
50|$|The {{training}} with voice writing equipment requires {{the person to}} pass dictation speed tests of up to 225 words a minute in the United States, as set forth by the National Verbatim Reporters Association (NVRA). Passing this speed is a requirement. A voice writer dictates and repeats the proceedings into a stenomask connected to a computer, and using <b>voice</b> <b>recognition</b> <b>software,</b> voice writers are able to offer realtime transcription.|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThis research investigates {{the use of}} inexpensive <b>voice</b> <b>recognition</b> systems hosted by micro-computers. The specific intent was to demonstrate a measurable and statistically significant improvement {{in the performance of}} relatively unsophisticated voice recognizers through the application of artificial intelligence algorithms to the <b>recognition</b> <b>software.</b> Two different artificial intelligence algorithms were studied, each with differing levels of sophistication. Results showed that artificial intelligence can increase recognizer system reliability. The degree of improvement in correct recognition percentage varied with the amount of sophistication in the artificial intelligence algorithm. [URL] Commander, United States Nav...|$|R
40|$|This paper {{examines}} {{the goal of}} providing transparent computer access to people with disabilities, especially those with visual impairments. First, four basic strategies to provide this population with access to technology are identified: (1) building features into the computer, operating system, or application programs; (2) using adaptive interfaces comprised of standard software or hardware products that provide modifications or alternate interfaces for accessibility; (3) establishing connectivity to personal assistive devices; and (4) developing custom adaptations. Application of these strategies is illustrated and discussed in case studies of people with visual impairments, physical impairments, and hearing impairments. Among the approaches reviewed in these case studies are Braille printers and computers, the Optacon, the Kurzweil Reading Machine, speech synthesizers, speech output software, magnification software, single-switch input systems, screen-based optical head pointing systems, <b>voice</b> <b>recognition</b> systems, StickyKey <b>software,</b> telephone devices for the deaf, the KEY plus keyboard system, and C-print. Discussion of future challenge considers {{the need to make}} graphic icons more accessible to the visually impaired in graphical user interface systems. (Contains 34 references.) (DB) Reproductions supplied by EDRS are the best that can be made from the ori inal document...|$|R
40|$|Research was {{conducted}} {{to determine if a}} relation exists between human exertion and the ability of speech <b>recognition</b> <b>software</b> to correctly recognize human speech. Participants were asked to use <b>voice</b> <b>recognition</b> technology to input a short newspaper article in 3 portions. 1 portion of the selected article was read while the participants were rested, another portion while they were lightly exerted, and the final portion while they were experiencing hard exertion. Recognition percentages were computed and compared for rested, lightly exerted, and moderately hard exerted states. The results identified a negative linear relation between physical exertion and recognition accuracy; the higher the level of exertion, the lower the accuracy rate. 1...|$|R
50|$|The {{software}} elements required for grammar checking {{are closely related}} to some of the development issues {{that need to be addressed}} for <b>voice</b> <b>recognition</b> <b>software.</b> In voice recognition, parsing can be used to help predict which word is most likely intended, based on part of speech and position in the sentence. In grammar checking, the parsing is used to detect words that fail to follow accepted grammar usage.|$|E
50|$|Adaptive Technology Evaluations {{consist of}} {{one-on-one}} assessments of adaptive technology with an experienced instructor. These evaluations {{are required by}} state agencies prior to state purchase. They are also a perfect way for individuals to test out adaptive hardware and software devices before they buy them. The Lighthouse provides evaluations on CCTVs, portable CCTVs, screen reading software, OCR scanning technology, screen magnification software, <b>voice</b> <b>recognition</b> <b>software,</b> QWERTY note takers, Braille note takers, and displays and printers.|$|E
50|$|Negroponte {{writes about}} the {{inadequacy}} of the interfaces that are currently used to interact with computers. He believes that the mouse is a mediocre interface for point and click, and inadequate for drawing. He instead prefers the interfaces of touch-screen technology and <b>voice</b> <b>recognition</b> <b>software.</b> His prediction that touch-screen technology would become a dominant interface has been proven correct by the rise in popularity of smartphones, tablets and {{an increasing number of}} ultrabooks.|$|E
40|$|This paper {{presents}} a brief survey on Automatic <b>Voice</b> <b>Recognition</b> {{so as to}} provide a technological perspective and {{an appreciation of the}} fundamental progress that has been accomplished in area of voice communication. The voice is a signal of infinite information. After years of research and development the accuracy of automatic <b>voice</b> <b>recognition</b> remains one of the important research challenges in respect to variations of the speakers, text and surroundings. Digital processing of <b>voice</b> <b>recognition</b> and speech signal is very important for fast and accurate automatic <b>voice</b> <b>recognition</b> technology. This review paper summarizes and compares some of the well known methods used in various stages of <b>voice</b> <b>recognition</b> system. It further helps to identify research topic and applications which are at the forefront of this exciting and challenging field of <b>voice</b> <b>recognition...</b>|$|R
40|$|There {{have been}} {{significant}} improvements in automatic <b>voice</b> <b>recognition</b> technology. However, existing systems still face difficulties, particularly when used by non-native speakers with accents. In this {{paper we address a}} problem of identifying the English accented speech of speakers from different backgrounds. Once an accent is identified the speech <b>recognition</b> <b>software</b> can utilise training set from appropriate accent and therefore improve the efficiency and accuracy of the speech recognition system. We introduced the Q factor, which is defined by the sum of relationships between frequencies of the formants. Four different accents were considered and experimented for this research. A scoring method was introduced in order to effectively analyse accents. The proposed concept indicates that the accent could be identified by analysing their formants. Griffith Sciences, School of Information and Communication TechnologyNo Full Tex...|$|R
40|$|In the {{advanced}} information society, the various interfaces are required. Especially, {{the accuracy of}} <b>voice</b> <b>recognition</b> becomes good. The current interface in Smartphone in equipped with <b>voice</b> <b>recognition.</b> However, such an interface can not understand the user’s mental state. We develop the Emotion Orientated Interface on Android Smartphone by using <b>voice</b> <b>recognition.</b> 開催日：平成 24 年 7 月 14 日　会場：広島市立大...|$|R
