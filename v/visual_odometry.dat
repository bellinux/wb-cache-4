684|22|Public
5|$|Near the aircraft, a {{point cloud}} in three {{dimensions}} is acquired by changing {{the orientation of the}} laser scanning sensors fixed on pan-tilt units. After filtering data to remove floor- or insufficiently large dot clusters, a registration technique with the model of the aircraft is used to estimate the static orientation of the robot. The robot moves and holds this orientation by considering its wheel odometry, its inertial unit and <b>visual</b> <b>odometry.</b>|$|E
50|$|<b>Visual</b> <b>odometry</b> is {{the process}} of {{determining}} equivalent odometry information using sequential camera images to estimate the distance traveled. <b>Visual</b> <b>odometry</b> allows for enhanced navigational accuracy in robots or vehicles using any type of locomotion on any surface.|$|E
50|$|Most {{existing}} {{approaches to}} <b>visual</b> <b>odometry</b> {{are based on}} the following stages.|$|E
40|$|A control {{strategy}} for tracking an omnidirectional target with a unicycle-like robot is proposed. An originality {{of the approach}} is that the target is allowed to move freely in the plane and perform motions which are not feasible by the nonholonomic robot. Control implementation involves the design of an estimator of the target's velocity, from <b>visual</b> and <b>odometry</b> measurements. Finally, simulation and experimental results are reported...|$|R
40|$|In recent years, vision-aided {{inertial}} odometry {{for state}} estimation has matured significantly. However, we still encounter challenges {{in terms of}} improving the computational efficiency and robustness of the underlying algorithms for applications in autonomous flight with micro aerial vehicles {{in which it is}} difficult to use high quality sensors and pow- erful processors because of constraints on size and weight. In this paper, we present a filter-based stereo <b>visual</b> inertial <b>odometry</b> that uses the Multi-State Constraint Kalman Filter (MSCKF) [1]. Previous work on stereo <b>visual</b> inertial <b>odometry</b> has resulted in solutions that are computationally expensive. We demonstrate that our Stereo Multi-State Constraint Kalman Filter (S-MSCKF) is comparable to state-of-art monocular solutions in terms of computational cost, while providing signifi- cantly greater robustness. We evaluate our S-MSCKF algorithm and compare it with state-of-art methods including OKVIS, ROVIO, and VINS-MONO on both the EuRoC dataset, and our own experimental datasets demonstrating fast autonomous flight with maximum speed of 17. 5 m/s in indoor and outdoor environments. Our implementation of the S-MSCKF is available at [URL] Submitted to RAL and ICRA 201...|$|R
40|$|This paper {{describes}} {{the current state}} of RatSLAM, a Simultaneous Localisation and Mapping (SLAM) system based on models of the rodent hippocampus. RatSLAM uses a competitive attractor network to fuse <b>visual</b> and <b>odometry</b> information. Energy packets in the network represent pose hypotheses, which are updated by odometry and can be enhanced or inhibited by visual input. This paper shows the effectiveness of the system in real robot tests in unmodified indoor environments using a learning vision system. Results are shown for two test environments; a large corridor loop and the complete floor of an office building. ...|$|R
5000|$|An {{alternative}} to feature-based methods is the [...] "direct" [...] or appearance-based <b>visual</b> <b>odometry</b> technique which minimizes an error directly in sensor space and subsequently avoids feature matching and extraction.|$|E
50|$|Optical flow {{was used}} by {{robotics}} researchers in many areas such as: object detection and tracking, image dominantplane extraction, movement detection, robot navigation and <b>visual</b> <b>odometry.</b> Optical flow information has been recognized as being useful for controlling micro air vehicles.|$|E
50|$|In {{robotics}} {{and computer}} vision, <b>visual</b> <b>odometry</b> {{is the process}} of determining the position and orientation of a robot by analyzing the associated camera images. It has been used {{in a wide variety of}} robotic applications, such as on the Mars Exploration Rovers.|$|E
40|$|Abstract — This work {{presents}} {{a method for}} increasing the accuracy of standard <b>visual</b> inertial <b>odometry</b> (VIO) by ef-fectively removing the angular drift that naturally occurs in feature-based VIO. In order to eliminate such drift, we propose to leverage the predominance of parallel lines in man-made environments by using the intersection of their image projections, known as vanishing points (VPs). First, an efficient inertial-based method is presented that accurately and efficiently detects such points. Second, a strategy {{to deal with these}} measurements within the framework of an EKF-based VIO system is presented. Furthermore, special care is taken in order to ensure the real-time execution of the estimator in order to comply with time-critical applications running on computationally constrained platforms. Experiments are performed in a mobile device on challenging environments and evaluated against the same VIO system without the use of VPs, demonstrating the superior accuracy when employing the proposed framework. I...|$|R
40|$|Abstract — Mapping the {{environment}} is crucial to enable path planning and obstacle avoidance for self-driving vehicles and other robots. In this paper, we concentrate on ground-based vehicles and present an approach which extracts static obstacles from depth maps computed out of multiple consecutive images. In contrast to existing approaches, our system does not require accurate <b>visual</b> inertial <b>odometry</b> estimation but solely relies on the readily available wheel odometry. To handle the resulting higher pose uncertainty, our system fuses obstacle detections over time and between cameras to estimate the free and occupied space around the vehicle. Using monocular fisheye cameras, {{we are able to}} cover a wider field of view and detect obstacles closer to the car, which are often not within the standard field of view of a classical binocular stereo camera setup. Our quantitative analysis shows that our system is accurate enough for navigation purposes of self-driving cars and runs in real-time. I...|$|R
40|$|Abstract — We {{propose a}} novel monocular <b>visual</b> {{inertial}} <b>odometry</b> algorithm that combines {{the advantages of}} EKF-based approaches with those of direct photometric error mini-mization methods. The method is based on sparse, very small patches and incorporates the minimization of photometric error directly into the EKF measurement model so that inertial data and vision-based surface measurements are used simultaneously during camera pose estimation. We fuse vision-based and inertial measurements almost at the raw-sensor level, allowing the estimated system state to constrain and guide image-space measurements. Our formulation allows for an efficient implementation that runs in real-time on a standard CPU and has several appealing and unique characteristics such as being robust to fast camera motion, in particular rotation, and not depending {{on the presence of}} corner-like features in the scene. We experimentally demonstrate robust and accurate performance compared to ground truth and show that our method works on scenes containing only non-intersecting lines. I...|$|R
50|$|In robotics, {{omnidirectional}} {{cameras are}} frequently used for <b>visual</b> <b>odometry</b> and {{to solve the}} simultaneous localization and mapping (SLAM) problems visually. Due to its ability to capture a 360-degree view, better results can be obtained for optical flow and feature selection and matching.|$|E
50|$|A key {{measure of}} AR systems is how realistically they {{integrate}} augmentations {{with the real}} world. The software must derive real world coordinates, independent from the camera, from camera images. That process is called image registration which uses different methods of computer vision, mostly related to video tracking. Many computer vision methods of augmented reality are inherited from <b>visual</b> <b>odometry.</b>|$|E
50|$|Even {{though the}} LAGR vehicle had a WAAS GPS, its {{position}} was never determined {{down to the}} width of the vehicle, so it was hard for the systems to re-use obstacle maps of areas the robots had previously traversed since the GPS continually drifted. The drift was especially severe if there was a forest canopy. A few teams developed <b>visual</b> <b>odometry</b> algorithms that essentially eliminated this drift.|$|E
40|$|Insects use visual {{estimates}} of flight speed {{for a variety}} of behaviors, including <b>visual</b> navigation, <b>odometry,</b> grazing landings and flight speed control, but the neuronal mechanisms underlying speed detection remain unknown. Although many models and theories have been proposed for how the brain extracts the angular speed of the retinal image, termed optic flow, we lack the detailed electrophysiological and behavioral data necessary to conclusively support any one model. One key property by which different models of motion detection can be differentiated is their spatiotemporal frequency tuning. Numerous studies have suggested that optic-flow-dependent behaviors are largely insensitive to the spatial frequency of a visual stimulus, but they have sampled only a narrow range of spatial frequencies, have not always used narrowband stimuli, and have yielded slightly different results between studies based on the behaviors being investigated. In this study, we present a detailed analysis of the spatial frequency dependence of the centering response in the bumblebee Bombus impatiens using sinusoidal and square wave patterns...|$|R
40|$|This paper {{describes}} a localization system for mobile robots moving in dynamic indoor environments, which uses probabilistic integration of <b>visual</b> appearance and <b>odometry</b> information. The approach {{is based on}} a novel image matching algorithm for appearancebased place recognition that integrates digital zooming, to extend the area of application, and a visual compass. Ambiguous information used for recognizing places is resolved with multiple hypothesis tracking and a selection procedure inspired by Markov localization. This enables the system to deal with perceptual aliasing or absence of reliable sensor data. It has been implemented on a robot operating in an office scenario and the robustness of the approach demonstrated experimentally...|$|R
40|$|The European V-Charge project {{seeks to}} develop fully {{automated}} valet parking and charging of electric vehicles using only low-cost sensors. One {{of the challenges}} is to implement robust visual localization using only cameras and stock vehicle sensors. We integrated four monocular, wide-angle, fisheye cameras on a consumer car and implemented a mapping and localization pipeline. <b>Visual</b> features and <b>odometry</b> are combined to build and localize against a keyframe-based three dimensional map. We report results for {{the first stage of}} the project, based on two months worth of data acquired under varying conditions, with the objective of localizing against a map created offline. © 2013 IEEE...|$|R
50|$|Near the aircraft, a {{point cloud}} in three {{dimensions}} is acquired by changing {{the orientation of the}} laser scanning sensors fixed on pan-tilt units. After filtering data to remove floor- or insufficiently large dot clusters, a registration technique with the model of the aircraft is used to estimate the static orientation of the robot. The robot moves and holds this orientation by considering its wheel odometry, its inertial unit and <b>visual</b> <b>odometry.</b>|$|E
5000|$|LAGR was {{managed in}} tandem with the DARPA Unmanned Ground Combat Vehicle - PerceptOR Integration Program (UPI) CMU NREC UPI Website. UPI {{combined}} advanced perception with a vehicle of extreme mobility. The best stereo algorithms and the <b>visual</b> <b>odometry</b> from LAGR were ported to UPI. In addition interactions between the LAGR PIs and the UPI team resulted in the incorporation of adaptive technology into the UPI codebase with a resultant improvement in performance of the UPI [...] "Crusher" [...] robots.|$|E
50|$|The goal of {{estimating}} the egomotion {{of a camera}} {{is to determine the}} 3D motion of that camera within the environment using a sequence of images taken by the camera. The process of estimating a camera's motion within an environment involves the use of <b>visual</b> <b>odometry</b> techniques on a sequence of images captured by the moving camera. This is typically done using feature detection to construct an optical flow from two image frames in a sequence generated from either single cameras or stereo cameras. Using stereo image pairs for each frame helps reduce error and provides additional depth and scale information.|$|E
40|$|This paper {{presents}} a new rat animat, a rat-sized bio-inspired robot platform currently being developed for embodied cognition and neuroscience research. The rodent animat is 150 mm x 80 mm x 70 mm {{and has a}} different drive, <b>visual,</b> proximity, and <b>odometry</b> sensors, x 86 PC, and LCD interface. The rat animat has a bio-inspired rodent navigation and mapping system called RatSLAM which demonstrates {{the capabilities of the}} platform and framework. A case study is presented of the robot's ability to learn the spatial layout of a figure of eight laboratory environment, including its ability to close physical loops based on <b>visual</b> input and <b>odometry.</b> A firing field plot similar to rodent 'non-conjunctive grid cells' is shown by plotting the activity of an internal network. Having a rodent animat the size of a real rat allows exploration of embodiment issues such as how the robot's sensori-motor systems and cognitive abilities interact. The initial observations concern the limitations of the deisgn as well as its strengths. For example, the visual sensor has a narrower field of view and is located much closer to the ground than for other robots in the lab, which alters the salience of visual cues and the effectiveness of different visual filtering techniques. The small size of the robot relative to corridors and open areas impacts on the possible trajectories of the robot. These perspective and size issues affect the formation and use of the cognitive map, and hence the navigation abilities of the rat animat...|$|R
40|$|International audienceIn this paper, {{the problem}} of multi-UAVs (Unmanned Arial Vehicles) Visual Simultaneous Localization and Mapping (SLAM) is {{considered}} by using a new framework for pose and map estimation using monocular vision and reduced communications capabilities. The problem of localization and mapping is solved by fusing monocular <b>visual</b> data with <b>odometry</b> measurements through Graph SLAM formulation. Using each robot’s map data representation, a proposal is made for a good and robust communication between UAVs to perform efficient data exchange while keeping SLAM performances. A mesh network is chosen to import solutions to wireless networking. Finally, some validation experiments are performed in an Ad Hoc Network and a Wireless Mesh Network using Better Approach to Mobile Ad Hoc Network (BATMAN) protocol...|$|R
30|$|Filter-based {{approach}} The filter-based approach uses EKF {{to propagate}} and update motion states of visual-inertial sensors. MSCKF in [111] uses an IMU to propagate the motion estimation {{of a vehicle}} and update this motion estimation by observing salient features from a monocular camera. Li and Mourikis [112] improved MSCKF, by proposing a real-time EKF-based VIO algorithm, MSCKF 2.0. This algorithm can achieve consistent estimation by ensuring correct observability properties of its linearized system model and performing online estimation of the camera-to-inertial measurement unit calibration parameters. Li et al. [113], Li and Mourikis [114] implemented real-time motion tracking on a cellphone using inertial sensing and a rolling-shutter camera. MSCKF algorithm is the core algorithm of Google’s Project Tango [URL] Clement et al. [115] compared two modern approaches: MSCKF and sliding window filter (SWF). SWF is more accurate and less sensitive to tuning parameters than MSCKF. However, MSCKF is computationally cheaper, has good consistency, and improves accuracies because more features are tracked. Bloesch et al. [116] presented a monocular <b>visual</b> inertial <b>odometry</b> algorithm by directly using pixel intensity errors of image patches. In this algorithm, by directly using the intensity errors as an innovation term, the tracking of multilevel patch features is closely coupled to the underlying EKF during the update step.|$|R
40|$|This thesis {{explored}} {{the utility of}} long-range stereo <b>visual</b> <b>odometry</b> for application on Unmanned Aerial Vehicles. Novel parameterisations and initialisation routines were developed for the long-range case of stereo <b>visual</b> <b>odometry</b> and new optimisation techniques were implemented to improve the robustness of <b>visual</b> <b>odometry</b> in this difficult scenario. In doing so, the applications of stereo <b>visual</b> <b>odometry</b> were expanded and shown to perform adequately in situations that were previously unworkable...|$|E
40|$|Odometry is an {{important}} input to robot navigation systems, and {{we are interested in}} the performance of vision-only techniques. In this paper we experimentally evaluate and compare the performance of wheel odometry, monocular feature-based <b>visual</b> <b>odometry,</b> monocular patch-based <b>visual</b> <b>odometry,</b> and a technique that fuses wheel odometry and <b>visual</b> <b>odometry,</b> on a mobile robot operating in a typical indoor environment...|$|E
30|$|<b>Visual</b> <b>odometry</b> only {{provides}} relative positioning, i.e., positioning {{relative to}} an earlier visited reference point. As a consequence, estimation errors are cumulative, and <b>visual</b> <b>odometry</b> methods are therefore susceptible to drift. The greater the distance traveled from the last absolute reference point (e.g., the known GPS coordinates of the starting address), the greater the positional error can become. While this may appear to limit the use of <b>visual</b> <b>odometry</b> to a short distances, the drift error can be bounded by combination with additional passive sensors (e.g., a magnetic compass for dead reckoning) or a priori known information (e.g., the local road map) [2, 3]. As such, <b>visual</b> <b>odometry</b> is still a prime candidate to supplement satellite navigation even for urban scenarios where signal reception may be unreliable for prolonged distances.|$|E
40|$|This paper {{presents}} the iRat (intelligent Rat animat technology), a robot designed for robotic and neuroscience teams {{as a tool}} for studies in navigation, embodied cognition, and neuroscience research. The rat animat has capabilities comparable to the popular standard Pioneer DX robots but is an order of magnitude smaller in size and weight. The robot‟s volume is approximately 0. 08 m 2 with a mass of 0. 5 kg and has <b>visual,</b> proximity, and <b>odometry</b> sensors, a differential drive, a 1 GHz x 86 computer, and LCD navigation pad interface. To facilitate the value of the platform to a broader range of researchers, the robot uses the Player-Stage framework, and C/C++, Python, and MATLAB APIs have been tested in real time. Two studies of neural simulation for robot navigation have confirmed the rat animat‟s capabilities. ...|$|R
40|$|International audienceLocal Bundle Adjustments were {{recently}} introduced for visual SLAM (Simultaneous Localization and Mapping). In Monocular Visual SLAM, the scale factor is not observable and the reconstruction scale drifts {{as time goes}} by. On long trajectory, this problem makes absolute localisation not usable. To overcome this major problem, data fusion is a possible solution. In this paper, we describe Weighted Local Bundle Adjustment(W-LBA) for monocular visual SLAM purposes. We show that W-LBA used with local covariance gives better results than Local Bundle Adjustment especially on the scale propagation. Moreover W-LBA is well designed for sensor fusion. Since odometer is a common sensor and is reliable to obtain a scale information, we apply W-LBA to fuse <b>visual</b> SLAM with <b>odometry</b> data. The method performance is shown {{on a large scale}} sequence...|$|R
40|$|Abstract — Despite recent progress, {{autonomous}} navigation on Micro Aerial Vehicles with {{a single}} frontal camera is still a challenging problem, especially in feature-lacking environ-ments. On a mobile robot with a frontal camera, monoSLAM can fail when {{there are not enough}} visual features in the scene, or when the robot, with rotationally dominant motions, yaws away from a known map toward unknown regions. To overcome such limitations and increase responsiveness, we present a novel parallel tracking and mapping framework that is suitable for robot navigation by fusing <b>visual</b> data with <b>odometry</b> measurements in a principled manner. Our framework can cope with a lack of visual features in the scene, and maintain robustness during pure camera rotations. We demonstrate our results on a dataset captured from the frontal camera of a quad-rotor flying in a typical feature-lacking indoor environment. I...|$|R
40|$|<b>Visual</b> <b>odometry</b> {{is a new}} {{navigation}} technology using video data. For long-range navigation, {{an intrinsic}} problem of <b>visual</b> <b>odometry</b> is the appearance of drift. The drift is caused by error accumulation, as <b>visual</b> <b>odometry</b> is based on relative measurements, and will grow unboundedly with time. The paper first reviews algorithms which adopt various methods to suppress this drift. However, {{as far as we}} know, no work has been done to statistically model and analyze the intrinsic properties of this drift. This paper uses an unbounded system model to represent the drift behavior of <b>visual</b> <b>odometry.</b> The model is composed of an unbounded deterministic part with unknown constant parameters, and a first-order Gauss-Markov process. A simple scheme is given to identify the unknown parameters as well as the statistics of the stochastic part from experimental data. Experiments and discussions are also provided. 1...|$|E
40|$|Abstract — In {{this paper}} we {{address the problem of}} visual motion {{estimation}} (<b>visual</b> <b>odometry)</b> from a single vehicle mounted camera. One of the basic issues of <b>visual</b> <b>odometry</b> is relative scale estimation. We propose a method to compute the relative scales of a path by solving a bundle adjustment optimization problem. We introduce a constricted parameterization of the bundle adjustment problem, where only the distances between neighboring cameras are optimized, while the rotation angles and translation directions stay fixed. We will present <b>visual</b> <b>odometry</b> results for image data of a vehicle mounted onmidirectional camera for a track of 1000 m length. I...|$|E
30|$|The use of Autonomous Underwater Vehicles (AUVs) for {{underwater}} tasks is {{a promising}} robotic field. These robots can carry visual inspection cameras. Besides serving {{the activities of}} inspection and mapping, the captured images {{can also be used}} to aid navigation and localization of the robots. <b>Visual</b> <b>odometry</b> is the process of determining the position and orientation of a robot by analyzing the associated camera images. It has been used in a wide variety of non-standard locomotion robotic methods. In this context, this paper proposes an approach to <b>visual</b> <b>odometry</b> and mapping of underwater vehicles. Supposing the use of inspection cameras, this proposal is composed of two stages: i) the use of computer vision for <b>visual</b> <b>odometry,</b> extracting landmarks in underwater image sequences and ii) the development of topological maps for localization and navigation. The integration of such systems will allow <b>visual</b> <b>odometry,</b> localization and mapping of the environment. A set of tests with real robots was accomplished, regarding online and performance issues. The results reveals an accuracy and robust approach to several underwater conditions, as illumination and noise, leading to a promissory and original <b>visual</b> <b>odometry</b> and mapping technique.|$|E
40|$|The goal of <b>visual</b> {{inertial}} <b>odometry</b> (VIO) is {{to estimate}} a moving vehicle's trajectory using inertial measurements and observations, obtained by a camera, of naturally occurring point features. One existing VIO estimation algorithm {{for use with}} a monocular system, is the multi-state constraint Kalman filter (MSCKF), proposed by Mourikis and Li [34, 29]. The way the MSCKF uses feature measurements drastically improves its performance, in terms of consistency, observability, computational complexity and accuracy, compared to other VIO algorithms [29]. For this reason, the MSCKF is chosen {{as the basis for}} the estimation algorithm presented in this thesis. A VIO estimation algorithm for a system consisting of an IMU, a monocular camera and a depth sensor is presented in this thesis. The addition of the depth sensor to the monocular camera system produces three-dimensional feature locations rather than two-dimensional locations. Therefore, the MSCKF algorithm is extended to use the extra information. This is accomplished using a model proposed by Dryanovski et al. that estimates the 3 D location and uncertainty of each feature observation by approximating it as a multivariate Gaussian distribution [11]. The extended MSCKF algorithm is presented and its performance is compared to the original MSCKF algorithm using real-world data obtained by flying a custom-built quadrotor in an indoor office environment. by Marissa N. Galfond. Thesis: S. M., Massachusetts Institute of Technology, Department of Aeronautics and Astronautics, 2014. Cataloged from PDF version of thesis. Includes bibliographical references (pages 93 - 97) ...|$|R
40|$|Unmanned ground {{vehicles}} (UGVs) {{will play}} an important role in the nation’s next-generation ground force. Advances in sensing, control, and computing have enabled a new generation of technologies that bridge the gap between manual UGV teleoperation and full autonomy. In this paper, we present current research on a unique command and control system for UGVs named PointCom (Point-and-Go Command). PointCom is a semi-autonomous command system for one or multiple UGVs. The system, when complete, will be easy to operate and will enable significant reduction in operator workload by utilizing an intuitive image-based control framework for UGV navigation and allowing a single operator to command multiple UGVs. The project leverages new image processing algorithms for monocular <b>visual</b> servoing and <b>odometry</b> to yield a unique, high-performance fused navigation system. Human Computer Interface (HCI) techniques from the entertainment software industry are being used to develop video-game style interfaces that require little training and build upon the navigation capabilities. By combining an advanced navigation system with an intuitive interface, a semi-autonomous control and navigation system is being created that is robust, user friendly, and less burdensome than many current generation systems...|$|R
40|$|This paper {{addresses}} {{the problem of}} tracking a group of mobile sensors {{in an environment where}} there is intermittent or no access to a localization service, such as the Global Positioning System. Example applications include tracking personnel underground or animals under dense tree canopies. We assume that each sensor uses inertial, <b>visual</b> or mechanical <b>odometry</b> to measure its relative movement as a series of displacement vectors. Each displacement vector suffers a small quantity of error which compounds, causing the overall accuracy of the positional estimate to decrease with time. The primary contribution of this paper is a novel offline method of counteracting this error by exploiting opportunistic radio encounters between sensors. We fuse encounter information with the displacement vectors to build a graph that models sensor mobility. We show that two dimensional sensor tracking is equivalent to finding an embedding of this graph in the plane. Finally, using radio, inertial and ground truth trace data, we conduct simulations to observe how the number of anchors, transmission range and radio noise affect the performance of the proposed model. We compare these results to those from a competing model in the literature. © 2012 ACM...|$|R
