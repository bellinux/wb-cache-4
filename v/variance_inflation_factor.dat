149|705|Public
50|$|<b>Variance</b> <b>inflation</b> <b>factor</b> for a multi-linear fit.|$|E
50|$|The {{square root}} of the <b>variance</b> <b>inflation</b> <b>factor</b> {{indicates}} how much larger the standard error is, compared with {{what it would be}} if that variable were uncorrelated with the other predictor variables in the model.|$|E
50|$|ExampleIf the <b>variance</b> <b>inflation</b> <b>factor</b> of a {{predictor}} variable were 5.27 (√5.27 = 2.3) {{this means that}} the standard error for the coefficient of that predictor variable is 2.3 times as large as it would be if that predictor variable were uncorrelated with the other predictor variables.|$|E
30|$|<b>Variance</b> <b>inflation</b> <b>factors</b> (VIF) {{between all}} {{independent}} variables were calculated to assess for multicollinearity.|$|R
3000|$|... 2 [*]=[*] 0.61. An {{analysis}} of the <b>variance</b> <b>inflation</b> <b>factors</b> (VIF) showed that no predictor exceeded the recommended level of 10 (all VIF[*]<[*] 3.29), suggesting no confounding of multicollinearity.|$|R
30|$|Factor {{analysis}} {{assumes the}} absence of multicollinearity and singularity. To check for this, <b>variance</b> <b>inflation</b> <b>factors</b> (VIFs) were computed (using SPSS). The values were reasonably low (<[*] 10); therefore, multicollinearity is not a problem.|$|R
50|$|In statistics, the <b>variance</b> <b>inflation</b> <b>factor</b> (VIF) quantifies the {{severity}} of multicollinearity in an ordinary least squares regression analysis. It provides an index that measures how much the variance (the square of the estimate's standard deviation) of an estimated regression coefficient is increased because of collinearity.|$|E
5000|$|Multicollinearity (as {{long as it}} is not [...] "perfect") can {{be present}} {{resulting}} in a less efficient, but still unbiased estimate. The estimates will be less precise and highly sensitive to particular sets of data. Multicollinearity can be detected from condition number or the <b>variance</b> <b>inflation</b> <b>factor,</b> among other tests.|$|E
5000|$|Some {{authors have}} {{suggested}} a formal detection-tolerance or the <b>variance</b> <b>inflation</b> <b>factor</b> (VIF) for multicollinearity:where [...] is {{the coefficient of}} determination of a regression of explanator j on all the other explanators. A tolerance of less than 0.20 or 0.10 and/or a VIF of 5 or 10 and above indicates a multicollinearity problem.|$|E
3000|$|... 3. The OLS-solution {{of system}} SyS 1 scr (Statistical and Econometric Appendix) was {{submitted}} to econometric controls from four standpoints: (a) <b>variance</b> <b>inflation</b> <b>factors,</b> (b) Breusch–Pagan–Godfrey heteroskedasticity test, (c) correlogram squared residuals, and (d) stationarity of residuals.|$|R
30|$|The Jarque-Bera {{normality}} {{test and}} chi-square test of {{goodness of fit}} indicated that the residuals were normally distributed. According to <b>variance</b> <b>inflation</b> <b>factors</b> (VIF), which all were less than 10, the models {{did not have the}} serious problem of multicollinearity.|$|R
30|$|In {{order to}} assess for the {{presence}} of collinearity (which happens when two variables are almost perfect linear combinations of one another), we calculated the <b>variance</b> <b>inflation</b> <b>factors</b> (VIFs). It is generally accepted that variables with VIFs greater than 10 merit further investigation [19].|$|R
5000|$|Obtain more data, if possible. This is the {{preferred}} solution. More data can produce more precise parameter estimates (with lower standard errors), {{as seen from the}} formula in <b>variance</b> <b>inflation</b> <b>factor</b> for the variance of the estimate of a regression coefficient in terms of the sample size and the degree of multicollinearity.|$|E
50|$|In statistics, {{the design}} effect (or {{estimates}} of unit variance) is an adjustment {{used in some}} kinds of studies, such as cluster randomised trials, {{to allow for the}} design structure. The adjustment inflates the variance of parameter estimates, and therefore their standard errors, which is necessary to allow for correlations among clusters of observations. It is similar to the <b>variance</b> <b>inflation</b> <b>factor</b> and is used in sample size calculations. The term was introduced by Leslie Kish in 1965.|$|E
30|$|The <b>Variance</b> <b>Inflation</b> <b>Factor</b> (VIF) for multicolinearity for columns 2, 3, and 4 are 1.25, 1.49, and 1.4, respectively.|$|E
40|$|Parameter {{estimates}} for equity studies tested for stability are described. Bootstrap simulation can test whether parameter estimates remain stable given {{changes in the}} sample data; fractional polynomials {{can be used to}} access functional form specification; and <b>variance</b> <b>inflation</b> <b>factors</b> can be used to test for multicollinearity...|$|R
3000|$|... = 3 {{and find}} <b>variance</b> <b>inflation</b> <b>factors</b> (VIF) for the {{attributes}} for each country. VIF {{is a common}} value which measures how much the variance of the estimated regression coefficients are inflated as compared to when the predictor variables are not linearly related, and it assumes that multicollinearity is high when VIF are larger than 5.|$|R
40|$|Inaccurate and invalid {{statistical}} inferences in {{regression analysis}} {{may be caused}} by multicollinearity due to the presence of high leverage points (HLP) in a data set. Therefore, it is important that high leverage point which is a form ofoutlier be detected because its existence can lead to misfitting of a regression model, thus resulting in inaccuracy of regression results. In this paper, several methods have been proposed to identify HLP in a financial accounting data set prior to conducting further analysis of regression and other multivariate analysis. The Pearson scorrelation coefficient and <b>variance</b> <b>inflation</b> <b>factors</b> (VIF) were used to measure the success of a detection method. Numerical analysis showed that common diagnostics like the twice-mean and thrice-mean rules failed to detect HLP in the given data set whilst robust approaches such as the potentials and diagnostic-robust generalized potentials (DRGP) methods were found to be successful in identifying high leverage point as indicated by lower values of the Pearson s correlation coefficient and <b>variance</b> <b>inflation</b> <b>factors...</b>|$|R
30|$|For all the models, {{the problem}} of {{multicollinearity}} was tested and found to be serious for variance and skewness (reflected in the <b>variance</b> <b>inflation</b> <b>factor</b> of 13.27 and 11.09, respectively). To solve the problem, skewness was dropped from the analysis and the test re-conducted. The test combined <b>Variance</b> <b>Inflation</b> <b>Factor</b> (VIF) and Eigen Values approaches. All the variance inflation factors were less than 2 and the condition number was 2.66, indicating that multicollinearity {{was no longer a}} problem.|$|E
3000|$|Likewise, <b>variance</b> <b>inflation</b> <b>factor</b> (VIF) of {{centralized}} variables (shown with suffix Cent: tamUseCent, tamPuCent, and tamPuCent, in the output, Appendix E) {{are below}} the cut point proposed by Kutner et al. [100] of β [...]...|$|E
30|$|The <b>Variance</b> <b>Inflation</b> <b>Factor</b> (VIF) {{values of}} the {{variables}} in the model, as shown in Tables 12 and 13 in the Appendix, are less than the critical values showing {{that there is no}} problem of collinearity.|$|E
30|$|<b>Variance</b> <b>Inflation</b> <b>Factors</b> (VIF) {{provide a}} measure of the {{increase}} in the variance of the estimated regression coefficients with respect to a situation where the prediction variables do not have a linear relationship. They allow describing the importance of multicollinearity (correlation between predictors) in a regression analysis. Multicollinearity is problematic because it can increase the variance of the regression coefficients, making them regressive and difficult to interpret.|$|R
30|$|For all {{logistic}} regression models, we report marginal effects {{to give an}} impression {{of the strength of}} the effects observed. In each case, <b>variance</b> <b>inflation</b> <b>factors</b> (vif) are calculated to evaluate the severity of multicollinearity. A value above 5 would suggest a problem (Sheskin 2011), but in our case they are always clearly below this threshold. All analyses have been conducted with the software packages SPSS^TM (version 22) and STATA^TM (version 12).|$|R
30|$|To analyse the {{influence}} of OTL on CK and PCK, multiple regression models were estimated. Data plots (residuals vs. fitted values, Q-Q plot, scale location plot, Cook’s Distance plot) do not indicate {{a violation of the}} prerequisites for regression models (especially linearity and homoscedasticity). Furthermore, the value of the Durbin-Watson statistic is close to 2, supporting the assumption of independent errors (Field 2009). The average <b>variance</b> <b>inflation</b> <b>factors</b> (VIF) show no indication of multicollinearity (mean VIF[*]<[*] 1.2), single VIF values[*]<[*] 1.37).|$|R
30|$|The <b>Variance</b> <b>Inflation</b> <b>Factor</b> (2.539 – 5.573) and Tolerance (. 179 –. 394) indices for all {{predictors}} do {{not show}} any critical values. Therefore, no multicollinearity between any variables exists (Bühner and Ziegler 2009, pp. 681 – 682).|$|E
30|$|To {{check for}} a {{potential}} multicollinearity problem, we examined the <b>Variance</b> <b>Inflation</b> <b>Factor</b> (VIF). All VIFs in the four regression models were below 2, meaning {{that there is no}} severe multicollinearity problem in the data (Hair et al. 2009).|$|E
30|$|The <b>variance</b> <b>inflation</b> <b>factor</b> (VIF) was {{examined}} {{for each of}} the predictor variables, as a test of multicollinearity within the model. No variables had a VIF greater than 2.5 and were therefore deemed to be sufficiently independent contributions to the model.|$|E
30|$|While {{multicollinearity}} can be {{of concern}} in general for multivariate statistical models, understanding the robustness of the findings about forbidden triads to multicollinearity is especially important here, as the proportion of forbidden triads {{and the proportion of}} closed triads can not vary completely independently, by definition. To assess the general level of multicollinearity for all independent variables, I include Table 5, with <b>variance</b> <b>inflation</b> <b>factors.</b> The VIF values indicate that multicollinearity is not a concern – all statistics are well within customary bounds (of VIF[*]>=[*] 4.00).|$|R
30|$|The overall fit of {{the model}} is about 38  % {{which can be treated}} as a {{moderate}} good fit. The R-squared reported is relatively higher than other similar studies (Munoz et al. 2014; Papadaki et al. 2002). The estimated model does not show any issue of collinearity as <b>variance</b> <b>inflation</b> <b>factors</b> (VIF) are less than 10.00. Heteroskedasticity and Jarque-Bera (JB) tests also do not bring any suspicious results, and Durbin-Watson statistics is around 2 meaning no possibilities of autocorrelation although it is argued that cross-sectional data is less prone to serial correlation problem.|$|R
3000|$|To {{check for}} {{multicollinearity}} problems, the tolerances, <b>variance</b> <b>inflation</b> <b>factors</b> (VIFs), and correlation coefficients were inspected. Tolerance values {{of less than}} [...]. 1 and VIFs of greater than 10 at the multivariate level could indicate a problem with multicollinearity in the SEM analysis (Kline 2005). The tolerance values ranged from [...]. 431 to [...]. 930, and the VIFs ranged from 1.075 to 2.322, indicating no problems with multicollinearity at the multivariate level. Correlation estimates of [...]. 850 or higher could indicate a problem with bivariate multicollinearity (Kline 2005). Absolute correlation coefficients among observed variables ranged from [...]. 007 to [...]. 652, indicating no problem with bivariate multicollinearity (see Table 8 in Appendix 2).|$|R
30|$|We have {{calculated}} the correlation matrix and, additionally, we have performed a multicollinearity test using the <b>Variance</b> <b>Inflation</b> <b>Factor</b> (VIF). Results {{are reported in}} Table  9 in the Appendix, and the low VIF values {{suggest that there is}} no collinearity among the variables considered.|$|E
40|$|Research {{hypotheses}} {{that include}} interaction effects {{should be of}} more interest to educational researchers, especially since issues related to centering and interpretation of the <b>variance</b> <b>inflation</b> <b>factor</b> have been introduced. The {{purpose of this paper}} was to examine interaction effects in the context of centered versus uncentered variables and the <b>variance</b> <b>inflation</b> <b>factor,</b> especially upon the interpretation of interaction effects. Results indicated that centering of variables was required when examining interaction effects, uncentered variables impacted the <b>variance</b> <b>inflation</b> <b>factor</b> values, and separate regression equations have important interpretation outcomes in the presence of non-significant interaction effects. istorically, hypotheses that specify testing interaction effects before examining main effects have appeared under the framework of analysis of variance. In the 1960 ’s with the emergence of multiple regression, coding for interaction effects was introduced. Faculty who taught multiple regression therefore usually included instruction on dummy coding to obtain a test of interaction effects (Fox, 1997). Today, depending upon the textbook used, analysis of variance with A x B interaction effect may be covered without any corresponding interaction effect presentation given for multiple regression (Hinkle, Wiersma, & Jurs, 1998). Much of the published research literature seems to only examine main effects or linear effects. I...|$|E
30|$|The usual {{specifications}} of the BARS, LINS, and BBR curves {{involve a}} high correlation between series cbe, atax, pdg, and their quadratic forms. In our application, the Pearson–Galtung {{coefficients for the}} corresponding pairs are as follows: 0.99913 cbe–cbe 2, 0.99843 atax–atax 2, and 0.98579 pdg–pdg 2. As expected, in regressions, the estimators register a great <b>variance</b> <b>inflation</b> <b>factor.</b>|$|E
40|$|Slack-variable {{models are}} {{compared}} against Scheffe's polynomial model for mixture experiments. The notion of model equivalence {{and the use}} of various diagnostic measures provide effective tools in making such comparisons, particularly when the experimental region is highly constrained. It is demonstrated that the choice of the best fitting model, through variable selection, depends on which mixture component is selected as a slack variable, and {{on the size of the}} fitted model. In addition, the equivalence of two well-known representations of a complete mixture model is shown to be valid. Two numerical examples are presented. Collinearity, column space, condition number, constrained mixture region, mixture components, model equivalence, L-pseudocomponents, variable selection, variance-decomposition proportions, <b>variance</b> <b>inflation</b> <b>factors,</b> well-formulated model,...|$|R
30|$|The IPO {{regressions}} are all {{estimated by}} the cross-sectional ordinary least squares (OLS) procedure. We perform a widely applied methodology to test for the OLS assumptions. First, the presence of heteroskedasticity is tested by the Breusch and Pagan test (1979) and the White test (1980). No severe heteroskedasticity is detected in the sample. However, {{if any of the}} tests suggest presence of mild heteroskedasticity, White’s heteroskedasticity consistent standard errors are used (White 1980). Second, the existence of multicollinearity is tested by the <b>variance</b> <b>inflation</b> <b>factors.</b> Last, the normality of residuals is tested by the Shapiro-Wilk test (Shapiro and Wilk 1964). When the Shapiro-Wilk test suggests the residuals are non-normally distributed, we use bootstrapping (1000 replications) procedure to estimate the t-statistics and p-values.|$|R
40|$|Well-conditioned {{models are}} important, {{particularly}} for practitioners {{who work with}} regression models for mixture experiments where parameter estimates are individually meaningful. In this article we investigate conditioning in second-order mixture models, using <b>variance</b> <b>inflation</b> <b>factors,</b> maximum and minimum eigenvalues of the information matrix and condition numbers to assess conditioning. A range of equivalent mixture models that lie "between" the Scheffé model (S-model) and the Kronecker model (K-model) is examined, and pseudocomponent transformations for lower bounds (L-pseudocomponents) and upper bounds (U-pseudocomponents) are also discussed. We prove that the maximum eigenvalue for the information matrix for the K-model is always smaller than that for any other model in the above range. We recommend in practice {{the use of the}} K-model, to reduce ill-conditioning, and the appropriate use of pseudocomponents...|$|R
