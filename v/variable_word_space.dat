3|1333|Public
2500|$|.....French spacing. The {{insertion}} of [...] fixed space {{such as an}} en or an em between sentences instead of a <b>variable</b> <b>word</b> <b>space.</b>|$|E
40|$|Three Trade Epitaphs’ was Christopher Wakeling's fourth annual {{contribution}} to the special edition. This piece is hand-set and printed in 10 point Univers 55 along with progressive sizes of Monotype Placard types. The printings are in black, white, red, blue and grey inks on Zerkall mould-made paper. It consists of three loose sheets printed one side, inserted into a red cover folder printed with white woodletter (Three | Trade | EPITAPHS). Finished size: 48 picas by 50 picas. Each epitaph is an ode to a former printing trade craftsman: Letterfounder, Compositor and Pressman. The short verses are extracted from The Trade, privately published in 1943 to ‘beneft the funds of the Printers’ Pension, Almshouse and Orphan Asylum Corporation’. Each is a personal typographical experiment exploring {{the relationship between the}} printed text and the ‘white’, non-printed space of the optical square format. All punctuation was removed from the original verses so that <b>variable</b> <b>word</b> <b>space,</b> together with line and page arrangement could be explored as a means of enhancing or re-enforcing the textural meaning and its grammatical sense. The natural rhythmical pattern of each ode forms the root for the linear vertical arrangement of the texts. Loose inserted with a small imprint card printed in red and black announcing its inclusion for ‘Parenthesis’ subscribers...|$|E
50|$|Awami Nastaliq (Awami {{derived from}} the {{adjectival}} form for Urdu word awam) is a Nasta'liq-script font designed by SIL's Peter Martin for Graphite aware applications. It requires a recent version of Firefox, LibreOffice, or XeTeX to render correctly, and aiming at {{a wide variety of}} languages of southwest Asia, including but not limited to Urdu. It also provides a number of user-selectable font features available via 4-byte character tags to produce character variants and <b>variable</b> <b>word</b> <b>spacing.</b>|$|R
40|$|Abstract. In 1984 T. Carlson and S. Simpson {{established}} an infinitary {{extension of the}} Hales-Jewett Theorem in which the leftmost letters of {{all but one of}} the words were required to be variables. (We call such <b>words</b> left <b>variable</b> <b>words.)</b> In this paper we extend the Carlson-Simpson result for left <b>variable</b> <b>words,</b> prove a corresponding result about right <b>variable</b> <b>words,</b> and determine precisely the extent to which left and right <b>variable</b> <b>words</b> can be combined in such extensions. The results mentioned so far all involve a finite alphabet. We show that the results for left <b>variable</b> <b>words</b> do not extend to words over an infinite alphabet, but that the results for right <b>variable</b> <b>words</b> do extend. 1...|$|R
5000|$|<b>Word</b> <b>spaces,</b> {{preceding}} or following punctuation, {{should be}} optically adjusted {{to appear to}} be of the same value as a standard <b>word</b> <b>space.</b> If a standard <b>word</b> <b>space</b> is inserted after a full point or a comma, then, optically, this produces a space of up to 50% wider than that of other <b>word</b> <b>spaces</b> within a line of type. This is because these punctuation marks carry space above them, which, when added to the adjacent standard <b>word</b> <b>spaces,</b> combines to create a visually larger space. Some argue that the [...] "additional" [...] space after a comma and full point serves as a [...] "pause signal" [...] for the reader. But this is unnecessary (and visually disruptive) since the pause signal is provided by the punctuation mark itself.|$|R
40|$|Dimensionality {{reduction}} {{has been}} shown to improve processing and information extraction from high dimensional data. <b>Word</b> <b>space</b> algorithms typically employ linear reduction techniques that assume the space is Euclidean. We investigate the effects of extracting nonlinear structure in the <b>word</b> <b>space</b> using Locality Preserving Projections, a reduction algorithm that performs manifold learning. We apply this reduction to two common <b>word</b> <b>space</b> models and show improved performance over the original models on benchmarks. ...|$|R
40|$|We {{present the}} S-Space Package, an open source {{framework}} for developing and evaluating <b>word</b> <b>space</b> algorithms. The package implements well-known <b>word</b> <b>space</b> algorithms, such as LSA, {{and provides a}} comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes <b>word</b> <b>space</b> benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate {{the efficiency of the}} reference implementations and also provide their results on six benchmarks. ...|$|R
50|$|Open Juncture {{is marked}} by <b>word</b> <b>space.</b>|$|R
40|$|Abstract — A {{hierarchical}} model incorporating motion patterns, proto symbols and words is proposed. The proto symbols abstract motion patterns, while {{the words are}} associated with the proto symbols stochastically. This paper describes the construction of a <b>word</b> <b>space,</b> where <b>words</b> are located in a multidimensional space based on dissimilarities among the words. The dissimilarity between two words can be calculated by using association probabilities that the words generate motion proto symbols. The <b>word</b> <b>space</b> encapsulates relations among the words such as similar or dissimilar pairs of <b>words.</b> The <b>word</b> <b>space</b> also allows motion recognition based on words. The validity of the constructed <b>word</b> <b>space</b> is demonstrated on a motion capture database. Moreover, the addition of the word associations is found to change the conventional proto symbol space so that the discrimination among the proto symbols is improved. I...|$|R
3000|$|Discriminating <b>variable</b> <b>words</b> from invariable {{words in}} the set of tags on the {{packages}} to be generalized [...]...|$|R
2500|$|... 1999: —modern {{mass-production}} commercial printing: {{a single}} <b>word</b> <b>space</b> between sentences ...|$|R
5000|$|One widened space, {{typically}} one-and-a-third {{to slightly}} less than twice as wide as a <b>word</b> <b>space.</b> This spacing was sometimes used in typesetting before the 19th century. It has also been used in other non-typewriter typesetting systems such as the Linotype machine and the TeX system. Modern computer-based digital fonts can adjust the spacing after terminal punctuation as well, creating a space slightly wider than a standard <b>word</b> <b>space.</b>|$|R
50|$|The <b>variable</b> <b>word</b> type may be {{affected}} by many morphological processes, such as prefixation, suffixation, infixation, reduplication, and glottalization.|$|R
50|$|Variations {{from the}} {{standard}} specification include <b>variable</b> <b>word</b> length and non-standard amplitude values. Some implementations also require non-standard re-synchronization.|$|R
50|$|Language {{can also}} be a factor that typographers would take into {{consideration}} for <b>word</b> <b>spacing.</b> For a language like Latin, “most boundaries are marked by grammatical tags, and a smaller space is therefore sufficient”. In English, the ability to read a line easily, instead of needing {{to make sense of it}} first, is also attributed by good <b>word</b> <b>spacing.</b>|$|R
30|$|The minimum {{asymptotic}} {{time needed}} for asymptotically thorough models is in Ω O(N⌈[*]log 2 (N)⌉) for <b>variable</b> <b>word</b> size computers.|$|R
5000|$|<b>Word</b> <b>spacing</b> in {{typography}} {{refers to}} the size of the <b>space</b> between <b>words.</b> It should be distinguished from letter-spacing (the spacing between the letters within each <b>word)</b> and sentence <b>spacing</b> (the spacing between sentences). Typographers may modify the spacing of letters or words in a body of type to aid readability and copy fit, or for aesthetic effect. In web browsers and standardized digital typography the <b>word</b> <b>spacing</b> is controlled by the CSS1 word-spacing property.|$|R
5000|$|... 1999: the Badger-in-the-bag game—modern {{mass-production}} commercial printing: {{a single}} <b>word</b> <b>space</b> between sentences ...|$|R
40|$|Abstract. We {{describe}} {{a system to}} tackle the Lexical Substitution task that exploits, as its only resource, co-occurrence statistics from a large PoS-tagged corpus. The system exploits the <b>word</b> <b>space</b> model formalism, and represents the word to be substituted by a composite vector {{that takes into account}} both the overall distribution of the word in the input corpus and its local context. As far as the precision and recall are concerned, the system is ranked among the highest positions in the Evalita competition, while it results winner in the mode p and mode r ranking. Key words: <b>word</b> <b>space</b> models, composition in <b>word</b> <b>space</b> models, corpus-based semantics...|$|R
40|$|We {{present the}} results of {{clustering}} experiments {{with a number of}} different evaluation sets using dependency based <b>word</b> <b>spaces.</b> Contrary to previous results we found a clear advantage using a parsed corpus over <b>word</b> <b>spaces</b> constructed with the help of simple patterns. We achieve considerable gains in performance over these spaces ranging between 9 and 13 % in absolute terms of cluster purity. ...|$|R
30|$|The minimum {{asymptotic}} space {{needed for}} asymptotically thorough and incompressible model is in Ω (N⌈[*]log 2 (N)⌉) for <b>variable</b> <b>word</b> size computers.|$|R
40|$|<b>Word</b> <b>space</b> models, in {{the sense}} of vector space models built on {{distributional}} data taken from texts, are used to model semantic relations between words. We argue that the high dimensionality of typical vector space models lead to unintuitive effects on modeling likeness of meaning and that the local structure of <b>word</b> <b>spaces</b> is where interesting semantic relations reside. We show that the local structure of <b>word</b> <b>spaces</b> has substantially different dimensionality and character than the global space and that this structure shows potential to be exploited for further semantic analysis using methods for local analysis of vector space structure rather than globally scoped methods typically in use today such as singular value decomposition or principal component analysis. ...|$|R
40|$|This paper {{proposes a}} <b>word</b> <b>spacing</b> model using a hidden Markov model (HMM) for re ning Korean raw text corpora. Previous {{statistical}} approaches for automatic <b>word</b> <b>spacing</b> have used models that {{make use of}} inaccurate probabilities {{because they do not}} consider the previous spacing state. We consider <b>word</b> <b>spacing</b> problem as a classi cation problem such as Part-of-Speech (POS) tagging and have experimented with various models considering extended context. Experimental result shows that the performance of the model becomes better as the more context considered. In case of the same number of parameters are used with other method, it is proved that our model is more eective by showing the better results...|$|R
40|$|Furstenberg and Katznelson applied {{methods of}} topological {{dynamics}} to Ramsey theory, obtaining a density {{version of the}} Hales-Jewett partition theorem. Inspired by their methods, but using spaces of ultrafilters instead of their metric spaces, we prove a generalization of a theorem of Carlson about <b>variable</b> <b>words.</b> We extend this result to partitions of finite or infinite sequences of <b>variable</b> <b>words,</b> and we apply these extensions to strengthen a partition theorem of Furstenberg and Katznelson about combinatorial subspaces of the set of words. 1...|$|R
50|$|Distributional {{semantic}} {{models that}} use linguistic items as context {{have also been}} referred to as <b>word</b> <b>space</b> models.|$|R
40|$|In {{collaboration}} with Gavagai, {{a company that}} develops automated and scalable methods for retrieving actionable intelligence from dynamic data, I have been studying semantic <b>word</b> <b>spaces</b> and topology. In this bachelor’s thesis, with help from computational topology, I introduce new ways to describe properties of these semantic <b>word</b> <b>spaces,</b> so called barcodes. I develop a measure to describe barcodes of betti number zero, prove its validity and discuss its implications...|$|R
40|$|Abstract. Furstenberg and Katznelson applied {{methods of}} topological {{dynamics}} to Ramsey theory, obtaining a density {{version of the}} Hales-Jewett partition theorem. Inspired by their methods, but using spaces of ultrafilters instead of their metric spaces, we prove a generalization of a theorem of Carlson about <b>variable</b> <b>words.</b> We extend this result to partitions of finite or infinite sequences of <b>variable</b> <b>words,</b> and we apply these extensions to strengthen a partition theorem of Furstenberg and Katznelson about combinatorial subspaces of the set of words. 1...|$|R
40|$|Abstract. In this paper, we {{will analyze}} the {{behavior}} of several parameters, namely type of contexts, similarity measures, and <b>word</b> <b>space</b> models, in the task of word similarity extraction from large corpora. The main objective of the paper will be to describe experiments comparing different extraction systems based on all possible combinations of these parameters. Special attention will {{be paid to the}} comparison between syntax-based contexts and windowing techniques, binary similarity metrics and more elaborate coefficients, as well as baseline <b>word</b> <b>space</b> models and Singular Value Decomposition strategies. The evaluation leads us to conclude that the combination of syntax-based contexts, binary similarity metrics, and a baseline <b>word</b> <b>space</b> model makes the extraction much more precise than other combinations with more elaborate metrics and complex models. ...|$|R
40|$|Recent {{work has}} pointed out the differ-ence between the {{concepts}} of semantic similarity and semantic relatedness. Im-portantly, some NLP applications depend on measures of semantic similarity, while others work better with measures of se-mantic relatedness. It has also been ob-served that methods of computing simi-larity measures from text corpora produce <b>word</b> <b>spaces</b> that are biased towards either semantic similarity or relatedness. De-spite these findings, there has been lit-tle work that evaluates the effect of vari-ous techniques and parameter settings in the <b>word</b> <b>space</b> construction from corpora. The present paper experimentally investi-gates how the choice of context, corpus preprocessing and size, and dimension re-duction techniques like singular value de-composition and frequency cutoffs influ-ence the semantic properties of the result-ing <b>word</b> <b>spaces.</b> ...|$|R
5000|$|... a <b>word</b> <b>space,</b> or letter space (the <b>space</b> {{between two}} <b>words</b> on a line, two letter spaces being ##) ...|$|R
50|$|The {{canonical}} {{word order}} of Macedonian is SVO (subject-verb-object), but <b>word</b> order is <b>variable.</b> <b>Word</b> order may be changed for poetic effect (inversion {{is common in}} poetry).|$|R
5000|$|Not all {{certified}} copies {{are prepared}} by notaries. In {{the case of}} certified copies by notaries, the certificate itself is subject to highly <b>variable</b> <b>wording.</b> In the U.S.: ...|$|R
50|$|Improve hand-eye {{coordination}} in writing following writing skills, e.g., clockwise and anti clockwise circling, angular writing, keeping symmetry and inter <b>word</b> <b>spacing.</b>|$|R
5000|$|Icelandic Sign Language has {{somewhat}} <b>variable</b> <b>word</b> order {{depending on}} the verb used. The basic word order is SVO however, with agreement verbs SOV and OSV word order is also accepted.|$|R
50|$|Marks of {{omission}} should {{consist of}} three full points. These {{should be set}} without any spaces, but be preceded and followed by <b>word</b> <b>spaces.</b>|$|R
40|$|The Carlson-Simpson lemma is a {{combinatorial}} statement {{occurring in}} the proof of the Dual Ramsey theorem. Formulated in terms of <b>variable</b> <b>words,</b> it informally asserts that given any finite coloring of the strings, there is an infinite sequence with infinitely many variables such that for every valuation, some specific set of initial segments is homogeneous. Friedman, Simpson, and Montalban asked about its reverse mathematical strength. We study the computability-theoretic properties and the reverse mathematics of this statement, and relate it to the finite union theorem. In particular, we prove the Ordered <b>Variable</b> <b>word</b> for binary strings in ACA 0. Comment: 11 page...|$|R
