735|817|Public
25|$|Surveilluminescent wand: {{a device}} for {{visualizing}} vision and seeing sight, {{by way of}} making visible the sightfield (time-reversed lightfield) of a camera or similar computer <b>vision</b> <b>sensor,</b> using time-exposure with array of surveilluminescent lights to make visible to one camera what another camera can see.|$|E
25|$|Areas of {{artificial}} intelligence deal with autonomous planning or deliberation for robotical systems to navigate through an environment. A detailed {{understanding of these}} environments is required to navigate through them. Information about the environment {{could be provided by}} a computer vision system, acting as a <b>vision</b> <b>sensor</b> and providing high-level information about the environment and the robot.|$|E
50|$|Key {{technology}} used by all teams was computer <b>vision,</b> <b>sensor</b> fusion, human-robot interaction,and simultaneous localization and mapping (SLAM).|$|E
40|$|In robot navigation, {{one of the}} {{important}} and fundamental issues is to reconstruct positions of landmarks or <b>vision</b> <b>sensors</b> locating around the robot. This paper proposes a method for reconstructing qualitative positions of multiple <b>vision</b> <b>sensors</b> from qualitative information ob-served by the <b>vision</b> <b>sensors,</b> i. e., motion direc-tions of moving objects. The process iterates the following steps: (1) observing motion direc-tions of moving objects from the <b>vision</b> <b>sensors,</b> (2) classifying the <b>vision</b> <b>sensors</b> into spatially classified pairs, (3) acquiring three point con-straints, and (4) propagating the constraints. The method have been evaluated with simula-tions. ...|$|R
50|$|Vision {{refers to}} {{processing}} data from any modality which uses {{the electromagnetic spectrum}} to produce an image. In humanoid robots {{it is used to}} recognize objects and determine their properties. <b>Vision</b> <b>sensors</b> work most similarly to the eyes of human beings. Most humanoid robots use CCD cameras as <b>vision</b> <b>sensors.</b>|$|R
5000|$|Infrared <b>vision</b> <b>sensors</b> for {{obstacle}} avoidance and detection of movements ...|$|R
50|$|Aspects of {{her work}} and {{personal}} life have been described in {{a book about the}} creation of the <b>vision</b> <b>sensor</b> company Foveon.|$|E
50|$|In a parts feeder, a <b>vision</b> <b>sensor</b> can {{eliminate}} the need for an alignment pallet. Vision-enabled insertion robots can precisely perform fitting and insertion operations of machine parts.|$|E
50|$|Seth Jared Teller (1964 - July 1, 2014) was an American {{computer}} scientist {{and professor at}} the Massachusetts Institute of Technology, whose research interests included computer <b>vision,</b> <b>sensor</b> networks, and robotics.|$|E
40|$|This paper {{proposes a}} method for reconstructing {{qualitative}} positions of multiple <b>vision</b> <b>sensors</b> from qualitative information observed by the <b>vision</b> <b>sensors,</b> i. e., motion directions of moving objects. In order to directly acquire the qualitative positions of points, the method proposed in this paper iterates the following steps: 1) observing motion directions (left or right) of moving objects with the <b>vision</b> <b>sensors,</b> 2) classifying the <b>vision</b> <b>sensors</b> into ######### ########## ##### based on the motion directions, 3) acquiring ##### ##### ###########, and 4) propagating the constraints. Compared with the previous methods, which reconstruct the environment structure from quantitative measurements and acquire qualitative representations by abstracting it, this paper focuses on how to acquire qualitative positions of landmarks from low-level, simple, and reliable information (that is, qualitative). The method has been evaluated with simulations and also verified with observation error...|$|R
30|$|Mobility in unknown (low-visibility) {{environment}} The robot {{should move}} {{even when the}} visibility for human operators and/or robot itself is quite low. In extreme environment with heavy smoke and fog, the <b>vision</b> <b>sensors</b> for localization, mapping, and navigation cannot obtain sufficient environmental information. Moreover, even when such <b>vision</b> <b>sensors</b> are broken, the robot keeps to move for continuing tasks or returning to the rescue base.|$|R
5000|$|Robocrane - RCS {{controlled}} a crane having {{six degrees}} of freedom. It incorporated tactile, proximity and <b>vision</b> <b>sensors.</b>|$|R
5000|$|Virtual Cinematography {{has evolved}} greatly since {{this time and}} can be found in use prolifically across a {{spectrum}} of digital media formats. Subsets technology components of Virtual Cinematography include [...] "computational photography, machine <b>vision,</b> <b>sensor</b> based volumetric video and image based rendering.|$|E
5000|$|Surveilluminescent wand: {{a device}} for {{visualizing}} vision and seeing sight, {{by way of}} making visible the sightfield (time-reversed lightfield) of a camera or similar computer <b>vision</b> <b>sensor,</b> using time-exposure with array of surveilluminescent lights to make visible to one camera what another camera can see.|$|E
50|$|VITAR (Vision based Tracked Autonomous Robot) {{consists}} of a tracked mobile robot equipped with a pan-tilt mounted <b>vision</b> <b>sensor,</b> an onboard PC, driver electronics, and a wireless link to a remote PC. It has been utilized to test vision based algorithms such as the HIS and the HIS-DMA.|$|E
40|$|The recent {{development}} of neuromorphic <b>vision</b> <b>sensors,</b> which provide an asynchronous, high-speed alternative to conventional cameras has {{lead to a}} considerable amount of research into their applicability to robotic control systems. However, algorithms for onboard control of mobile robotic platforms such as automobiles or aircraft using these sensors are lacking and in fact almost all existing implementations keep the sensor stationary. This research has several objectives. First, to develop a rigorous understanding of how to use asynchronous temporal contrast <b>vision</b> <b>sensors</b> for heading regulation and tracking {{in such a way as}} to fully leverage the remarkable properties of these sensors including high bandwidth, low latency and low power consumption. Second, to provide a theoretical and experimental comparison between neuromorphic <b>vision</b> <b>sensors</b> and conventional cameras in the context of this problem. Finally, to describe and test algorithms for high-speed motion planning in cluttered environments using neuromorphic <b>vision</b> <b>sensors.</b> by Erich Mueller. Thesis: Ph. D., Massachusetts Institute of Technology, Department of Aeronautics and Astronautics, February 2016. Cataloged from PDF version of thesis. "February 2015. "Includes bibliographical references (pages 162 - 173) ...|$|R
40|$|Abstract. In this paper, {{we propose}} our ideas for {{realising}} a Distributed Vision System {{for building a}} map of a large environment {{with a team of}} mobile robots equipped only with <b>vision</b> <b>sensors.</b> The <b>vision</b> <b>sensors</b> used in this work are cata-dioptric omnidirectional vision systems. The mirrors of these catadioptric systems have proles expressly designed for the robot's body and for the robot's task. We report preliminary experiments on the interactions between the vision systems of the dierent robots. ...|$|R
40|$|In recent years, various {{practical}} systems {{using multiple}} <b>vision</b> <b>sensors</b> have been proposed. In this paper, as {{an application of}} such vision systems, we propose a real-time human tracking system consisting of multiple omnidirectional <b>vision</b> <b>sensors</b> (ODVSs). The system measures people's locations by N-ocular stereo, which {{is an extension of}} trinocular stereo, from omnidirectional images taken with the ODVSs. In addition, the system employs several compensation methods for observation errors in order to achieve robust measurement. We have evaluated the proposed methods in the experimentation using four compact ODVSs we have originally developed. 1. Introduction Recent progress of multimedia and computer graphics is developing practical applications based on simple computer vision techniques. Especially, the practical approach recently focused on is to use multiple <b>vision</b> <b>sensors</b> with simple visual processing. For example, several systems track people or automobiles in the real environm [...] ...|$|R
5000|$|Intelligent {{welding robot}} for chassis : It {{automatically}} recognize welding with laser <b>vision</b> <b>sensor</b> in the welding process for chassis. It {{was the first}} automatic production line application of Korean patent. The technology was developed in 1996 and operated in the production line of Daewoo Motor Co. from 1997.|$|E
50|$|Controlled active vision can {{be defined}} as a {{controlled}} motion of a <b>vision</b> <b>sensor</b> can maximize the performance of any robotic algorithm that involves a moving <b>vision</b> <b>sensor.</b> It is a hybrid of control theory and conventional vision. An application of this framework is real-time robotic servoing around static or moving arbitrary 3-D objects. See Visual Servoing. Algorithms that incorporate the use of multiple windows and numerically stable confidence measures are combined with stochastic controllers in order to provide a satisfactory solution to the tracking problem introduced by combining computer vision and control. In the case where there is an inaccurate model of the environment, adaptive control techniques may be introduced. The above information and further mathematical representations of controlled active vision {{can be seen in the}} thesis of Nikolaos Papanikolopoulos.|$|E
50|$|Visual servoing, {{also known}} as vision-based robot control and {{abbreviated}} VS, is a technique which uses feedback information extracted from a <b>vision</b> <b>sensor</b> (visual feedback) to control the motion of a robot. One of the earliest papers that talks about visual servoing was from the SRI International Labs in 1979.|$|E
40|$|Vision based {{observation}} of {{a moving target}} {{is one of the}} important problems and hot issues of mobile robot system. Dynamical <b>vision</b> <b>sensors</b> which is constituted with multiple robots to obtain improved observational results {{has been shown to be}} a good substitution of single <b>vision</b> <b>sensors.</b> Thus, a new active cooperative observation (ACO) method based on two dynamical <b>vision</b> <b>sensors</b> is proposed. The most characteristics of the proposed method is that data fusion and path planning algorithm are simultaneously implemented and combined with each other by the optimal observation formation so that the influence of relative positions among MVSs and target on the cooperative observation result can be fully considered to improve the observation result. Finally, for verifying the validity and feasibility of the proposed method, experiments on a multiple rotor flying robots test-bed are conducted and analyzed, respectively...|$|R
30|$|Xiangyu Weng {{currently}} pursues his PhD in Control Science and Engineering at the College of Automation, Harbin Engineering University. His research interest includes intelligent control, control engineering, and <b>vision</b> <b>sensors.</b>|$|R
40|$|As {{an attempt}} to steer away from {{developing}} an autonomous robot with complex centralised intelligence, this thesis proposes an intelligent environment infrastructure where intelligences are distributed in the environment through collaborative <b>vision</b> <b>sensors</b> mounted in a physical architecture, forming a wireless sensor network, to enable the navigation of unintelligent robots within that physical architecture. The aim is to avoid the bottleneck of centralised robot intelligence that hinders the application and exploitation of autonomous robot. A bio-mimetic snake algorithm is proposed to coordinate the distributed <b>vision</b> <b>sensors</b> for the generation of a collision free Reference-snake (R-snake) path during the path planning process. By following the R-snake path, a novel Accompanied snake (A-snake) method that complies with the robot's nonholonomic constraints for trajectory generation and motion control is introduced to generate real time robot motion commands to navigate the robot from its current position to the target position. A rolling window optimisation mechanism subject to control input saturation constraints is carried out for time-optimal control along the A-snake. A comprehensive simulation software and a practical distributed intelligent environment with <b>vision</b> <b>sensors</b> mounted on a building ceiling are developed. All the algorithms proposed in this thesis are first verified by the simulation and then implemented in the practical intelligent environment. A model car with less on-board intelligence is successfully controlled by the distributed <b>vision</b> <b>sensors</b> and demonstrated superior mobility. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
50|$|The VNsight visible/near {{infrared}} sensor is a low-light-level TV (LLLTV) {{integrated into the}} Apache's Modernized Pilot Night <b>Vision</b> <b>Sensor</b> (M-PNVS) and Pathfinder dedicated pilotage sensor (the M-PNVS adapted for cargo and utility aircraft). The additional imaging capability in this wavelength complements the long wave infrared wavelength of the existing sensor and adds significant tactical advantages.|$|E
50|$|The {{concept of}} STDP {{has been shown}} to be a proven {{learning}} algorithm for forward-connected artificial neural networks in pattern recognition. Recognising traffic, sound or movement using Dynamic <b>Vision</b> <b>Sensor</b> (DVS) cameras has been a recent area of research. Correct classifications with a high degree of accuracy with only minimal learning time has been shown.|$|E
50|$|Areas of {{artificial}} intelligence deal with autonomous planning or deliberation for robotical systems to navigate through an environment. A detailed {{understanding of these}} environments is required to navigate through them. Information about the environment {{could be provided by}} a computer vision system, acting as a <b>vision</b> <b>sensor</b> and providing high-level information about the environment and the robot.|$|E
40|$|Bio-inspired {{asynchronous}} event-based <b>vision</b> <b>sensors</b> {{are currently}} introducing {{a paradigm shift}} in visual information processing. These new sensors rely on a stimulus-driven principle of light acquisition similar to biological retinas. They are event-driven and fully asynchronous, thereby reducing redundancy and encoding exact times of input signal changes, leading to a very precise temporal resolution. Approaches for higher-level computer vision often rely on the realiable detection of features in visual frames, but similar definitions of features for the novel dynamic and event-based visual input representation of silicon retinas have so far been lacking. This article addresses the problem of learning and recognizing features for event-based <b>vision</b> <b>sensors,</b> which capture properties of truly spatiotemporal volumes of sparse visual event information. A novel computational architecture for learning and encoding spatiotemporal features is introduced based {{on a set of}} predictive recurrent reservoir networks, competing via winner-take-all selection. Features are learned in an unsupervised manner from real-world input recorded with event-based <b>vision</b> <b>sensors.</b> It is shown that the networks in the architecture learn distinct and task-specific dynamic visual features, and can predict their trajectories over time...|$|R
40|$|In {{this paper}} {{we present a}} set of novel methods for {{image-based}} modeling using omnidirectional <b>vision</b> <b>sensors.</b> The basic idea is to directly and efficiently acquire plenoptic representations by using omnidirectional <b>vision</b> <b>sensors.</b> The three methods, in order of increasing complexity, are direct memorization, discrete interpolation, and smooth interpolation. Results of these methods are compared visually with ground-truth images taken from a standard camera walking along the same path. The experimental results demonstrate that our methods are successful at generating high-quality virtual images. In particular, the smooth interpolation technique approximates the plenoptic function most closely. A comparative analysis of the computational {{costs associated with the}} three methods is also presented...|$|R
40|$|Abstract: In {{this paper}} we show how a {{combination}} of low dimensional <b>vision</b> <b>sensors</b> {{can be used to}} aid the higher level visual processing task of colour blob tracking, carried out by a conventional vision system. The processing elements are neuromorphic analog VLSI (aVLSI) <b>vision</b> <b>sensors</b> capable of computing motion and estimating the spatial position of high-contrast moving targets. In these devices both sensing and processing is done on the chip’s focal plane. The neuromorphic sensors can calculate optical flow, position of sharpest edge, and motion of sharpest edge, in real-time. The processing capability of the system is investigated in a mobile robotics application. Firstly, for visualization and evaluation purposes, a correlation analysis is performed between the data collected from the neuromorphic <b>vision</b> <b>sensors</b> and the standard vision system of the autonomous robot, then, we process the multiple neuromorphic sensory signals with a standard auto-regression method in order to achieve a higher level vision processing task at a much higher update rate. At the end we argue why this result is of great relevance for the application domain of reactive and lightweight mobile robotics, {{at the hands of a}} soccer robot, where the fastes...|$|R
50|$|The Apache Arrowhead (also Modernized Target Acquisition and Designation Sight/Pilot Night <b>Vision</b> <b>Sensor</b> or M-TADS/PNVS), is an {{integrated}} targeting and night vision system developed by Lockheed Martin for the Boeing AH-64 Apache attack helicopter. It uses second-generation long-wave Forward looking infrared (FLIR) sensors with three fields of view, a charge-coupled device TV camera, dual {{field of view}} pilotage FLIR, electronic zoom, target tracker and auto-boresight.|$|E
50|$|The company’s product {{portfolio}} includes In-Sight, a vision system that combines a camera, software and processor into one compact unit; VisionPro vision software; Checker, a single-purpose <b>vision</b> <b>sensor</b> {{used to provide}} high performance at certain common vision tasks, such as checking for {{the presence or absence}} of parts and features; DataMan, a family of fixed mount and handheld ID readers used to identify and track items by reading 1D and 2D Data Matrix codes.|$|E
50|$|The AKMSN {{model is}} derived from the AKMS and {{features}} an accessory rail used to mount a night <b>vision</b> <b>sensor</b> as seen on the AKML and additionally a flash hider and bipod. The left arm of the AKMSN’s folding stock is bent outwards {{in order to avoid the}} sight mount bracket during folding and the sling loop was moved further to the rear. Similarly to the AKMN-1, the AKMSN-1 can mount the multi-model night vision scope 1PN51 and the AKMSN2 the multi-model night vision scope 1PN58.|$|E
40|$|This thesis {{describes}} {{the design and}} implementation of a system for checking the boxes completeness. Thesis is divided into several parts. The first part includes an introduction into the manufacturing process and also devices, for which the system is designed. The second part consists of system design and a description of hardware components used for the system. The third part deals {{with the creation of}} a scheme used for connection an electrical switchboard. The fourth part is focused on a design of mechanical constructions, such as construction plan used for <b>vision</b> <b>sensors</b> connection, lighting system and design mechanical parts of stopper. A simulator of the system for checking the boxes completeness is described in the fifth part. It is used for a demonstration of functionality and controllability of the system. The sixth part deals with enabling of a communication between particular sub-systems, such as <b>vision</b> <b>sensors,</b> control panel and control system. The seventh part is focused on creation of the software required for the control system, HMI panel and deals with the configuration of pair <b>vision</b> <b>sensors.</b> In the final part there are some illustrations of designed system in real process of manufacturing...|$|R
40|$|The {{invention}} is {{a method}} of using computer vision to control systems consisting {{of a combination of}} holonomic and nonholonomic degrees of freedom such as a wheeled rover equipped with a robotic arm, a forklift, and earth-moving equipment such as a backhoe or a front-loader. Using <b>vision</b> <b>sensors</b> mounted on the mobile system and the manipulator, the system establishes a relationship between the internal joint configuration of the holonomic degrees of freedom of the manipulator and the appearance of features on the manipulator in the reference frames of the <b>vision</b> <b>sensors.</b> Then, the system, perhaps with the assistance of an operator, identifies the locations of the target object in the reference frames of the <b>vision</b> <b>sensors.</b> Using this target information, along with the relationship described above, the system determines a suitable trajectory for the nonholonomic degrees of freedom of the base to follow towards the target object. The system also determines a suitable pose or series of poses for the holonomic degrees of freedom of the manipulator. With additional visual samples, the system automatically updates the trajectory and final pose of the manipulator so as to allow for greater precision in the overall final position of the system...|$|R
50|$|The company’s product {{portfolio}} includes photoelectric sensors, light grids, inductive, capacitive and magnetic sensors, opto-electronic protective devices, <b>vision</b> <b>sensors,</b> detection, ranging and identification solutions such as bar code scanners and RFID readers, analyzers for gas and liquid analysis {{as well as}} gas flow measuring devices.|$|R
