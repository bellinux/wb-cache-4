3573|556|Public
5|$|Kata is a {{term used}} to {{describe}} the posture and movement associated with taiko performance. The term is used in martial arts in a similar way: for example, both traditions include the idea that the hara is the center of being. Author Sean Bender argues that kata is the primary feature that distinguishes different taiko groups from one another, and is a key factor in judging the quality of performance. For this reason, many practice rooms intended for taiko contain mirrors to provide <b>visual</b> <b>feedback</b> to players. An important part of kata in taiko is keeping the body stabilized while performing, and can be accomplished by keeping a wide, low stance with the legs, with the left knee bent over the toes and keeping the right leg straight. It is important that the hips face the drum and the shoulders are relaxed. Some teachers note a tendency to rely on the upper body while playing, and emphasize the importance of the holistic use of the body during performance.|$|E
5|$|The player flies {{around in}} a fighter with a first-person, in-cockpit view with a fully {{customizable}} fixed head-up display (HUD) as the visual interface. The HUD displays video communications and relevant data on the ship's status and performance, weapons, objectives, and targets. It can also warn players from which direction missiles are locking onto them from, thus becoming an aide for launching countermeasures or taking evasive maneuvers. Players have to maneuver into position and shoot through both shields and hull to destroy enemy ships. While hull damage is unrecoverable, shields recharge over time. With the game supporting force feedback technology, joystick players will find their controllers vibrating or putting up resistance when they engage the afterburners or collide with objects. Similarly, certain events, such as engaging afterburners and firing powerful weapons, will shake the screen {{as a form of}} <b>visual</b> <b>feedback.</b>|$|E
25|$|The motion {{controller}} features vibration-based haptic technology. In {{addition to}} providing a tracking reference, the controller's orb light {{can be used to}} provide <b>visual</b> <b>feedback,</b> simulating aesthetic effects such as the muzzle flash of a gun, or the paint on a brush.|$|E
40|$|ObjectiveDirect haptic (force or tactile) {{feedback}} is negligible in current surgical robotic systems. The relevance of haptic feedback in robot-assisted performances of surgical tasks is controversial. We studied {{the effects of}} <b>visual</b> force <b>feedback,</b> a haptic feedback surrogate, on tying surgical knots with fine sutures {{similar to those used}} in cardiovascular surgery. MethodsBy using a modified da Vinci robotic system (Intuitive Surgical, Inc, Sunnyvale, Calif) equipped with force-sensing instrument tips and real-time <b>visual</b> force <b>feedback</b> overlays in the console image, 10 surgeons each tied 10 knots with and 10 knots without <b>visual</b> force <b>feedback.</b> Four surgeons had significant prior da Vinci experience, and the remaining 6 surgeons did not. Performance parameters, including suture breakage and secure knots, peak and standard deviation of applied forces, and completion times using 5 - 0 silk sutures, were recorded. Chi-square and Student t test analyses determined the differences between groups. ResultsAmong surgeon subjects with robotic experience, no differences in measured performance parameters were found between robot-assisted knot ties executed with and without <b>visual</b> force <b>feedback.</b> Among surgeons without robotic experience, however, <b>visual</b> force <b>feedback</b> was associated with lower suture breakage rates, peak applied forces, and standard deviations of applied forces. <b>Visual</b> force <b>feedback</b> did not impart differences in knot completion times or loose knots for either surgeon group. ConclusionsVisual force feedback resulted in reduced suture breakage, lower forces, and decreased force inconsistencies among novice robotic surgeons, although elapsed time and knot quality were unaffected. In contrast, <b>visual</b> force <b>feedback</b> did not affect these metrics among surgeons experienced with the da Vinci system. These results suggest that <b>visual</b> force <b>feedback</b> primarily benefits novice robot-assisted surgeons, with diminishing benefits among experienced surgeons...|$|R
40|$|The {{growing demand}} for haptic {{technologies}} {{in recent years has}} motivated novel approaches in developing haptic interfaces and control algorithms. Based on the force reflecting nature and energy flow directions, those interfaces are categorized in active and semi-active groups. Semi-active interfaces, in general, have the advantage of addressing safety concerns which adversely affects their active counterparts. This thesis presents the development of semi-active haptic interfaces using Magnetorheological (MR) -dampers. The ability of MR-Dampers in producing controllable resistance forces is the key reason for their utilization in interfaces. Our semi-active haptic interfaces are consisted of linear and rotary MR-Dampers. Each of the MR-Dampers is modeled using Bouc-Wen model. The parameters of the mathematical equation of the MR-Damper are identified experimentally. The concept of Digital Resistance Map (DRM) is developed as main strategy for activating MR-Dampers for semi-active interfaces. A preliminary study is carried out to verify the potentials of MR-Dampers and DRM for implementing in semi-active haptic system. Next, the DRM concept is expanded and introduced as a haptic rendering algorithm. The DRM is a high-fidelity haptic rendering algorithm and proved to be effective to create comprehensive force feedback for operators. MATLAB/Simulink is used for implementing several DRM scenarios for generating haptic enabled virtual environments. Several experiments are conducted to demonstrate the effectiveness of the interface and rendering algorithm. A human subject experiment is also included as a further investigation of the proposed system. The interface is integrated with virtual reality to provide the human operators with haptic and <b>visual</b> <b>feedbacks.</b> The obtained results confirm that the proposed system is able to generate understandable haptic and <b>visual</b> <b>feedbacks</b> to help the operators to explore virtual environments. Also, it is found that, to obtain the best human performances, haptic and <b>visual</b> <b>feedbacks</b> need to be combined...|$|R
40|$|International audienceThis paper {{presents}} an immersive application where users receive sound and <b>visual</b> <b>feedbacks</b> on {{their interactions with}} a virtual environment. In this application, the users play the part of conductors of an orchestra of factory machines since each of their actions on interaction devices triggers a pair of visual and audio responses. Audio stimuli were spatialized around the listener. The application was exhibited during the 2013 Science and Music day and designed {{to be used in}} a large immersive system with head tracking, shutter glasses and a 10. 2 loudspeaker configuration...|$|R
25|$|<b>Visual</b> <b>feedback</b> {{has also}} been studied in {{relation}} to visual extinction. Patients were asked to touch a known target in a darkened room. A light attached to the patient’s hand was sometimes briefly illuminated, {{to provide information about}} where the hand was in relation to the target. In some of these trials, a distracting light was also lit, which induced an extinction event in the patient. Although the patient reported in such cases that he had not seen the indicator light on his hand, their performance was correspondingly better, similar to the results when <b>visual</b> <b>feedback</b> had been available. Although the patient was not cognizant of having received the information, they were able to correctly act upon it, {{in a manner similar to}} blindsight.|$|E
25|$|Research into {{treating}} the condition with Mirror <b>Visual</b> <b>Feedback</b> is being undertaken at the Royal National Hospital for Rheumatic Disease in Bath. Patients are {{taught how to}} desensitize in the most effective way, then progress to using mirrors to rewrite the faulty signals in the brain that appear responsible for this condition.|$|E
25|$|A {{video game}} is an {{electronic}} game that involves interaction with a user interface to generate <b>visual</b> <b>feedback</b> {{on a video}} device such as a TV screen or computer monitor. The word video in video game traditionally referred to a raster display device, but as of the 2000s, it implies any type of display device that can produce two- or three-dimensional images. Some theorists categorize video games as an art form, but this designation is controversial.|$|E
40|$|We are {{developing}} a visually-guided autonomous underwater vehicle. We have achieved a position-based visual servo control of fixed and slow moving targets using <b>visual</b> position <b>feedback</b> and sensor-based orientation <b>feedback.</b> The <b>visual</b> position <b>feedback</b> has been implemented on a stereo camera system. We use a compass and an inclinometer for orientation feedback. We have also implemented a computed torque controller using Euler parameters to represent the orientation state, for vehicle motion control. Using Euler parameters eliminates singularities in the model and the controller. Preliminary experimental results of visual servo control are reported. ...|$|R
40|$|Various {{cognitive}} control {{processes are}} generally assumed {{to take place}} in the production of written language. The studies reported in this thesis aimed at assessing the extent to which visual information is involved in such control operations that might underlie the production of number agreement. Sentences with various linguistic characteristics and target agreements were dictated to French speaking university students who were simply asked to write them down. They were required to do this in different conditions according to whether they could see what they were writing or not, and according to whether they were constrained by an additive cognitive load or not. Using a pen tablet allowed us to record and to analyze their writing online. Thanks to different measures based on errors, pauses (with the assumption that long pauses could reflect a pre-graphic control of the agreement), corrections (with the assumption that such corrections could reflect a post-graphic control of the agreement) and writing speed, we show that <b>visual</b> <b>feedbacks</b> impact the control of grammatical agreements. When they are suppressed, the proportion of errors increases but the writing speed decreases as well as the proportion of pre- and post-graphic controls. In addition, we showed that <b>visual</b> <b>feedbacks</b> impact the agreements control in various ways depending {{on the nature of the}} agreement and, in case of verbal agreement, on the linear distance with the subject. (LOGO 3) [...] UCL, 200...|$|R
40|$|A smart Unmanned Ground Vehicle (UGV) is {{designed}} and developed for some application specific missions to operate predominantly in hazardous environments. In our work, {{we have developed}} a small and lightweight vehicle to operate in general cross-country terrains in or without daylight. The UGV can send <b>visual</b> <b>feedbacks</b> to the operator at a remote location. Onboard infrared sensors can detect the obstacles around the UGV and sends signals to the operator. Comment: In proceedings of 2 nd National Conference on Recent Trends in Information Systems (ReTIS- 08), pp. 222 - 225, Feb 7 - 9, 2008, Kolkat...|$|R
25|$|While {{auditory}} feedback {{is most important}} during speech acquisition, it may be activated less if the model has learned a proper feedforward motor command for each speech unit. But {{it has been shown}} that {{auditory feedback}} needs to be strongly coactivated in the case of auditory perturbation (e.g. shifting a formant frequency, Tourville et al. 2005). This is comparable to the strong influence of <b>visual</b> <b>feedback</b> on reaching movements during visual perturbation (e.g. shifting the location of objects by viewing through a prism).|$|E
25|$|In {{a review}} article by Pierce & Buxbaum (2002), they {{concluded}} that the evidence for Hemispheric Activation Approaches, which focuses on moving the limb {{on the side of}} the neglect, has conflicting evidence in the literature. The authors note that a possible limitation in this approach is the requirement for the patients to actively move the neglected limb, which may not be possible for many patients. Constraint-Induced Therapy (CIT), appears to be an effective, long-term treatment for improving neglect in various studies. However, the use of CIT is limited to patients who have active control of wrist and hand extension. Prism Glasses, Hemispatial Glasses, and Eye-Patching have all appear to be effective in improving performance on neglect tests. Caloric Stimulation treatment appears to be effective in improving neglect; however, the effects are generally short-term. The review also suggests that Optokinetic Stimulation is effective in improving position sense, motor skills, body orientation, and perceptual neglect on a short-term basis. As with Caloric Stimulation treatment, long-term studies will be necessary to show its effectiveness. A few Trunk Rotation Therapy studies suggest its effectiveness in improving performance on neglect tests as well as the Functional Independence Measure (FIM). Some less studied treatment possibilities include treatments that target Dorsal Stream of visual processing, Mental Imagery Training, and Neck Vibration Therapy. Trunk rotation therapies aimed at improving postural disorders and balance deficits in patients with unilateral neglect, have demonstrated optimistic results in regaining voluntary trunk control when using specific postural rehabilitative devices. One such device is the Bon Saint Côme apparatus, which uses spatial exploratory tasks in combination with auditory and <b>visual</b> <b>feedback</b> mechanisms to develop trunk control. The Bon Saint Côme device has been shown to be effective with hemiplegic subjects due to the combination of trunk stability exercises, along with the cognitive requirements needed to perform the postural tasks.|$|E
500|$|As a god, {{the player}} can teach their {{creature}} to perform {{tasks such as}} stocking the village store or performing miracles. The creature is taught what and when to eat, and how to attack or impress enemy villages. Fighting skills may be taught in one-on-one battles with other creatures; attack and defence abilities can be improved. Teaching is performed using a reinforcement learning system: if the creature does something the player does not want, it can be discourage with a slap. If the creature does something the player approves of, it can be stroked. The creature remembers the response to various actions and gradually changes its behaviour accordingly. With time and repetition, it can perform complex functions that allow it {{to serve as the}} player's avatar. Three types of leashes are used to command the creature to go to a specific place, and can be tied to a building to restrict movement. One leash encourages the creature to pay attention when actions are demonstrated; the others encourage either benevolent or malevolent behaviour. [...] The game reinforces the creature's choices and learning by providing <b>visual</b> <b>feedback,</b> and the creature has an alignment separate from the player's. Evil wolves sport glowing eyes and large fangs and claws; good ones turn a shade of purple and glow gently.|$|E
40|$|This paper {{presents}} an immersive application where users receive sound and <b>visual</b> <b>feedbacks</b> on {{their interactions with}} a virtual environment. In this application, the users play the part of conductors of an orchestra of factory machines since each of their actions on interaction devices triggers a pair of visual and audio responses. Audio stimuli were spatialized around the listener. The application was exhibited during the 2013 Science and Music day and designed {{to be used in}} a large immersive system with head tracking, shutter glasses and a 10. 2 loudspeaker configuration. Comment: Sonic Interaction for Virtual Environments, Minneapolis : United States (2014...|$|R
50|$|Direct <b>Visual</b> <b>Feedback's</b> method applies more {{directly}} to improving exterior physical health and performance. Instead of using technically advanced measuring instruments for feedback, DVF {{relies on the}} human eye. One method {{that has been used}} to test DVF is by placing a laser beam on the hand and connecting it to a monitor, when the trainee raises his arm from waist height to shoulder height he is focused on the external reference showing exactly what path the arm took from point A(waist height) to point B(shoulder height). Studies have shown that not only is a higher level of performance often achieved faster with an external rather than an internal attention focus, but the skill is retained better.|$|R
40|$|Biofeedback {{has been}} used {{extensively}} in {{physical medicine and rehabilitation}} of human joints to facilitate recovery to normal function after injury and treatments [1]. Audio and <b>visual</b> <b>feedbacks</b> are intended to encourage patients to perform rehabilitation exercises with more attention, more accurately, and more frequently by adding enter-tainment to the execution of physical exercises. The signals on the position and orientation of the body seg-ments involved in the movement exercise should provide users with valuable feedback on the quality of their performance. This can be displayed in the basic form of numbers (direct inclinations or joint angles, general scores, etc.), geometrical entities or simple bar plots [2], up to complete immersive virtual environments typical of video-games [3 - 7]...|$|R
500|$|Work on DJ Hero 2 started {{some months}} after the first game was completed, as the {{development}} team had endured heavy working hours to meet the release deadline. [...] FreeStyleGames' creative director Jamie Jackson said that DJ Hero 2 took about seven months to complete. While {{they were able to}} easily build on the existing framework for DJ Hero, FreeStyleGames wanted {{to do more than just}} include new mixes, and sought to include additional features such as the Battle Mixes into the game. [...] Many of the game's new features are a result of feedback from players, including the game's social modes and improvements in the game's interface to provide a cleaner look and instantaneous <b>visual</b> <b>feedback</b> to the player. The team also worked at improving the game's graphics; the appearance of DJ Heros graffiti-based menus gave the impression of being a hip-hop game, but FreeStyleGames wanted to distance DJ Hero 2 from that. They also wanted to clean up the gameplay's interface, providing better feedback to the player and making it easily to tell when they were winning or losing. [...] While DJ Hero 2 does not use Kinect or PlayStation Move motion-sensing technology, Jackson had not ruled them out for a future iteration of the game.|$|E
2500|$|... (In)famous for its terseness, ed gives {{almost no}} <b>visual</b> <b>feedback,</b> {{and has been}} called (by Peter H. Salus) [...] "the most user-hostile editor ever created", even when {{compared}} to the contemporary (and notoriously complex) TECO. For example, the message that ed will produce in case of error, or when it wants to make sure the user wishes to quit without saving, is [...] "?". [...] It does not report the current filename or line number, or even display the results of a change to the text, unless requested. Older versions (ca. 1981) did not even ask for confirmation when a quit command was issued without the user saving changes. This terseness was appropriate in the early versions of Unix, when consoles were teletypes, modems were slow, and memory was precious. As computer technology improved and these constraints were loosened, editors with more <b>visual</b> <b>feedback</b> became the norm.|$|E
2500|$|Intensive-care {{ventilators}} — These ventilators {{are larger}} and usually run on AC power (though virtually all contain a battery to facilitate intra-facility transport {{and as a}} back-up {{in the event of}} a power failure). [...] This style of ventilator often provides greater control of a wide variety of ventilation parameters (such as inspiratory rise time). [...] Many ICU ventilators also incorporate graphics to provide <b>visual</b> <b>feedback</b> of each breath.|$|E
40|$|Security of an {{information}} system is only as strong as its weakest element. Popular elements of such system include hardware, software, network and people. Current approaches to computer security problems usually exclude people in their studies {{even though it is}} an integral part of these systems. To fill that gap, this paper discusses crucial people-related problems in computer security and proposes a method of improving security in such systems by integrating people tightly into the whole system. The integration is implemented via visualization to provide <b>visual</b> <b>feedbacks</b> and capture people's awareness of their actions and consequent results. By doing it, we can improve system usability, shorten user's learning curve, and hence enable user uses computer systems more securely...|$|R
40|$|Stereo {{vision is}} one {{critical}} tool in {{minimally invasive surgery}} (MIS) for enhancing perception of depth of organs which greatly improves the operation efficiency. Augmented stereo views results from superimposing 3 D anatomical models with real organ views. A reliable distributed framework for relaying stereoscopic <b>visual</b> <b>feedbacks</b> between a telerobotic server and a client station is proposed. The distributed components are based on DirectX, Visual C#, and Window sockets. We used a multi-threaded execution to promote concurrency in grabbing, transmitting, receiving, processing, and displaying image data using head-mounted display (HMD) technology. The client station provides components that support augmented reality (AR), i. e. superimposing animated graphic model with real views from the operating site. Design and performance issues of proposed multi-threaded execution for streaming of stereo data in a distributed and modular framework is presented...|$|R
40|$|Haptic {{information}} in robotic surgery can significantly improve clinical outcomes and help detect hard soft-tissue inclusions that indicate potential abnormalities. Visual representation of tissue stiffness information is a cost-effective technique. Meanwhile, direct force feedback, although considerably {{more expensive than}} visual representation, is an intuitive method of conveying information regarding tissue stiffness to surgeons. In this study, real-time <b>visual</b> stiffness <b>feedback</b> by sliding indentation palpation is proposed, validated, and compared with force feedback involving human subjects. In an experimental tele-manipulation environment, a dynamically updated color map depicting the stiffness of probed soft tissue is presented via a graphical interface. The force feedback is provided, aided by a master haptic device. The haptic device uses data acquired from an F/T sensor attached to the end-effector of a tele-manipulated robot. Hard nodule detection performance is evaluated for 2 modes (force <b>feedback</b> and <b>visual</b> stiffness <b>feedback)</b> of stiffness feedback on an artificial organ containing buried stiff nodules. From this artificial organ, a virtual-environment tissue model is generated based on sliding indentation measurements. Employing this virtual-environment tissue model, we compare the performance of human participants in distinguishing differently sized hard nodules by force <b>feedback</b> and <b>visual</b> stiffness <b>feedback.</b> Results indicate that the proposed distributed visual representation of tissue stiffness can be used effectively for hard nodule identification. The representation {{can also be used}} as a sufficient substitute for force feedback in tissue palpation...|$|R
2500|$|A {{video game}} is an {{electronic}} game that involves interaction with a user interface to generate <b>visual</b> <b>feedback</b> {{on a video}} device. In 1948, ten years before William Higinbotham's Tennis for Two was developed, Thomas T. Goldsmith Jr. and Estle R. Mann co-patented the [...] "Cathode-Ray Tube Amusement Device," [...] making it the earliest documented video game. Primitive by modern standards in video gaming, the amusement device, however, required players to overlay pictures or illustrations of targets such as airplanes {{in front of the}} screen, dovetailing the game's action.|$|E
2500|$|An {{improved}} {{version is}} currently in development at Loughborough University, which will add <b>visual</b> <b>feedback</b> by showing {{a representation of the}} bowler projected onto a screen. [...] The ball will be released as the virtual bowler’s hand reaches the hole from which the ball is released. [...] This version of the machine is intended for mass production, so it can be sold to cricket clubs around the world. In the winter of 2009/10 the ECB acquired 20 of the machines, one for each county and two for the performance centre at Loughborough.|$|E
2500|$|Several {{laboratories}} {{have managed}} to record signals from monkey and rat cerebral cortices to operate BCIs to produce movement. Monkeys have navigated computer cursors on screen and commanded robotic arms to perform simple tasks simply by thinking about the task and seeing the <b>visual</b> <b>feedback,</b> but without any motor output. In May 2008 photographs that showed a monkey at the University of Pittsburgh Medical Center operating a robotic arm by thinking were published {{in a number of}} well-known science journals and magazines. Other research on cats has decoded their neural visual signals.|$|E
40|$|Sketching, as an {{intuitive}} tool for creation and edition of 3 D prototypes, {{is a topic}} of increasing interest to the community. These approaches {{are based on the}} natural ability of humans to quickly draw in 2 D some characteristic curves of 3 D objects. Unfortunately, some 3 D modeling operations- like positioning different components- and the modeling in front of large displays have still not reached the same level of intuitively in sketching systems. The main difficulty is to leverage the intuitive 2 D gesture abilities of humans and lift them to 3 D operations. We present a new approach, based on a virtual 3 D paper sheet metaphor and the use of a 6 degrees of freedom (DOF) device. With the associated interaction processes and <b>visual</b> <b>feedbacks,</b> it allows the user to quickly create and edit some sketched 3 D models...|$|R
40|$|Mandibular angle {{reduction}} {{is a popular}} and efficient procedure widely used to alter the facial contour. The primary surgical instruments, the reciprocating saw, and the round burr, employed in the surgery have a common feature: operating at a high speed. Generally, inexperienced surgeons need a long-time practice {{to learn how to}} minimize the risks caused by the uncontrolled contacts and cutting motions in manipulation of instruments with high-speed reciprocation or rotation. A virtual reality-based surgical simulator for the mandibular angle reduction was designed and implemented on a compute unified device architecture (CUDA) -based platform in this paper. High-fidelity <b>visual</b> and haptic <b>feedbacks</b> are provided to enhance the perception in a realistic virtual surgical environment. The impulse-based haptic models were employed to simulate the contact forces and torques on the instruments. It provides convincing haptic sensation for surgeons to control the instruments under different reciprocation or rotation velocities. The real-time methods for bone removal and reconstruction during surgical procedures have been proposed to support realistic <b>visual</b> <b>feedbacks.</b> The simulated contact forces were verified by comparing against the actual force data measured through the constructed mechanical platform. An empirical study based on the patient-specific data was conducted to evaluate the ability of the proposed system in training surgeons with various experiences. The results confirm the validity of our simulator. © 2012 IEEE. Mandibular angle {{reduction is}} a popular and efficient procedure widely used to alter the facial contour. The primary surgical instruments, the reciprocating saw, and the round burr, employed in the surgery have a common feature: operating at a high speed. Generally, inexperienced surgeons need a long-time practice to learn how to minimize the risks caused by the uncontrolled contacts and cutting motions in manipulation of instruments with high-speed reciprocation or rotation. A virtual reality-based surgical simulator for the mandibular angle reduction was designed and implemented on a compute unified device architecture (CUDA) -based platform in this paper. High-fidelity <b>visual</b> and haptic <b>feedbacks</b> are provided to enhance the perception in a realistic virtual surgical environment. The impulse-based haptic models were employed to simulate the contact forces and torques on the instruments. It provides convincing haptic sensation for surgeons to control the instruments under different reciprocation or rotation velocities. The real-time methods for bone removal and reconstruction during surgical procedures have been proposed to support realistic <b>visual</b> <b>feedbacks.</b> The simulated contact forces were verified by comparing against the actual force data measured through the constructed mechanical platform. An empirical study based on the patient-specific data was conducted to evaluate the ability of the proposed system in training surgeons with various experiences. The results confirm the validity of our simulator. © 2012 IEEE...|$|R
40|$|We {{presented}} the technologies and algorithms {{to build a}} web-based visualization and steering system to monitor the dynamics of remote parallel simulations executed on a Linux Cluster. The polynomial time based algorithm to optimally utilize distributed computing resources over a network to achieve maximum frame-rate was also proposed. Keeping up with the advancements in modern web technologies, we have developed an Ajax-based web frontend which allows users to remotely access and control ongoing computations via a web browser facilitated by <b>visual</b> <b>feedbacks</b> in real-time. Experimental results are also given from sample runs mapped to distributed computing nodes and initiated by users at different geographical locations. Our preliminary results on frame-rates illustrated that system performance was affected by network conditions of the chosen mapping loop including available network bandwidth and computing capacities. The underlying programming framework of our system supports mixed-programming mode and is flexible to integrate most serial or parallel simulation code written in different programming languages such as Fortran, C and Java...|$|R
2500|$|One study {{describes}} {{the use of}} electropalatography (EPG) to treat a patient with severe acquired apraxia of speech. [...] EPG is a computer-based tool for assessment and treatment of speech motor issues. [...] The program allows patients to see the placement of articulators during speech production thus aiding them in attempting to correct errors. Originally {{after two years of}} speech therapy, the patient exhibited speech motor and production problems including problems with phonation, articulation, and resonance. [...] This study showed that EPG therapy gave the patient valuable <b>visual</b> <b>feedback</b> to clarify speech movements that had been difficult for the patient to complete when given only auditory feedback.|$|E
2500|$|Apple's {{lawsuits}} with Samsung in the Netherlands and HTC in the United Kingdom both led to failure: both courts {{ruled the}} patents to be invalid, citing the similar lock screen on the N1, a mobile phone manufactured by the Swedish company Neonode, as prior art for Apple's design. The British court specifically ruled that Apple's lock screen was an [...] "obvious improvement" [...] over {{that of the}} Neonode N1 due to its additional <b>visual</b> <b>feedback</b> through an on-screen slider graphic (unlike the N1, which only displayed a written instruction explaining the gesture). Early work on touchscreen technology from the University of Maryland Human – Computer Interaction Lab was also cited as prior art, in particular a 1991 touchscreen slider developed by Catherine Plaisant ...|$|E
2500|$|The {{most direct}} of related disorders, {{deafferentation}} occurs when sensory {{input from the}} body is reduced or absent, without affecting efferent, or motor, neurons. The most famous case of this disorder is [...] "IW", who lost all sensory input from below the neck, resulting in temporary paralysis. He was forced to learn to control his movement all over again using only his conscious body image and <b>visual</b> <b>feedback.</b> As a result, when constant visual input is lost during an activity, such as walking, it becomes {{impossible for him to}} complete the task, which may result in falling, or simply stopping. IW requires constant attention to tasks to be able to complete them accurately, demonstrating how automatic and subconscious the process of integrating touch and proprioception into the body schema actually is.|$|E
40|$|International audienceVirtual reality {{technologies}} have been experimented {{for several years}} for post-stroke motor rehabilitation, but there is too little diffusion of these systems among medical facilities and none among patients. Our objective {{is the development of}} an interactive system to assist motor rehabilitation of the upper limb after a stroke, which retains the medical benefits of traditional post-stroke methods while reducing human costs (usable with minimal supervision) and materials (general public), and facilitating active patient participation. System architecture, 3 D interactions and virtual content are based on an iterative, user-centered design methodology with patients and therapists. The system allows users to perform repetitive and intensive tasks with the upper limb. The paretic hand is tracked with a low-cost depth sensor. Kinematic performance is monitored and <b>visual</b> <b>feedbacks</b> are proposed. Preliminary tests were conducted on a non-immersive prototype, with eight patients and a target pointing task. The results showed good usability and high acceptance from the users...|$|R
40|$|Haptic {{guidance}} by a {{force feedback}} device is a technology which provides additional proprioceptive cues during visuo-motor learning tasks. The effects of {{two types of}} haptic guidance - control in position (HGP) or in force (HGF) – on visuo-manual tracking (“following”) of trajectories are still under debate. Three training techniques of haptic guidance (HGP, HGF or NHG control condition without haptic guidance) were evaluated. Movements produced by adults were assessed in terms of shapes (dynamic time warping) and kinematics criteria (number of velocity peaks and mean velocity) {{before and after the}} training sessions. Trajectories consisted of two Arabic and two Japanese-inspired letters. Results revealed both types of haptic guidance do not influence the shape quality, mainly guided by <b>visual</b> <b>feedbacks.</b> Moreover, the use of HGF globally improves the fluidity of the four movements while no significant improvement was found for HGP or NHG [...] These results suggest that learned information for this specific motor activity could be stored as internal inverse model andencoded in force coordinates...|$|R
40|$|In {{this paper}} we present an {{innovative}} haptic device that combines the electro-tactile stimulation {{with the force}} and <b>visual</b> <b>feedbacks</b> {{in order to improve}} the perception of a virtual world. We discuss about the sensation evoked in a user by the haptic, force, and visual interface provided by this device, implemented as a special glove, equipped with sensors and actua- tors connected to a PC. The techniques used to recreate tactile and kines- thetic sensations are based on an innovative use of cutaneous stimulation integrated with actuators and 3 D modelling techniques. We discuss about the specificity of haptic interfaces, their controllers, their open problems. We present results about generating the sensation of touching virtual ob- jects with our device. Experiments show also that, using a multi-modal sensorial pattern of stimulation, the subject perceives more realistically the virtual object. We discuss about possible use of the same technique as a way to interface intelligent robots...|$|R
