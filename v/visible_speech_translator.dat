0|178|Public
40|$|Purpose This study {{investigated}} whether {{and to what}} extent iconic co-speech gestures contribute to information from <b>visible</b> <b>speech</b> to enhance degraded speech comprehension at different levels of noise-vocoding. Previous studies of the contributions of these 2 visual articulators to speech comprehension have only been performed separately. Method Twenty participants watched videos of an actress uttering an action verb and completed a free-recall task. The videos were presented in 3 speech conditions (2 -band noise-vocoding, 6 -band noise-vocoding, clear), 3 multimodal conditions (speech + lips blurred, <b>speech</b> + <b>visible</b> <b>speech,</b> <b>speech</b> + <b>visible</b> <b>speech</b> + gesture), and 2 visual-only conditions (<b>visible</b> <b>speech,</b> <b>visible</b> <b>speech</b> + gesture). Results Accuracy levels were higher when both visual articulators were present compared with 1 or none. The enhancement effects of (a) <b>visible</b> <b>speech,</b> (b) gestural information on top of <b>visible</b> <b>speech,</b> and (c) both <b>visible</b> <b>speech</b> and iconic gestures were larger in 6 -band than 2 -band noise-vocoding or visual-only conditions. Gestural enhancement in 2 -band noise-vocoding did not differ from gestural enhancement in visual-only conditions...|$|R
40|$|We have {{implemented}} a facial animation system {{to carry out}} <b>visible</b> <b>speech</b> synthesis. Using this system, {{it is possible to}} manipulate control parameters to synthesize a sequence of speech articulations. In addition, it is possible to synthesize novel articulations, such as one that is half way between /ha! and Ida!. Given the importance ofvisible information in face-toface communication, <b>visible</b> <b>speech</b> synthesis is being developed to control and manipulate <b>visible</b> <b>speech.</b> Experiments have shown that this <b>visible</b> <b>speech</b> is particularly important when the auditory speech is degraded, because ofnoise, bane:width filtering, or hearing impairment (Massaro, 1987). The strong influence of <b>visible</b> <b>speech</b> is not limited to situations with degraded auditory input, however; it occurs even when <b>visible</b> <b>speech</b> is paired with perfectly intelligible speech sounds. The influence of <b>visible</b> <b>speech</b> is easily experienced in a demonstration o...|$|R
50|$|In 2015, was {{developed}} Blabber Messenger - <b>speech</b> <b>translator</b> for 23 languages.|$|R
40|$|Purpose: This study {{investigated}} whether {{and to what}} extent iconic co-speech gestures contribute to information from <b>visible</b> <b>speech</b> to enhance degraded speech comprehension at different levels of noise-vocoding. Previous studies of the contributions of these 2 visual articulators to speech comprehension have only been performed separately. Method: Twenty participants watched videos of an actress uttering an action verb and completed a free-recall task. The videos were presented in 3 speech conditions (2 -band noise-vocoding, 6 -band noise-vocoding, clear), 3 multimodal conditions (speech + lips blurred, <b>speech</b> + <b>visible</b> <b>speech,</b> <b>speech</b> + <b>visible</b> <b>speech</b> + gesture), and 2 visual-only conditions (<b>visible</b> <b>speech,</b> <b>visible</b> <b>speech</b> + gesture). Results: Accuracy levels were higher when both visual articulators were present compared with 1 or none. The enhancement effects of (a) <b>visible</b> <b>speech,</b> (b) gestural information on top of <b>visible</b> <b>speech,</b> and (c) both <b>visible</b> <b>speech</b> and iconic gestures were larger in 6 -band than 2 -band noise-vocoding or visual-only conditions. Gestural enhancement in 2 -band noise-vocoding did not differ from gestural enhancement in visual-only conditions. Conclusions: When perceiving degraded speech in a visual context, listeners benefit more from having both visual articulators present compared with 1. This benefit was larger at 6 -band than 2 -band noise-vocoding, where listeners can benefit from both phonological cues from <b>visible</b> <b>speech</b> and semantic cues from iconic gestures to disambiguate speech...|$|R
40|$|OBJECTIVE: To test {{a simple}} method for {{improving}} consistency among raters for the perceptual evaluation of pathological voice quality by providing <b>visible</b> <b>speech</b> (spectrogram) as additional information because, to date, the interrater variability still limits the widespread clinical {{use of the}} best available rating system. DESIGN: Experimental comparison between 2 different ways (with and without the addition of <b>visible</b> <b>speech)</b> of perceptual rating by trained professionals of recorded pathological voices. Furthermore, the correlation between acoustical (jitter, shimmer, and noise-harmonic ratio) and perceptual parameters was investigated in both rating conditions. SUBJECTS: Six experts evaluated 70 recorded pathological voices using the GIRBAS (grade, instability, roughness, breathiness, asthenicity, and strain) scale in 2 separate sessions: first, conventionally, without <b>visible</b> <b>speech</b> as additional information, and several months later, with <b>visible</b> <b>speech</b> as additional information. MAIN OUTCOME MEASURES: The kappa interrater agreement and the correlation coefficient between GIRBAS scores and acoustic measures. RESULTS: We found a significant effect of <b>visible</b> <b>speech</b> on the agreement between the raters. The interrater agreement according to kappa statistics was significantly stronger {{with the addition of}} <b>visible</b> <b>speech</b> than without for rating grade, roughness, and breathiness. The correlation between acoustical and perceptual parameters showed no significant effect of <b>visible</b> <b>speech.</b> CONCLUSIONS: The addition of <b>visible</b> <b>speech</b> to the perceptual evaluation of pathological voices is an interesting clinical asset to enhance its reliability. The addition of <b>visible</b> <b>speech</b> to the clinical setting is feasible, since affordable computer programs are currently available that can provide the spectrogram in quasi-real time while conversing with the patient. The acoustical analysis might be applied in addition to perceptual rating in a multidimensional approach to assess voice quality. status: publishe...|$|R
40|$|Learning {{the sounds}} of a foreign {{language}} is difficult. The experiments reported here investigated whether <b>visible</b> <b>speech</b> can help. The focus of the experiments was on whether <b>visible</b> <b>speech</b> affects perception as well as the production of foreign speech sounds. The first experiment examined whether <b>visible</b> <b>speech</b> assists in the detection of a syllable within an unfamiliar foreign phrase. It was found that a syllable was more likely to be detected within a phrase when the participants could see the speaker’s face. The second experiment investigated whether judgments about the duration of a foreign language phrase would be more accurate with <b>visible</b> <b>speech</b> compared to a sound only condition. It was found that in the <b>visible</b> <b>speech</b> condition participant’s estimates of phrase duration correlated positively with actual duration, whereas in the sound only condition there was a negative correlation. Furthermore, with <b>visible</b> <b>speech,</b> estimates were close to the actual durations whereas those in the sound only condition tended to underestimate duration. The results are discussed with respect to previous findings and future applications. 1...|$|R
40|$|Abstract—We {{present a}} novel {{approach}} to synthesizing accurate <b>visible</b> <b>speech</b> based on searching and concatenating optimal variable-length units in a large corpus of motion capture data. Based {{on a set of}} visual prototypes selected on a source face and a corresponding set designated for a target face, we propose a machine learning technique to automatically map the facial motions observed on the source face to the target face. In order to model the long distance coarticulation effects in <b>visible</b> <b>speech,</b> a large-scale corpus that covers the most common syllables in English was collected, annotated and analyzed. For any input text, a search algorithm to locate the optimal sequences of concatenated units for synthesis is desrcribed. A new algorithm to adapt lip motions from a generic 3 D face model to a specific 3 D face model is also proposed. A complete, end-to-end <b>visible</b> <b>speech</b> animation system is implemented based on the approach. This system is currently used in more than 60 kindergarten through third grade classrooms to teach students to read using a lifelike conversational animated agent. To evaluate the quality of the <b>visible</b> <b>speech</b> produced by the animation system, both subjective evaluation and objective evaluation are conducted. The evaluation results show that the proposed approach is accurate and powerful for <b>visible</b> <b>speech</b> synthesis. Index Terms—Face animation, character animation, visual <b>speech,</b> <b>visible</b> <b>speech,</b> coarticulation effect, virtual human. æ...|$|R
5000|$|A Popular Manual of <b>Visible</b> <b>Speech</b> and Vocal Physiology (1889) ...|$|R
40|$|Grammatical Framework (GF) is a grammar {{formalism}} {{which supports}} interlingua-based translation, library-based grammar engineering, and compilation to speech recognition grammars. We show how these features {{can be used}} in the construction of portable high-precision domain-specific <b>speech</b> <b>translators...</b>|$|R
40|$|We {{present a}} {{multimodal}} interactive data exploration tool that fa-cilitates discrimination between <b>visible</b> <b>speech</b> tokens. The multi-modal tool uses visualization and sonification (non-speech sound) of data. <b>Visible</b> <b>speech</b> tokens is {{a class of}} multidimensional data {{that have been used}} extensively in designing talking head that has been used in training of deaf individuals by watching <b>speech</b> [1]. <b>Visible</b> <b>speech</b> tokens (consonants), referred to as categories, dif-fer along a set of pre-measured feature dimensions such as mouth height, mouth narrowing, jaw rotation and upper-lip retraction. The data set was visualized with a series of 1 D scatter-plots that differed in color for each category. Sonification was performed by mapping three qualities of the data (within-category variabil...|$|R
5000|$|<b>Visible</b> <b>Speech</b> (a phonetic script) — no {{specific}} language; developed {{to aid the}} deaf and teach them to speak properly ...|$|R
40|$|Baldi, a computer-animated talking head is introduced. The {{quality of}} his <b>visible</b> <b>speech</b> has been {{repeatedly}} modified and evaluated to accurately simulate naturally talking humans. Baldi's <b>visible</b> <b>speech</b> can be appropriately aligned with either synthesized or natural auditory speech. Baldi has had great success in teaching vocabulary and grammar to children with language challenges and training speech distinctions to children with hearing loss and to adults learning a new language. We demonstrate these learning programs and also demonstrate several other potential application areas for Baldi...|$|R
50|$|Additionally he {{was also}} the creator of <b>Visible</b> <b>Speech</b> which was used to help the deaf learn to talk, and was the father of Alexander Graham Bell.|$|R
30|$|Speech {{synthesis}} (SS) is {{the artificial}} production of human speech, {{which is the}} core part of text-to-speech (TTS) systems that convert text content in a specific language into a speech waveform [1]. SS {{can be used in}} several applications, including educational applications, <b>speech</b> to <b>speech</b> <b>translators,</b> and applications for telecommunications and multimedia.|$|R
50|$|Other alphabets, such as Hangul, {{may have}} their own phonetic extensions. There also exist {{featural}} phonetic transcription systems, such as Alexander Melville Bell's <b>Visible</b> <b>Speech</b> and its derivatives.|$|R
50|$|Massaro, DW. and Light, J. Using <b>visible</b> <b>speech</b> for {{training}} perception {{and production of}} speech for hard of hearing individuals. Journal of Speech, Language, and Hearing Research, 2004, 47(2), 304-320.|$|R
50|$|Modern {{phonetics}} {{begins with}} attempts—such {{as those of}} Joshua Steele (in Prosodia Rationalis, 1779) and Alexander Melville Bell (in <b>Visible</b> <b>Speech,</b> 1867)—to introduce systems of precise notation for speech sounds.|$|R
5000|$|... #Caption: Fig. 1. Bell learned {{acoustics}} {{from his}} father, Alexander Melville Bell, who created diagrams {{of how the}} human mouth formed consonants and vowels for his book on <b>Visible</b> <b>Speech.</b>|$|R
40|$|Globalization and the {{continued}} increase in international travel and commerce have made automatic speech-to-speech translation systems an attractive area of research and development. With handheld devices becoming more powerful, the idea of <b>speech</b> to <b>speech</b> <b>translators</b> on PDAs is becoming a practical proposition. New Zealand welcomes a number of tourists {{from all over the}} world and tourism is a very important industry for New Zealand. New Zealand, a wealthy Pacific nation is dominated by two cultural groups New Zealanders of European descent, and the minority Maori, but Migration patterns have changed, with most incomers coming from Asia and Pacific island states, rather than from the UK and Australia. Officials estimate that Asians will make up 13 % of the population by 2021. Developing a handheld <b>speech</b> to <b>speech</b> <b>translator</b> is therefore not only of economic significance but also of considerable social and cultural importance...|$|R
50|$|In 1924, Steiner gave two {{intensive}} {{workshops on}} {{different aspects of}} eurythmy; transcripts of his talks during these workshops are published as Eurythmy as <b>Visible</b> <b>Speech</b> and Eurythmy as Visible Singing.|$|R
40|$|Gestures and <b>visible</b> <b>speech</b> cues {{are often}} {{available}} to listeners to aid their {{comprehension of the}} speaker’s meaning. However, visual communication cues are not always beneficial over-and-above the audible speech cues. My goal is to outline several types of constraints which operate in the human cognitive processing system that bear on this question: When do visual language cues (<b>visible</b> <b>speech</b> and gestures) provide an aid to comprehension, and when do they not? Research on visual-spoken language comprehension carried out in my lab over recent years is described and recommendations will be made concerning the design of multi-modal interfaces...|$|R
50|$|Text In {{her first}} solo exhibition, Auerbach showed {{a series of}} text-based {{drawings}} that explored various linguistic systems including calligraphy, Morse code, semaphore signals, the Ugaritic alphabet and Alexander Melville Bell's <b>visible</b> <b>speech.</b>|$|R
50|$|Rogers {{served as}} the {{director}} at the Clarke School from 1867 to 1886. She worked at the school with Alexander Graham Bell, who implemented his father’s <b>Visible</b> <b>Speech</b> System to teach instructors in the oral method of teaching.|$|R
2500|$|Chief George Henry Martin Johnson (Onwanonsyshon) of the {{aboriginal}} Six Nations Mohawk Reserve, near Bell's home in Brantford, Ontario awarded him {{the title of}} Honorary Chief {{for his work in}} translating the unwritten Mohawk language into <b>Visible</b> <b>Speech</b> symbols (c. 1870); ...|$|R
5000|$|Melville's {{works on}} <b>Visible</b> <b>Speech</b> became highly notable, and were {{described}} by Édouard Séguin as being [...] "...a greater invention than the telephone by his son, Alexander Graham Bell". Melville saw numerous applications for his invention, including its worldwide {{use as a}} universal language. However, although heavily promoted at the Second International Congress on Education of the Deaf in Milan, Italy in 1880, {{after a period of}} a dozen years or so in which it was applied to the education of the deaf, <b>Visible</b> <b>Speech</b> was found to be more cumbersome, and thus a hindrance, to the teaching of speech to the deaf, compared to other methods, and eventually faded from use.|$|R
40|$|For speech {{perception}} {{and production of}} a new language, we examined whether 1) {{they would be more}} easily learned by ear and eye relative to by ear alone, and 2) whether viewing the tongue, palate, and velum during production is more beneficial for learning than a standard frontal view of the speaker. In addition, we determine whether differences in learning under these conditions are due to enhanced receptive learning from additional visual information, or to more active learning motivated by the visual presentations. Test stimuli were two similar vowels in Mandarin and two similar stop consonants in Arabic, presented in different word contexts. Participants were tested with auditory speech and were either trained 1) unimodally with just auditory speech or bimodally with both auditory and visual speech, and 2) a standard frontal view versus an inside view of the vocal tract. The visual speech was generated by the appropriate multilingual versions of Baldi [1]. The results test the effectiveness of <b>visible</b> <b>speech</b> for learning a new language. Preliminary results indicate that <b>visible</b> <b>speech</b> can contribute positively to acquiring new speech distinctions and promoting active learning. Index Terms: <b>visible</b> <b>speech</b> synthesis, pronunciation trainin...|$|R
40|$|The paper {{describes}} the Spoken Language Translator (SLT) system, a prototype automatic <b>speech</b> <b>translator.</b> SLT is currently capable of translating spoken English queries {{in the domain}} of air travel planning into either Swedish or French, using a vocabulary of about 1200 words. We present an overview of the system's architecture, concentrating on how rationally con-structed balanced corpora are used to allow rapid development of high-quality limited-domain translation systems. ...|$|R
40|$|We {{present an}} {{overview}} of the development environment for Regulus, an Open Source platform for construction of grammar-based speech-enabled systems, focussing on recent work whose goal has been to introduce uniformity between text and speech views of Regulus-based applications. We argue the advantages of being able to switch quickly between text and speech modalities in interactive and offline testing, and describe how the new functionalities enable rapid prototyping of spoken dialogue systems and <b>speech</b> <b>translators.</b> ...|$|R
40|$|BabelDr ([URL] is a {{joint project}} of Geneva’s Faculty of Translation and Interpretation (FTI) and University Hospitals (HUG), that has been active since July 2015. The goal is to develop methods that allow rapid {{prototyping}} of medium-vocabulary web-enabled medical <b>speech</b> <b>translators,</b> with particular emphasis on languages spoken by victims of the current European refugee crisis. A demonstrator system freely available on the project site translates spoken French medical examination questions into four languages...|$|R
40|$|We present REGULUS, an Open Source {{environment}} which compiles typed unification grammars into context free grammar language models {{compatible with}} the Nuance Toolkit. The environ- ment includes a large general unification grammar of English and corpus-based tools for creating efficient domainspecific recognisers from it. We will demo applications built using the system, including a <b>speech</b> <b>translator</b> and a command and control system for a simulated robotic domain, and show how the development environment {{can be used to}} edit and extend them...|$|R
40|$|In {{this paper}} we {{describe}} how the translation methodology adopted for the Spoken Language Translator (SLT) ad-dresses {{the characteristics of}} the speech translation task in a context where it is essential to achieve easy customization to new languages and new domains. We then discuss the issues that arise in any attempt to evaluate a <b>speech</b> <b>translator,</b> and present the results of such an evalu-ation carried out on SLT for several lan-guage pairs. 1 The nature of the speech translation tas...|$|R
2500|$|The Massachusetts Charitable Mechanic Association (a.k.a. the Association of the Mechanics of Boston) awarded two {{gold medals}} to Bell, as {{exhibitor}} #626 registered to the New England Telephone Company of Boston, MA, {{for both the}} telephone and <b>Visible</b> <b>Speech,</b> twinning {{the results of the}} Centennial Exposition held in Philadelphia two years earlier (1878); ...|$|R
40|$|Language {{processing}} {{is influenced}} by multiple sources of information. We examined whether the performance in simultaneous interpreting would be improved when providing two sources of information, the auditory speech as well as corresponding lip-movements, in comparison to presenting the auditory speech alone. Although there was an improvement in sentence recognition when presented with <b>visible</b> <b>speech,</b> {{there was no difference}} in performance between these two presentation conditions when bilinguals simultaneously interpreted from English to German or from English to Spanish. The reason why visual speech did not contribute to performance could be the presentation of the auditory signal without noise (Massaro, 1998). This hypothesis should be tested in the future. Furthermore, it should be investigated if an effect of <b>visible</b> <b>speech</b> can be found for other contexts, when visual information could provide cues for emotions, prosody, or syntax...|$|R
40|$|Proceedings of the 9 th International Conference on Auditory Display (ICAD), Boston, MA, July 7 - 9, 2003. We {{present a}} {{multimodal}} interactive data exploration tool that facilitates discrimination between <b>visible</b> <b>speech</b> tokens. The multimodal tool uses visualization and sonification (non-speech sound) of data. <b>Visible</b> <b>speech</b> tokens is {{a class of}} multidimensional data {{that have been used}} extensively in designing talking head that has been used in training of deaf individuals by watching <b>speech</b> [1]. <b>Visible</b> <b>speech</b> tokens (consonants), referred to as categories, differ along a set of pre-measured feature dimensions such as mouth height, mouth narrowing, jaw rotation and upper-lip retraction. The data set was visualized with a series of 1 D scatter-plots that differed in color for each category. Sonification was performed by mapping three qualities of the data (within-category variability, between category variability, and category identity) to three sound parameters (noise amplitude, duration, and pitch). An experiment was conducted to assess the utility of multimodal information compared to visual information alone for exploring this multidimensional data set. Tasks involved answering a series of questions to determine how well each feature or a set of features discriminate among categories, which categories are discriminated and how many. Performance was assessed by measuring accuracy and reaction time to 36 questions varying in scale of understanding and level of dimension integrality. Scale varied at three levels (ratio, ordinal, and nominal) and integrality also varied at three levels (1, 2, and 3 dimensions). A between-subjects design was used by assigning subjects to either the multimodal group or visual only group. Results show that accuracy is better for the multimodal group as the number of dimensions required to answer a question (integrality) increased. Also, accuracy was 10 % better for the multimodal group for ordinal questions. For discriminating <b>visible</b> <b>speech</b> tokens, sonification provides useful information in addition to that given by visualization, particularly for representing three dimensions simultaneously...|$|R
40|$|We {{describe}} {{attempts to}} synthesize <b>visible</b> <b>speech</b> in real-time on a Macintosh^TM personal computer, and {{to enable the}} user to color {{the text of the}} speech to be synthesized emotionally, according to the user's wishes, the representation of the text, or the semantics of the utterance. The animated <b>visible</b> <b>speech</b> will be demonstrated, in real time, using a variety of on-screen agents and faces. The speech synthesizer used is Apple Computer's MacinTalkPro 2 ®, running on a Quadra AV® computer. Introduction Researchers in disciplines as diverse as ethology, psychology, speech therapy, interpersonal communications and human-machine interface design agree that facial expressions enhance communication. Facial expressions convey both emotion (Ekman and Friesen, 1986) and other communicative signals (Chovil, 1991). A multimodal form of interaction with a computer on-screen agent is likely {{to improve the quality of}} the dialogue, both in terms of intelligibility and with regard to user-friendline [...] ...|$|R
5000|$|Data converters: {{language}} <b>translators,</b> <b>speech</b> processing, URL shorteners...|$|R
