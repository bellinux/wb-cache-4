1071|1225|Public
5|$|A {{helmet-mounted sight}} (HMS) {{developed}} by Luoyang Electro-Optics Technology Development Centre of AVIC {{was developed in}} parallel with the JF-17; it was first tested on Prototype 04 in 2006. It was dubbed as EO HMS, (Electro-Optical Helmet Mounted Sight) and was first revealed {{to the public in}} 2008 at the 7th Zhuhai Airshow, where a partial mock-up was on display. The HMS tracks the pilot's head and eye movements to guide missiles towards the pilot's <b>visual</b> <b>target.</b> An externally carried day/night laser designator targeting pod may be integrated with the avionics to guide laser-guided bombs (LGBs). An extra hardpoint may be added under the starboard air intake, opposite the cannon, for such pods. To reduce the numbers of targeting pods required, the aircraft's tactical data link can transmit target data to other aircraft not equipped with targeting pods. The communication systems comprise two VHF/UHF radios; the VHF radio has the capacity for data linking for communication with ground control centres, airborne early warning and control aircraft and combat aircraft with compatible data links for network-centric warfare, and improved situation awareness.|$|E
25|$|Cogan occulomotor apraxia {{is a rare}} {{disorder}} of development. Affected children have difficulty moving their eyes only to a new <b>visual</b> <b>target,</b> so they will turn their head past the target to “drag” the eyes to the new object of interest, then turn the head back. This tendency becomes evident in late infancy and toddler years, and mostly improves with time. This contrasts to the oculomotor difficulties evident in children with A-T, which are not evident in early childhood but emerge over time. Cogan’s oculomotor apraxia is generally an isolated problem, or {{may be associated with}} broader developmental delay.|$|E
500|$|The F-4N (updated F-4Bs) with {{smokeless}} {{engines and}} F-4J aerodynamic improvements started in 1972 under a U.S. Navy-initiated refurbishment program called [...] "Project Bee Line" [...] with 228 converted by 1978. The F-4S model {{resulted from the}} refurbishment of 265 F-4Js with J79-GE-17 smokeless engines of 17,900lbf (79.379kN), AWG-10B radar with digitized circuitry for improved performance and reliability, Honeywell AN/AVG-8 <b>Visual</b> <b>Target</b> Acquisition Set or VTAS (world's first operational Helmet Sighting System), classified avionics improvements, airframe reinforcement and leading edge slats for enhanced maneuvering. The USMC also operated the RF-4B with reconnaissance cameras with 46 built.|$|E
30|$|IVA-CPT {{normally}} balances auditory and <b>visual</b> <b>targets.</b> In this work, {{the test}} was modified by varying the distribution for the auditory and <b>visual</b> <b>targets</b> between the blocks. Therefore, while the classic test uses the same distribution for auditory and <b>visual</b> <b>targets</b> within a single block, we used different distributions to induce different loads during the test.|$|R
40|$|The human {{visual system}} {{is capable of}} {{tracking}} multiple <b>visual</b> <b>targets</b> {{under a variety of}} task constraints and configurations. For nearly two decades, the psychophysical literature has shown that moving, occluded <b>visual</b> <b>targets</b> [...] targets that are momentarily invisible as they pass behind an occluding bar [...] are differentially represented by the visual system compared to their moving, non-occluded counterparts. Here, I sought to examine the neurophysiological basis of this behavioral difference in response to occluded versus non-occluded <b>visual</b> <b>targets.</b> I used brain imaging to conduct modern retinotopic mapping experiments in human participants. Once· their early visual cortices were mapped, I was able characterize the neural representations for both targets and distractors as well as during moments of occlusion and non-occlusion. The results show that, using our method, we can distinguish <b>visual</b> <b>targets</b> from distractors; furthermore, {{there appears to be a}} representation in retinotopically organized early visual cortex for <b>visual</b> <b>targets</b> that have momentarily disappeared from the visual field due to occlusion...|$|R
40|$|Eye {{and head}} {{contributions}} to orienting gaze shifts have been primarily studied using <b>visual</b> <b>targets.</b> Consequently, relatively {{little is known}} about the kinematics of eye and head movements in gaze shifts to acoustic targets. Although early work in nonhuman primates indicates that orienting responses to acoustic and <b>visual</b> <b>targets</b> ar...|$|R
5000|$|... #Subtitle level 2: Smooth pursuit in {{the absence}} of a <b>visual</b> <b>target</b> ...|$|E
5000|$|The JPDAF is one {{of several}} {{techniques}} used for <b>visual</b> <b>target</b> tracking in the field of Computer Vision ...|$|E
50|$|Northrop Grumman {{has tested}} the {{installation}} of a MS-177 camera on an E-8C to provide real time <b>visual</b> <b>target</b> confirmation.|$|E
30|$|Reaction time to <b>visual</b> <b>targets</b> was {{slower than}} to {{auditory}} ones and {{was positively correlated}} to performance on both <b>visual</b> and auditory <b>targets.</b>|$|R
40|$|The {{participants}} {{in the present study}} had to make speeded elevation discrimination responses to <b>visual</b> <b>targets</b> presented to the left or right of central fixation, following the presentation of a task-irrelevant auditory cue on either the same or opposite side. In Experiment 1, when the cues were presented from in front of the participants (from the same azimuthal positions as the <b>visual</b> <b>targets),</b> the standard crossmodal exogenous spatial cuing effect was observed: That is, the participants responded significantly faster in the elevation discrimination task for <b>visual</b> <b>targets</b> when both auditory cues and <b>visual</b> <b>targets</b> were presented on the same side. Experiment 2, in which the auditory cues were either presented from the front or rear, replicated the exogenous spatial cuing effect for frontal <b>visual</b> <b>targets</b> from both front and rear cue-space, with participants responding significantly more rapidly to front targets when cues were presented on the same side, no matter whether the cues were presented from the front or rear. Importantly, the results of Experiment 3 showed that the participants had little difficulty in correctly discriminating the location from which the sounds were presented. Thus, taken together, the results of the three experiments reported here demonstrate that the exact co-location of auditory cues and <b>visual</b> <b>targets</b> is not necessary to attract spatial attention. Implications for the design of real-world warning signals are discussed...|$|R
40|$| {{shown that}} RB can occur even if two <b>visual</b> <b>targets</b> are similar but not|$|R
50|$|The initial {{design was}} based on the Raduga Kh-58 (AS-11 'Kilter'), but it had to be {{abandoned}} because the missile speed was too high for <b>visual</b> <b>target</b> acquisition.|$|E
5000|$|Pre-test: For example, the {{pre-test}} measures the observer's ability {{to point to}} the <b>visual</b> <b>target</b> directly in front of them before prism exposure. This task can be completed with ease and accuracy by normal, healthy individuals.|$|E
5000|$|This was {{the only}} {{daylight}} precision bombing mission in formation by the 330th BG during July. The primary <b>visual</b> <b>target</b> was the Nakajima aircraft plant near Nagoya. When the formation got there it was cloud covered. The formation then traveled to the secondary <b>visual</b> <b>target</b> and it was cloud covered as well. The formations then proceeded {{to the city of}} Tsu, Mie, about 38 mi southwest of Nagoya, the primary radar target. This mission had what the Royal Air Force called a [...] "Master of Ceremonies" [...] when one aircraft took off before the others and was over the various targets to direct traffic. Despite flying to three targets in formation, all aircraft returned safely.|$|E
40|$|Here we {{investigated}} {{the influence of}} angular separation between <b>visual</b> and motor <b>targets</b> on concurrent adaptation to two opposing visuomotor rotations. We inferred the extent of generalisation between opposing visuomotor rotations at individual target locations based on whether interference (negative transfer) was present. Our main finding was that dual adaptation occurred to opposing visuomotor rotations when each was associated with different <b>visual</b> <b>targets</b> but shared a common motor target. Dual adaptation could have been achieved either within a single sensorimotor map (i. e. with different mappings associated with different ranges of visual input), or by forming two different internal models (the selection {{of which would be}} based on contextual information provided by target location). In the present case, the pattern of generalisation was dependent on the relative position of the <b>visual</b> <b>targets</b> associated with each rotation. <b>Visual</b> <b>targets</b> nearest the workspace of the opposing visuomotor rotation exhibited the most interference (i. e. generalisation). When the minimum angular separation between <b>visual</b> <b>targets</b> was increased, the extent of interference was reduced. These results suggest that the separation in the range of sensory inputs is the critical requirement to support dual adaptation within a single sensorimotor mapping. © 2011 Springer-Verlag...|$|R
3000|$|<b>Visual</b> <b>targets</b> {{associated}} with the auditory prime words were selected based on the definitions provided in Chinese Wordnet (Huang et al. 2010) and Ministry of Education Revised Chinese Dictionary (1994). All the visual probes are unambiguous words based on Ministry of Education Revised Chinese Dictionary (1994). For these <b>visual</b> <b>targets,</b> a prime-target semantic association questionnaire was conducted, in which 24 participants (7 males and 17 females, aged between 19 – 23, mean age[*]=[*] 21.05, SD[*]=[*] 1.36) rated the degree of semantic association between the prime and the target words on a seven-point scale (1 [*]=[*]not semantically associated, 7 [*]=[*]highly semantically associated). A lexical decision experiment (20 participants, paid 100 New Taiwan Dollars, Mandarin-speaking undergraduate students at National Taiwan Normal University, 6 males and 14 females, aged between 18 – 22, mean age[*]=[*] 20.45, SD[*]=[*] 1.15) was then conducted {{to ensure that the}} <b>visual</b> <b>targets</b> induced comparable response times when they are presented in isolation. All the pretests were completed by different native Mandarin Chinese speakers in Taiwan. Details about these pretests and the actual experimental stimuli are provided as Additional file 1 (pretest details, auditory prime words and <b>visual</b> <b>targets,</b> and sentential materials in which the prime words were embedded) at the first author’s personal website ([URL] and at Research Gate [URL] [...]...|$|R
40|$|International audienceRecent {{evidence}} suggests that planning a reaching movement entails similar stages and common networks irrespective of whether the target location is defined through visual or proprioceptive cues. Here we test whether the transformations that convert the sensory information regarding target location into the required motor output are common for both types of reaches. To do so, we adaptively modified these sensorimotor transformations through exposure to displacing prisms and hypothesized {{that if they are}} common to both types of reaches, the aftereffects observed for reaches to <b>visual</b> <b>targets</b> would generalize to reaches to a proprioceptive target. Subjects (n = 16) were divided into two groups that differed with respect to the sensory modality of the <b>targets</b> (<b>visual</b> or proprioceptive) used in the pre- and posttests. The adaptation phase was identical for both groups and consisted of movements toward <b>visual</b> <b>targets</b> while wearing 10. 5 ° horizontally displacing prisms. We observed large aftereffects consistent with the magnitude of the prism-induced shift when reaching toward <b>visual</b> <b>targets</b> in the posttest, but no significant aftereffects for movements toward the proprioceptive target. These results provide evidence that distinct, differentially adaptable sensorimotor transformations underlie the planning of reaches to <b>visual</b> and proprioceptive <b>targets...</b>|$|R
5000|$|Potts {{earned his}} Ph.D. in 1994 from the Institute of Cognitive and Decision Sciences at the University of Oregon (Eugene, Oregon). His thesis was titled, [...] "Neural systems of <b>visual</b> <b>target</b> detection: Human event-related {{potential}} studies." [...] He completed a post-doctoral fellowship at Harvard Medical School, where he later became an instructor.|$|E
5000|$|Contextual {{information}} plays a {{large role}} in discourse comprehension, but the issue that many psychologists have been trying to solve is how contextual information is used. Models have been proposed to explain how contextual information is used to decide the appropriate meaning of an ambiguous word such as [...] "cast". The [...] "selective access model" [...] suggests that depending on the context of the sentence determines which meaning of the word [...] "cast" [...] comes to mind (orthopedic cast or cast of characters in a play). The [...] "ordered access model" [...] suggests that the more dominant meaning of the word is the meaning formulated first when dealing with an ambiguous word, so the orthopedic cast would be the one called to mind. Through a series of experiments, Glucksberg found that these models might have produced these outcomes in experiments because of backwards priming, which is when a <b>visual</b> <b>target</b> word influences the initial ambiguous word. For example, the ambiguous word [...] "cast" [...] is heard by a listener and then they see the word [...] "actress". While processing the auditory statement, the <b>visual</b> <b>target</b> is available during the mental representation of the ambiguous word, thus bringing about the [...] "cast of characters" [...] meaning to the word rather than the more dominant one. Glucksberg's solution was to use non-words as the <b>visual</b> <b>target</b> so it would eliminate backward priming. He found that context can constrain lexical access using essentially the same paradigms used by others who did not find such evidence.|$|E
50|$|The Orbital Express Demonstration Manipulator System (OEDMS), {{provided}} by MDA Corp., was the mission’s integrated robotics solution. It consisted primarily of a 6-DOF rotary joint robotic arm, its flight avionics (the Manipulator Control Unit or MCU) and arm vision system, two On-Orbit Replaceable Units (ORUs) and their spacecraft attachment interfaces, a <b>visual</b> <b>target</b> and grapple fixture installed on NEXTSat, and the Manipulator Ground Segment.|$|E
30|$|Previous {{studies have}} shown that the time of {{accurate}} response to <b>visual</b> <b>targets</b> is slower than the time of accurate response to auditory targets [18]. The reaction times to auditory targets are consistent with previous studies [19] but the results of <b>visual</b> <b>targets</b> were 1.5 more than Niruba & Maruthy [19] results. However, both results are consistent with Shelton & Kumar [20] where they found that the reaction time on simple visual stimuli was 331  ms, while the reaction time to auditory stimuli was 284 on a control group of healthy people.|$|R
40|$|Novel assays {{were used}} to assess inter alia whether the {{hippocampus}} is involved in detecting novelty per se or in an associative mismatch process. During training, rats received two audiovisual sequences (tone–left constant light and click– left flashing light). In both sham-operated control rats and those with excitotoxic hippocampal lesions, novel <b>visual</b> <b>targets</b> provoked an orienting response that habituated during training. Moreover, like sham-operated rats, rats with hippocampal lesions acquired associations between the elements of two audiovisual sequences. However, subsequent test trials in which the auditory stimuli preceding the <b>visual</b> <b>targets</b> were switched (click–left constant light and tone–left flashing light) provoked renewed orienting to the <b>visual</b> <b>targets</b> in sham-operated rats but not in hippocampal rats. These results support the view that hippocampal damage results in a failure to detect (or act on) mismatches that are generated when an auditory stimulus associatively evokes the memory of one visual stimulus and a different (familiar) visual stimulus is present in the environment...|$|R
40|$|Cues {{that direct}} {{selective}} {{attention to a}} spatial location have been observed to increase baseline neural activity in visual areas that represent a to-be-attended stimulus location. Analogous attention-related baseline shifts have also been observed in response to attention-directing cues for non-spatial stimulus features. It has been proposed that baseline shifts with preparatory attention may serve as the mechanism by which attention modulates the responses to subsequent <b>visual</b> <b>targets</b> that match the attended location or feature. Using functional MRI, we localized color- and motion-sensitive visual areas in individual subjects and investigated the relationship between cue-induced baseline shifts and the subsequent attentional modulation of task-relevant target stimuli. Although attention-directing cues often led to increased background neural activity in feature specific visual areas, these increases {{were not correlated with}} either behavior in the task or subsequent attentional modulation of the <b>visual</b> <b>targets.</b> These findings cast doubt on the hypothesis that attention-related shifts in baseline neural activity result in selective sensory processing of <b>visual</b> <b>targets</b> during feature-based selective attention...|$|R
50|$|Brightness is an {{attribute}} of visual perception {{in which a}} source appears to be radiating or reflecting light. In other words, brightness is the perception elicited by the luminance of a <b>visual</b> <b>target.</b> It is not necessarily proportional to luminance. This is a subjective attribute/property of an object being observed {{and one of the}} color appearance parameters of color appearance models. Brightness refers to an absolute term and {{should not be confused with}} Lightness.|$|E
50|$|Optic ataxia is the {{inability}} to guide the hand toward an object using visual information where {{the inability}} cannot be explained by motor, somatosensory, visual field deficits or acuity deficits. Optic ataxia is seen in Bálint's syndrome where it is characterized by an impaired visual control of the direction of arm-reaching to a <b>visual</b> <b>target,</b> accompanied by defective hand orientation and grip formation. It is considered a specific visuomotor disorder, independent of visual space misperception.|$|E
50|$|Arrington founded Xtreme Procision (XP) in 2010, a {{state-of-the-art}} football training {{system that will}} develop the world’s next generation of football players. Xtreme Procision offers football training camps nationwide, as well as football training products with <b>visual</b> <b>target</b> zones to aid in accelerating development. Xtreme Procision unveiled the XP Locker {{in the summer of}} 2015, a digital platform used to track the development of athletes through the capturing of video and metric data associated with performing football drills.|$|E
40|$|AbstractDetermining {{the precise}} moment a visual {{stimulus}} appears is difficult because visual response latencies vary. This temporal uncertainty could cause localization errors to brief <b>visual</b> <b>targets</b> presented before and during eye movements if the oculomotor system cannot determine {{the position of the}} eye at the time the stimulus appeared. We investigated the effect of varying neural processing time on localization accuracy for perisaccadic <b>visual</b> <b>targets</b> that differed in luminance. Although systematic errors in localization were observed, the effect of luminance was surprisingly small. We explore several hypotheses that may explain why processing delays are not more disruptive to localization performance...|$|R
40|$|The human {{visual system}} uses saccadic and vergence eye {{movements}} to foveate <b>visual</b> <b>targets.</b> To mimic {{this aspect of}} the biological visual system the PC/BC-DIM neural network is used as an omni-directional basis function network for learning and performing sensory-sensory and sensory-motor transformations without using any hardcoded geometric information. A hierarchical PC/BC-DIM network is used to learn a head-centred representation of <b>visual</b> <b>targets</b> by dividing the whole problem into independent subtasks. The learnt head-centred representation is then used to generate saccade and vergence motor commands. The performance of the proposed system is tested using the iCub humanoid robot simulator...|$|R
40|$|We {{compared}} sensorimotor adaptation in {{the visual}} and the auditory modality. Subjects pointed to <b>visual</b> <b>targets</b> while receiving direct spatial information about fingertip position {{in the visual}} modality, or they pointed to <b>visual</b> <b>targets</b> while receiving indirect information about fingertip position in the visual modality, or they pointed to auditory targets while receiving indirect information about fingertip position in the auditory modality. Feedback was laterally shifted to induce adaptation, and aftereffects were tested with both target modalities and both hands. We found that aftereffects of adaptation were smaller when tested with the non-adapted hand, i. e., intermanual transfer was incomplete. Furthermore, aftereffects were smaller when tested in the non-adapted target modality, i. e., intermodal transfer was incomplete. Aftereffects were smaller following adaptation with indirect rather than direct feedback, {{but they were not}} smaller following adaptation with auditory rather than <b>visual</b> <b>targets.</b> From this we conclude that the magnitude of adaptive recalibration rather depends on the method of feedback delivery (indirect versus direct) than on the modality of feedback (visual versus auditory) ...|$|R
5000|$|Prism Exposure: During prism exposure, {{the initial}} {{attempts}} at {{pointing to the}} target are off- target because the observer’s visual field has been laterally shifted in one direction. The initial pointing errors during prism exposure occur {{in the same direction}} of the visual shift. For example, if the prismatic goggles displace the visual field to the right, the initial pointing errors would occur {{to the right of the}} <b>visual</b> <b>target</b> until a sensory-motor adaptation known as the ‘direct effect of prism adaptation’ occurs.|$|E
50|$|Carpenter's work focusses on {{mechanisms}} of decision. Measurement of saccadic latency, the time taken {{to choose a}} <b>visual</b> <b>target</b> and initiate an eye movement, is a reliable method for obtaining reaction time data. This work has inspired a model referred to as LATER (Linear Approach to Threshold with Ergodic Rate) to explain the decision mechanism. Technological advances enable oculomotor {{measurements to be made}} both quickly and non-invasively, using micro-devices which have many clinical applications. He also has professional interests in vision in general, motor systems, and physiological {{mechanisms of}} consciousness.|$|E
50|$|During gaze shifts, {{for example}} when an object {{appears in the}} periphery, human usually move both their eye and head to capture the object of interest. In experiments, in which {{participants}} needed to shift their gaze to detect a <b>visual</b> <b>target,</b> patients with schizophrenia exhibit abnormal eye-head coordination, and no modulation of saccadic latency (the delay between onset of the stimulus in the periphery {{and the start of}} the gaze shift) occurred, which is usually task dependent in healthy controls as they adjust to different task in terms of saccadic latency.|$|E
40|$|A. In the {{presence}} of <b>visual</b> <b>targets,</b> the fly shows more walks between the stripes than in their absence. B. Median stripe deviation is different in the three groups. Red line denotes the value for random walks. C. Centrophobism during pauses is still present in all three groups. D. Centrophobism while moving is eliminated by narrow stripes. E. Median speed is not significantly affected by <b>visual</b> <b>targets.</b> F. The number of pauses is lower in the wide stripe condition {{as compared to the}} two other conditions. Asterisks denote significant differences after a MANOVA analysis. Bars represent means and error bars standard errors, n =  20 in each group...|$|R
40|$|Determining {{the precise}} moment a visual {{stimulus}} appears is difficult because visual response latencies vary. This temporal uncertainty could cause localization errors to brief <b>visual</b> <b>targets</b> presented before and during eye movements if the oculomotor system cannot determine {{the position of the}} eye at the time the stimulus appeared. We investigated the effect of varying neural processing time on localization accuracy for perisaccadic <b>visual</b> <b>targets</b> that differed in luminance. Although systematic errors in localization were observed, the effect of luminance was surprisingly small. We explore several hypotheses that may explain why processing delays are not more disruptive to localization performance. © 2001 Elsevier Science Ltd. All rights reserved...|$|R
50|$|The U.S. Navy {{employs the}} Low-Cost Modular Target (LCMT), a modular barge made from pontoons, {{scaffolding}} and large colored sails as <b>visual</b> <b>targets,</b> {{which can be}} shot at with guns or a variety of missiles. It is usually towed by a HSMST.|$|R
