36|291|Public
5000|$|However, {{commercial}} pressures {{have led}} some CAs to introduce [...] "domain validated" [...] certificates. Domain validated certificates existed before <b>validation</b> <b>standards,</b> and generally only require some proof of domain control. In particular, domain validated certificates do not assert {{that a given}} legal entity has any relationship with the domain, although the domain may resemble a particular legal entity.|$|E
50|$|SAXS is {{a rapidly}} growing area of {{structure}} determination, both {{as a source of}} approximate 3D structure for initial or difficult cases and as a component of hybrid-method structure determination when combined with NMR, EM, crystallographic, cross-linking, or computational information. There is great interest in the development of reliable <b>validation</b> <b>standards</b> for SAXS data interpretation and for quality of the resulting models, but there are as yet no established methods in general use. Three recent steps in this direction are the creation of a Small-Angle Scattering Validation Task Force committee by the worldwide Protein DataBank and its initial report, a set of suggested standards for data inclusion in publications, and an initial proposal of statistically derived criteria for automated quality evaluation.|$|E
40|$|Analytical methods {{validation}} is {{a mandatory}} step {{to evaluate the}} ability of developed methods to provide accurate results for their routine application. Validation usually involves <b>validation</b> <b>standards</b> or quality control samples that are prepared in placebo or reconstituted matrix made of a mixture of all the ingredients composing the drug product except the active substance or the analyte under investigation. However, {{one of the main}} concerns that can be made with this approach is that it may lack an important source of variability that come from the manufacturing process. The question that remains {{at the end of the}} validation step is about the transferability of the quantitative performance from <b>validation</b> <b>standards</b> to real authentic drug product samples. In this work, this topic is investigated through three case studies. Three analytical methods were validated using the commonly spiked placebo <b>validation</b> <b>standards</b> at several concentration levels as well as using samples coming from authentic batch samples (tablets and syrups). The results showed that, depending on the type of response function used as calibration curve, there were various degrees of differences in the results accuracy obtained with the two types of samples. Nonetheless the use of spiked placebo <b>validation</b> <b>standards</b> was showed to mimic relatively well the quantitative behaviour of the analytical methods with authentic batch samples. Adding these authentic batch samples into the validation design may help the analyst to select and confirm the most fit for purpose calibration curve and thus increase the accuracy and reliability of the results generated by the method in routine application. Peer reviewe...|$|E
30|$|The goal of {{the study}} is to {{investigate}} 18 F-fluorodeoxyglucose positron emission tomography (18 F-FDG-PET)’s ability to delineate the viable portion of a tumor in an animal model using cross-sectional histology as the <b>validation</b> <b>standard.</b>|$|R
25|$|Griffith JF, Freeberg FE: Empirical and {{experimental}} bases for selecting the low volume {{eye irritation test}} as the <b>validation</b> <b>standard</b> for in vitro methods. In: Goldber AM (Ed): In Vitro Toxicology: Approaches to Validation. New York, Mary Ann Libert, 1987, pp.303–311.|$|R
40|$|This paper {{gives an}} {{overview}} of the antenna measurement techniques and facility sharing activity of the Antenna Centre of Excellence (ACE) within the EU's 6 'th framework program for research - in particular, the facility comparison campaign with the DTU-ESA 12 GHz <b>validation</b> <b>standard</b> antenna (VAST 12) ...|$|R
40|$|Abstract: Current Isothermal Titration Calorimetry (ITC) {{data in the}} {{literature}} have relatively high errors in the measured enthalpies of protein-ligand binding reactions. There {{is a need for}} universal <b>validation</b> <b>standards</b> for titration calorimeters. Several inorganic salt co-precipitation and buffer protonation reactions have been suggested as possible enthalpy standards. The performances of several commercial calorimeters, including the VP-ITC, ITC 200, and Nano ITC-III, were validated using these suggested standard reactions...|$|E
40|$|The {{number of}} genome-scale {{metabolic}} models {{has been rising}} quickly in recent years, and the scope of their utilization encompasses {{a broad range of}} applications from metabolic engineering to biological discovery. However the reconstruction of such models remains an arduous process requiring a high level of human intervention. Their utilization is further hampered by the absence of standardized data and annotation formats and the lack of recognized quality and <b>validation</b> <b>standards.</b> |$|E
40|$|This paper {{discusses}} how {{content validity}} {{can be achieved}} {{in the development of}} an appraisal instrument within the hospitality industry. Decision criteria for making employment decisions (e. g., select, promote, dismiss) constitute a "test " which, if challenged in court, must meet <b>validation</b> <b>standards</b> set forth in the Uniform Guidelines for Employee Selection. In the case of a performance rating used to promote or dismiss an employee, the rating must meet the minimal criterion of content validity. Key Words: Content Validity, Tests...|$|E
40|$|The {{analytical}} {{calculation of}} the self-referred lacunarity {{is used as a}} <b>validation</b> <b>standard</b> of the computational algorithm. In this supplementary material to our article (see cond-mat/ 0407079) we present a detailed calculation for two simple shapes, namely a square box and a cross. Comment: 12 pages, 10 figures and 1 Tabl...|$|R
40|$|International audienceWe {{report on}} {{sensitive}} and specific detection and quantification of a template in a molecularly imprinted polymer (MIP) using Raman microspectroscopy. The beta-blocking drug S-propranolol and its enantiomer, R-propranolol, {{were used as}} target molecules since the selectivity of this MIP is well established and serves as an appropriate <b>validation</b> <b>standard.</b> Specific peaks originating in the template were identified in the Raman spectrum, allowing quantification of bound target molecule. We demonstrate that label-free monitoring can be achieved from volumes as small as 1 µm 3 of MIP, based on a single identifying peak...|$|R
40|$|In this paper, the DTU-ESA 12 GHz <b>Validation</b> <b>Standard</b> (VAST 12) Antenna and a {{dedicated}} measurement campaign {{carried out in}} 2007 - 2008 for the definition of its accurate reference pattern are first described. Next, a comparison between {{the results from the}} three involved measurement facilities is presented. Then, an accurate reference pattern of the VAST 12 antenna is formed by averaging the three results taking into account the estimated uncertainties of each result. Finally, the potential use of the reference pattern for benchmarking of antenna measurement facilities is outlined...|$|R
40|$|This note {{deals with}} {{validation}} and verification issues in aircraft noise prediction. We use two different comprehensive simulation software: FLIGHTand PANAM. The comparison {{is done on}} the basis of extensive flight data taken on an Airbus A 319 - 100 operated by Lufthansa. This note aims to contribute to the establishment of rational <b>validation</b> <b>standards</b> as well as realistic accuracy margins on integral noise metrics. The computer codes are briefly described. Results are shown for a variety of microphone positions, located sideways and directly along selected departure and approach flight ground tracks...|$|E
40|$|This paper {{documents}} the GIS interface {{developed for the}} model network and GIS databases, {{the development of the}} travel model and validation results, {{and the use of the}} travel time and speed study data in model development and validation. This model update was a collaborative process between the consultants and city staff. The 1998 Lincoln Model was developed using a new software package, TP+ and VIPER. The 1998 Lincoln Model meets the vast majority of typical <b>validation</b> <b>standards</b> and those set forth by City staff. The results are reasonable in total and for stratification of functional class, area type, and volume grou...|$|E
40|$|While newly {{available}} {{electronic transmission}} methods can increase timeliness and completeness of infectious disease reports, {{limitations of this}} technology may unintentionally compromise detection of, and response to, bioterrorism and other outbreaks. We reviewed implementation experiences for five electronic laboratory systems and identified problems with data transmission, sensitivity, specificity, and user interpretation. The results suggest a need for backup transmission methods, <b>validation,</b> <b>standards,</b> preserving human judgment in the process, and provider and end-user involvement. As illustrated, challenges encountered in deployment of existing electronic laboratory reporting systems could guide further refinement and advances in infectious disease surveillance. U 18 -HS 10399 /HS/AHRQ HHS/United State...|$|E
5000|$|... #Subtitle level 2: CCB <b>Standards</b> <b>Validation</b> and Verification Process ...|$|R
5000|$|... <b>validation</b> of {{technical}} <b>standards</b> and adoption of uniform technical prescriptions for railway material (APTU), ...|$|R
40|$|Pattern {{recognition}} is a mature field; however, {{in recent years}} it has developed a special interest. Many methods have been developed exclusively focusing on this subject; all of them, however, are centered on certain types of patterns. In this paper, a new method, which aims to be as flexible as possible, is proposed. This technique {{is based on a}} <b>validation</b> <b>standard</b> method used in electromagnetic compatibility named feature selective validation. The final objective of developing the algorithm is to make it adaptable to diverse types of patterns. An application on the recognition of transient patterns is exposed as an example. Nevertheless, a wide range of other types of signals is likely to be treated with the same logic. Postprint (published version...|$|R
40|$|Abstract Background Health {{technology}} {{assessments of}} surgical interventions frequently require {{the inclusion of}} non-randomised evidence. Literature search strategies employed to identify this evidence often exclude a methodological component because of uncertainty surrounding the use of appropriate search terms. This can result in the retrieval {{of a large number}} of irrelevant records. Methodological filters would help to minimise this, making literature searching more efficient. Methods An objective approach was employed to develop MEDLINE and EMBASE filters, using a reference standard derived from screening the results of an electronic literature search that contained only subject-related terms. Candidate terms for MEDLINE (N = 37) and EMBASE (N = 35) were derived from examination of the records of the reference standard. The filters were validated on two sets of studies that had been included in previous health technology assessments. Results The final filters were highly sensitive (MEDLINE 99. 5 %, EMBASE 100 %, MEDLINE/EMBASE combined 100 %) with precision ranging between 16. 7 % – 21. 1 %, specificity 35. 3 % – 43. 5 %, and a reduction in retrievals of over 30 %. Against the <b>validation</b> <b>standards,</b> the individual filters retrieved 85. 2 % – 100 % of records. In combination, however, the MEDLINE and EMBASE filters retrieved 100 % against both <b>validation</b> <b>standards</b> with a reduction in retrieved records of 28. 4 % and 30. 1 % Conclusion The MEDLINE and EMBASE filters were highly sensitive and substantially reduced the number of records retrieved, indicating that they are useful tools for efficient literature searching. </p...|$|E
40|$|Managing the {{validation}} {{and migration}} from SAS ® version 9. 1. 3 to 9. 2 {{on a new}} server presents many challenges. In May, 2010, such a project was completed. The manager of such a project should take the following into account: (1) purchasing hardware and software, (2) following company <b>validation</b> <b>standards,</b> (3) regular meetings with IT, Quality and other personnel, (4) {{the time required to}} write and execute validation documents, (5) file migration from the old to the new server, (6) start-up of the new SAS system, and (7) hiring personnel with validation experience, if needed. The successful manager should not underestimate the amount of time and resources to accomplish this type of project...|$|E
40|$|Physical {{disabled}} {{access is}} something that most cultural institutions such as museums consider very seriously. Indeed, there are normally legal requirements to do so. However, online disabled access is still a relatively novel and developing field. Many cultural organizations have not yet considered the issues in depth and web developers are not necessarily experts either. The interface for websites is normally tested with major browsers, but not with specialist software like text to audio converters for the blind or against the relevant accessibility and <b>validation</b> <b>standards.</b> We consider {{the current state of}} the art in this area, especially with respect to aspects of particular importance to the access of cultural heritage. Comment: 10 pages, see [URL]...|$|E
40|$|AbstractIn {{an attempt}} to develop a {{verification}} and <b>validation</b> <b>standard</b> for building fire evacuation models, Ronchi et al. (2013) at the United States’ National Institute of Standards and Technology (NIST) recommended a set of seventeen verification tests. We found that the application of these verification tests allowed us to make rather significant improvements to our simulation code (PEDFLOW) for {{approximately half of the}} recommended tests (Table 1). In some cases, we added capabilities that did not exist before. In other cases, we found anomalous behaviors and adjusted the existing code to remove these unexplained behaviors. This paper summarizes the work on the verification tests, highlighting the lessons learned and modifications made. We also discuss some modifications we recommend to the NIST verification tests, as well as demonstrate how to make these tests suitable for all pedestrian flow models (not just building fire evacuation) ...|$|R
30|$|Having {{understood}} the model’s dynamics, {{the next step}} is to independently validate its correctness within the application domain using <b>standard</b> <b>validation</b> framework (Pitchforth and Mengersen 2013).|$|R
5000|$|The National Institute of Standards and Technology (NIST) National Voluntary Laboratory Accreditation Program (NVLAP) accredits CMTLs to meet Cryptographic Module <b>Validation</b> Program (CMVP) <b>standards</b> and procedures.|$|R
40|$|The {{development}} of appropriate validation techniques {{is critical to}} assess uncertainties associated with satellite-data-based products, to identify needed product improvements and to allow products to be used appropriately. At regional to global scales, there are several outstanding issues in the {{development of}} robust validation methodologies, including the need to increase the quality and economy of product validation by developing and promoting international <b>validation</b> <b>standards</b> and protocols. This paper describes a protocol developed to validate a regional southern Africa burned-area product derived from Moderate Resolution Imaging Spectroradiometer (MODIS) 500 m time series data. The protocol is based upon interpretations by members of the Southern Africa Fire Network (SAFNet) of multitemporal Landsat Enhanced Thematic Mapper plu...|$|E
40|$|This {{contribution}} {{deals with}} validation and verification issues in aircraft noise prediction. We use two different comprehensive simulation software: PANAM, developed at DLR and FLIGHT, {{developed at the}} University of Manchester. The comparison is done {{on the basis of}} extensive flight data taken on an Airbus A 319 - 100 operated by Lufthansa. The flight recorder data have been synchronized with noise measurements on the ground, at 25 different observer locations. This paper aims to contribute to the establishment of rational <b>validation</b> <b>standards,</b> as well as realistic accuracy margins on integral noise metrics. The computer codes are briefly described. Results are shown for a variety of microphone positions, located sideways and directly along selected departure and approach flight ground tracks...|$|E
40|$|The {{validation}} of automated image registration and segmentation {{is crucial for}} accurate and reliable mapping of brain connectivity and function in three-dimensional (3 D) data sets. While <b>validation</b> <b>standards</b> are necessarily high and routinely met in the clinical arena, they have to date been lacking for high-resolution microscopy data sets obtained from the rodent brain. Here we present a tool for optimized automated mouse atlas propagation (aMAP) based on clinical registration software (NiftyReg) for anatomical segmentation of high-resolution 3 D fluorescence images of the adult mouse brain. We empirically evaluate aMAP as a method for registration and subsequent segmentation by validating it against the performance of expert human raters. This study therefore establishes a benchmark standard for mapping the molecular function and cellular connectivity of the rodent brain...|$|E
40|$|The AIAA Aeroelastic Prediction Workshop (AePW) {{was held}} in April, 2012, {{bringing}} together communities of aeroelasticians and computational fluid dynamicists. The objective in conducting this workshop on aeroelastic prediction was to assess state-of-the-art computational aeroelasticity methods as practical tools for the prediction of static and dynamic aeroelastic phenomena. No comprehensive aeroelastic benchmarking <b>validation</b> <b>standard</b> currently exists, greatly hindering validation and state-of-the-art assessment objectives. The workshop was a step towards assessing {{the state of the}} art in computational aeroelasticity. This was an opportunity to discuss and evaluate the effectiveness of existing computer codes and modeling techniques for unsteady flow, and to identify computational and experimental areas needing additional research and development. Three configurations served as the basis for the workshop, providing different levels of geometric and flow field complexity. All cases considered involved supercritical airfoils at transonic conditions. The flow fields contained oscillating shocks and in some cases, regions of separation. The computational tools principally employed Reynolds-Averaged Navier Stokes solutions. The successes and failures of the computations and the experiments are examined in this paper...|$|R
40|$|Small interfering RNA (siRNA) {{screening}} approaches have provided useful tools for the validation of genetic functions; however, image-based siRNA screening using multiwell plates requires {{large numbers of}} cells and time, which could be the barrier in application for gene mechanisms study using human adult cells. Therefore, we developed the advanced method with the cell-defined siRNA microarray (CDSM), for functional analysis of genes in small scale within slide glass using human bone marrow stromal cells (hBMSCs). We designed cell spot system with biomaterials (sucrose, gelatin, poly-l-lysine and matrigel) to control the attachment of hBMSCs inside spot area on three-dimensional (3 D) hydrogel-coated slides. The p 65 expression {{was used as a}} <b>validation</b> <b>standard</b> which described our previous report. For the optimization of siRNA mixture, first, we detected five kinds of commercialized reagent (Lipofectamine 2000, RNAi-Max, Metafectine, Metafectine Pro, TurboFectin 8. 0) via validation. Then, according to quantification of p 65 expression, we selected 2  μl of RNAi-Max as the most effective reagent condition on our system. Using same <b>validation</b> <b>standard,</b> we optimized sucrose and gelatin concentration (80  mM and 0. 13 %), respectively. Next, we performed titration of siRNA quantity (2. 66 – 5. 55  μM) by reverse transfection time (24  h, 48  h, 72  h) and confirmed 3. 75  μM siRNA concentration and 48  h as the best condition. To sum up the process for optimized CDSM, 3  μl of 20  μM siRNA (3. 75  μM) was transferred to the 384 -well V-bottom plate containing 2  μl of dH 2 O and 2  μl of 0. 6  M sucrose (80  mM). Then, 2  μl of RNAi-Max was added and incubated for 20  min at room temperature after mixing gently and centrifugation shortly. Five microliters of gelatin (0. 26 %) and 2  μl of growth factor reduced phenol red-free matrigel (12. 5 %) were added and mixed by pipetting gently. Finally, optimized siRNA mixture was printed on 3 D hydrogel-coated slides and cell-defined attachment and siRNA reverse transfection were induced. The efficiency of this CDSM was verified using three siRNAs (targeting p 65, Slug, and N-cadherin), with persistent gene silencing for 5  days. We obtained the significant and reliable data with effective knock-down in our condition, and suggested our method as the qualitatively improved siRNA microarray screening method for hBMSCs...|$|R
50|$|Visual Web Developer Express is a {{freeware}} web {{development tool}} that allows developers {{to evaluate the}} web development and editing capabilities of the other Visual Studio editions at no charge. Its main function is to create ASP.NET websites. It has a WYSIWYG interface, drag-and-drop user interface designer, enhanced HTML and code editors, a limited database explorer, support for CSS, JavaScript and XML, and integrated, design-time <b>validation</b> for <b>standards</b> including XHTML 1.0/1.1 and CSS 2.1.|$|R
40|$|Abstract. Cardiac {{dynamics}} suppression is a {{main issue}} for visual improvement and computation of tissue mechanical properties in IntraVascular UltraSound (IVUS). Although {{in recent times}} several motion compensation techniques have arisen, {{there is a lack}} of objective evaluation of motion reduction in in vivo pullbacks. We consider that the assessment protocol deserves special attention for the sake of a clinical applicability as reliable as possible. Our work focuses on defining a quality measure and a validation protocol assessing IVUS motion compensation. On the grounds of continuum mechanics laws we introduce a novel score measuring motion reduction in in vivo sequences. Synthetic experiments validate the proposed score as measure of motion parameters accuracy; while results in in vivo pullbacks show its reliability in clinical cases. Key words: <b>validation</b> <b>standards,</b> quality measures, IVUS motion compensation, conservation laws, Fourier development. ...|$|E
40|$|The {{abundance}} of models of complex networks {{and the current}} insufficient <b>validation</b> <b>standards</b> {{make it difficult to}} judge which models are strongly supported by data and which are not. We focus here on likelihood maximization methods for models of growing networks with many parameters and compare their performance on artificial and real datasets. While high dimensionality of the parameter space harms the performance of direct likelihood maximization on artificial data, this can be improved by introducing a suitable penalization term. Likelihood maximization on real data shows that the presented approach is able to discriminate among available network models. To make large-scale datasets accessible to this kind of analysis, we propose a subset sampling technique and show that it yields substantial model evidence in a fraction of time necessary for the analysis of the complete data. Comment: 8 pages, 5 figures, 2 table...|$|E
30|$|Another {{important}} issue in PET based radiotherapy planning {{is to determine}} whether 18 F-FDG-PET imaging can delineate the viable portion of a heterogeneous tumor from the necrotic portion. The <b>validation</b> <b>standards</b> {{reported in the literature}} are too simplified to adequately address this aspect of FDG-PET imaging [6]. In most clinical studies, surgical specimens were used as the “gold standard”, and results derived from the tumor contouring methods were compared to the external dimensions of the surgical specimen. There are two major drawbacks to such a validation standard. First, tumors are usually irregularly shaped, which makes a comparison in dimensions quite coarse. Second, a tumor specimen’s outer dimensions provide no information about the tumor’s internal biological heterogeneity. Tumors of similar dimensions may have different internal distributions of viable tumor tissues, and {{there are only a few}} studies taking into account such biological variation [8]. Tumors may shrink during histological processing, which further complicates the overall validation process [9].|$|E
5000|$|Open Badges is {{the name}} of a group of {{specifications}} and open technical standards originally developed by the Mozilla Foundation with funding from the MacArthur Foundation. The Open Badges standard describes a method for packaging information about accomplishments, embedding it into portable image files as a digital badge, and establishing an infrastructure for badge <b>validation.</b> The <b>standard</b> was originally maintained by the Badge Alliance Standard Working Group, but transitioned officially to the IMS Global Learning Consortium ...|$|R
40|$|Within the European Union network "Antenna Center of Excellence" – ACE (2004 - 2007), two intercomparison campaigns among {{different}} European measurement systems, using the 12 GHz <b>Validation</b> <b>Standard</b> (VAST 12) antenna, were carried out. These campaigns {{are described in}} the companion paper “Dedicated measurement campaign for definition of accurate reference pattern of the VAST 12 antenna”. The second campaign was performed by Technical University of Denmark (DTU) in Denmark, SAAB Microwave Systems in Sweden and Technical University of Madrid (UPM) in Spain. This campaign consisted {{of a large number}} of measurements with slightly different configurations in each of the three institutions (2 spherical near field systems and one compact range). The purpose of this paper is to evaluate the accuracy of the different facilities using this large number of acquisitions. The acquisitions were performed systematically varying in applied scanning scheme, measurement distances, signal level and so on. The results are analyzed by each institution combining the measurement results in near or far field and extracting from these measurements: a “best” pattern, an evaluation of possible sources of errors (i. e. reflections, mechanical and electrical uncertainties) and an estimation of the items of the uncertainty budget...|$|R
40|$|Background: The Centers for Disease Control and Prevention (CDC) {{recommends}} hand hygiene compliance {{by health}} care workers as a primary way to prevent the transmission of healthcare associated infections (HAI). A new strategy to measure hand hygiene compliance involves using patient surveys during healthcare visits to report on hand hygiene compliance. This study evaluated the feasibility of using patient surveys to monitor hand hygiene compliance in outpatient clinics of a US Department of Defense medical center. ^ Methods: A cross sectional study {{was used to compare}} patient observations to a nurse researcher 2 ̆ 7 s observations. Patients (n= 112) were asked to complete a brief questionnaire after an outpatient visit. Data on hand hygiene compliance of the health care provider(s) present at the outpatient visit were recorded on the questionnaire. A nurse researcher, who served as the <b>validation</b> <b>standard,</b> was present during the healthcare visit and recorded observations of hand hygiene practices of the health care practitioner. ^ Results: Cohen 2 ̆ 7 s kappa analysis of inter-rater agreement showed consistent agreement with a kappa of 0. 57 (0. 47 - 0. 66, 95...|$|R
