19|108|Public
40|$|In {{the present}} paper, the multiaxial fatigue life {{assessment}} of notched structural components is performed by employing a strain-based multiaxial fatigue criterion. Such a criterion, {{depending on the}} critical plane concept, is extended by implementing the control volume concept reated to the Strain Energy Density (SED) approach: a material point located at a certain distance from the notch tip {{is assumed to be}} the <b>verification</b> <b>point</b> where to perform the above assessment. Such a distance, measured along the notch bisector, is a function of both the biaxiality ratio (defined as the ratio between the applied shear stress amplitude and the normal stress amplitude) and the control volume radii under Mode I and Mode III. Once the position of the <b>verification</b> <b>point</b> is determined, the fatigue lifetime is assessed through an equivalent strain amplitude, acting on the critical plane, together with a unique material reference curve (i. e. the Manson-Coffin curve). Some uniaxial and multiaxial fatigue data related to V-notched round bars made of titanium grade 5 alloy (Ti- 6 Al- 4 V) are examined to validate the present criterion...|$|E
40|$|Abstract In {{this paper}} we present our C/C++-based design {{environment}} for hardware/software co-verification. Our {{approach is to}} use C/C++ to describe both hardware and software throughout the design flow. Our methodology supports the efficient mapping of C/ C++ functional descriptions directly into hardware and software. The advantages of a C/C++-based flow from the <b>verification</b> <b>point</b> of view are presented. The use of C/C++ to model {{all parts of the}} system provides great flexibility and enables faster simulation compared to existing methodologies. We show how co-verification can be done efficiently and effectively at the various levels of abstraction, how coverification can be used to drive co-design through performance estimation and give an example of implementation for the 8051 architecture. 1...|$|E
40|$|Numerical hydrodynamics model MOHID {{based on}} General Ocean Turbulence Model formula {{was used to}} study the tidal current in Makassar waters. The data used in this model are tidal {{elevation}} and bathymetry data from GEBCO. The tidal current simulation was conducted for 30 days. Model simulation {{results show that the}} tidal current velocity ranged between 0, 001 m/s – 0, 012 m/s. The current velocity during neap tide is smaller than the velocity during spring tide. The pattern of tidal current in Makassar waters is dominantly flows towards westward and then turned heading northward direction. The simulation result of tide-induced residual current in <b>verification</b> <b>point</b> shows that the flows is ebb-dominant at a rate of 0, 005 m/s heading westward direction...|$|E
40|$|In {{this paper}} we {{introduce}} the automatic verification of authentication & key exchange protocol its name suggest/show the paper summery now It is preferable for authentication and key exchange protocols {{to be verified}} automatically and rapidly in accordance with security requirements. In order to meet these requirements, we proposed the automatic security verification method for the protocols based on Bellaire et al. 's model and showed the <b>verification</b> <b>points</b> of security properties to verify their security efficiently. We show the novel <b>verification</b> <b>points</b> for each security property in the authentication and key exchange protocols {{in accordance with the}} aforementioned revisions. In addition, we describe the relations among the six <b>verification</b> <b>points,</b> explain how the proposed method verifies the aforementioned protocols by providing one example and show the validity of the proposed method by verifying the security challenges...|$|R
5000|$|An {{executable}} {{script is}} needed for each test case. It should be written according to test case description which defines the PDU sequence and <b>verification</b> <b>points.</b>|$|R
50|$|Rational Service Tester for SOA Quality {{offers a}} {{customizable}} functional test report {{which provides a}} global test verdict, and shows all the executed test steps, service operation calls and answers, {{as well as the}} detailed results of each <b>verification</b> <b>points.</b>|$|R
40|$|Abstract [...] As {{commonly}} observed {{throughout the}} world, the meteorological parameters at coastal area {{are influenced by}} both rotation of wind direction and sea breezes wind vectors features. Theoretically, this atmospheric condition describes difficulties in predicting on ground concentration of pollutant using the acceptable method of dispersion under the turbulence properties. This research applies the air dispersion modeling using ISCT 3 software in order to predict on ground concentration of NO 2 from selected petrochemical plants in Kertih, Terengganu, located at North East of Peninsular Malaysia Meteorological data of year 2008 obtained from the Kuala Terengganu Meteorology Station was used as input to the ISCT 3 software. This meteorology station is located approximately 95 km north-west off the study site which contains the pollutant sources and <b>verification</b> <b>point.</b> Th...|$|E
40|$|ABSTRACT. Tree Regular model {{checking}} is {{the name}} of a family of techniques for analyzing infinitestate systems in which states are represented by trees and sets of states by tree automata. From the <b>verification</b> <b>point</b> of view, the central problem is to compute the set of reachable states providing a given transition relation. A main obstacle is that this set is in general not computable in a finite time. In this paper, we propose a new CounterExample Guided Abstraction Refinement technique {{that can be used to}} check whether a set of state can be reached from the initial set. Contrary to existing techniques, our approach relies on equational abstraction to ease the definition of approximations and on a specific model of tree automata to avoid heavy backward refinement steps. ...|$|E
40|$|International audienceBiometric {{traits are}} {{permanently}} {{associated with a}} user. Though this is an advantage from identity <b>verification</b> <b>point</b> of view, if such biometric data is compromised, it cannot {{be replaced by a}} new one and becomes unusable in the system. This limitation can be overcome by combining biometrics with cryptographic techniques to induce revocability in biometric systems. In this paper, a multi-biometrics based cryptographic key regeneration scheme is proposed which combines information from iris and face to obtain a long cryptographic key having high entropy. The biometric information fusion is carried in feature domain using weighted feature level fusion technique. With the proposed system, we obtain 210 -bit keys with 183 -bit entropy (which is significantly higher than the 83 -bit entropy obtained for iris), at a False Acceptance Rate of 0 % and a False Rejection Rate of 0. 91...|$|E
40|$|Step-by-step {{procedures}} were developed for high integrity manual and machine welding of aluminum alloys. Detailed instructions are given for each step with tables and graphs to specify materials and dimensions. Throughout work sequence, processing procedure designates manufacturing <b>verification</b> <b>points</b> and inspection points...|$|R
40|$|Abstract—Security in a {{peer-to-peer}} (P 2 P) {{system is}} not considered, although it has many potential threats. In {{order to make the}} whole P 2 P system secure, various security functions require to be taken into consideration for these threats, respectively. We take up authentication and key exchange protocols as a target of the security functions in the P 2 P system. These protocols can bear one duty in order to realize the secure P 2 P system. It is preferable for authentication and key exchange protocols to be verified automatically and rapidly in accordance with security requirements. In order to meet this requirement, we proposed the security verification method for the aforementioned protocols based on Bellare et al. ’s model and showed the <b>verification</b> <b>points</b> of security properties to verify their security efficiently. However, there are three weaknesses in the aforementioned paper. In this paper, (1) we describe the relations of the six <b>verification</b> <b>points,</b> (2) explain how the proposed method verifies the aforementioned protocols by providing one example and (3) show the validity of the proposed method by verifying the security of 87 authentication and key exchange protocols that were generated automatically. Keywords-security verification method; authentication and key exchange protocols; <b>verification</b> <b>points...</b>|$|R
50|$|The TSA {{periodically}} publishes some links, so {{that all}} previously issued time-stamp tokens depend on the published link {{and that it is}} practically impossible to forge the published values. By publishing widely witnessed links, the TSA creates unforgeable <b>verification</b> <b>points</b> for validating all previously issued time-stamps.|$|R
40|$|Recent {{advancement}} in hardware design urged using a transac-tion based model {{as a new}} intermediate design level. Supporters for the Transaction Level Modeling (TLM) trend claim its efficiency in terms of rapid prototyping and fast simulation {{in comparison to the}} classical RTL-based approach. Intuitively, from a <b>verification</b> <b>point</b> of view, faster simulation induces better coverage results. This is driven by two factors: coverage measurement and simulation guid-ance. In this paper, we propose to use an abstract model of the de-sign, written in the Abstract State Machines Language (AsmL), in order to provide an adequate way for measuring the functional cov-erage. Then, we use this metric in defining the fitness function of a genetic algorithm proposed to improve the simulation efficiency. Fi-nally, we compare our coverage and simulation results to: (1) ran-dom simulation at TLM; and (2) the Specman tool of Verisity at RTL. 1...|$|E
40|$|Abstract. Automatic {{analysis}} of Hybrid Systems poses formidable chal-lenges both from a modeling {{as well as}} from a <b>verification</b> <b>point</b> of view. We present a case study on automatic verification of a Turbogas Control System (TCS) using an extended version of the Murϕ verifier. TCS is the heart of ICARO, a 2 MW Co-generative Electric Power Plant. For large hybrid systems, as TCS is, the modeling effort accounts for {{a significant part of the}} whole verification activity. In order to ease our mo-deling effort we extended the Murϕ verifier by importing the C language long double type (finite precision real numbers) into it. We give experimental results on running our extended Murϕ on our TCS model. For example using Murϕ we were able to compute an admissible range of values for the variation speed of the user demand of electric power to the turbogas. ...|$|E
40|$|In {{this paper}} we present our C/C++-based design {{environment}} for hardware/software co-verification. Our {{approach is to}} use C/C++ to describe both hardware and software throughout the design flow. Our methodology supports the efficient mapping of C/ C++ functional descriptions directly into hardware and software. The advantages of a C/C++-based flow from the <b>verification</b> <b>point</b> of view are presented. The use of C/C++ to model {{all parts of the}} system provides great flexibility and enables faster simulation compared to existing methodologies. We show how co-verification can be done efficiently and effectively at the various levels of abstraction, how coverification can be used to drive co-design through performance estimation and give an example of implementation for the 8051 architecture. 1. INTRODUCTION With shrinking device sizes, microprocessors, digital signal processors, memory and custom logic are being integrated into a single chip to form systems-on-chip. Verification of such syst [...] ...|$|E
50|$|During the {{recording}} phase, the user may introduce <b>verification</b> <b>points,</b> which capture an expected system state, {{such as a}} specific value in a field, or a given property of an object, such as enabled or disabled. During playback, any discrepancies between the baseline captured during recording and the actual result achieved during playback are noted in the Rational Functional Tester log. The tester can then review the log to determine if an actual software bug was discovered.|$|R
40|$|Currently {{the county}} {{management}} has great social, economic, environmental and institution challenges to being aimed, {{so that the}} municipalities reach social and economic development, without however promoting the environmental degradation, basic assumption to sustainability. To such objectives are reached, it makes necessary the cartographic document use with compatible precision and accuracy with the cartographic laws. This work is resulted of {{the assessment of the}} cartographic accuracy of digital orthophoto of the Goiânia city, produced in 2006 and of an orthorectified image based on Quickbird satellite image acquired in 2002. Using itself of methodology usually applied for analysis of cartographic accuracy, sixty four ground <b>verification</b> <b>points</b> in all city area, using themselves GPS receivers had been measured, and when evaluated the positioning of the ground <b>verification</b> <b>points</b> with digital orthophoto and with orthorectified image, were verified that the cartographic documents have accuracy according the laws of the national cartography. Our conclusions show that this work is very viable and necessary to be applied in any cartographic products due the low time consuming in work field using GPS receivers. Cartographic documents with quality assessed are the support for the planning and territorial management. Pages: 1771 - 177...|$|R
40|$|In a {{ubiquitous}} environment, it {{is preferable}} for authentication and key exchange protocols to be optimized automatically {{in accordance with}} security requirements. In this paper, we propose a security verification method for authentication and key exchange protocols {{that is based on}} Bellare et al. 's model. In particular, we show the <b>verification</b> <b>points</b> of one security property for authentication protocols and five security properties for key exchange protocols. We show that this method is valid by verifying the security of four typical examples of the authentication and key exchange protocols and the 87 authentication and key exchange protocols which were generated automatically. Key words...|$|R
40|$|This book {{describes}} {{the life cycle}} process of IP cores, from specification to production, including IP modeling, verification, optimization, and protection. Various trade-offs in the design process are discussed, including  those associated {{with many of the}} most common memory cores, controller IPs  and system-on-chip (SoC) buses. Readers will also benefit from the author’s practical coverage of new verification methodologies. such as bug localization, UVM, and scan-chain.   A SoC case study is presented to compare traditional verification with the new verification methodologies. ·         Discusses the entire life cycle process of IP cores, from specification to production, including IP modeling, verification, optimization, and protection; ·         Introduce a deep introduction for Verilog for both implementation and <b>verification</b> <b>point</b> of view.   ·         Demonstrates how to use IP in applications such as memory controllers and SoC buses. ·         Describes a new verification methodology called bug localization; ·         Presents a novel scan-chain methodology for RTL debugging; ·         Enables readers to employ UVM methodology in straightforward, practical terms...|$|E
40|$|System-on-Chip (SoC) is a {{promising}} paradigm to implement safety-critical embedded systems, but it poses significant challenges from a design and <b>verification</b> <b>point</b> of view. In particular, in a mixed-criticality system, low criticality applications must be prevented from interfering with high criticality ones. In this paper, we {{introduce a new}} design methodology for SoC that provides strong isolation guarantees to applications with different criticalities. A set of certificates describing the assumed application behavior is extracted from a functional Architectural Analysis and Design Language (AADL) specification. Our tools then automatically generate hardware wrappers that enforce at run-time the behavior described by the certificates. In particular, we employ run-time monitoring to formally check all data communication in the system, and we enforce timing reservations for both computation and communication resources. Verification is greatly simplified because certificates are much simpler than the components used to implement low-criticality applications. The effectiveness of our methodology is proven on a case study consisting of a medical pacemaker...|$|E
40|$|The fully {{automatic}} Graphical User Interface tool for any application using novel model based test suite generation techniques for a GUI. They {{are unable to}} control response time and time intervals are based on relationship between GUI events handlers and test cases with their responsibilities. We present a novel prioritization algorithm that enhances event handlers for the automated GUI tool. The proposed tool generates GUI events, it Captures and Playback event responses to automatic <b>verification</b> <b>point</b> of {{the results for the}} test cases which are written to a log file and corresponding report will be generated. This novel algorithm was able to detect new test suite and ordering of test cases to reduce a GUI fault integration defects. The number of faults detected for a single event are found after generating test cases for the application. The Average Percentage of Fault Detection (APFD) and charts has been used to show the effectiveness of proposed algorithm to find fault detection rate...|$|E
40|$|Land cover change {{classification}} from 2000 - 2014 for {{the footprints}} 125050, 125051, 126049, 126050, 129046, 130047, 131046 using dense time stacks of Landsat data (2 - 9 image per year). The classification was performed using the support vector machines (SVM) classifier. Classification results, training <b>points,</b> and <b>verification</b> <b>points</b> are provided for each footprint. Details on the classification {{can be found}} in the publication "Mapping the expansion of boom crops in Mainland Southeast Asia using dense time stacks of Landsat data" (to be published in MDPI Remote Sensing, currently under revision). This study was supported by the National Aeronautics and Space Administration’s (NASA) Land-Cover and Land-Use Change Program (LCLUC) Grant No. NNX 14 AD 87 G...|$|R
40|$|This paper {{presents}} an outdoor Radio Environment Map (REM) for the television band {{applied to the}} UPCCampus Nord, which has a Manhattan like structure. A measurement campaign of the received TV signal {{in the streets and}} squares of the campus was carried out. Two specific interpolation algorithms were analysed in the study, namely Kriging and {{a modified version of the}} Inverse Distance Weighted (IDW) algorithm. In addition a set of <b>verification</b> <b>points</b> were also measured and used to check the goodness of the interpolation methods used for REM implementation. Finally, a MATLAB based application was developed, which allows the user to obtain the list of occupied and free TV channels in any outdoor position of the Campus. Peer ReviewedPostprint (published version...|$|R
40|$|Initial results {{obtained}} from the registration of LANDSAT- 4 data to LANDSAT- 2 MSS data are documented and compared with {{results obtained}} from a LANDSAT- 2 MSS-to-LANDSAT- 2 scene-to-scene registration (using the same LANDSAT- 2 MSS data as the base data set in both procedures). RMS errors calculated on the control points used {{in the establishment of}} scene-to-scene mapping equations are compared to error computed from independently chosen <b>verification</b> <b>points.</b> Models developed to estimate actual scene-to-scene registration accuracy based on the use of electrostatic plots are also presented. Analysis of results indicates a statistically significant difference in the RMS errors for the element contribution. Scan line errors were not significantly different. It appears that a modification to the LANDSAT- 4 MSS scan mirror coefficients is required to correct the situation...|$|R
40|$|AbstractThe timed {{concurrent}} constraint {{programming language}} (tccp in short) was introduced for modeling reactive systems. This language {{allows one to}} model in a very intuitive way typical ingredients of these systems such as timeouts, preemptions, etc. However, there is no natural way for modeling other desirable features such as functional computations, for example for calculating arithmetic results. In fact, although it is certainly possible to implement such kind of operations, each single step of the computation takes time in tccp, and avoiding interferences with the intended overall behavior of the (reactive) system is quite involved. In this paper, we propose an extension of tccp for modeling instantaneous computations which improves the expressiveness of the language, {{in the sense that}} operations that are cumbersome to implement in pure tccp, are executed by calling an efficient, external functional engine, while the tccp programmer can focus on the pure, and usually more complex, reactive part of the system. We also describe a case study which motivates the work, and discuss how the new capability presented here can also be used as a new tool for developers from the <b>verification</b> <b>point</b> of view...|$|E
40|$|International audienceDynamic reconfigurations {{increase}} {{the availability and}} the reliability of component-based systems by allowing their architectures to evolve at runtime. Recently we have proposed a temporal pattern logic, called FTPL, to characterize the correct reconfigurations of component-based systems under some temporal and architectural constraints. As component-based architectures evolve at runtime, {{there is a need}} to check these FTPL constraints on the fly, even if only a partial information is expected. Firstly, given a generic component-based model, we review FTPL from a runtime <b>verification</b> <b>point</b> of view. To this end we introduce a new four-valued logic, called RV-FTPL (Runtime Verification for FTPL), characterizing the "potential" (un) satisfiability of the architectural constraints in addition to the basic FTPL semantics. Potential true and potential false values are chosen whenever an observed behaviour has not yet lead to a violation or satisfiability of the property under consideration. Secondly, we present a prototype developed to check at runtime the satisfiability of RV-FTPL formulas when reconfiguring a Fractal component-based system. The feasability of a runtime property enforcement is also shown. It consists in supervising on the fly the reconfiguration execution against desired RV-FTPL properties. The main contributions are illustrated on the example of a HTTP server architecture...|$|E
40|$|Abstract- As {{commonly}} observed {{throughout the}} world, the meteorological parameters at coastal area {{are influenced by}} both rotation of wind direction and sea breezes wind vectors features. Theoretically, this atmospheric condition describes difficulties in predicting on ground concentration of pollutant using the acceptable method of dispersion under the turbulence properties. This research applies the air dispersion modeling using ISCT 3 software in order to predict on ground concentration of NO 2 from selected petrochemical plants in Kertih, Terengganu, located at North East of Peninsular Malaysia Meteorological data of year 2008 obtained from the Kuala Terengganu Meteorology Station was used as input to the ISCT 3 software. This meteorology station is located approximately 95 km north-west off the study site which contains the pollutant sources and <b>verification</b> <b>point.</b> The modeling domains covered a 20 x 20 km 2 area centre of the petrochemical industry with grid spacing of 500 meter each as dummy receptors. During verification process, the significance improvement through the optimization analysis of wind direction proven that the correlation coefficient of predicted over the actual NO 2 concentration improve from 0. 68 to 0. 91. The average maximum monthly and yearly on ground concentration NO 2 obtained is at 13. 97 ug/m 3 and 6. 91 ug/m 3 respectively. The annual value is much below the Malaysian and WHO guidelines which is at 90 ug/m 3 and 40 ug/m 3 respectively. No benchmarking could be gauged on the monthly value since no guideline is available...|$|E
40|$|A {{city can}} not be designed" Watanabe [1]: our {{ambition}} can be at the maximum to guide someway and in some part its growth. So as planners need tools to aid an open design with uncertain goals. This research group begin to develop such a tool at high level of abstraction (Fioravanti 2008), {{with the aim of}} investigating the potentiality of a collaboration among complementary research domains. The present work reports about early implementation results of an innovative approach developed by the authors, for representation of design knowledge. It has been identified in the Urban Design Ontology (Montenegro and Duarte 2009) some design entities and their internal relationships that have been formalized and visualized by means of an intuitive interface. As a matter of fact, this approach, by means of inference engines allows coherence's check and constraint <b>verification,</b> <b>pointing</b> out incompatibility between initial design program and each partial specialist design solution and/or the overall shared one...|$|R
40|$|The {{satellites}} of {{the program}} CBERS offer the possibility of off nadir imagery (lateral inclination until 32 ° of its mirror), allowing to take images with stereoscopy. The objective of this work was a quantitative evaluation of the precision and the exactness of the altimetric information extracted from a stereoscopic pair of HR-CCD sensor in the satellite CBERS- 2. Statistical tests were applied {{for the analysis of}} the quality of a Digital Terrain Model (DTM) generated through techniques of digital photogrammetry. The error of the observations had average of 46, 86 meters. Moreover, it was observed something that may be a bias quality control of approximately 38 meters in these models, what lowered for 27, 03 meters the average of errors in the <b>verification</b> <b>points</b> [...] Following classification in the Decree Law 89817 - Standards of Cartographic Accuracy, it was concluded {{that it is possible to}} use these altimetric information for cartographic document generation Class A in scale 1 : 250000 or minus. Pages: 1211 - 121...|$|R
40|$|Sampling rate {{of current}} signal {{acquisition}} systems are singular. Aiming at this shortcoming, {{a method of}} multi-channel data acquisition(DAQ) with adjustable sampling rate is presented. The method realizes the cut-off frequency of anti-aliasing filter controlled by program {{with the help of}} switched-capacitor; by independently pulsing sampling signal of different ADCs, 16 -channel sampling rate are adjustable within the range 50 ksps, 25 ksps, 10 ksps, 5 ksps, 1 ksps. Theoretical analysis and experimental <b>verification</b> <b>pointing</b> at the proposed method are implemented: theoretical analysis shows that parameters of the filter meet the design requirements; experimental results show that cut-off frequency of the anti-aliasing filter matches variable sampling rate very well; choosing appropriate sampling rate according to the characteristics of the measured signal not only can well restore the measured signal, but also prevents system resources from waste. This method can meet needs of testing various signals with different frequency at the same time.   </p...|$|R
40|$|Software {{engineering}} research {{is driven by}} the aim of making software development more dynamic, flexible and evolvable. Nowadays the emphasis is on the evolution of pre-existing sub-systems and component and service-based development, where often only a part of the system is totally under control of the designer, most components being remotely operated by external vendors. In this context, we tackle the following problem: given the formal specification of the (incomplete) system, say it p, already built, how to characterize collaborators of p to be selected, based on a given communication interface L, so that a given property φ is satisfied. Using properties described by temporal logic formulae and systems by CCS processes, if φ is the formula to be satisfied by the complete system, an efficient and automatic procedure is defined to identify a formula ψ such that, for each existing process q satisfying ψ, the process (pq) â̂-L satisfies φ. Important features of this result are simplicity of the derived property ψ, compared to the original one, and scalability of the verification process. Such characteristics are necessary for applying the method to both incremental design and system evolution scenarios where p is already in place, and one needs to understand the specification of the functionality of the new component that should correctly interact with p. Indeed, in general, finding a suitable partner for p is easier than finding a complete system satisfying the global property. Moreover, in this paper it is shown how ψ can be used also to select a set of possible candidate processes q through a property-directed and structural heuristic. From the <b>verification</b> <b>point</b> of view, the description of the lacking component through a logic formula guarantees correctness of the integration with p of any process that exhibits a behaviour compliant with the inferred formula...|$|E
40|$|Software {{verification}} is {{the process}} of checking a software system to make sure it meets its specifications. A software system can be either of sequential or concurrent type. The most important part in a sequential system from the <b>verification</b> <b>point</b> of view is the relationship between the system’s inputs and outputs. More formally, verification of a sequential system can be expressed in the following form: If a program starts in a specific state which satisfies a certain condition(precondition), then it eventually terminates and the program variables at the final state satisfy some given relation with the corresponding values {{at the beginning of the}} execution [20]. But the story in verification of concurrent software is different. Many concurrent software use parallelism in order to make calculations more efficient. Parallelism is used to distribute a large amount of computations between different processing units to finish them in a shorter time. Input and output values are still enough for specifying the behavior of these sequential processes but are absolutely not enough for specifying the behaviors of the concurrent system. This is mainly because of the interactions between processes which can not only be expressed with the help of inputs and outputs. One important aspect in verification of concurrent programs is to directly verify them against an abstract specification of overall functionality. For example, a concurrent implementation of a familiar data type abstraction, such as a queue, could be verified to conform to a simple abstract specification of such a data type. This has been accomplished for finite-state programs and some verification tools like SPIN [3, 13] are already supporting it, but to our knowledge there are no approaches for handling unbounded data domains in specifications and implementations as is the case for a work-stealing double ended queue(deque) implementation. In this project we present a technique for automatically verifying that a concurrent workstealing deque conforms to an abstract specification of its functionality and we mathematically prove our technique's correctness. We also demonstrate its use by applying it to a famous implementation of a work-stealing deque data structure presented by Arora, et al. [2...|$|E
40|$|The Landsat 7 (L 7) Enhanced Thematic Mapper (ETM+) sensor was {{launched}} on April 15 th, 1999 {{and has been}} in operation for over nine years. It has six reflective solar spectral bands located in the visible and shortwave infrared part of the electromagnetic spectrum (0. 5 - 2. 5 micron) at a spatial resolution of 30 m. The on-board calibrators are used to monitor the on-orbit sensor system changes. The ETM+ performs solar calibrations using on-board Full Aperture Solar Calibrator (FASC) and the Partial Aperture Solar Calibrator (PASC). The Internal Calibrator Lamp (IC) lamps, a blackbody and shutter optics constitute the on-orbit calibration mechanism for ETM+. On 31 May 2003, a malfunction of the scan-line corrector (SLC) mirror assembly resulted in the loss of approximately 22 % of the normal scene area. The missing data affects most of the image with scan gaps varying in width from one pixel or less near the centre of the image to 14 pixels along the east and west edges of the image, creating a wedge-shaped pattern. However, the SLC failure has no impacts on the radiometric performance of the valid pixels. On December 18, 1999, the Moderate Resolution Imaging Spectroradiometer (MODIS) Proto-Flight Model (PFM) {{was launched}} on-board the NASA's EOS Terra spacecraft. Terra MODIS has 36 spectral bands with wavelengths ranging from 0. 41 to 14. 5 micron and collects data over a wide field of view angle (+/- 55 deg) at three nadir spatial resolutions of 250 m, 500 in 1 km for bands 1 to 2, 3 to 7, and 8 to 36, respectively. It has 20 reflective solar bands (RSB) with spectral wavelengths from 0. 41 to 2. 1 micron. The RSB radiometric calibration is performed by using on-board solar diffuser (SD), solar diffuser stability monitor (SDSM), space-view (SV), and spectro-radiometric calibration assembly (SRCA). Through the SV port, periodic lunar observations are used to track radiometric response changes at different angles of incidence (AOI) of the scan mirror. As a part of the AM Constellation satellites, Terra MODIS flies approximately 30 minutes behind L 7 ETM+ in the same orbit. The orbit of L 7 is repetitive, circular, sunsynchronous, and near polar at a nominal altitude of 705 km (438 miles) at the Equator. The spacecraft crosses the Equator from north to south on a descending node between 10 : 00 AM and 10 : 15 AM. Circling the Earth at 7. 5 km/sec, each orbit takes nearly 99 minutes. The spacecraft completes just over 14 orbits per day, covering the entire Earth between 81 degrees north and south latitude every 16 days. The longest continuous imaging swath that L 7 sensor can collect is for a 14 -minute subinterval contact period which is equivalent to 35 full WRS- 2 scenes. On the other hand, Terra can provide the entire corresponding orbit with wider swath at any given ETM+ collection without contact time limitation. There are six spectral matching band pairs between MODIS (bands 3, 4, 1, 2, 6, 7) and ETM+ (bands 1, 2, 3, 4, 5, 7) sensor. MODIS has narrower spectral responses than ETM+ in all the bands. A short-term radiometric stability was evaluated using continuous ETM+ scenes within the contact period and the corresponding half orbit MODIS scenes. The near simultaneous earth observations (SNO) were limited by the smaller swath size of ETM+ (187 km) as compared to MODIS (2330 km). Two sets of continuous granules for MODIS and ETM+ were selected and mosaiced based on pixel geolocation information for non cloudy pixels over the North American continent. The Top-of- Atmosphere (TOA) reflectances were computed for the spectrally matching bands between ETM+ and MODIS over the regions of interest (ROI). The matching pixel pairs were aggregated from a finer to a coarser pixel resolution and the TOA reflectance values covering a wide dynamic range of the sensors were compared and analyzed. Considering the uncertainties of the absolute calibration of the both sensors, radiometric stability was verified for the band pairs. The Railroad Valley Playa, Nada (RVPN) was included in the path of this continuous orbit, which served as a <b>verification</b> <b>point</b> between the shortterm and the long-term trending results from previous studies. This work focuses on monitoring the short-term on-orbit stability of MODIS and the ETM+ RSB. It also provides an assessment of the absolute calibration differences between the two sensors over their wide dynamic ranges...|$|E
30|$|Instead of {{calibrating}} for {{a specific}} task, the present study proposes obtaining robotic parameters via a calibration table. We design the calibration table and a dual-arm system to improve the accuracy of a manipulator using an optimization method to adjust DH parameters. The proposed procedure has three steps in Fig.  1. First, real data are obtained during operation via a Vicon camera system, the calibration table, and encoders. These data are the robot’s real positions and joint angles obtained at the calibration points. Then, the summation of errors between all ideal positions and real positions is calculated. Second, calibration is conducted via optimization. An optimization framework that uses deviations of DH parameters as the design variables is then formulated to minimize the position errors. Third, the compensation is applied and verified. The robot arm is controlled {{to move to the}} <b>verification</b> <b>points,</b> which are different from the calibration points, with optimized DH parameters. Then, the position of real points is captured using the Vicon camera system and the errors are determined by comparing ideal and real positions. Finally, the accuracy improvement is assessed.|$|R
40|$|Abstract — We {{motivate the}} {{capability}} approach to network denial-of-service (DoS) attacks, {{and evaluate the}} TVA architecture which builds on capabilities. With our approach, rather than send packets to any destination at any time, senders must first obtain “permission to send ” from the receiver, which provides the permission {{in the form of}} capabilities to those senders whose traffic it agrees to accept. The senders then include these capabilities in packets. This enables <b>verification</b> <b>points</b> distributed around the network to check that traffic has been authorized by the receiver and the path in between, and hence to cleanly discard unauthorized traffic. To evaluate this approach, and to understand the detailed operation of capabilities, we developed a network architecture called TVA. TVA addresses a wide range of possible attacks against communication between pairs of hosts, including spoofed packet floods, network and host bottlenecks, and router state exhaustion. We use simulations to show the effectiveness of TVA at limiting DoS floods, and an implementation on Click router to evaluate the computational costs of TVA. We also discuss how to incrementally deploy TVA into practice. I...|$|R
40|$|Abstract [...] -Denial of Service (DoS) attacks {{constitutes}} {{one of the}} rn are Distributed Denial of Service (DDoS) attacks, whose impact can be proportionally severe. Because of {{the seriousness}} of the problem many defense mechanisms have been proposed to combat these attacks. This paper presents a structural approach to the DDoS problem by introducing new technique of Implicit Token Scheme where in we distinguish between legitimate packets and the packets from unauthorized users and prevent network congestion. This scheme enables <b>verification</b> <b>points</b> to be distributed around the network which in turn checks that traffic has been certified as legitimate by both end points and the path in between, and to cleanly discard unauthorized traffic. The use of perimeter routers is highlighted in this approach which forms the backbone of connecting networks and cannot be compromised in any way. To achieve their security we propose a cryptographic algorithm, hash function for configuring the routers which will make the startup configuration or the running configuration of the router unintelligible to an attacker. Keywords [...] DoS, token, TCP, IT...|$|R
