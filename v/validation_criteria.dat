422|310|Public
50|$|Grounded theory method {{according}} to Glaser emphasizes induction or emergence, {{and the individual}} researcher's creativity within a clear frame of stages, while Strauss {{is more interested in}} <b>validation</b> <b>criteria</b> and a systematic approach.|$|E
50|$|RIBA Visiting Boards {{continue}} to assess courses for exemption from the RIBA's examinations in architecture. Under arrangements made in 2011 the <b>validation</b> <b>criteria</b> are jointly {{held by the}} RIBA and the Architects Registration Board, but unlike the ARB, the RIBA also validates courses outside the UK.|$|E
50|$|Content/Document management—Systems that {{automate}} {{the process}} of creating web content and/or documents. Roles such as editors, graphic designers, writers and producers can be explicitly modeled along with the tasks in the process and <b>validation</b> <b>criteria.</b> Commercial vendors started either to support documents (e.g. Documentum) or to support web content (e.g. Interwoven) but as the Internet grew these functions merged and vendors now perform both functions.|$|E
40|$|Abstract: A new <b>criterion</b> for {{clusters}} <b>validation</b> {{is proposed}} in the paper {{and based on the}} new cluster <b>validation</b> <b>criterion</b> a clustering ensmble framework is proposed. The main idea behind the framework is to extract the most stable clusters in terms of the defined criteria. Employing this new cluster <b>validation</b> <b>criterion,</b> the obtained ensemble is evaluated on some well-known and standard data sets. The empirical studies show promising results for the ensemble obtained using the proposed criterion comparing with the ensemble obtained using the standard clusters <b>validation</b> <b>criterion...</b>|$|R
40|$|Part 5 : Learning and Novel AlgorithmsInternational audienceIn {{this paper}} a new <b>criterion</b> for {{clusters}} <b>validation</b> is proposed. This new cluster <b>validation</b> <b>criterion</b> {{is used to}} approximate the goodness of a cluster. The clusters which satisfy a threshold of this measure are selected to participate in clustering ensemble. For combining the chosen clusters, a co-association based consensus function is applied. Since the Evidence Accumulation Clustering method cannot derive the co-association matrix from a subset of clusters, a new EAC based method which is called Extended EAC, EEAC, is applied for constructing the co-association matrix from the subset of clusters. Employing this new cluster <b>validation</b> <b>criterion,</b> the obtained ensemble is evaluated on some well-known and standard data sets. The empirical studies show promising results for the ensemble obtained using the proposed criterion comparing with the ensemble obtained using the standard clusters <b>validation</b> <b>criterion...</b>|$|R
3000|$|... test-set = 0.93). The {{configuration}} cost of {{the hypothesis}} exceeded the limit of 17 bits but can be accepted as the model achieves other <b>validation</b> <b>criterion</b> [11, 12].|$|R
50|$|INP validates each {{data field}} {{as it is}} entered. Validation types vary from simple alphabetic/numeric through ranges of numbers to tables of all {{allowable}} values. If a field is incorrect INP displays the <b>validation</b> <b>criteria,</b> beeps, and returns you {{to the start of}} the offending field. A mechanism exists to override validation checks. An optional audit trail of changes is maintained for reference and recovery.|$|E
50|$|The {{database}} access program is already compiled. It reads database description files at run time. The record layout and <b>validation</b> <b>criteria</b> are specified in one schema file. The specifications for the screen displays are entered into format files. Both {{are created by}} the user with any text editor. A variety of formats may be created to view or update different parts of a database record.|$|E
5000|$|In 2010, the Psychological Society of Ireland (PSI) {{limited the}} term of its {{validation}} of the College's psychology program to one year, {{in the course of}} which it required new <b>validation</b> <b>criteria</b> to be met. Given the uncertainty {{about the future of the}} program's PSI validation, the College arranged for students who wished to complete the degree under PSI validation to transfer to a similar program at Dublin Business School. All of the affected students elected to transfer out of the program.|$|E
3000|$|Other {{measures}} {{have been developed to}} assess the model quality. The Akaike information criterion (AIC), the Chi-square test, the cross <b>validation</b> <b>criterion,</b> and others are methods used to compare models with different numbers of predictor variables [34]. These methods are useful in the model selection, one of the steps of the analysis of the 2 [...]...|$|R
40|$|This article {{introduces}} a framework to determine and allocate capital reserves to multiple dependent business lines, {{with or without}} overall reserve level constraints. The proposed methodology emphasizes {{the role of the}} loss function in the <b>validation</b> <b>criterion</b> and its conditional interpretation. Univariate and multivariate examples are discussed in detail. Copyright (c) The Journal of Risk and Insurance, 2009. ...|$|R
30|$|Futures Maps are {{customer}} specific. If the Futures Map {{takes into}} account customers’ interests and key customers understand it, {{the validity of the}} Futures Map increases. In this sense, futures research is first of all an applied science serving actors in their decision making. Relevant actors have to understand the validated Futures Maps (<b>validation</b> <b>criterion</b> 5). In some cases, it is required that just key customers of the study understand the map (<b>validation</b> <b>criterion</b> 6). Sometimes these key customers might even require that e.g., their competitors are not able to use the map. This does not imply incomprehensibility of results for third parties, but usually confidentiality: the resulting intellectual property belongs to the organization that commissioned the study. Confidentiality makes applied research different to academic research. As a rule it excludes validation procedures like peer reviews and the integration of the results into the knowledge corpus of the field.|$|R
5000|$|The {{overriding}} {{aim of the}} book, and {{the import}} of the word [...] "methods" [...] in its original title, is a detailed specification of <b>validation</b> <b>criteria</b> for linguistic analysis. These criteria lend themselves to differing forms of presentation that have sometimes been taken as competing. Harris showed how they are complementary. (An analogy may be drawn to intersecting parameters in optimality theory.) [...] "It is not that grammar is {{one or another of}} these analyses, but that sentences exhibit simultaneously all of these properties." [...] Harris's treatment of these as tools of analysis rather than theories of language, and his way of using them to work toward an optimal presentation for this purpose or that, contributed to the perception that he was engaged in [...] "hocus-pocus" [...] with no expectation that there was any absolute truth to the matter.|$|E
40|$|In the {{software}} development cycle, validation {{is the important}} stage which is held in final stage especially in intelligent system. Validation obtains the validity, credibility and trustworthy of the system. It is needed {{to ensure that the}} intelligent system has same manner as human expertsâ€™. Whilst with the importance of validation stage, determining the <b>validation</b> <b>criteria</b> is also important. This paper presents the evaluation of <b>validation</b> <b>criteria</b> which is commonly used in intelligent system validation process. The evaluation is carried out by reviewing the literature of intelligent system validation process. The result shows that the <b>validation</b> <b>criteria</b> have its own characteristic so it requires for understanding the <b>validation</b> <b>criteria</b> characteristics, purposes of validation and also the intelligent system itself to hold validation process...|$|E
3000|$|Correlations between {{subscale}} {{scores from the}} 30 -item version and from the 15 -item version ranged from [...]. 90 (anger-aggression) to [...]. 96 (anxiety-withdrawal). Further analyses comparing patterns for external <b>validation</b> <b>criteria</b> for scores in the 15 -item version and in the 30 -item version showed no differences in effects. Due to the psychometric limitation of the 30 -item scale, and the approximate equivalence regarding external <b>validation</b> <b>criteria,</b> subscales scoreswere computed using only the 5 items per subscale depicted on Model 3.|$|E
40|$|AbstractThe {{problems}} of smoothing data through a transform in the Fourier domain and of retrieving a function from its Fourier coefficients are analyzed {{in the present}} paper. For both of them a solution, based on regularization tools, is known. Aim of the paper is to prove strong results of convergence of the regularized solution and optimality of the Generalized Cross <b>Validation</b> <b>criterion</b> for choosing the regularization parameter...|$|R
40|$|In this paper, {{we propose}} a {{numerical}} algorithm for filtering and robust signal differentiation. The numerical procedure {{is based on}} the solution of a simplified linear optimization problem. A compromise between smoothing and fidelity with respect to the measurable data is achieved by the computation of an optimal regularization parameter that minimizes the Generalized Cross <b>Validation</b> <b>criterion</b> (GCV). Simulation results are given to highlight the effectiveness of the proposed procedure...|$|R
40|$|Over {{the past}} few years, the notion of {{stability}} in data clustering has received growing attention as a cluster <b>validation</b> <b>criterion</b> in a sample-based framework. However, recent work has shown that as the sample size increases, any clustering model will usually become asymptotically stable. This led {{to the conclusion that}} stability is lacking as a theoretical and practical tool. The discrepancy between this conclusion and the success of stability in practice has remained an open question, which we attempt to address. Our theoretical approach is that stability, as used by cluster validation algorithms, is similar in certain respects to measures of generalization in a model-selection framework. In such cases, the model chosen governs the convergence rate of generalization bounds. By arguing that these rates are more important than the sample size, we are led to the prediction that stability-based cluster validation algorithms should not degrade with increasing sample size, despite the asymptotic universal stability. This prediction is substantiated by a theoretical analysis as well as some empirical results. We conclude that stability remains a meaningful cluster <b>validation</b> <b>criterion</b> over finite samples. ...|$|R
30|$|We {{additionally}} used time {{to complete}} the training mission as objective <b>validation</b> <b>criteria.</b> As time is critical in firefighting our reasoning was that well coordinated teams would be faster.|$|E
30|$|The reduced 15 -item version {{replicates}} the three-factor structure, shows good {{psychometric properties}} and meets external <b>validation</b> <b>criteria.</b> Further research {{should focus on}} the invariance of the factor structure of the SCBE- 30 between cultures.|$|E
40|$|In {{this chapter}} the Likelihood Ratio (LR) {{inference}} model will be introduced, the theoretical aspects of probabilities {{will be discussed}} and the validation framework for LR methods used for forensic evidence evaluation will be presented. Prior to introducing the validation framework, following questions will be addressed: “which aspects of a forensic evaluation scenario need to be validated?‿, “what {{is the role of}} the LR as part of a decision process?‿ and “how to deal with uncertainty in the LR calculation?‿ The answers to these questions are necessary to define the validation strategy based on the <b>validation</b> <b>criteria.</b> The questions: “what to validate?‿ focusing on defining <b>validation</b> <b>criteria</b> and methods, and “how to validate?‿ dealing with the implementation of a validation protocol, form the core of this chapter. The validation framework described can be used to provide assistance to the forensic practitioners, when determining the suitability and applicability of the LR method developed in the forensic practice by introducing performance characteristics, performance metrics, <b>validation</b> <b>criteria</b> and the decision thresholds. This chapter will start with the introduction of the LR inference model, followed by the validation framework proposed...|$|E
40|$|The {{objective}} {{of this study is}} to validate a flow prediction model for a hydrometric station using a multistation criterion in addition to standard single-station performance criteria. In this contribution we used cluster analysis to identify the regional flow height, i. e., water-level patterns and validate the output of an artificial neural network (ANN) model of the Alportel River in Portugal. Measurements of precipitation, temperature, and flow height were used as input variables to the ANN model with a lead time of 12 h. The lead time of 12 h is assumed to be appropriate for a short-term hydrological prediction since it is meaningful for physical processes. The ANN model with three inputs, four hidden neurons, and ten epochs was tested using the new model-validation criterion. The high performance of the model (i. e., Nash-Sutcliffe coefficient is equal to 0. 922) was confirmed by the cluster-analysis criterion. It can be concluded that a multistation-based approach can be used as an additional <b>validation</b> <b>criterion</b> and might result in a rejection of a model which initially passed a single-station <b>validation</b> <b>criterion...</b>|$|R
40|$|In this paper, {{spreadsheet}} for {{the calculation}} of re-entrant and co-axial cavities for Klystron and Multiple Beam Klystron(MBK) have been built using equivalent circuit model. The results from the Spreadsheet were compared with those from MAFIA and MicrowaveStudio (MWS), and the errors are within 5 % if the <b>validation</b> <b>criterions</b> are satisfied for the equivalent model. So the spreadsheet {{could be used in}} the preliminary design of cavity resonator for both klystron and MBK...|$|R
30|$|The method <b>validation</b> {{performance}} <b>criteria</b> {{indicate the}} method’s suitability for trace level quantification of Pb, Cd, and Cr in fish feed samples.|$|R
40|$|Abstract. Clustering {{techniques}} {{have been a}} valuable tool for several data analysis applications. However, {{one of the main}} difficulties associated with clustering is the validation of the results obtained. Both clustering algorithms and <b>validation</b> <b>criteria</b> present an inductive bias, which can favor datasets with particular characteristics. Besides, different runs of the same algorithm using the same data set may produce different clusters. In this work, traditional clustering and validation techniques are combined with a Genetic Algorithm (GA) to build clusters that better approximate the real distribution of the dataset. The GA employs a fitness function that combines two <b>validation</b> <b>criteria.</b> Such combination allows the GA to improve the evaluation of the candidate solutions. Furthermore, this combined approach avoids the individual weaknesses of each criterion. A set of experiments are run to compare the proposed model with other clustering algorithms, with promising results. ...|$|E
40|$|The {{requirements}} {{to be checked}} in validation are listed below. "CCS " indicates that the contractual requirements for these items will also be checked by Contract Compliance Screening (CCS). CCS requirements are not always the same as data <b>validation</b> <b>criteria.</b> "CADRE " indicates that CADRE checks for these items in CLP-Low/Medium Organic electronic dat...|$|E
40|$|Abstract—A sensor <b>validation</b> <b>criteria</b> {{based on}} the sensor’s object {{localization}} accuracy is proposed. Assuming that the true probability distribution of an object or event in space @�A is known and a spatial likelihood function (SLF) @�A for the same object or event in space is obtained from a sensor, then the expected value of the SLF...|$|E
3000|$|The {{coherent}} generator groups’ formation with maximum average silhouette {{value is}} considered more appropriate coherency identification [45]. Hence, more coherent generator groups formation will surely ensure more transient stability after islanding {{and will help}} in healing the system and avoiding a further blackout. A few researchers have recently used this coherency <b>validation</b> <b>criterion</b> for coherent generator groups’ formation evaluation. Therefore, it {{can be regarded as}} a measure of the stability of the newly formed islands [44, 45] [...]...|$|R
40|$|This paper {{presents}} a cross validation method for selection of statistics for Approximate Bayesian Computing, and for related estimation {{methods such as}} the Method of Simulated Moments. The method uses simulated annealing to minimize the cross <b>validation</b> <b>criterion</b> over a combinatorial search space that may contain many, many elements. An example, for which optimal statistics are known from theory, shows that the method is able to select optimal statistics out of a large set of candidate statistics...|$|R
40|$|In this paper, {{we develop}} an upper {{bound for the}} SPARSEVA (SPARSe Estimation based on a <b>VAlidation</b> <b>criterion)</b> {{estimation}} error in a general scheme, i. e., when the cost function is strongly convex and the regularized norm is decomposable {{for a pair of}} subspaces. We show how this general bound can be applied to a sparse regression problem to obtain an upper bound for the traditional SPARSEVA problem. Numerical results are used to illustrate the effectiveness of the suggested bound...|$|R
30|$|The {{individual}} {{results of}} the <b>validation</b> <b>criteria</b> and comparison with the method of the Ph.Eur. (HPLC and PC data) are presented below (the complete validation data {{can be found in}} the tables in the Online Resource 1). The results are expressed with 2 standard deviations (k = 2) meaning that 95.4 % of the results are in the mean values.|$|E
40|$|Though {{scientific}} {{software is}} an engine for scientific progress and provides data for critical decisions, {{the testing of}} scientific software is often anything but scientific. Our research examines the factors that complicate the testing of scientific software, including {{the complexity of the}} subject matter, inadequate <b>validation</b> <b>criteria,</b> a high demand for correctness, and the lack of testing expertise among scientists...|$|E
40|$|Bioanalytical method {{validation}} is {{a mandatory}} step {{to evaluate the}} ability of developed methods to provide accurate results for their routine application in order to trust the critical decisions that will be made with them. Even if several guidelines exist to help perform bioanalytical method validations, {{there is still the}} need to clarify the meaning and interpretation of bioanalytical method <b>validation</b> <b>criteria</b> and methodology. Yet, different interpretations can be made of the validation guidelines {{as well as for the}} definitions of the <b>validation</b> <b>criteria.</b> This will lead to diverse experimental designs implemented to try fulfilling these criteria. Finally, different decision methodologies can also be interpreted from these guidelines. Therefore, the risk that a validated bioanalytical method may be unfit for its future purpose will depend on analysts personal interpretation of these guidelines. The objective of this review is thus to discuss and highlight several essential aspects of methods validation, not only restricted to chromatographic ones but also to ligand binding assays owing to their increasing role in biopharmaceutical industries. The points that will be reviewed are the common <b>validation</b> <b>criteria,</b> which are selectivity, standard curve, trueness, precision, accuracy, limits of quantification and range, dilutional integrity and analyte stability. Definitions, methodology, experimental design and decision criteria are reviewed. Two other points closely connected to method validation are also examined: incurred sample reproducibility testing and measurement uncertainty as they are highly linked to bioanalytical results reliability. Their additional implementation is foreseen to strongly reduce the risk of having validated a bioanalytical method unfit for its purpose. Peer reviewe...|$|E
40|$|A period {{timing device}} {{suitable}} for processing laser Doppler anemometer signals {{has been described}} here. The important features of this instrument are: it is inexpensive, simple to operate, and easy to fabricate. When the concentration of scattering particles is low the Doppler signal {{is in the form}} of a burst and the Doppler frequency is measured by timing the zero crossings of the signal. But the presence of noise calls for the use of <b>validation</b> <b>criterion,</b> and a 5 – 8 cycles comparison has been used in this instrument. <b>Validation</b> <b>criterion</b> requires the differential count between the 5 and 8 cycles to be multiplied by predetermined numbers that prescribe the accuracy of measurement. By choosing these numbers to be binary numbers, much simplification in circuit design has been accomplished since this permits the use of shift registers for multiplication. Validation accuracies of 1. 6 %, 3. 2 %, 6. 3 %, and 12. 5 % are possible with this device. The design presented here is for a 16 -bit processor and uses TTL components. By substituting Schottky barrier TTLs the clock frequency can be increased from about 10 to 30 MHz resulting in an extension in the range of the instrument. Review of Scientific Instruments is copyrighted by The American Institute of Physics...|$|R
40|$|International audienceCluster {{validation}} {{is commonly}} {{used to determine the}} optimal number of clusters in a data set. Despite the success of distance-based validity indexes, their efficacy decreases rapidly when dealing with high-dimensional data. The present paper introduces a feature-based cluster <b>validation</b> <b>criterion</b> which can cope with said situation. In contrast to distance-based methods, our criterion evaluates similarity in terms of shared relevant features between data. The idea is based on the identification of the ``core'' features which are correlated within the description of each of the discovered clusters. The individual quality of each cluster is then evaluated through the frequency of the core features with respect to that of the non-core features within the cluster, while the between-cluster isolation is measured by means of the overlap coefficient between clusters, considering only the core features within the clusters. The overall clustering quality is measured by a weighted combination of the within and between cluster correlation coefficients, which enables choosing an appropriate number of clusters according to the purpose of clustering. Furthermore, our validation can prune out unreliable clusters which have no correlated features and thus no specific description of their content. Extensive experiments on the Reuters- 21578 collection are conducted to show the effectiveness of our <b>validation</b> <b>criterion...</b>|$|R
40|$|For ridge {{regression}} the {{degrees of freedom}} are commonly calculated by the trace of the matrix that transforms the vector of observations on the dependent variable into the {{ridge regression}} estimate of its expected value. For a fixed ridge parameter this is unobjectionable. When the ridge parameter is optimized on the same data, by minimization of the generalized cross <b>validation</b> <b>criterion</b> or Mallows, additional degrees of freedom are used however. We give formulae that take this into account. This allows of a proper assessment of ridge regression in competitions for the best predictor...|$|R
