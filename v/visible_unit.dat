14|107|Public
50|$|In Gaussian-Bernoulli RBM, the <b>visible</b> <b>unit</b> {{conditioned}} on hidden units {{is modeled}} as a Gaussian distribution.|$|E
5000|$|Since the RBM has {{the shape}} of a bipartite graph, with no intra-layer connections, the hidden unit activations are {{mutually}} independent given the <b>visible</b> <b>unit</b> activations and conversely, the <b>visible</b> <b>unit</b> activations are mutually independent given the hidden unit activations. That is, for [...] visible units and [...] hidden units, the conditional probability of a configuration of the visible units , given a configuration of the hidden units , is ...|$|E
50|$|Headed by the Assistant Chief of Administration, this is {{the least}} <b>visible</b> <b>unit</b> of the bureau but one that is {{possibly}} the most essential. It consists of eight major divisions.|$|E
5000|$|The <b>visible</b> <b>units</b> of RBM can be multinomial, {{although}} the hidden units are Bernoulli. In this case, the logistic function for <b>visible</b> <b>units</b> {{is replaced by}} the softmax function ...|$|R
3000|$|... where ∥·∥ 2 denotes L 2 norm. W∈R^I × J, σ∈R^I × 1, b∈R^I × 1, and c∈R^J × 1 are model {{parameters}} of the GBRBM, indicating the weight matrix between <b>visible</b> <b>units</b> and hidden units, the standard deviations associated with Gaussian <b>visible</b> <b>units,</b> a bias vector of the <b>visible</b> <b>units,</b> and a bias vector of hidden units, respectively. The fraction bar in Equation 2 denotes the element-wise division.|$|R
5000|$|... #Caption: Diagram of a {{restricted}} Boltzmann machine with three <b>visible</b> <b>units</b> and four hidden units (no bias units).|$|R
5000|$|As the 1970s {{turned to}} the 1980s, Hancock drifted away from the band as he moved into his electro-oriented phase, and they ceased {{operation}} as a <b>visible</b> <b>unit.</b> The band reunited with Hancock for the 1998 album Return of the Headhunters.|$|E
5000|$|The {{standard}} type of RBM has binary-valued (Boolean/Bernoulli) hidden and visible units, {{and consists of}} a matrix of weights [...] (size m×n) associated with the connection between hidden unit [...] and <b>visible</b> <b>unit</b> , as well as bias weights (offsets) [...] for the visible units and [...] for the hidden units. Given these, the energy of a configuration (pair of boolean vectors) [...] is defined as ...|$|E
5000|$|Order of Titus {{award is}} the only award {{presented}} by the Chief of Chaplains to recognize outstanding performance of ministry by chaplains and chaplain assistants. The Order of Titus is awarded for meritorious contributions to the unique and highly <b>visible</b> <b>Unit</b> Ministry Team Observer Controller Program. The award recognizes the great importance of realistic, doctrinally guided combat ministry training in ensuring the delivery of prevailing religious support to the American Soldier.|$|E
5000|$|Re-update {{the hidden}} units in {{parallel}} given the reconstructed <b>visible</b> <b>units</b> {{using the same}} equation as in step 2.|$|R
5000|$|From , {{sample a}} {{reconstruction}} [...] of the <b>visible</b> <b>units,</b> then resample the hidden activations [...] from this. (Gibbs sampling step) ...|$|R
5000|$|Update the <b>visible</b> <b>units</b> in {{parallel}} given the hidden units: [...] [...] is the bias of [...] This {{is called the}} [...] "reconstruction" [...] step.|$|R
5000|$|A {{restricted}} Boltzmann machine [...] is an undirected {{graphical model}} with stochastic visible variable and stochastic hidden variables. Each visible variable {{is connected to}} each hidden variable. The energy function of the model is defined as where [...] are model parameters: [...] represents the symmetric interaction term between <b>visible</b> <b>unit</b> [...] and hidden unit [...] and [...] are bias terms. The joint distribution {{of the system is}} defined as where [...] is a normalizing constant.The conditional distribution over hidden [...] and [...] can be derived as logistic function in terms of model parameters.|$|E
50|$|A Boltzmann {{machine is}} a type of {{stochastic}} neural network invented by Geoffrey Hinton and Terry Sejnowski in 1985. Boltzmann machines {{can be seen as the}} stochastic, generative counterpart of Hopfield nets. They are named after the Boltzmann distribution in statistical mechanics. The units in Boltzmann machines are divided into two groups-visible units and hidden units. General Boltzmann machines allow connection between any units. However, learning is impractical using general Boltzmann Machines because the computational time is exponential to the size of the machine. A more efficient architecture is called restricted Boltzmann machine where connection is only allowed between hidden unit and <b>visible</b> <b>unit,</b> which is described in the next section.|$|E
40|$|Subjects are the {{principle}} <b>visible</b> <b>unit</b> in most degree courses. Students {{tend to focus}} on passing subjects, sometimes to the detriment of understanding the concepts, their context and relative importance to the subject, degree and profession. The material being taught is effectively fragmented by the mechanism used to deliver it. This paper outlines a view of degree courses that promotes threads as a <b>visible</b> <b>unit,</b> to put back into place the context and importance of material, by drawing the focus beyond the subject boundaries to the body of knowledge or theory that the material belongs to. Links between these threads are considered to promote learning efficacy and to more easily facilitate course changes. INTRODUCTION There is a tension in technology-based degree courses between maintaining relevance and course stability. Whilst potentially new material grows rapidly, existing material does not become irrelevant at nearly the same rate. Careful selection of material to be incl [...] ...|$|E
5000|$|... #Caption: NMF as a {{probabilistic}} graphical model: <b>visible</b> <b>units</b> (...) {{are connected}} to hidden units (...) through weights , so that [...] is generated from a probability distribution with mean [...]|$|R
40|$|We are {{interested}} in exploring the possibility and benefits of structure learning for deep models. As the first step, this paper investigates the matter for Restricted Boltzmann Machines (RBMs). We conduct the study with Replicated Softmax, a variant of RBMs for unsupervised text analysis. We present a method for learning what we call Sparse Boltzmann Machines, where each hidden unit is connected to {{a subset of the}} <b>visible</b> <b>units</b> instead of all of them. Empirical results show that the method yields models with significantly improved model fit and interpretability as compared with RBMs where each hidden unit is connected to all <b>visible</b> <b>units...</b>|$|R
5000|$|... #Caption: [...] A {{graphical}} representation of an example Boltzmann machine. Each undirected edge represents dependency. In this example there are 3 hidden <b>units</b> and 4 <b>visible</b> <b>units.</b> This {{is not a}} restricted Boltzmann machine.|$|R
30|$|Current {{literature}} on shelf-space management mainly addresses the demand side by modeling {{the effect of}} space-elastic demand. In this case, a retailer’s profit is maximized under shelf-space constraints by defining the number of facings for each product (i.e., first <b>visible</b> <b>unit</b> of an item in the front row; Hübner and Kuhn 2012; Kök et al. 2015). Existing models do not account for replenishment frequencies and costs, or options for leveraging backroom inventory (Hübner and Kuhn 2012; Bianchi-Aguiar et al. 2016).|$|E
30|$|The DAE has six layers in total. The {{number of}} the layers was {{determined}} using a development set. The number of nodes in each layer is set to be 2048 except for input and output layers 2. The network is initialized using the same RBMs as used for initializing the DNNs described in the last subsection, which were trained using reverberant speech. The lower three layers were initialized using the weights {{of the first three}} RBMs and the hidden unit biases. The upper three layers were initialized using the transpose of the weights mentioned above and the <b>visible</b> <b>unit</b> biases. While the RBM we used for the initialization of the output layer originally has 40 * 11 nodes in the upper layer, we used only the 40 nodes corresponding to the center frame.|$|E
3000|$|... (1) Shelf-space {{decision}} Shelf-space {{planning is}} a mid-term task and typically executed every six months, requiring a retailer to assign shelf space and shelf quantities to listed products under {{the constraints of}} limited shelf size (Hübner and Kuhn 2012; Hübner et al. 2013; Bianchi-Aguiar et al. 2016). The results of these decisions are visualized in a planogram which displays the number of facings, display orientation and position on the shelf for every item (cf. Fig.  1). A facing is the first <b>visible</b> <b>unit</b> of an item {{in the front row}} of a shelf. Behind each facing, there is certain quantity of stock, i.e., additional units of the respective item. The number of facings and the stock per facing determine the total shelf quantity of an item. Furthermore, items can be displayed lengthwise or crosswise (cf. Drèze et al. 1994). Particularly when single units of an item are stored in cartons, the retailer must decide on the display orientation of the respective carton. Figure  1, left, shows the difference between facings and shelf quantities. For example, item a gets 2 facings with a stock of 4 units each, resulting in a total quantity of 2 · 4 = 8 units. Item b gets 1 facing with a stock of 6 units and a total shelf quantity of 6. The right of Fig.  1 explains the difference between length- and crosswise display orientation.|$|E
5000|$|... #Caption: [...] A {{graphical}} {{representation of a}} Boltzmann machine with a few weights labeled. Each undirected edge represents dependency and is weighted with weight [...] In this example there are 3 hidden units (blue) and 4 <b>visible</b> <b>units</b> (white). This is not a restricted Boltzmann machine.|$|R
5000|$|... #Caption: Graphical {{representation}} of a restricted Boltzmann machine. The four blue units represent hidden units, and the three red <b>units</b> represent <b>visible</b> states. In restricted Boltzmann machines there are only connections (dependencies) between hidden and <b>visible</b> <b>units,</b> and none between units of the same type (no hidden-hidden, nor visible-visible connections).|$|R
40|$|Abstract. Probabilistic {{models of}} textures {{should be able}} to {{synthesize}} specific textural structures, prompting the use of filter-based Markov ran-dom fields (MRFs) with multi-modal potentials, or of advanced variants of restricted Boltzmann machines (RBMs). However, these complex mod-els have practical problems, such as inefficient inference, or their large number of model parameters. We show how to train a Gaussian RBM with full-convolutional weight sharing for modeling repetitive textures. Since modeling the local mean intensities plays a key role for textures, we show that the covariance of the <b>visible</b> <b>units</b> needs to be sufficiently small – smaller than was previously known. We demonstrate state-of-the-art texture synthesis and inpainting performance with many fewer, but structured features being learned. Inspired by Gibbs sampling infer-ence in the RBM and the small covariance of the <b>visible</b> <b>units,</b> we further propose an efficient, iterative deterministic texture inpainting method. ...|$|R
40|$|In recent years, {{there has}} been an {{explosion}} of interest in computing using clusters of commodity, shared nothing computers. In this paper, we describe the design of TidyFS, a simple and small distributed file system that provides the abstractions necessary for data parallel computations on clusters. Similar to other large-scale distributed file systems such as the Google File System (GFS) and the Hadoop Distributed File System (HDFS), the prototypical workload for this file system is high-throughput, writeonce, sequential I/O. The primary user <b>visible</b> <b>unit</b> of storage in this system is the stream, which is a sequence of partitions distributed across the local storage of machines in the cluster. The mapping of streams to sequences of partitions is performed by the TidyFS metadata server, which also tracks the locations of each of the partition replicas in the system, the state of each storage machine in the cluster, and per-stream and per-partition attributes. The metadata server is implemented as a state machine and replicated for scalability and fault tolerance. In addition to the metadata server, the system is comprised of a graphical user interface which enables users and administrators to view the state of the system and a small service installed on each cluster machine responsible for replication, validation, and garbage collection. Clients read and write partitions directly to get the best possible I/O performance. ...|$|E
40|$|In modern {{linguistics}} since American Structuralism onwards {{and especially}} in the contributions of generative morphologists, the notion of ‘head’ as “the most important constituent” has been the main target of the analysis devoted to compounds. Once discarded the criterion of the position of the head, inasmuch as it is a parameter depending on the single languages, several other criteria have been used for its identification as a constituent whose properties are transmitted to the whole compound, by means of the so-called “percolation” either a) of the grammatical category (cf. Lieber 1981) - e. g. the nominal category trasmitted from the head utpalam ‘Nymphaea’ to the compound nīlotpalam ‘blu Nymphaea’ or b) of the morphological features (cf. Booij 2007), such as the gender - e. g. nīlotpalam is a neuter noun because utpalam is neuter; or c) of the semantic type (cf. Allen 1978), i. e. of the kind of objects that the compound denotes - e. g. the nīlotpalam is an utpalam. Nevertheless, such classifications often imply a clearcut notion of ‘adjective’ and ‘noun’, which - as semantics studies (e. g. Kamp 1975; Dixon 1982) have parallelously shown - are categories far from granted and relying on a subordinate-principal opposition, whose shortcomings had already been pointed out by the first Pāṇini's commentators. Moreover, in some well-known occurrences of karmadhārayas such as rājarṣi and nīlalohita, or the supposed comparison-based compounds of the kind śastrīśyāma- and puruṣavyāghra-, it is compulsory to combine more than one test and to postulate some additional rule to arrive at a clear identification of the 'head'. Similar difficulties arise in the analysis of (‘headless’) bahuvrīhis. Analogously, the extension of the notion of head to the derivational suffixes raises several doubts about the classical parameters adopted for identifying the head of a syntactic structure. Among these (which are listed e. g. by Bauer 1990 : 2 f. - cf. Scalise-Vogel 2010 : 8), perhaps the most evidently misleading could be “the head is an obligatory constituent in the phrase”. In point of fact, if we consider for instance a taddhita-formation such as vāstraḥ in the sense of vastrena parivṛtaḥ rathaḥ “a chariot covered by a cloth” in accordance with Pāṇini’s rule A 4. 2. 10, undoubtedly the suffixal head is always an obligatory constituent but the most <b>visible</b> <b>unit</b> in the derivative word (i. e. the lexical material which is actually involved - cf. A 4. 1. 82) is not the head, but the nominal stem vastra- “cloth”, modified according to the specific affixation rule (vástra- + áṆ > vāstráḥ). In the proposed communication we will try to face some of the linguistic problems here mentioned, by interpreting them in the light of Pāṇini’s marked choice of concentrating on the ‘non-head’ constituent of these complex words - precisely on their upasarjana. Both in compounds and in secondary derivatives, indeed, the upasarjana loses its original independence of meaning but determines the meaning of another linguistic form to which it is subordinated. Thus even a upamāna-word such as śas̄trī- can be considered {{as a part of the}} list of the subordinate lexemes which can modify the sāmānyavacanas which are lexically headed by śyāma (A 2. 1. 55), or a upamita-word such as puruṣa- in the list of the 'kind of' vyāghras (A 2. 1. 56). This joint paper is crucially based on the problems tackled in Kātyāyana and Patañjali’s commentaries on A 2. 1. 55 - 56 (particularly about the co-reference of the two constituents of these compounds) in the attempt of putting them in closer relation with the other rules involving the upasarjana-category. The aim is that of spelling out the core features of Pāṇini's upasarjana-based account of morphological subordination, highligting the morphological, semantic and lexical implications of his choice...|$|E
2500|$|The Saudi Arabian Royal Guard Regiment {{is one of}} {{the more}} <b>visible</b> <b>units</b> [...] Originally an {{independent}} military force, the Royal Guards were incorporated into the Army in 1964. However, the Royal Guards still retained their unique mission of protecting the House of Saud. Units of the Royal Guard protect the King of Saudi Arabia at all times.|$|R
5000|$|A deep Boltzmann {{machine has}} a {{sequence}} of layers of hidden units.There are only connections between adjacent hidden layers, {{as well as between}} <b>visible</b> <b>units</b> and hidden units in the first hidden layer. The energy function of the system adds layer interaction terms to the energy function of general restricted Boltzmann machine and is defined by ...|$|R
5000|$|The Saudi Arabian Royal Guard Regiment {{is one of}} {{the more}} <b>visible</b> <b>units</b> [...] Originally an {{independent}} military force, the Royal Guards were incorporated into the Army in 1964. However, the Royal Guards still retained their unique mission of protecting the House of Saud. Units of the Royal Guard protect the King of Saudi Arabia at all times.|$|R
40|$|A {{specific}} type of neural network, the Restricted Boltzmann Machine (RBM), is implemented for classification and feature detection in machine learning. RBM is characterized by separate layers of <b>visible</b> and hidden <b>units,</b> which are able to learn efficiently a generative model of the observed data. We study a "hybrid" version of RBM's, in which hidden units are analog and <b>visible</b> <b>units</b> are binary, and we show that thermodynamics of <b>visible</b> <b>units</b> are equivalent to those of a Hopfield network, in which the N <b>visible</b> <b>units</b> are the neurons and the P hidden units are the learned patterns. We apply the method of stochastic stability to derive the thermodynamics of the model, by considering a formal extension of this technique {{to the case of}} multiple sets of stored patterns, which may act as a benchmark for the study of correlated sets. Our results imply that simulating the dynamics of a Hopfield network, requiring the update of N neurons and the storage of N(N- 1) / 2 synapses, can be accomplished by a hybrid Boltzmann Machine, requiring the update of N+P neurons but the storage of only NP synapses. In addition, the well known glass transition of the Hopfield network has a counterpart in the Boltzmann Machine: It corresponds to an optimum criterion for selecting the relative sizes of the hidden and visible layers, resolving the trade-off between flexibility and generality of the model. The low storage phase of the Hopfield model corresponds to few hidden units and hence a overly constrained RBM, while the spin-glass phase (too many hidden units) corresponds to unconstrained RBM prone to overfitting of the observed data. Comment: 15 pages, 2 figure...|$|R
40|$|Restricted Boltzmann {{machines}} (RBMs) {{and their}} variants are usually trained by contrastive divergence (CD) learning, but the training procedure is an unsupervised learning approach, without any guidances {{of the background}} knowledge. To enhance the expression ability of traditional RBMs, in this paper, we propose pairwise constraints restricted Boltzmann machine with Gaussian <b>visible</b> <b>units</b> (pcGRBM) model, in which the learning procedure is guided by pairwise constraints {{and the process of}} encoding is conducted under these guidances. The pairwise constraints are encoded in hidden layer features of pcGRBM. Then, some pairwise hidden features of pcGRBM flock together and another part of them are separated by the guidances. In order to deal with real-valued data, the binary <b>visible</b> <b>units</b> are replaced by linear units with Gausian noise in the pcGRBM model. In the learning process of pcGRBM, the pairwise constraints are iterated transitions between <b>visible</b> and hidden <b>units</b> during CD learning procedure. Then, the proposed model is inferred by approximative gradient descent method and the corresponding learning algorithm is designed in this paper. In order to compare the availability of pcGRBM and traditional RBMs with Gaussian <b>visible</b> <b>units,</b> the features of the pcGRBM and RBMs hidden layer are used as input 'data' for K-means, spectral clustering (SP) and affinity propagation (AP) algorithms, respectively. A thorough experimental evaluation is performed with sixteen image datasets of Microsoft Research Asia Multimedia (MSRA-MM). The experimental results show that the clustering performance of K-means, SP and AP algorithms based on pcGRBM model are significantly better than traditional RBMs. In addition, the pcGRBM model for clustering task shows better performance than some semi-supervised clustering algorithms. Comment: 13 page...|$|R
40|$|Unsupervised feature {{learning}} {{has emerged as}} a promising tool in learning representations from unlabeled data. However, it is still challenging to learn useful high-level features when the data contains a significant amount of irrelevant patterns. Although feature selection can be used for such complex data, it may fail when we have to build a learning system from scratch (i. e., starting from the lack of useful raw features). To address this problem, we propose a point-wise gated Boltzmann machine, a unified generative model that combines {{feature learning}} and feature selection. Our model performs not only feature selection on learned high-level features (i. e., hidden units), but also dynamic feature selection on raw features (i. e., <b>visible</b> <b>units)</b> through a gating mechanism. For each example, the model can adaptively focus on a variable subset of visible nodes corresponding to the task-relevant patterns, while ignoring the <b>visible</b> <b>units</b> corresponding to the task-irrelevant patterns. In experiments, our method achieves improved performance over state-of-the-art in several visual recognition benchmarks. 1...|$|R
40|$|The {{restricted}} Boltzmann machine (RBM) is {{a flexible}} model for complex data. How-ever, using RBMs for high-dimensional multi-nomial observations poses significant com-putational diculties. In {{natural language processing}} applications, words are naturally modeled by K-ary discrete distributions, whereK {{is determined by the}} vocabulary size and can easily be in the hundred thousands. The conventional approach to training RBMs on word observations is limited because it re-quires sampling the states of K-way softmax <b>visible</b> <b>units</b> during block Gibbs updates, an operation that takes time linear in K. In this work, we address this issue with a more gen-eral class of Markov chain Monte Carlo op-erators on the <b>visible</b> <b>units,</b> yielding updates with computational complexity independent of K. We demonstrate the success of our approach by training RBMs on hundreds of millions of word n-grams using larger vocab-ularies than previously feasible with RBMs and by using the learned features to improve performance on chunking and sentiment clas-sification tasks, achieving state-of-the-art re-sults on the latter. 1...|$|R
5000|$|A deep Boltzmann machine (DBM) {{is a type}} of binary {{pairwise}} Markov random field (undirected probabilistic graphical model) {{with multiple}} layers of hidden random variables. It is a network of symmetrically coupled stochastic binary units. It comprises a set of <b>visible</b> <b>units</b> [...] and layers of hidden units [...] No connection links units of the same layer (like RBM). For the , the probability assigned to vector [...] is ...|$|R
40|$|The {{recurrent}} temporal restricted Boltzmann ma-chine (RTRBM) is a probabilistic time-series model. The topology of the RTRBM graphical model, however, assumes full connectivity be-tween all {{the pairs}} of <b>visible</b> <b>units</b> and hidden units, thereby ignoring the dependency structure within the observations. Learning this structure {{has the potential}} for not only improving the pre-diction performance, but also revealing impor-tant dependency patterns in the data. For ex-ample, given a meteorological dataset, we could identify regional weather patterns. In this work, we propose a new class of RTRBM, which we refer to as the structured RTRBM (SRTRBM), which explicitly uses a graph to model the de-pendency structure. Our technique is related to methods such as graphical lasso, which are used to learn the topology of Gaussian graphical mod-els. We also develop a spike-and-slab version of the RTRBM, and combine it with the SRTRBM to learn dependency structures in datasets with real-valued observations. Our experimental re-sults using synthetic and real datasets demon-strate that the SRTRBM can significantly im-prove the prediction performance of the RTRBM, particularly when the number of <b>visible</b> <b>units</b> is large {{and the size of the}} training set is small. It also reveals the dependency structures underly-ing our benchmark datasets. 1...|$|R
40|$|Abstract. In {{an effort}} to better {{understand}} the complex courtship behaviour of pigeons, we have built a model learned from motion capture data. We employ a Conditional Restricted Boltzmann Machine (CRBM) with binary latent features and real-valued <b>visible</b> <b>units.</b> The units are conditioned on information from previous time steps to capture dynamics. We validate a trained model by quantifying the characteristic “headbobbing” present in pigeons. We also show how to predict missing data by marginalizing out the hidden variables and minimizing free energy. ...|$|R
