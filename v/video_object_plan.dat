0|2222|Public
40|$|In this paper, {{we present}} an {{efficient}} video data model to represent moving trajectories of <b>video</b> <b>objects</b> and spatiotemporal {{relationships among the}} <b>video</b> <b>objects.</b> A <b>video</b> clips is segmented into a set of common appearance intervals (CAI) s. a CAI is a time interval that <b>video</b> <b>objects</b> appear together. Transitions among CAIs record the appearance/disappearance of <b>video</b> <b>objects.</b> Depending on the properties of <b>video</b> <b>objects,</b> they are classified as foreground and background <b>video</b> <b>objects.</b> Foreground <b>video</b> <b>objects</b> are further divided into moving <b>video</b> <b>objects</b> and static <b>video</b> <b>objects.</b> Different models are designed to capture these <b>video</b> <b>objects</b> and spatio-temporal relationships among foreground <b>video</b> <b>objects.</b> 1...|$|R
30|$|As the {{distance}} between <b>video</b> <b>object</b> and viewer/camera increases, interesting ratio of the <b>video</b> <b>object</b> decreases.|$|R
40|$|Abstract—Object-based video representation, {{such as the}} one {{suggested}} by the MPEG- 4 standard, offers a framework that is better suited for object-based video indexing and retrieval. In such a framework, the concept of a “key frame ” is replaced by that of a “key <b>video</b> <b>object</b> plane. ” In this paper, we propose a method for key <b>video</b> <b>object</b> plane selection using the shape information in the MPEG- 4 compressed domain. The shape of the <b>video</b> <b>object</b> (VO) is approximated using the shape coding modes of I, P, and B <b>video</b> <b>object</b> planes (VOPs) without decoding the shape informa-tion in the MPEG- 4 bit stream. Two popular shape distance mea-sures, the Hamming and Hausdorff distance measures, are modi-fied to measure the similarities between the approximated shapes of the <b>video</b> <b>objects.</b> Although they feature different computational and implementation complexity tradeoffs, the corresponding algo-rithms achieve essentially the same performance levels in selecting key <b>video</b> <b>object</b> planes that represent efficiently the salient content of the <b>video</b> <b>objects.</b> Index Terms—Key frame selection, key <b>video</b> <b>object</b> plane selec-tion, object-based video retrieval, video databases, video summa-rization. I...|$|R
40|$|This paper {{introduces}} {{a new form}} of representation for three-dimensional <b>video</b> <b>objects.</b> We propose to provide the capability to encode depth data for <b>video</b> <b>object</b> planes, as they are used in the MPEG- 4 video standard. We introduce a technique to extract depth and texture data from <b>video</b> <b>objects,</b> that are captured with multiple-camera configurations. The <b>video</b> <b>object</b> plane then represents an unwrapped surface of a 3 D object, which contains all texture data visible from any of the cameras. Using the depth data, it is possible to reconstruct different viewpoints from this texture surface. The technique can be used for efficient multiview encoding of <b>video</b> <b>objects</b> and for viewpoint adaptation towards single <b>video</b> <b>objects,</b> e. g. in mixing natural and synthetic images. Furthermore, scene composition becomes more flexible if depth information can be used for a natural background scene as well. 1. INTRODUCTION The MPEG- 4 video verification model (VM) encodes for each <b>video</b> <b>object</b> plane (VOP [...] ...|$|R
40|$|A robust <b>video</b> <b>object</b> based {{watermarking}} scheme, {{based on}} Zernike and Hu moments, is proposed in this paper. Firstly, a human <b>video</b> <b>object</b> detector {{is applied to}} the initial image. Zernike and the Hu moments of each human <b>video</b> <b>object</b> are estimated and an invariant function for watermarking is incorporated. Then, the watermark is generated modifying the moment values of each human <b>video</b> <b>object.</b> In the detection scheme, a neural network classifier is initially used in order to extract possible watermarked human <b>video</b> <b>objects</b> from each received input image. Then, a watermark detection procedure is applied for <b>video</b> <b>object</b> authentication. A full experiment confirms the promising performance of the proposed scheme. Furthermore, the performances of the two types of moments are extensively investigated under several attacks, verifying the robustness of Zernike moments comparing to Hu moments. © Springer-Verlag Berlin Heidelberg 2006...|$|R
40|$|Summarization of {{the video}} content is {{necessary}} {{in order to reduce the}} large amount of data involved in video retrieval. In a frame-based digital video retrieval framework, this is achieved by representing the content of a video sequence using "key frames". Similarly, in an object-based framework, such as the one suggested by the MPEG- 4 standard, <b>video</b> <b>object</b> planes can be used for summarization of <b>video</b> <b>object</b> content. In this paper, we propose a method for key <b>video</b> <b>object</b> plane selection using the compressed domain shape information in MPEG- 4. Two popular shape distance measures, the Hamming and Hausdorff distance measures, are employed to measure the similarities between the approximated shapes of the <b>video</b> <b>objects.</b> The corresponding algorithms, with different implementation complexity and computation tradeoffs, select key <b>video</b> <b>object</b> planes that represent efficiently the salient content of the <b>video</b> <b>objects...</b>|$|R
40|$|Abstract—This paper {{presents}} an intuitive method for synthesizing videos by directly manipulating <b>video</b> <b>objects</b> without using 3 D models. The proposed method extracts a <b>video</b> <b>object</b> from each <b>video</b> frame and creates locally consistent video sequences using a 2 D motion graph, where its node {{corresponds to the}} extracted <b>video</b> <b>object</b> and its edge represents a motion transition between a pair of nodes. Our primary contribution lies in a sophisticated construction of the 2 D motion graph using shape matching techniques, and its search {{that allows us to}} intuitively synthesize a new video sequence by manipulating feature points extracted from the <b>video</b> <b>objects</b> through the 2 D screen space. The method further employs a deformation technique to interpolate between <b>video</b> <b>objects</b> with relatively different shapes, and thus can increase available motion transitions by inserting intervening <b>video</b> <b>objects</b> into the 2 D motion graph. Several examples have been generated to demonstrate that this approach can create the user-intended motions of the <b>video</b> <b>objects</b> easily by clicking and dragging the feature points. Keywords- 2 D motion graphs; video synthesis; feature point tracking; I...|$|R
40|$|We {{present a}} novel scheme for object-based video {{sequence}} representation using appearance spaces. Our scheme enables fully automatic extraction of semantic <b>video</b> <b>objects</b> {{for a class}} of sequences, and their supervised organization in an object-class hierarchy. The hierarchy {{can be used for}} generic classification of query <b>video</b> <b>objects,</b> and transcoding using semantics of <b>video</b> <b>objects...</b>|$|R
40|$|In this paper, {{we present}} the OB-SPECK {{algorithm}} (objectbased set partitionedembedded block coder) for wavelet coding of arbitrary shaped <b>video</b> <b>objects.</b> Based on the SPECK algorithmintroduced in [1], the shape informationof the <b>video</b> <b>object</b> in the wavelet domain is {{integrated into the}} coding process for efficient coding of a <b>video</b> <b>object</b> of arbitrary shape...|$|R
40|$|This paper {{presents}} a Learning Vector Quantization (LVQ) -based temporal tracking method for semi-automatic <b>video</b> <b>object</b> segmentation. A semantic <b>video</b> <b>object</b> is initialized using user assistance in a reference frame to give initial classification of <b>video</b> <b>object</b> and its background regions. The LVQ training approximates <b>video</b> <b>object</b> and background classification {{and use them}} for automatic segmentation of the <b>video</b> <b>object</b> on the following frames thus performing temporal tracking. For LVQ training input, we sampling each pixel of a video frame as a 5 -dimensional vector combining 2 -dimensional pixel position (X,Y) and 3 -dimensional HSV color space. This paper also demonstrates experiments using some MPEG- 4 standard test video sequences to evaluate {{the accuracy of the}} proposed method...|$|R
40|$|In {{this paper}} we propose an {{algorithm}} to partition MPEG- 4 <b>video</b> <b>objects</b> into temporal segments with uniform activity levels. These segments {{can be used for}} efficient summarization of <b>video</b> <b>objects</b> and/or retrieval of <b>video</b> <b>objects</b> by their motion. The proposed algorithm is based on the shape and texture coding modes of inter coded <b>video</b> <b>object</b> planes. Because the employed filtering techniques are simple and full decoding of the MPEG- 4 bitstream is not required, the algorithm is computationally efficient. 1. Introduction Recent advancements in video coding technology have led to the emergence of the MPEG- 4 [1] standard, which enables access to individual <b>video</b> <b>objects</b> within the <b>video</b> sequences. Consequently, <b>object</b> based <b>video</b> retrieval is replacing frame based video retrieval. Motion {{is one of the key}} low level features employed by video retrieval systems. Global motion of the <b>video</b> <b>object</b> that is represented by MPEG- 4 can be easily obtained by extracting the location of the video [...] ...|$|R
40|$|This paper {{presents}} a 3 -D shape-adaptive directional wavelet coding technique for object-based scalable video coding. This technique includes 3 -D object-based directional threading and extensions of weighted adaptive lifting (WAL) scheme. The 3 -D object-based directional threading, which unifies {{the concept of}} temporal motion threading and 2 -D spatial directional threading, provides the opportunities to align a series of <b>video</b> <b>object</b> planes to form a 3 -D <b>video</b> <b>object</b> and exploit the spatio-temporal correlation inside the 3 -D <b>video</b> <b>object.</b> The WAL scheme, which is extended from 2 -D frame-based image coding to 3 -D object-based video coding, is employed to decompose the 3 -D <b>video</b> <b>object</b> into a 3 -D multispatio-temporal resolution <b>video</b> <b>object</b> pyramid for object-based scalable video coding. Experimental {{results show that the}} proposed 3 -D shape-adaptive directional wavelet coding technique consistently outperforms MPEG- 4 and other wavelet-based schemes for coding arbitrarily shaped <b>video</b> <b>objects...</b>|$|R
40|$|A robust <b>video</b> <b>object</b> based {{watermarking}} scheme, {{based on}} Zernike and Hu moments, is proposed in this paper. Firstly, a human <b>video</b> <b>object</b> detector {{is applied to}} the initial image. Zernike and the Hu moments of each human <b>video</b> <b>object</b> are estimated and an invariant function for watermarking is incorporated. Then, the watermark is generated modifying the moment values o...|$|R
40|$|Object-based video representation, {{such as the}} one {{suggested}} by the MPEG- 4 standard, offers a framework that is better suited for object-based video indexing and retrieval. In such a framework, the concept of a "key frame" is replaced by that of a "key <b>video</b> <b>object</b> plane". In this paper, we propose a method for key <b>video</b> <b>object</b> plane selection using the shape information in the MPEG- 4 compressed domain. The shape of the <b>video</b> <b>object</b> is approximated using information on the shape coding modes in the MPEG- 4 bitstream. Two popular shape distance measures, the Hamming and Hausdorff distance measures, are modified to measure the similarities between the approximated shapes of the <b>video</b> <b>objects.</b> Although they feature different computational and implementation complexity tradeoffs, the corresponding algorithms achieve essentially the same performance levels in selecting key <b>video</b> <b>object</b> planes that represent efficiently the salient content of the <b>video</b> <b>objects.</b> Key words: Key video [...] ...|$|R
40|$|This paper {{introduces}} {{a new form}} of representation for three-dimensional (3 -D) <b>video</b> <b>objects.</b> We have developed a technique to extract disparity and texture data from <b>video</b> <b>objects</b> that are captured simultaneously with multiple-camera configurations. For this purpose, we derive an “area of interest” (AOI) for each of the camera views, which represents an area on the <b>video</b> <b>object's</b> surface that is best visible from this specific camera viewpoint. By combining all AOIs, we obtain the <b>video</b> <b>object</b> plane as an unwrapped surface of a 3 -D object, containing all texture data visible from any of the cameras. This texture surface can be encoded like any 2 -D <b>video</b> <b>object</b> plane, while the 3 -D information is contained in the associated disparity map. It is then possible to reconstruct different viewpoints from the texture surface by simple disparity-based projection. The merits of the technique are efficient multiview encoding of single <b>video</b> <b>objects</b> and support for viewpoint adaptation functionality, which is desirable in mixing natural and synthetic images. We have performed experiments with the MPEG- 4 video verification model, where the disparity map is encoded by use of the tools provided for grayscale alpha data encoding. Due to its simplicity, the technique is suitable for applications that require real-time viewpoint adaptation toward <b>video</b> <b>objects...</b>|$|R
40|$|In {{this paper}} a fully {{automatic}} scheme for embedding visually recognizable watermark patterns to <b>video</b> <b>objects</b> is proposed. The architecture consists of 3 main modules. During the first module unsupervised <b>video</b> <b>object</b> extraction is performed, by analyzing stereoscopic pairs of frames. In the second module each <b>video</b> <b>object</b> is decomposed into three levels with ten subbands, using the Shape Adaptive Discrete Wavelet Transform (SA-DWT) and {{three pairs of}} subbands are formed (HL 3, HL 2), (LH 3, LH 2) and (HH 3, HH 2). Next Qualified Significant Wavelet Trees (QSWTs) are estimated for the specific pair of subbands with the highest energy content. QSWTs are derived from the Embedded Zerotree Wavelet (EZW) algorithm and they are high-energy paths of wavelet coefficients. Finally during the third module, visually recognizable watermark patterns are redundantly embedded to the coefficients of the highest energy QSWTs and the inverse SA-DWT is applied to provide the watermarked <b>video</b> <b>object.</b> Performance of the proposed <b>video</b> <b>object</b> watermarking system is tested under various signal distortions such as JPEG lossy compression, sharpening, blurring and adding different types of noise. Furthermore the case of transmission losses for the watermarked <b>video</b> <b>objects</b> is also investigated. Experimental results on real life <b>video</b> <b>objects</b> indicate the efficiency and robustness of the proposed schem...|$|R
40|$|In {{this paper}} a fully {{automatic}} system for embedding visually recognizable watermark patterns to <b>video</b> <b>objects</b> is proposed. The architecture consists of 3 main modules. During the first module unsupervised <b>video</b> <b>object</b> extraction is performed, by analyzing stereoscopic pairs of frames. In the second module each <b>video</b> <b>object</b> is decomposed into three levels with ten subbands, using the Shape Adaptive Discrete Wavelet Transform (SA-DWT) and {{three pairs of}} subbands ar...|$|R
40|$|We {{introduce}} {{a new form of}} representation for three-dimensional <b>video</b> <b>objects.</b> We have developed a technique to extract disparity and texture data from <b>video</b> <b>objects,</b> that are captured simultaneously with multiple-camera configurations. As a result, we obtain the <b>video</b> <b>object</b> plane as an unwrapped surface of a 3 D object, containing all texture data visible from any of the cameras. This texture surface can be encoded like any 2 D <b>video</b> <b>object</b> plane, while the 3 D information is contained in the associated disparity map. It is then possible to reconstruct different viewpoints from the texture surface by simple disparity-based projection. The merits of the technique are efficient multiview encoding of single <b>video</b> <b>objects,</b> and support for viewpoint adaptation functionality, which is desirable in mixing natural and synthetic images. We have performed experiments with the MPEG- 4 video verification model, where the disparity map is encoded by use of the tools provided for grayscale alpha data encoding. Due to its simplicity, the technique is capable for applications with requirement for real-time viewpoint adaptation towards <b>video</b> <b>objects...</b>|$|R
40|$|<b>Video</b> <b>object</b> {{tracking}} play {{an important}} role in security surveillance in current scenario. The explosion of successful digital device, the ease of use of high quality and economical video cameras, and the increasing need for computerized video analysis has generated a great deal of interest in video tracking methods. There are three techniques for video analysis: exposure of interesting moving target, tracking of such target from frame to frame, and analysis of target tracks to identify their activities. The successful <b>video</b> <b>object</b> tracking system faced a problem of false detection of moving <b>video</b> <b>object.</b> The false <b>video</b> <b>object</b> detection arises due to drastic change of background of moving video. For the maintenance of background updating various authors proposed a method for automatic background updating. In this paper we study of different <b>video</b> <b>object</b> tracking method using background updating factor...|$|R
40|$|This paper {{presents}} a novel deformation system for <b>video</b> <b>objects.</b> The {{system is designed}} to minimize the amount of user interaction, while providing flexible and precise user control. It has a keyframe-based user interface. The user only needs to manipulate the <b>video</b> <b>object</b> at some keyframes. Our algorithm will smoothly propagate the editing result from the keyframes to the rest frames and automatically generate the new <b>video</b> <b>object.</b> The algorithm is able to preserve the temporal coherence {{as well as the}} shape features of the <b>video</b> <b>objects</b> in the original video clips. We demonstrate the potential of our system with a variety of examples. 1...|$|R
50|$|MPEG-4 Part 2 is H.263 {{compatible}} in {{the sense}} that a basic H.263 bitstream is correctly decoded by an MPEG-4 Video decoder. (MPEG-4 Video decoder is natively capable of decoding a basic form of H.263.) In MPEG-4 Visual, {{there are two types of}} <b>video</b> <b>object</b> layers: the <b>video</b> <b>object</b> layer that provides full MPEG-4 functionality, and a reduced functionality <b>video</b> <b>object</b> layer, the <b>video</b> <b>object</b> layer with short headers (which provides bitstream compatibility with base-line H.263). MPEG-4 Part 2 is partially based on ITU-T H.263. The first MPEG-4 Video Verification Model (simulation and test model) used ITU-T H.263 coding tools together with shape coding.|$|R
40|$|To support {{heterogeneous}} application types a video {{digital library}} will contain {{a large number}} of <b>video</b> <b>objects</b> with various lengths and display requirements. Multi-user access to the same <b>video</b> <b>objects</b> is required in order to increase the availability of video information and to make full use of the limited computing and storage resources. The access frequency and delay sensitivity of <b>video</b> <b>objects</b> require special methods to guarantee smooth playback of <b>video</b> <b>objects</b> and to minimize average waiting time. We propose an integrated approach to bu#er and disk management for dynamic loading and simultaneous delivery of multiple <b>video</b> <b>objects</b> to multiple users. The allocation of bu#er and disk resources in this study is based on qualityof service variables suchasaverage waiting time, display continuity, and viewer enrollment. To Appear in the Special Issue of the Journal of Parallel and Distributed Computing on Distributed Multimedia Systems # This researchwas supported in part b [...] ...|$|R
40|$|This paper {{describes}} a novel appearance based scheme for extraction and representation of <b>video</b> <b>objects.</b> The tracking algorithm used for <b>video</b> <b>object</b> extraction {{is based upon}} a new eigen-space update scheme. We propose a scheme for organisation of <b>video</b> <b>objects</b> in an appearance based hierarchy. Appearance based hierarchy is constructed using a new SVD based eigen-space merging algorithm. The hierarchy enables approximate query resolution. Experiments performed on {{a large number of}} video sequences have yielded promising results. 1...|$|R
40|$|VII. CONCLUSION In {{this paper}} we {{presented}} an integrated schema for semantic object segmentation and content-based object search {{based on the}} region-based <b>video</b> <b>object</b> model. We first discussed AMOS, a generic <b>video</b> <b>object</b> segmentation system which combines low level automatic region segmentation with user input for defining and tracking semantic <b>video</b> <b>objects.</b> Our experiments and performance evaluation have shown very good segmentation results. Using the region-based <b>video</b> <b>object</b> model, an object query model which effectively combines local region-level features and spatial-temporal structures is then presented. Experiments have shown promising results and great potential for developing advanced video search tools for semantic video representations such as MPEG- 4. In the future work, we will include multiple objects tracking...|$|R
40|$|MPEG- 4 video {{consists}} of various <b>video</b> <b>objects,</b> rather than frames, allowing a true interactivity and manipulation of separate arbitrary shape object. Software-based encoding of MPEG- 4 <b>video</b> <b>objects</b> {{can be carried}} out by wing parallel processing with efficient scheduling scheme to speedup the computation. In this paper, we propose two dynamic scheduling algorithms which have different scheduling costs and performance levels. The algorithms assign the multiple <b>video</b> <b>objects</b> encoding tasks to the cluster of workstations with proper load balancing. The algorithms allow user on-line interactions and perform the concurrent encoding on the <b>video</b> <b>objects</b> to achieve real-time speed. The experimental results, while showing real-time encoding rates, exhibit trade-offs between load balancing, overhead scheduling cost and global performance...|$|R
40|$|A {{semi-automatic}} algorithm {{to extract}} the semantic <b>video</b> <b>object</b> in image sequences is proposed. Different schemes are used to get the initial <b>video</b> <b>object</b> in the first frame and other frames of a sequence. In the first frame, two polygons are input by the user to specify {{the area in which}} the object boundary is located. Then the <b>video</b> <b>object</b> is extracted automatically based on only the first frame. In the following frames, the image frame is segmented into intensity homogeneous regions. The moving regions are detected by a morphological filter, non-moving regions are selected by the object model from the previous frame. These regions form the initial <b>video</b> <b>object.</b> In each frame, after the initial object is available, the edges which belong to the <b>video</b> <b>object</b> of interest are selected by a local object contour model. Finally, an active contour model (snake) is applied {{to extract the}} final object contour. 1...|$|R
40|$|<b>Video</b> <b>objects</b> are {{temporal}} in nature. A <b>video</b> <b>object</b> {{is composed}} {{of a set of}} video frames which are related in a total time ordering. By imposing additional timing constraints among video frames, various presentation operators on <b>video</b> <b>objects</b> could be defined. A core set of presentation operators include Play, Pause, Resume, Fast Forward, Fast Backward, Slow Motion, and Stop. A video database system must be able to support the temporal ordering of video frames and the temporal constraints required by any presentation operators. In this paper, we demonstrate how our Four Dimensional Information Space (4 DIS) temporal database system is used to model <b>video</b> <b>objects</b> and the presentation operators. Since time is defined as a first class object in 4 DIS, querying operators and constraints for time objects are supported. These temporal querying operators and constraints are used to describe the timing requirements of <b>video</b> <b>objects</b> and the presentation operators. This allows users to describe [...] ...|$|R
40|$|Abstract: Video {{surveillance}} {{process takes}} video as an input, processes the video frames and performs actions accordingly. The video surveillance process consists of many phases. <b>Video</b> <b>object</b> segmentation and tracking are two crucial {{building blocks of}} smart surveillance systems. Threshold decision is a complex problem for <b>video</b> <b>object</b> segmentation with a multibackground model. Some conditions like nonrigid object movement, target appearance variations due to changes in illumination, and background mess make robust <b>video</b> <b>object</b> tracking complex. Multiple thresholds might be needed in connection with more refined algorithms. To make it completely automatic for variant conditions, First an automatic threshold decision technique that can automatically and specifically determine the threshold values for dynamic backgrounds is proposed. Second, a <b>video</b> <b>object</b> tracking framework based on a particle filter is proposed with the probability function composed of diffusion distance for measuring color histogram similarity and motion clue from <b>video</b> <b>object</b> segmentation. The proposed framework will track multiple moving objects under drastic changes in illumination and background mess. ...|$|R
40|$|The {{extraction}} of meaningful <b>objects</b> from <b>video</b> sequences {{is becoming increasingly}} important in many multimedia applications such as video compression or video post-production. The goal of this thesis is to review, evaluate and build upon the wealth of recent work {{on the problem of}} <b>video</b> <b>object</b> segmentation in the context of probabilistic techniques for generic <b>video</b> <b>object</b> segmentation. Methods are suggested that solve this problem using formal probabilistic learning techniques, this allows principled justification of methods applied to the problem of segmenting <b>video</b> <b>objects.</b> By applying a simple, but effective, evaluation methodology the impact of all aspects of the <b>video</b> <b>object</b> segmentation process are quantitatively analysed. This research focuses on the application of feature spaces and probabilistic models for <b>video</b> <b>object</b> segmentation are investigated. Subsequently, an efficient region-based approach to object segmentation is described along with an evaluation of mechanisms for updating such a representation. Finally, a hierarchical Bayesian framework is proposed to allow efficient implementation and comparison of combined region-level and object-level representational schemes...|$|R
40|$|This paper {{presents}} an efficient segmentation approach for non-rigid <b>video</b> <b>object.</b> We propose to formulate the <b>video</b> <b>object</b> segmentation problem as the maximum {{a posteriori probability}} (MAP) problem and define the probabilistic models {{in terms of the}} object’s density function. Furthermore, in order to accurately represent the density function for <b>video</b> <b>object</b> with arbitrary shape and complex texture, we employ a non-parametric method to estimate the density function. Our proposed density estimation mostly relies on the object’s color features and requires no time-consuming motion estimation. In addition, we further employ an efficient mean-shift procedure in the MAP optimization step to largely reduce the computational cost. Our experiments demonstrate that the segmentation results are very promising even when the <b>video</b> <b>objects</b> are severely deformed or occluded...|$|R
40|$|This paper {{presents}} a n ew object-based video compression approach. It consists on pr edicting <b>video</b> <b>objects</b> motions throughout the scene. Neural networks {{are used to}} carry out the prediction step. A multi-step- ahead prediction is performed to predict the <b>video</b> <b>objects</b> trajectories over the sequence. In order to reduce video data, only the background of the video sequence is transmitted with the different detected <b>video</b> <b>objects</b> as well as their initial properties such as placement and dimensions. Experimental results show the effectiveness of the proposed approach in terms of the compression rates...|$|R
40|$|Embedding generic shape {{information}} into probabilistic spatio-temporal <b>video</b> <b>object</b> segmentation is of pivotal importance to achieving better segmentation, since it provides valuable perceptual clues for humans in both distinguishing and recognising objects. Recently a probabilistic spatio-temporal <b>video</b> <b>object</b> segmentation algorithm incorporating shape {{information has been}} proposed, though since it is restricted to only pixel features, {{the probability of a}} pixel belonging to a certain cluster is directly correlated with its spatial location, which theoretically limits the segmentation performance of the technique. To address this problem, this paper proposes a new probabilistic spatio-temporal <b>video</b> <b>object</b> segmentation algorithm that incorporates generic shape information based on its region. Experimental results reveal a significant performance improvement in arbitrary-shaped <b>video</b> <b>object</b> segmentation compared with other contemporary methods for a variety of standard video test sequence...|$|R
40|$|Abstract. In {{this paper}} we {{describe}} the first full implementation of a content-based indexing and retrieval system for MPEG- 2 and MPEG- 4 videos. We con-sider a video {{as a collection of}} spatiotemporal segments called video objects; each <b>video</b> <b>object</b> is a sequence of <b>video</b> <b>object</b> planes. A set of representative <b>video</b> <b>object</b> planes is used to index each <b>video</b> <b>object.</b> During the database population, the operator, using a semi-automatic outlining tool we developed, manually selects <b>video</b> <b>objects</b> and insert some semantical information. Low-level visual features like color, texture, motion and geometry are automatically computed. The system has been implemented on a commercial relational DBMS and is based on the client server paradigm. The database population client is a stand-alone Windows application; the querying of the system is performed with a web-based client. To test the system we indexed a number of videos taken from Italian soccer championship games. A few examples of queries in this par-ticular domain are reported and results appear very promising. 1 Introduction and Related Wor...|$|R
40|$|This paper {{describes}} an MPEG- 4 video compliant {{framework for the}} creation, encoding and decoding of video scenes composed of multiple <b>video</b> <b>objects.</b> The generated scenes can be compliantly encoded and the bitstreams can be decoded resulting in individual <b>video</b> <b>objects</b> that can be independently accessed in the decoded scene. 1...|$|R
3000|$|In the {{proposed}} biometrics hiding method, {{one of the}} initial steps includes detection of the QSWTs {{for a pair of}} subbands of the host <b>video</b> <b>object.</b> Towards this direction, let us assume that the host <b>video</b> <b>object</b> is decomposed into two levels using the DWT to provide three pairs of subbands: [...]...|$|R
40|$|The {{increasing}} {{availability of}} object-based video content requires new technologies for automatically extracting and matching {{of the low}} level features of arbitrarily shaped video. In this paper, we propose a shape retrieval method for <b>video</b> <b>objects.</b> Our method takes into account not only the still shape features but also the shape deformations that may occur in the lifespan of <b>video</b> <b>objects.</b> We define a new shape similarity measure {{that is based on}} the shape similarity of the representative temporal instances of <b>video</b> <b>objects.</b> We also propose shape deformation features that are based on the variances of the still shape features. The proposed visual features can be derived directly from the MPEG- 4 compressed domain or computed from the shape masks of the <b>video</b> <b>objects</b> in the spatial domain. Our experiments show that the proposed method offers good retrieval performance results...|$|R
