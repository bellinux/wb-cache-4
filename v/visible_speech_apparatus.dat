0|195|Public
40|$|Purpose This study {{investigated}} whether {{and to what}} extent iconic co-speech gestures contribute to information from <b>visible</b> <b>speech</b> to enhance degraded speech comprehension at different levels of noise-vocoding. Previous studies of the contributions of these 2 visual articulators to speech comprehension have only been performed separately. Method Twenty participants watched videos of an actress uttering an action verb and completed a free-recall task. The videos were presented in 3 speech conditions (2 -band noise-vocoding, 6 -band noise-vocoding, clear), 3 multimodal conditions (speech + lips blurred, <b>speech</b> + <b>visible</b> <b>speech,</b> <b>speech</b> + <b>visible</b> <b>speech</b> + gesture), and 2 visual-only conditions (<b>visible</b> <b>speech,</b> <b>visible</b> <b>speech</b> + gesture). Results Accuracy levels were higher when both visual articulators were present compared with 1 or none. The enhancement effects of (a) <b>visible</b> <b>speech,</b> (b) gestural information on top of <b>visible</b> <b>speech,</b> and (c) both <b>visible</b> <b>speech</b> and iconic gestures were larger in 6 -band than 2 -band noise-vocoding or visual-only conditions. Gestural enhancement in 2 -band noise-vocoding did not differ from gestural enhancement in visual-only conditions...|$|R
40|$|We have {{implemented}} a facial animation system {{to carry out}} <b>visible</b> <b>speech</b> synthesis. Using this system, {{it is possible to}} manipulate control parameters to synthesize a sequence of speech articulations. In addition, it is possible to synthesize novel articulations, such as one that is half way between /ha! and Ida!. Given the importance ofvisible information in face-toface communication, <b>visible</b> <b>speech</b> synthesis is being developed to control and manipulate <b>visible</b> <b>speech.</b> Experiments have shown that this <b>visible</b> <b>speech</b> is particularly important when the auditory speech is degraded, because ofnoise, bane:width filtering, or hearing impairment (Massaro, 1987). The strong influence of <b>visible</b> <b>speech</b> is not limited to situations with degraded auditory input, however; it occurs even when <b>visible</b> <b>speech</b> is paired with perfectly intelligible speech sounds. The influence of <b>visible</b> <b>speech</b> is easily experienced in a demonstration o...|$|R
40|$|Purpose: This study {{investigated}} whether {{and to what}} extent iconic co-speech gestures contribute to information from <b>visible</b> <b>speech</b> to enhance degraded speech comprehension at different levels of noise-vocoding. Previous studies of the contributions of these 2 visual articulators to speech comprehension have only been performed separately. Method: Twenty participants watched videos of an actress uttering an action verb and completed a free-recall task. The videos were presented in 3 speech conditions (2 -band noise-vocoding, 6 -band noise-vocoding, clear), 3 multimodal conditions (speech + lips blurred, <b>speech</b> + <b>visible</b> <b>speech,</b> <b>speech</b> + <b>visible</b> <b>speech</b> + gesture), and 2 visual-only conditions (<b>visible</b> <b>speech,</b> <b>visible</b> <b>speech</b> + gesture). Results: Accuracy levels were higher when both visual articulators were present compared with 1 or none. The enhancement effects of (a) <b>visible</b> <b>speech,</b> (b) gestural information on top of <b>visible</b> <b>speech,</b> and (c) both <b>visible</b> <b>speech</b> and iconic gestures were larger in 6 -band than 2 -band noise-vocoding or visual-only conditions. Gestural enhancement in 2 -band noise-vocoding did not differ from gestural enhancement in visual-only conditions. Conclusions: When perceiving degraded speech in a visual context, listeners benefit more from having both visual articulators present compared with 1. This benefit was larger at 6 -band than 2 -band noise-vocoding, where listeners can benefit from both phonological cues from <b>visible</b> <b>speech</b> and semantic cues from iconic gestures to disambiguate speech...|$|R
40|$|OBJECTIVE: To test {{a simple}} method for {{improving}} consistency among raters for the perceptual evaluation of pathological voice quality by providing <b>visible</b> <b>speech</b> (spectrogram) as additional information because, to date, the interrater variability still limits the widespread clinical {{use of the}} best available rating system. DESIGN: Experimental comparison between 2 different ways (with and without the addition of <b>visible</b> <b>speech)</b> of perceptual rating by trained professionals of recorded pathological voices. Furthermore, the correlation between acoustical (jitter, shimmer, and noise-harmonic ratio) and perceptual parameters was investigated in both rating conditions. SUBJECTS: Six experts evaluated 70 recorded pathological voices using the GIRBAS (grade, instability, roughness, breathiness, asthenicity, and strain) scale in 2 separate sessions: first, conventionally, without <b>visible</b> <b>speech</b> as additional information, and several months later, with <b>visible</b> <b>speech</b> as additional information. MAIN OUTCOME MEASURES: The kappa interrater agreement and the correlation coefficient between GIRBAS scores and acoustic measures. RESULTS: We found a significant effect of <b>visible</b> <b>speech</b> on the agreement between the raters. The interrater agreement according to kappa statistics was significantly stronger {{with the addition of}} <b>visible</b> <b>speech</b> than without for rating grade, roughness, and breathiness. The correlation between acoustical and perceptual parameters showed no significant effect of <b>visible</b> <b>speech.</b> CONCLUSIONS: The addition of <b>visible</b> <b>speech</b> to the perceptual evaluation of pathological voices is an interesting clinical asset to enhance its reliability. The addition of <b>visible</b> <b>speech</b> to the clinical setting is feasible, since affordable computer programs are currently available that can provide the spectrogram in quasi-real time while conversing with the patient. The acoustical analysis might be applied in addition to perceptual rating in a multidimensional approach to assess voice quality. status: publishe...|$|R
40|$|Learning {{the sounds}} of a foreign {{language}} is difficult. The experiments reported here investigated whether <b>visible</b> <b>speech</b> can help. The focus of the experiments was on whether <b>visible</b> <b>speech</b> affects perception as well as the production of foreign speech sounds. The first experiment examined whether <b>visible</b> <b>speech</b> assists in the detection of a syllable within an unfamiliar foreign phrase. It was found that a syllable was more likely to be detected within a phrase when the participants could see the speaker’s face. The second experiment investigated whether judgments about the duration of a foreign language phrase would be more accurate with <b>visible</b> <b>speech</b> compared to a sound only condition. It was found that in the <b>visible</b> <b>speech</b> condition participant’s estimates of phrase duration correlated positively with actual duration, whereas in the sound only condition there was a negative correlation. Furthermore, with <b>visible</b> <b>speech,</b> estimates were close to the actual durations whereas those in the sound only condition tended to underestimate duration. The results are discussed with respect to previous findings and future applications. 1...|$|R
40|$|Abstract—We {{present a}} novel {{approach}} to synthesizing accurate <b>visible</b> <b>speech</b> based on searching and concatenating optimal variable-length units in a large corpus of motion capture data. Based {{on a set of}} visual prototypes selected on a source face and a corresponding set designated for a target face, we propose a machine learning technique to automatically map the facial motions observed on the source face to the target face. In order to model the long distance coarticulation effects in <b>visible</b> <b>speech,</b> a large-scale corpus that covers the most common syllables in English was collected, annotated and analyzed. For any input text, a search algorithm to locate the optimal sequences of concatenated units for synthesis is desrcribed. A new algorithm to adapt lip motions from a generic 3 D face model to a specific 3 D face model is also proposed. A complete, end-to-end <b>visible</b> <b>speech</b> animation system is implemented based on the approach. This system is currently used in more than 60 kindergarten through third grade classrooms to teach students to read using a lifelike conversational animated agent. To evaluate the quality of the <b>visible</b> <b>speech</b> produced by the animation system, both subjective evaluation and objective evaluation are conducted. The evaluation results show that the proposed approach is accurate and powerful for <b>visible</b> <b>speech</b> synthesis. Index Terms—Face animation, character animation, visual <b>speech,</b> <b>visible</b> <b>speech,</b> coarticulation effect, virtual human. æ...|$|R
5000|$|A Popular Manual of <b>Visible</b> <b>Speech</b> and Vocal Physiology (1889) ...|$|R
40|$|We {{present a}} {{multimodal}} interactive data exploration tool that fa-cilitates discrimination between <b>visible</b> <b>speech</b> tokens. The multi-modal tool uses visualization and sonification (non-speech sound) of data. <b>Visible</b> <b>speech</b> tokens is {{a class of}} multidimensional data {{that have been used}} extensively in designing talking head that has been used in training of deaf individuals by watching <b>speech</b> [1]. <b>Visible</b> <b>speech</b> tokens (consonants), referred to as categories, dif-fer along a set of pre-measured feature dimensions such as mouth height, mouth narrowing, jaw rotation and upper-lip retraction. The data set was visualized with a series of 1 D scatter-plots that differed in color for each category. Sonification was performed by mapping three qualities of the data (within-category variabil...|$|R
5000|$|<b>Visible</b> <b>Speech</b> (a phonetic script) — no {{specific}} language; developed {{to aid the}} deaf and teach them to speak properly ...|$|R
40|$|Baldi, a computer-animated talking head is introduced. The {{quality of}} his <b>visible</b> <b>speech</b> has been {{repeatedly}} modified and evaluated to accurately simulate naturally talking humans. Baldi's <b>visible</b> <b>speech</b> can be appropriately aligned with either synthesized or natural auditory speech. Baldi has had great success in teaching vocabulary and grammar to children with language challenges and training speech distinctions to children with hearing loss and to adults learning a new language. We demonstrate these learning programs and also demonstrate several other potential application areas for Baldi...|$|R
50|$|Additionally he {{was also}} the creator of <b>Visible</b> <b>Speech</b> which was used to help the deaf learn to talk, and was the father of Alexander Graham Bell.|$|R
50|$|Other alphabets, such as Hangul, {{may have}} their own phonetic extensions. There also exist {{featural}} phonetic transcription systems, such as Alexander Melville Bell's <b>Visible</b> <b>Speech</b> and its derivatives.|$|R
50|$|Massaro, DW. and Light, J. Using <b>visible</b> <b>speech</b> for {{training}} perception {{and production of}} speech for hard of hearing individuals. Journal of Speech, Language, and Hearing Research, 2004, 47(2), 304-320.|$|R
50|$|Modern {{phonetics}} {{begins with}} attempts—such {{as those of}} Joshua Steele (in Prosodia Rationalis, 1779) and Alexander Melville Bell (in <b>Visible</b> <b>Speech,</b> 1867)—to introduce systems of precise notation for speech sounds.|$|R
5000|$|... #Caption: Fig. 1. Bell learned {{acoustics}} {{from his}} father, Alexander Melville Bell, who created diagrams {{of how the}} human mouth formed consonants and vowels for his book on <b>Visible</b> <b>Speech.</b>|$|R
50|$|In 1924, Steiner gave two {{intensive}} {{workshops on}} {{different aspects of}} eurythmy; transcripts of his talks during these workshops are published as Eurythmy as <b>Visible</b> <b>Speech</b> and Eurythmy as Visible Singing.|$|R
40|$|Gestures and <b>visible</b> <b>speech</b> cues {{are often}} {{available}} to listeners to aid their {{comprehension of the}} speaker’s meaning. However, visual communication cues are not always beneficial over-and-above the audible speech cues. My goal is to outline several types of constraints which operate in the human cognitive processing system that bear on this question: When do visual language cues (<b>visible</b> <b>speech</b> and gestures) provide an aid to comprehension, and when do they not? Research on visual-spoken language comprehension carried out in my lab over recent years is described and recommendations will be made concerning the design of multi-modal interfaces...|$|R
50|$|Text In {{her first}} solo exhibition, Auerbach showed {{a series of}} text-based {{drawings}} that explored various linguistic systems including calligraphy, Morse code, semaphore signals, the Ugaritic alphabet and Alexander Melville Bell's <b>visible</b> <b>speech.</b>|$|R
40|$|WHO SAID AS THE FIRST? MORPHOLOGICAL DETERMINANTS OFHUMAN SPEECH. The {{evolution}} of human speech was probably involved into development of brain structures responsible for cultural behaviour, specially tool-making. Moreover, {{it has been}} sugested that only Homo erectus species had the <b>speech</b> <b>apparatus</b> formed the way which allowed to articulate some sounds...|$|R
50|$|Rogers {{served as}} the {{director}} at the Clarke School from 1867 to 1886. She worked at the school with Alexander Graham Bell, who implemented his father’s <b>Visible</b> <b>Speech</b> System to teach instructors in the oral method of teaching.|$|R
2500|$|Chief George Henry Martin Johnson (Onwanonsyshon) of the {{aboriginal}} Six Nations Mohawk Reserve, near Bell's home in Brantford, Ontario awarded him {{the title of}} Honorary Chief {{for his work in}} translating the unwritten Mohawk language into <b>Visible</b> <b>Speech</b> symbols (c. 1870); ...|$|R
5000|$|Melville's {{works on}} <b>Visible</b> <b>Speech</b> became highly notable, and were {{described}} by Édouard Séguin as being [...] "...a greater invention than the telephone by his son, Alexander Graham Bell". Melville saw numerous applications for his invention, including its worldwide {{use as a}} universal language. However, although heavily promoted at the Second International Congress on Education of the Deaf in Milan, Italy in 1880, {{after a period of}} a dozen years or so in which it was applied to the education of the deaf, <b>Visible</b> <b>Speech</b> was found to be more cumbersome, and thus a hindrance, to the teaching of speech to the deaf, compared to other methods, and eventually faded from use.|$|R
40|$|For speech {{perception}} {{and production of}} a new language, we examined whether 1) {{they would be more}} easily learned by ear and eye relative to by ear alone, and 2) whether viewing the tongue, palate, and velum during production is more beneficial for learning than a standard frontal view of the speaker. In addition, we determine whether differences in learning under these conditions are due to enhanced receptive learning from additional visual information, or to more active learning motivated by the visual presentations. Test stimuli were two similar vowels in Mandarin and two similar stop consonants in Arabic, presented in different word contexts. Participants were tested with auditory speech and were either trained 1) unimodally with just auditory speech or bimodally with both auditory and visual speech, and 2) a standard frontal view versus an inside view of the vocal tract. The visual speech was generated by the appropriate multilingual versions of Baldi [1]. The results test the effectiveness of <b>visible</b> <b>speech</b> for learning a new language. Preliminary results indicate that <b>visible</b> <b>speech</b> can contribute positively to acquiring new speech distinctions and promoting active learning. Index Terms: <b>visible</b> <b>speech</b> synthesis, pronunciation trainin...|$|R
50|$|The lips {{serve for}} {{creating}} different sounds—mainly labial, bilabial, and labiodental consonant sounds {{as well as}} vowel rounding—and thus {{are an important part}} of the <b>speech</b> <b>apparatus.</b> The lips enable whistling and the performing of wind instruments such as the trumpet, clarinet, flute, and saxophone. People who have hearing loss may unconsciously or consciously lip read to understand speech without needing to perceive the actual sounds.|$|R
2500|$|The Massachusetts Charitable Mechanic Association (a.k.a. the Association of the Mechanics of Boston) awarded two {{gold medals}} to Bell, as {{exhibitor}} #626 registered to the New England Telephone Company of Boston, MA, {{for both the}} telephone and <b>Visible</b> <b>Speech,</b> twinning {{the results of the}} Centennial Exposition held in Philadelphia two years earlier (1878); ...|$|R
40|$|Language {{processing}} {{is influenced}} by multiple sources of information. We examined whether the performance in simultaneous interpreting would be improved when providing two sources of information, the auditory speech as well as corresponding lip-movements, in comparison to presenting the auditory speech alone. Although there was an improvement in sentence recognition when presented with <b>visible</b> <b>speech,</b> {{there was no difference}} in performance between these two presentation conditions when bilinguals simultaneously interpreted from English to German or from English to Spanish. The reason why visual speech did not contribute to performance could be the presentation of the auditory signal without noise (Massaro, 1998). This hypothesis should be tested in the future. Furthermore, it should be investigated if an effect of <b>visible</b> <b>speech</b> can be found for other contexts, when visual information could provide cues for emotions, prosody, or syntax...|$|R
40|$|Proceedings of the 9 th International Conference on Auditory Display (ICAD), Boston, MA, July 7 - 9, 2003. We {{present a}} {{multimodal}} interactive data exploration tool that facilitates discrimination between <b>visible</b> <b>speech</b> tokens. The multimodal tool uses visualization and sonification (non-speech sound) of data. <b>Visible</b> <b>speech</b> tokens is {{a class of}} multidimensional data {{that have been used}} extensively in designing talking head that has been used in training of deaf individuals by watching <b>speech</b> [1]. <b>Visible</b> <b>speech</b> tokens (consonants), referred to as categories, differ along a set of pre-measured feature dimensions such as mouth height, mouth narrowing, jaw rotation and upper-lip retraction. The data set was visualized with a series of 1 D scatter-plots that differed in color for each category. Sonification was performed by mapping three qualities of the data (within-category variability, between category variability, and category identity) to three sound parameters (noise amplitude, duration, and pitch). An experiment was conducted to assess the utility of multimodal information compared to visual information alone for exploring this multidimensional data set. Tasks involved answering a series of questions to determine how well each feature or a set of features discriminate among categories, which categories are discriminated and how many. Performance was assessed by measuring accuracy and reaction time to 36 questions varying in scale of understanding and level of dimension integrality. Scale varied at three levels (ratio, ordinal, and nominal) and integrality also varied at three levels (1, 2, and 3 dimensions). A between-subjects design was used by assigning subjects to either the multimodal group or visual only group. Results show that accuracy is better for the multimodal group as the number of dimensions required to answer a question (integrality) increased. Also, accuracy was 10 % better for the multimodal group for ordinal questions. For discriminating <b>visible</b> <b>speech</b> tokens, sonification provides useful information in addition to that given by visualization, particularly for representing three dimensions simultaneously...|$|R
40|$|We {{describe}} {{attempts to}} synthesize <b>visible</b> <b>speech</b> in real-time on a Macintosh^TM personal computer, and {{to enable the}} user to color {{the text of the}} speech to be synthesized emotionally, according to the user's wishes, the representation of the text, or the semantics of the utterance. The animated <b>visible</b> <b>speech</b> will be demonstrated, in real time, using a variety of on-screen agents and faces. The speech synthesizer used is Apple Computer's MacinTalkPro 2 ®, running on a Quadra AV® computer. Introduction Researchers in disciplines as diverse as ethology, psychology, speech therapy, interpersonal communications and human-machine interface design agree that facial expressions enhance communication. Facial expressions convey both emotion (Ekman and Friesen, 1986) and other communicative signals (Chovil, 1991). A multimodal form of interaction with a computer on-screen agent is likely {{to improve the quality of}} the dialogue, both in terms of intelligibility and with regard to user-friendline [...] ...|$|R
40|$|Pathologies of the <b>speech</b> <b>apparatus</b> are a {{great concern}} in {{nowadays}} society, having experienced a large increase due to the persistence of certain social habits. The praecox diagnose of these pathologies may yield spectacular results {{in the treatment of}} serious diseases, avoiding more drastic interventions which can not always grant absolute cure. The possibility of screening a large number of patients whose speech, electroglotographic or endoscopic records could be sent through Internet to specialized care services from primary attention centres may open an important way for early diagnose and treatment. This project is oriented to develop and complement Diagnose-Aid tools with the possibilities offered by Internet to facilitate this kind of actions. A computer platform to record and store patient data for its transmission from primary attention centres to specialized care services is to be defined. A screening procedure based on the use of a Computer-Aided Diagnose System is to be designed, based on estimation of biomechanical parameters. Data gathered following the before mentioned procedures will be used in improving current models of the <b>speech</b> <b>apparatus</b> to optimise the Diagnose-Aid methods. The system is to be experimented in two hospitals of the Area of Madrid...|$|R
40|$|In this paper, we {{describe}} research {{to extend the}} capability of an existing talking head, Baldi, to be multilingual. We use parsimonious client/server architecture to impose autonomy in the functioning of an auditory speech module and a visual speech synthesis module. This scheme enables the implementation and the joint application of text-to-speech synthesis and facial animation in many languages simultaneously. Additional languages {{can be added to}} the system by defining a unique phoneme set and unique phoneme definitions for the <b>visible</b> <b>speech</b> for each language. The accuracy of these definitions is tested in perceptual experiments in which human observers identify auditory speech in noise presented alone or paired with the synthetic versus a comparable natural face. We illustrate the development of an Arabic talking head, Badr, and demonstrate how the empirical evaluation enabled the improvement of the <b>visible</b> <b>speech</b> synthesis from one version to another. Ó 2005 Elsevier B. V. All rights reserved...|$|R
5|$|Spoken {{language}} {{relies on}} human physical {{ability to produce}} sound, which is a longitudinal wave propagated through the air at a frequency capable of vibrating the ear drum. This ability depends on the physiology of the human speech organs. These organs consist of the lungs, the voice box (larynx), and the upper vocal tract– the throat, the mouth, and the nose. By controlling the {{different parts of the}} <b>speech</b> <b>apparatus,</b> the airstream can be manipulated to produce different speech sounds.|$|R
2500|$|A diphthong ( [...] or [...] from Greek: , diphthongos, {{literally}} [...] "two sounds" [...] or [...] "two tones"), {{also known}} as a gliding vowel, is a combination of two adjacent vowel sounds within the same syllable. Technically, a diphthong is a vowel with two different targets: that is, the tongue (and/or other parts of the <b>speech</b> <b>apparatus)</b> moves during the pronunciation of the vowel. In many dialects of English, the phrase no highway cowboys [...] has five distinct diphthongs, one in every syllable.|$|R
50|$|Spoken {{language}} {{relies on}} human physical {{ability to produce}} sound, which is a longitudinal wave propagated through the air at a frequency capable of vibrating the ear drum. This ability depends on the physiology of the human speech organs. These organs consist of the lungs, the voice box (larynx), and the upper vocal tract - the throat, the mouth, and the nose. By controlling the {{different parts of the}} <b>speech</b> <b>apparatus,</b> the airstream can be manipulated to produce different speech sounds.|$|R
25|$|The most {{prominent}} of constructed scripts may be Glagolitic, Korean Hangul and the International Phonetic Alphabet. Some, {{such as the}} Shavian alphabet, Quikscript, Alphabet 26, and the Deseret alphabet, were devised as English spelling reforms. Others, including Alexander Melville Bell's <b>Visible</b> <b>Speech</b> and John Malone's Unifon were developed for pedagogical use. Blissymbols were developed as a written international auxiliary language. Shorthand systems may be considered constructed scripts.|$|R
50|$|Specific {{language}} impairment (SLI) {{is diagnosed}} when a child's language does not develop normally {{and the difficulties}} cannot {{be accounted for by}} generally slow development, physical abnormality of the <b>speech</b> <b>apparatus,</b> autism spectrum disorder, acquired brain damage or hearing loss. Twin studies have shown that it is under geneticinfluence. Although language impairment can result from a single-gene mutation, this is unusual. More commonly SLI results from the combined influence of multiple genetic variants, each of which is found in the general population, as well as environmental influences.|$|R
40|$|We {{present a}} {{multimodal}} interactive data exploration tool that facilitates discrimination between <b>visible</b> <b>speech</b> tokens. The multimodal tool uses visualization and sonification (non-speech sound) of data. <b>Visible</b> <b>speech</b> tokens is {{a class of}} multidimensional data {{that have been used}} extensively in designing talking head that has been used in training of deaf individuals by watching <b>speech</b> [1]. <b>Visible</b> <b>speech</b> tokens (consonants), referred to as categories, differ along a set of pre-measured feature dimensions such as mouth height, mouth narrowing, jaw rotation and upper-lip retraction. The data set was visualized with a series of 1 D scatter-plots that differed in color for each category. Sonification was performed by mapping three qualities of the data (within-category variability, between category variability, and category identity) to three sound parameters (noise amplitude, duration, and pitch). An experiment was conducted to assess the utility of multimodal information compared to visual information alone for exploring this multidimensional data set. Tasks involved answering a series of questions to determine how well each feature or a set of features discriminate among categories, which categories are discriminated and how many. Performance was assessed by measuring accuracy and reaction time to 36 questions varying in scale of understanding and level of dimension integrality. Scale varied at three levels (ratio, ordinal, and nominal) and integrality also varied at three levels (1, 2, and 3 dimensions). A between-subjects design was used by assigning subjects to either the multimodal group or visual only group. Results show that accuracy is better for the multimodal group as the number of dimensions required to answer a question (integrality) increased. Also, accura [...] ...|$|R
