47|123|Public
5000|$|VDX (standing for <b>Virtual</b> <b>Document</b> eXchange) is a {{software}} product for interlibrary loan (ILL) and document request management. [...] VDX {{was developed by}} UK company Fretwell-Downing Informatics, a company which in 2005 {{was taken over by}} OCLC PICA, itself wholly acquired by OCLC Online Computer Library Center in 2007.|$|E
5000|$|From the CMIS Specification 1.1, page: [...] "... {{this data}} model {{does not cover}} all the {{concepts}} that a full-function ECM repository ... transient entities (such as programming interface objects), administrative entities (such as user profiles), and extended concepts (such as compound or <b>virtual</b> <b>document,</b> work flow and business process, event and subscription) are not included." ...|$|E
5000|$|Keynote {{is based}} on the tree data {{structure}} concept, allowing [...] "nodes" [...] in a tree panel (much like the tree panel in Windows file managers) to represent separate fields within a single text file. Import and export of KeyNote files to and from TreePad files is fully supported. Each field is then treated as a separate <b>virtual</b> <b>document</b> within the tree. Individual documents within the tree can be edited in Rich Text Format.|$|E
40|$|This paper {{explores the}} issue of {{publishing}} information under the form of <b>Virtual</b> <b>Documents</b> that include data and fragments of documents from remote sources, especially from databases and SGML Documents databases. Because they are dynamically generated, <b>Virtual</b> <b>Documents</b> are automatically updated when the source information changes, and they allow for sharing, reusing and adapting information for various contexts. The paper focuses on implementation of <b>virtual</b> <b>documents</b> from the authoring and architecture perspective...|$|R
40|$|Information Retrieval {{systems such}} as web search engines offer {{convenient}} keyword-based search interfaces. In contrast, relational database systems require the user to learn SQL and to know the schema of the underlying data even to pose simple searches. We propose an architecture that supports highly efficient keyword-based search over relational databases: A relational database is ”crawled ” in advance, text-indexing <b>virtual</b> <b>documents</b> that correspond to interconnected database content. At query time, the text index supports keyword-based searches with interactive response, identifying database objects corresponding to the <b>virtual</b> <b>documents</b> matching the query. Our system, EKSO, creates <b>virtual</b> <b>documents</b> from joining relational tuples and uses the DB 2 Net Search Extender for indexing and keyword-search processing. Experimental results show that index size is manageable and database updates (which are propagated incrementally as recomputed <b>virtual</b> <b>documents</b> to the text index) do not significantly hinder query performance. We also present a user study confirming the superiority of keyword-based search over SQL {{for a range of}} database retrieval tasks. 1...|$|R
40|$|With the {{increasing}} importance of Web publishing, {{there has been}} considerable interest {{in the production of}} <b>virtual</b> <b>documents</b> on demand. The bulk of this work has used existing documents annotated with meta-data as a source. We suggest that more exibility and functionality can be obtained if <b>virtual</b> <b>documents</b> are generated instead from raw data. This capability can be achieved by using natural language generation techniques. In this paper, we describe a project concerned with automatically generating natural language descriptions of museum artefacts directly from a museum's Collection Information System...|$|R
40|$|Accurate {{web page}} {{classification}} often depends crucially on information gained from neighboring {{pages in the}} local web graph. Prior work has exploited the class labels of nearby pages to improve performance. In contrast, in this work we utilize a weighted combination {{of the contents of}} neighbors to generate a better <b>virtual</b> <b>document</b> for classification. In addition, we break pages into fields, finding that a weighted combination of text from the target and fields of neighboring pages is able to reduce classification error by more than a third. We demonstrate performance on a large dataset of pages from the Open Directory Project and validate the approach using pages from a crawl from the Stanford WebBase. Interestingly, we find no value in anchor text and unexpected value in page titles (and especially titles of parent pages) in the <b>virtual</b> <b>document...</b>|$|E
40|$|We {{provide a}} {{framework}} that allows one to study structural properties of hypertext in connection with formal language theory. We model hypertext as a transformation device (an atransducer) that transforms a link-following into a sequence of matched pairs: basic linkable units. Then, we address the following questions: What can hypertext do? What structure is formed when a link-following is done? What structure is built when a <b>virtual</b> <b>document</b> is constructed? We show that the set of all link-followingsin hypertext is a regular set. Then, the set of all possible outputs of link-followings is shown to be context-free, which means that constructing virtual documents is essentially same as generating words of a contextfree language. KEYWORDS: Hypertext structure, Dexter model, regular set, context-free language, Hypertext models, link-following, atransducer, <b>virtual</b> <b>document.</b> INTRODUCTION Structure information in hypertext is important in many ways. One can make use of underlying hyperte [...] ...|$|E
40|$|Colloque avec actes et comité de lecture. internationale. /[URL] audienceDocument {{evolution}} is usually performed {{by creating a}} new document which explicitly details changes to specific paragraphs inside other documents content. Obtaining (<b>virtual)</b> <b>document</b> versions corresponding to its tsate at a specific date is left to the document users, who manually extract fromlibrary collections, and compose, the pieces of text to obtain teh desired version. But {{this can be a}} very tedious and difficult task when changes are numerous. We propose a solution to dynamically generate <b>virtual</b> <b>document</b> versions on user demand, respecting the library documents integrity. REFernecs to other documents and modification relationships can be automatically detected and are modelled as typed links (XLink) in a relationship graph. In this paper, we focus on the version generation process,consisting in a dynamic document composition based on a graph exploration. This solution has already shown its adequacy with a legislative digital library...|$|E
40|$|In keyword search over data graphs, {{an answer}} is a {{non-redundant}} subtree that contains the keywords of the query. Ranking of answers should take into account both their tex-tual relevance and the significance of their semantic struc-ture. A novel method for answers priors is developed and {{used in conjunction with}} query-dependent features. Since the space of all possible answers is huge, efficiency is also a major problem. A new algorithm that drastically cuts down the search space is presented. It generates candidate an-swers by first selecting top-n roots and top-n nodes for each query keyword. The selection is by means of a novel con-cept of <b>virtual</b> <b>documents</b> with weighted term frequencies. Markov random field models are used for ranking the <b>virtual</b> <b>documents</b> and then the generated answers. The proposed approach outperforms existing systems on a standard eval-uation framework...|$|R
40|$|In this report, {{we present}} a {{language}} for producing <b>virtual</b> <b>documents,</b> where dynamic information objects can be retrieved from various sources, transformed, and included along with static information in SGML documents. The language uses a tree-like structure for the representation of information objects, and allows querying without a complete knowledge of the structure or the types of information...|$|R
40|$|Many {{analytical}} or computational applications create {{documents and}} display screens {{in response to}} user queries “dy-namically ” or in “real time”. Hypermedia features must be generated “just in time ” – automatically and dynamically. Conversely, the hypermedia features may have to cause target documents to be generated or re-generated. This paper presents a just-in-time hypermedia framework to provide hypermedia support of <b>virtual</b> <b>documents...</b>|$|R
40|$|This paper {{explores the}} issue of {{representing}} textual information {{in the form of}} virtual documents that include data and fragments of documents from remote sources — especially from databases and SGML document databases. Virtual documents are dynamically generated, and therefore always present up-todate information when they are instantiated [...] The benefit of this paradigm is that it allows information to be shared, reused, and adapted for various contexts. A <b>Virtual</b> <b>document</b> specification defines how to find and retrieve information objects from databases or from existing documents, and how to assemble it into another document. Virtual documents can be used to create HTML pages that contain information from one or several remote or local databases, to assembly parts of existing documents into a new one, or to define various views of the same information according to various needs. This paper focuses on the prototype implementation of virtual documents from the perspectives of document authoring and architecture. We propose an SGML syntax for Information Object that includes OQL-queries for retrieving fragments of existing documents, transformations on an intermediate tree representation, and output mapping to the <b>virtual</b> <b>document</b> structures...|$|E
40|$|A {{major issue}} in many domains is to present {{information}} to people that is tailored to their need, {{in such a way}} that it supports them in their tasks. In this paper, we present the <b>Virtual</b> <b>Document</b> Planner (VDP), a platform we developed for generating tailored interactive multimedia presentations in the surveillance domain. Integrated with the surveillance operators ’ graphical interface, the VDP provides tailored information delivery mechanisms that adapt the operators ’ information rich environment to their tasks and information needs...|$|E
40|$|In {{this paper}} we present Taylor, {{a search engine}} {{designed}} to reduce the effort required by a user for the perusal of documents. This is accomplished by delivering document passages to the user, rather than a list of pointers to documents. We discussed our strategy, called extraction/gathering, where passages are extracted according to the query and the document structure, and gathered and delivered using <b>virtual</b> <b>document</b> technology. We describe Taylor {{and one of its}} application to a collection of Java-related documents...|$|E
40|$|The DelaunayMM {{project is}} {{concerned}} with creating a customizable layout-driven approach to querying multimedia digital libraries. DelaunayMM layouts can be fully specified from scratch by a layout designer to accommodate different classes of users and tasks. In this paper, we overview the main aspects of the DelaunayMM project and present our approach to the layout specification and enhancement of <b>virtual</b> <b>documents...</b>|$|R
5000|$|XQuery 3.1 {{became a}} W3C Recommendation on March 21, 2017. [...] "The {{mission of the}} XML Query project is to provide {{flexible}} query facilities to extract data from real and <b>virtual</b> <b>documents</b> on the World Wide Web, therefore finally providing the needed interaction between the Web world and the database world. Ultimately, collections of XML files will be accessed like databases".|$|R
40|$|Web ontologies are {{structural}} {{frameworks for}} organizing information in semantics web and provide shared concepts. Ontology formally represents knowledge or information about particular entity {{as a set}} of concepts within a particular domain on semantic web. Web ontology helps to describe concepts within domain and also help us to enables semantic interoperability between two different applications byusing Falcons concept search. We can facilitate concept searching and ontologies reusing. Constructing <b>virtual</b> <b>documents</b> is a keyword based search in ontology. The proposed method helps us to find how search engine help user to find out ontologies in less time so we can satisfy their needs. It include some supportive technologies with new technique is to constructing <b>virtual</b> <b>documents</b> of concepts for keywordbased search and based on population scheme we rank the concept and ontologies, a way to generate structured snippets according to query. In this concept we can report the user feedback and usabilityevolution...|$|R
40|$|We {{describe}} the automatic transformation {{of a traditional}} electronic document into a augmented, <b>virtual</b> <b>document.</b> After converting the content into a “small-scale ” hyperbook structure with an ontology and textual fragments, we calculate semantic similarity relations between the concepts of this hyperbook and a reference hyperbook. We finally rebuild the document by involving the retrieved hyperlinks. The aim {{is to show that}} the integration process also works without a highly detailed ontological structure of the source document. Categories and Subject Descriptor...|$|E
40|$|Abstract. In {{this paper}} {{we present a}} model and an {{adaptation}} architecture for context-aware multimodal documents. A compound <b>virtual</b> <b>document</b> describes the {{different ways in which}} multimodal information can be structured and presented. Physical features are associated to media instances, while properties describe the context. Concrete documents are instantiated from virtual documents by selecting and synchronizing proper media instances based on the user context: the situation, the environment, the device and the available communication resources. The relations between the context features and the media properties are described by a rule based system...|$|E
40|$|Digital {{libraries}} {{are often}} viewed {{as a collection of}} documents from which users can retrieve documents for their needs. As an extension to this conventional notion of digital libraries, we introduce a new document architecture where links {{play a key role in}} helping users create knowledge. It allows for easy creation of a new composite document, called a <b>virtual</b> <b>document,</b> whose parts can be geographically dispersed. This paper describes the concept of virtual documents and a digital library system architecture geared toward applications where an essential element is teacher-student interactions through an exchange of knowledge in the form of multimedia documents. 1...|$|E
40|$|We {{present a}} {{framework}} for the design and management of multimodal <b>document</b> databases: <b>virtual</b> <b>documents</b> describe the dierent facets under which informa-tion can be delivered and presented, in terms of media, content, and presentation format. Concrete documents are composed and instantiated from virtual ones by aggregating and synchronizing media instances based on the user context, i. e., the situation, the environment, the devices and communication resources available. 1...|$|R
40|$|Abstract Ontology mapping is {{a crucial}} task for the facil-itation of {{information}} exchange and data integration. A mapping system can {{use a variety of}} similarity measures to determine concept correspondences. This paper proposes the integration of word-sense disambiguation techniques into lexical similarity measures. We propose a disambiguation methodology which entails the creation of <b>virtual</b> <b>documents</b> from concept and sense definitions, including their neigh-bourhoods. The specific terms are weighted according to their origin within their respective ontology. The document simi-larities between the concept document and sense documents are used to disambiguate the concept meanings. First, we evaluate to what extent the proposed disambiguation method can improve the performance of a lexical similarity metric. We observe that the disambiguation method improves the performance of each tested lexical similarity metric. Next, we demonstrate the potential of a mapping system utilizing the proposed approach through the comparison with con-temporary ontology mapping systems. We observe a high performance on a real-world data set. Finally, we evaluate how the application of several term-weighting techniques on the <b>virtual</b> <b>documents</b> can affect the quality of the gen-erated alignments. Here, we observe that weighting terms according to their ontology origin leads to the highest per-formance...|$|R
40|$|Abstract. We present ToX – the Toronto XML Engine – a {{repository}} for XML data and metadata, which supports real and <b>virtual</b> XML <b>documents.</b> Real documents are stored as files or mapped into relational or object databases, {{depending on their}} structuredness; indices are defined according to the storage method used. <b>Virtual</b> <b>documents</b> can be remote documents, defined as arbitrary WebOQL queries, or views, defined as queries over documents registered in the system. The system catalog contains metadata for the documents, especially their schemata, used for query processing and optimization. Queries can range over both the catalog and the documents, and multiple query languages are supported. In this paper we describe the architecture and main of ToX; we present our indexing and storage strategies, including two novel techniques; and we discuss our query processing strategy. The project started recently and is under active development. ...|$|R
40|$|Abstract. Adaptation/personalization {{is one of}} {{the main}} issues for web services. Adaptive web {{applications}} have the ability to deal with different users’ needs for enhancing usability and comprehension and for dealing with large repositories. We propose an open-ended adaptive hypermedia environment which is based on the <b>virtual</b> <b>document</b> and semantic web approaches and which is able to manage adaptive techniques at knowledge level. In this paper, we have focused on the way to specify and to manage adaptation in this environment. We propose an approach which is based on a unique evaluation principle of links/contents per document and where the author may assign user stereotypes to adaptive techniques...|$|E
40|$|Building {{public trust}} and {{confidence}} through openness is a goal of the DOE Carlsbad Field Office for the Waste Isolation Pilot Plant (WIPP). The objective of the <b>virtual</b> <b>document</b> described in {{this paper is to}} give the public an overview of the waste characterization steps, an understanding of how waste characterization instrumentation works, and the type and amount of data generated from a batch of drums. The document is intended to be published on a web page and/or distributed at public meetings on CDs. Users may gain as much information as they desire regarding the transuranic (TRU) waste characterization program, starting at the highest level requirements (drivers) and progressing to more and more detail regarding how the requirements are met. Included are links to: drivers (which include laws, permits and DOE Orders); various characterization steps required for transportation and disposal under WIPP's Hazardous Waste Facility Permit; physical/chemical basis for each characterization method; types of data produced; and quality assurance process that accompanies each measurement. Examples of each type of characterization method in use across the DOE complex are included. The original skeleton of the document was constructed in a PowerPoint presentation and included descriptions of each section of the waste characterization program. This original document had a brief overview of Acceptable Knowledge, Non-Destructive Examination, Non-Destructive Assay, Small Quantity sites, and the National Certification Team. A student intern was assigned the project of converting the document to a virtual format and to discuss each subject in depth. The resulting product is a fully functional <b>virtual</b> <b>document</b> that works in a web browser and functions like a web page. All documents that were referenced, linked to, or associated, are included on the virtual document's CD. WIPP has been engaged in a variety of Hazardous Waste Facility Permit modification activities. During the public meetings, discussion centered on proposed changes to the characterization program. The philosophy behind the <b>virtual</b> <b>document</b> is to show the characterization process as a whole, rather than as isolated parts. In addition to public meetings, other uses for the information might be as a training tool for new employees at the WIPP facility to show them where their activities fit into the overall scheme, as well as an employee review to help prepare for waste certification audits...|$|E
40|$|Abstract—Web ontologies provide shared {{concepts}} {{for describing}} do-main entities and thus enable semantic interoperability between applica-tions. To facilitate concept sharing and ontology reusing, we developed Falcons Concept Search, a novel keyword-based ontology search engine. In this paper, we illustrate how the proposed mode of interaction helps users quickly find ontologies that satisfy {{their needs and}} present several supportive techniques including a new method of constructing virtual documents of concepts for keyword search, a popularity-based scheme to rank concepts and ontologies, {{and a way to}} generate query-relevant structured snippets. We also report the results of a usability evaluation as well as user feedback. Index Terms—Indexing, ontology ranking, ontology search, snippet gen-eration, <b>virtual</b> <b>document.</b> I...|$|E
40|$|XML {{applications}} are developing rapidly. There is {{an urgent need}} for XML repositories to store XML documents, to retrieve and restructure them. This paper studies some of the requirements for such XML repository, i. e. the requirements for a data model, a query language and an architecture. We discuss various models and propose using Ozone, a model developed at Stanford, and an extension for managing links. We also present an architecture that couples smoothly the repository with a cache for managing external or <b>virtual</b> <b>documents...</b>|$|R
40|$|As legal {{information}} is processed by computer systems, computation is increasingly {{contributing to the}} functioning of legal systems. Consequently, legal informatics has a large role to play, providing theories and models supporting computable law, as well as critical reflections on it. Indeed, in recent decades research on legal informatics has expanded to address multiple operations and aspects of the law, including text retrieval, <b>virtual</b> <b>documents,</b> the deductive derivation of legal conclusions, conceptual structures, argumentation, case-based reasoning, interpretation, dialogues, theory construction, adaptive attitudes, and institutions...|$|R
40|$|Computer systems {{should provide}} what you want, {{when you want}} it (the WYWWYWI Principle, {{pronounced}} "why why why"), but they frequently do not. Our research encourages a new philosophy of design based on the WYWWYWI principle, and the tools for authors to provide this easily. Comprehensive metainformation embodies the WYWWYWI principle. Metainformation includes the structural relationships, content-based relationships, user-declared link-based relationships, and metadata around an element of interest. Combined, the metainformation {{goes a long way}} towards establishing the full semantics for (the meaning of and context around) a system's elements. We take a three-pronged approach to providing metainformation on a grand scale. First, we provide a systematic methodology for systems analysts to determine the relationships around elements of interest in their information domains [...] -Relationship Analysis. Relationship Analysis will result in a comprehensive set of a domain's structural relationships. Second, we provide a Metainformation Engine, which automatically generates sets of structural and content-based relationships around elements of interest as links, as well as metadata within static and <b>virtual</b> <b>documents.</b> Third, we provide an infrastructure for widespread link-based services within both static and <b>virtual</b> <b>documents.</b> This approach provides the inspiration as well as a sound foundation for a ubiquitous embracing of the WYWWYWI principle in the everyday systems people use, both on the Web and beyond...|$|R
40|$|Abstract. The {{importance}} of reuse {{of information is}} well recognised for electronic publishing. However, it is rarely achieved satisfactorily because {{of the complexity of}} the task: integrating different formats, handling updates of information, addressing document author’s need for intuitiveness and simplicity, etc. An approach which addresses these problems is to dynamically generate and update documents through a descriptive definition of virtual documents. In this paper we present a document interpreter that allows gathering information from multiple sources, and combining it dynamically to produce a <b>virtual</b> <b>document.</b> Two strengths of our approach are: the generic information objects that we use, which enables access to distributed, heterogeneous data sources; and the interpreter’s evaluation strategy, which permits a minimum of re-evaluation of the information objects from the data sources...|$|E
40|$|Rule {{evolution}} is usually performed {{by creating a}} new document which explicitly details changes to specic parts inside other rules's content. Obtaining (<b>virtual)</b> <b>document</b> versions corresponding to a rules's state at a specic date is thus left to document users, who manually extract from library collections, and compose, the pieces of text needed to obtain the desired version. When changes are numerous {{this can be a}} tedious task. We propose a solution to dynamically generate virtual rule versions on user demand, respecting the library documents integrity. References to other documents and modication relationships can be automatically detected and are modelled as typed links modelled with XLink in a relationships graph. This graph can be used to query relationships, to create hypertext, and to dynamically generate rule versions...|$|E
40|$|To {{support a}} {{pedagogical}} approach {{based on the}} collaborative writing of hyperbooks, we have developed several hyperbook models and systems based on the <b>virtual</b> <b>document</b> idea. In these hyperbooks, domain ontologies are used to organize the information contents and to infer interface documents. This article first presents our hyperbook models, together with observations made when using them with student. Then we discuss some of the lessons we have drawn {{about the use of}} ontologies in these environments. These lessons concern both the engineering of hyperbooks and the creation of domain ontologies. Finally we show how to use ontologies to extend the hyperbook concept to take into account several points of view, to integrate hyperbooks into a library of hyperbooks, to personalize hyperbooks, and to generate documentation about a topic...|$|E
40|$|Abstract Digital library {{systems and}} other {{analytic}} or computational applications create documents and display screens {{in response to}} user queries “dynamically ” or in “real time. ” These “virtual documents ” do not exist in advance, and thus hypermedia features (links, comments, and bookmark anchors) must be generated “just in time”—automatically and dynamically. In addition, accessing the hypermedia features may cause target documents to be generated or regenerated. This article describes the specific challenges for <b>virtual</b> <b>documents</b> and dynamic hypermedia functionality: dynamic regeneration, and dynamic anchor re-identification and re-location. It presents Just-in-time Hypermedia Engine to support just-in-time hypermedia across digital library an...|$|R
40|$|This {{communication}} introduces DoMDL, {{a powerful}} and flexible document model capable to represent multi-edition, structured, multimedia documents that can be disseminated in multiple manifestation formats. This model also allows any document {{to be associated with}} multiple metadata descriptions in different formats and to include semantic relationships with other documents and parts of them. The paper discusses also how the OpenDLib Digital Library Management System exploits this model to abstract from the specific organization and structure of the documents that are imported from different heterogeneous information sources in order to provide <b>virtual</b> <b>documents</b> that fulfill the needs of the different DL user communities...|$|R
40|$|This paper {{describes}} the VideoSTAR experimental database {{system that is}} being designed to support video applications in sharing and reusing video data and meta-data. VideoSTAR provides four di erent repositories: for media les, <b>virtual</b> <b>documents,</b> video structures, and video annotations/user indexes. It also provides a generic video data model relating data in the di erent repositories to each other, and it o ers a powerful application interface. VideoSTAR concepts have been evaluated by developing a number of experimental video tools, such as a video player, a video annotator, a video authoring tool, a video structure and contents browser, and a video query tool. ...|$|R
