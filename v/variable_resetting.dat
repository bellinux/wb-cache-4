1|29|Public
40|$|Cratonic eclogites and garnet pyroxenites {{from the}} Kaapvaal craton have {{heterogeneous}} Hf-Nd-Sr-(O) isotope ratios that define a positive Hf-Nd isotope array and a negative Nd-Sr isotope array. Isotopic variability encompasses depleted (mid-ocean ridge basalt and ocean-island basalt) to enriched mantle compositions (Group I and II kimberlites) and overlaps {{with that of}} the Kaapvaal craton garnet peridotite xenoliths. Isotopic heterogeneity at Roberts Victor is less extreme than previously reported and ranges from eclogites with a highly depleted MORB-like signature to enriched eclogites similar to Group II and transitional kimberlites and Group II megacrysts (εHf=- 32 · 8). Much of this similarity may well be due to partial or complete resetting during entrainment. For the majority of eclogites and garnet pyroxenites the Lu-Hf system records 'older' mantle events than the Sm-Nd system, but neither necessarily records the protolith age. Both the Lu-Hf and the Sm-Nd systems are prone to being reset by entrainment in high-temperature kimberlite and/or basaltic magmas (e. g. Kaapvaal) and emplacement in orogenic belts (e. g. Beni Bousera). In the case of one eclogite from Roberts Victor the Sm-Nd cpx-gt mineral isochron age (963 · 1 ± 42 · 3 Ma) differs from the Lu-Hf cpx-gt mineral isochron age (1953 ± 13 Ma) by 1 Ga and the Rb-Sr clinopyroxene model age (3 · 15 Ga) is 1 Ga older than the Lu-Hf age and the reconstructed whole-rock isochron age. Ironically, it may be that, in this instance, the Rb-Sr system gives a better indication of protolith age than Sm-Nd or Lu-Hf. Overall <b>variable</b> <b>resetting</b> of isotope systems between protolith formation in the Archaean (> 2 · 5 Ga) and kimberlite and/or basalt entrainment (≤ 0 · 2 Ga) masks our understanding of the exact protolith age of eclogites. 23 page(s...|$|E
40|$|By {{establishing}} a relation between information erasure and continuous phase transitions we generalise the Landauer bound to analog computing systems. The entropy production per {{degree of freedom}} during erasure of an analog <b>variable</b> (<b>reset</b> to standard value) is given by the logarithm of the configurational volume measured in units of its minimal quantum. As a consequence every computation has to be carried on with {{a finite number of}} bits and infinite precision is forbidden by the fundamental laws of physics, since it would require an infinite amount of energy. Comment: 5 pages, no figures, to appear in Phys. Rev. ...|$|R
30|$|Upon {{receiving}} messages, {{the system}} will update the buffer table. At {{the beginning of a}} communication cycle, all <b>variables</b> <b>reset</b> to their initial value or ϕ. Upon receiving a message, if the variable’s current history information contains ϕ in the buffer table, then {{the system will}} use the C-State-based CRC method to validate the transmitted message before updating the associated variable entry in the buffer table. On the other hand, if the variable in the buffer table has already been initialized or updated, then the system will use the variable’s history to check state inconsistency. The history-based approach performs better than the C-State-based approach [22].|$|R
40|$|Neurons encode {{information}} and communicate via action potentials, which are generated following the summation of synaptic events. It is commonly assumed that action potentials reset the membrane potential completely, allowing {{another round of}} synaptic integration to begin. We show here that the con-ductances underlying the action potential act instead as a <b>variable</b> <b>reset</b> of synaptic integration. The strength of this reset is cell typeÐspeciÞc and depends on the kinetics, location, and timing of the synaptic input. As a consequence, distal synapses, as well as inputs mediated by N-methyl-D-aspartate receptor activation, can contribute disproportionately to synaptic integration during action potential Þring. Most neurons fire action potentials (APs) continuously in vivo. As a consequence, APs and synaptic potentials are constantly inter-acting during normal brain function. The pi-oneering work of Eccles and colleagues i...|$|R
40|$|AbstractData-flow {{analysis}} to identify “dead” <b>variables</b> and <b>reset</b> {{them to an}} “undefined” value is an effective technique for fighting state explosion in the enumerative verification of concurrent systems. Although this technique is well-adapted to imperative languages, it is not directly applicable to value-passing process algebras, in which <b>variables</b> cannot be <b>reset</b> explicitly due to the single-assignment constraints of the functional programming style. This paper addresses this problem by performing data-flow analysis on an intermediate model (Petri nets extended with state variables) into which process algebra specifications can be translated automatically. It also addresses important issues such as avoiding the introduction of useless reset operations and handling shared read-only variables that child processes inherit from their parents...|$|R
25|$|At a restart marker, block-to-block {{predictor}} <b>variables</b> are <b>reset,</b> and the bitstream is synchronized to a byte boundary. Restart markers provide {{means for}} recovery after bitstream error, such as transmission over an unreliable network or file corruption. Since the runs of macroblocks between restart markers may be independently decoded, these runs may be decoded in parallel.|$|R
40|$|In {{this paper}} we {{describe}} a new tool, SReach, which solves probabilistic bounded reachability problems for {{two classes of}} stochastic hybrid systems. The first one is (nonlinear) hybrid automata with parametric uncertainty. The second one is probabilistic hybrid automata with additional randomness for both transition probabilities and <b>variable</b> <b>resets.</b> Standard approaches to reachability problems for linear hybrid systems require numerical solutions for large optimization problems, and become infeasible for systems involving both nonlinear dynamics over the reals and stochasticity. Our approach encodes stochastic information by using random variables, and combines the randomized sampling, a δ-complete decision procedure, and statistical tests. SReach utilizes the δ-complete decision procedure to solve reachability problems in a sound manner, i. e., it always decides correctly if, for a given assignment to all random variables, the system actually reaches the unsafe region. The statistical tests adapted guarantee arbitrary small error bounds between probabilities estimated by SReach and real ones. Compared to standard simulation-based methods, our approach supports non-deterministic branching, increases the coverage of simulation, and avoids the zero-crossing problem. We demonstrate our method's feasibility by applying SReach to three representative biological models and to additional benchmarks for nonlinear hybrid systems with multiple probabilistic system parameters...|$|R
30|$|Similar to Quartz programs, the AIF {{description}} adds {{an implicit}} default reaction: if no action has determined {{the value of}} the variable in the current macro step, then a variable either gets a default value or stores its previous value, depending on the declaration of the variable (obviously, this is the case if the guards of all immediate assignments in the current step and the guards of all delayed assignments in the preceding step of a variable are evaluated to false). Thereby, event <b>variables</b> are <b>reset</b> to a default value while memorized variables store their value of the previous step.|$|R
40|$|Abstract. The paper {{shows the}} decidability of the {{reachability}} problem for pla-nar, monotonic, linear hybrid automata without resets. These automata are a spe-cial class of linear hybrid automata {{with only two}} variables, whose flows in all states is monotonic along some direction in the plane, and in which the continu-ous <b>variables</b> are not <b>reset</b> on a discrete transition. ...|$|R
40|$|Abstract — This paper {{proposes a}} novel {{threshold}} control approach for creating multi-scroll chaotic attractors. The general jerk circuit {{is used as}} an example to show the working principle of this method. The controlled jerk circuit can emerge various limit cycles and n−scroll chaotic attractors by adjusting the upper threshold, lower threshold, and the width of inner saturated plateau. The dynamical mechanism of the threshold control is then further explored by analyzing the system dynamical behaviors. In particular, this method is effective and simple to implement since we only need monitor a single state <b>variable</b> and <b>reset</b> it if it exceeds the thresholds. It indicates the potential engineering applications for various chaos-based information systems. I...|$|R
40|$|In this work, {{we present}} a {{numerical}} approach for simulating wave propagation in unbounded domains which combines discontinuous Galerkin methods with arbitrary high order time integration (ADER-DG) and a stabilized modification of perfectly matched layers (PML). Here, the ADER-DG method is applied to Bérenger’s formulation of PML. The instabilities caused by the original PML formulation are treated by a fractional step method that allows to monitor whether waves are damped in PML region. In grid cells where waves are amplified by the PML, the contribution of damping terms is neglected and auxiliary <b>variables</b> are <b>reset.</b> Results of 2 D simulations in acoustic media with constant and discontinuous material parameters are presented to illustrate {{the performance of the}} method...|$|R
40|$|The paper {{shows the}} decidability of the {{reachability}} problem for planar, monotonic, linear hybrid automata without resets. These automata are a special class of linear hybrid automata {{with only two}} variables, whose flows in all states is monotonic along some direction in the plane, and in which the continuous <b>variables</b> are not <b>reset</b> on a discrete transition. We also prove the undecidability for the same class of automata in 4 -dimensions...|$|R
40|$|This paper {{reports on}} how an {{electrical}} {{model for the}} core of the Arm II machine was created and how to use this model. We wanted to get a model for the electrical characteristics of the ARM II core, in order to simulate this machine and to assist in the design of a future machine. We wanted this model to be able to simulate saturation, <b>variable</b> loss, and <b>reset.</b> Using the Hodgdon model and the circuit analysis program MicroCap IV, this was accomplished. This paper is written {{in such a way as}} to allow someone not familiar with the project to understand it...|$|R
40|$|This paper {{addresses}} {{the requirements for}} wide dynamic range cameras in automotive machine vision applications. The paper describes the architecture of a CMOS image sensor pixel and explores mechanisms for overcoming the effects of noise and saturation to achieve wide dynamic range. The linear response curve of a standard pixel is used to illustrate the effect of integration time on dynamic range. Alternative methods are presented for expanding dynamic range of image sensors, including multiple integration and logarithmic response. Each method is shown to have significant disadvantages. Autobrite ® proprietary technology from Sensata Technologies is described. An explanation of <b>variable</b> height/multiple <b>reset</b> and barrier voltage demonstrates how Autobrite offers superior interscene and intrascene dynamic range. A description of the control mechanism shows how Autobrite provides adaptive and programmable wide dynamic range. The paper concludes that only cameras equipped with Autobrite match the dynamic range of the image sensor to both the scene and the application. Autobrite-enabled cameras offer superior wide dynamic range performance for capturing complete and accurate visua...|$|R
40|$|The RESET {{test for}} {{functional}} misspecification is generalised to cover systems of equations, and {{the properties of}} 7 versions are studied using Monte Carlo methods. The Rao F -test clearly exhibits the best performance as regards correct size, whilst the commonly used LRT (uncorrected for degrees-of-freedom), and LM and Wald tests (both corrected and uncorrected) behave badly even in single equations. The Rao test exhibits correct size even in ten equation systems, which is better than previous research concerning autocorrelation tests. The power of the test is low, however, {{when the number of}} equations grows and the correlation between the omitted <b>variables</b> and the <b>RESET</b> proxies is small...|$|R
40|$|Traditional mutual {{exclusion}} locks are not resilient to failures: {{if there is}} a power outage, the memory is wiped out. Thus, when the system comes back on, the lock will have to be restored to the initial state, i. e., all processes are rolled back to the Remainder section and all <b>variables</b> are <b>reset</b> to their initial values. Recently, Golab and Ramaraju showed that we can improve this state of the art by exploiting the Non-Volatile RAM (NVRAM). They designed algorithms that, by maintaining shared variables in NVRAM, allow processes to recover from crashes on their own without a need for a global reset, even though a crash can wipe out the local memory of a process. We present a Recoverable Mutual Exclusion algorithm using the commonly supported CAS primitive. The main features of our algorithm are that it satisfies FCFS, it ensures that each process recovers in a wait-free manner, {{and in the absence of}} failures, it guarantees a worst-case Remote Memory Reference (RMR) complexity of O(lg n) on both Cache Coherent (CC) and Distributed Shared Memory (DSM) machines, where n is the number of processes for which the algorithm is designed. This bound matches the Omega(lg n) RMR lower bound by Attiya, Hendler, and Woelfel for Mutual Exclusion algorithms that use comparison primitives...|$|R
40|$|The {{size and}} power of various {{generalization}} of the RESET test for functional misspecification are investigated, using the “Bootsrap critical values”, in systems ranging from one to ten equations. The properties of 8 versions of the test are studied using Monte Carlo methods. The results are then compared with another study of Shukur and Edgerton (2002), in which they used the asymptotic critical values instead and found that in general only one version of the tests works well regarding size properties. In our study, when applying the bootstrap critical values, we find that all the tests exhibits correct size even in large systems. The power of the test is low, however, {{when the number of}} equations grows and the correlation between the omitted <b>variables</b> and the <b>RESET</b> proxies is small...|$|R
40|$|International audienceIn {{a series}} of two papers, we {{investigate}} the mechanisms by which complex oscillations are generated {{in a class of}} nonlinear dynamical systems with resets modeling the voltage and adaptation of neurons. This first paper presents mathematical analysis showing that the system can support bursts of any period as a function of model parameters, and that these are organized in a period-incrementing structure. In continuous dynamical systems with resets, such structures are complex to analyze. In the present context, we use the fact that bursting patterns correspond to periodic orbits of the adaptation map that governs the sequence of values of the adaptation <b>variable</b> at the <b>resets.</b> Using a slow-fast approach, we show that this map converges towards a piecewise linear discontinuous map whose orbits are exactly characterized. That map shows a period-incrementing structure with instantaneous transitions. We show that the period-incrementing structure persists for the full system with non-constant adaptation, but the transitions are more complex. We investigate the presence of chaos at the transitions...|$|R
40|$|In {{a series}} of two papers, we {{investigate}} the mechanisms by which complex oscillations are generated {{in a class of}} nonlinear dynamical systems with resets modeling the voltage and adaptation of neurons. This first paper presents mathematical analysis showing that the system can support bursts of any period as a function of model parameters. In continuous dynamical systems with resets, period-incrementing structures are complex to analyze. In the present context, we use the fact that bursting patterns correspond to periodic orbits of the adaptation map that governs the sequence of values of the adaptation <b>variable</b> at the <b>resets.</b> Using a slow-fast approach, we show that this map converges towards a piecewise linear discontinuous map whose orbits are exactly characterized. That map shows a period-incrementing structure with instantaneous transitions. We show that the period-incrementing structure persists for the full system with non-constant adaptation, but the transitions are more complex. We investigate the presence of chaos at the transitions...|$|R
50|$|Series I bonds have a {{variable}} yield based on inflation. The interest rate {{consists of two}} components: {{the first is a}} fixed rate which will remain constant {{over the life of the}} bond. The second component is a <b>variable</b> rate <b>reset</b> every six months from the time the bond is purchased based on the current inflation rate. New rates are published on May 1 and November 1 of every year. The fixed rate is determined by the Treasury Department (0.00% in May 2017); the variable component is based on the Consumer Price Index (CPI-U) from a six-month period ending one month prior to the reset time (0.98% in May 2017, reflecting the CPI-U from September to March, published in mid-April, for an effective annual inflation rate of 1.96%). As an example, if someone purchases a bond in February, they will lock in the current fixed rate forever, but the inflation component will be based on the rate published the previous November. In August, six months after the purchase month, the inflation component will now change to the rate that was published in May while the fixed rate remains locked. Interest accrues monthly, in full, on the first day of the month (i.e., a Savings Bond will have the same value on July 1 as on July 31, but on August 1 its value will increase for the August interest accrual). The fixed portion of the rate has varied from as much as 3.6% to 0%. During times of deflation (during part of 2009 and again in 2015), the negative inflation portion can wipe out the return of the fixed portion, but the combined rate cannot go below 0% and the bond will not lose value.|$|R
40|$|This paper {{presents}} {{the analysis of}} the energy savings potential in an existing K- 5 school in hot and humid climates. Previous paper (Im and Haberl 2008 b) presented a calibrated simulation procedure for an existing K- 5 school in hot and humid area, and the first step of the energy savings potential analysis by applying the energy savings measures recommended as in the ASHRAE Advanced Energy Design Guides for K- 12 Schools. As an effort to investigate more energy savings potential for the school building, several other energy savings measures and renewable energy measures were applied to the target building. Those measures include: increased glazing U-value, VFD application for the HVAC system, cold deck <b>reset,</b> <b>variable</b> speed for pumps, high-efficiency boiler, skylights, and the application of solar thermal and PV systems. The final simulation results show that the estimated Energy Use Index (EUI) of the school by applying all the measures but the solar thermal and PV systems would be 29. 9 kBtu/sqft (i. e., 38. 6 % energy savings against the baseline school). In addition, solar thermal and PV systems were designed to provide half of the electricity demand and all the SWH demand of the school building, respectively. The final EUI for the school with the solar thermal and PV systems was estimated to be 15 kBtu/sqft...|$|R
40|$|This paper {{focuses on}} the {{conceptual}} design of a two spool compressor for the NASA Large Civil Tilt Rotor engine, which has a design-point pressure ratio goal of 30 : 1 and an inlet weight flow of 30. 0 lbm/sec. The compressor notional design requirements of pressure ratio and low-pressure compressor (LPC) and high pressure ratio compressor (HPC) work split {{were based on a}} previous engine system study to meet the mission requirements of the NASA Subsonic Rotary Wing Projects Large Civil Tilt Rotor vehicle concept. Three mean line compressor design and flow analysis codes were utilized for the conceptual design of a two-spool compressor configuration. This study assesses the technical challenges of design for various compressor configuration options to meet the given engine cycle results. In the process of sizing, the technical challenges of the compressor became apparent as the aerodynamics were taken into consideration. Mechanical constraints were considered in the study such as maximum rotor tip speeds and conceptual sizing of rotor disks and shafts. The rotor clearance-to-span ratio in the last stage of the LPC is 1. 5 % and in the last stage of the HPC is 2. 8 %. Four different configurations to meet the HPC requirements were studied, ranging from a single stage centrifugal, two axi-centrifugals, and all axial stages. Challenges of the HPC design include the high temperature (1, 560 deg R) at the exit which could limit the maximum allowable peripheral tip speed for centrifugals, and is dependent on material selection. The mean line design also resulted in the definition of the flow path geometry of the axial and centrifugal compressor stages, rotor and stator vane angles, velocity components, and flow conditions at the leading and trailing edges of each blade row at the hub, mean and tip. A mean line compressor analysis code was used to estimate the compressor performance maps at off-design speeds and to determine the required <b>variable</b> geometry <b>reset</b> schedules of the inlet guide vane and variable stators that would result in the transonic stages being aerodynamically matched with high efficiency and acceptable stall margins based on user specified maximum levels of rotor diffusion factor and relative velocity ratio...|$|R
40|$|Floating {{codes are}} codes {{designed}} to store multiple values in a Write Asymmetric Memory, with applications to flash memory. In this model, a memory {{consists of a}} block of n cells, with each cell in one of Q states { 0 ̇ 1 ⋯q- 1 }. The cells are used to represent k variable values from an &ell-ary alphabet. Cells can move from lower values to higher values easily, but moving any cell from a higher value to a lower value requires first resetting the entire block to an all 0 state. Reset operations are to be avoided; generally a block can only experience a large but finite number of resets before wearing out entirely. A code here corresponds to a mapping from cell states to variable values, and a transition function that gives how to rewrite cell states when a variable is changed. Previous work has focused on developing codes that maximize the worst-case number of variable changes, or equivalently cell rewrites, that can be experienced before resetting. In this paper, we introduce the problem of maximizing the expected number of <b>variable</b> changes before <b>resetting,</b> given an underlying Markov chain that models variable changes. We demonstrate that codes designed for expected performance can differ substantially from optimal worst-case codes, and suggest constructions for some simple cases. We then study the related question {{of the performance of}} random codes, again focusing on the issue of expected behavior. © 2006 IEEE...|$|R
40|$|This paper {{presents}} {{a new approach}} to control variables in chilled water loops for energy conservation. Based on the analysis of the relationships between differential pressure and chilled water flow rate in chilled water loop, a series of optimal set points can be found to <b>reset</b> the controlled <b>variables.</b> With these <b>reset</b> set points, chilled water pumps only provide the necessary energy for water distribution loops. An optimization problem accompanied with its constraints is proposed for direct-return chilled water loops. A simple algorithm is provided to find the optimal set points, which can be used in real time to control the variable speed drive pumps to save energy under part-load conditions. With some modification, the proposed approach can be extended to different type of chilled water loops. An application example shows that the chilled water loop operating under the optimal set points has the potential energy savings compared with the conventional method. NOMENCLATURE a 0,a 1,a 2 : curve fitting constant of pump energy; c(HVL) : constraint function of HVL; Cpw: specific heat of water under constant pressure; gc: a constant; HBL: head friction loss of piping branch; HCL: head friction loss of cooling coil; HP: distribution pump head; HPL: head friction loss of pipings; HVL: head friction loss of control valve; HVL, 0 : pump head; K 1 : friction factor of cooling coil; K 2 : friction factor of pipes; m: the total number of subsystems; mw: water flow rate; n: the total number of coils; Ppump: power consumption of pump; Qcoil: cooling load of coil; TCHWR: chilled water return temperature; TCHWS: chilled water supply temperature; ηmotor: efficiency factor of motor; ηpump: efficiency factor of pump; ηVSD: efficiency factor of VSD; Subscript B: branch; i: the ith coil/terminal; j: the jth coil/terminal; i. j: the ith coil in the jth subsystem; m: the measured data; des: the designed data...|$|R
40|$|Social pension {{systems in}} most {{countries}} in Eastern Europe and the former Soviet Union face severe financial pressure. Aging populations are increasing that pressure, which stems mainly from in the%design in the %in the flaws and incompatible incentives in the systems. The authors describe {{the features of the}} ion systems that have led to the current dire predicament: a big discrepancy between system and demographic dependency ratios, unsustainable targeted replacement rates, the high contribution rates needed, growing evasion, and growing deficits. Radical basic reform is inevitable, they say, but may not be politically feasible or even advisable in the short run. After reviewing experience in other countries, they conclude that restructuring and downsizing the social ion system will leave adequate but affordable (thus sustainable) benefits and will allow for the creation and growth of private pension funds. The shortcomings of company-based defined benefit plans (limited portability, restricted vesting, inadequate funding) suggest that transitional economies should opt in the longer run for non-employer, defined contribution plans based on individual capitalization accounts with full immediate vesting, full portability, and full funding. To cope with the need for a targeted replacement rate, such schemes could operate with <b>variable</b> contribution rates, <b>reset</b> each year in accord with the salary growth of each worker, the cumulative investment return on his/her acount, and the targeted pension benefit. Once private pension funds are established, long-term financial resources should accumulate rapidly. They can then {{play a major role in}} modernizing securities markets, stimulating innovation, fostering better accounting and auditing standards, and promoting more disclosure of information. They could also greatly help improve corporate governance and the monitoring of corporate performance. Their"voice"in corporate affairs could be exercised more effectively through collective bodies. They could thus help create more robust structures of corporate governance, lower monitoring costs, and avoid the problems caused by"free riding". Contractual Savings,Pensions&Retirement Systems,Payment Systems&Infrastructure,Non Bank Financial Institutions,Banks&Banking Reform,Non Bank Financial Institutions,Economic Stabilization,Contractual Savings,Banks&Banking Reform,Pensions&Retirement Systems...|$|R
40|$|The Mesoproterozoic Kibara belt in Central Africa has {{recently}} been redefined and subdivided into the Karagwe–Ankole belt (KAB) and the Kibara belt (KIB), separated by Palaeoproterozoic (Rusizian) terranes. The KIB and KAB are characterised {{by the presence of}} numerous rare metal mineralised (Nb–Ta–Sn) pegmatites and Sn–W mineralised quartz veins that are related to the youngest granite generation, i. e. the G 4 -granites in Rwanda, which formed at 986 ± 10 Ma. The pegmatites of the Gatumba area (western Rwanda) have historically been mined for their columbite-tantalite and cassiterite mineralisation, but contain also beryl, apatite, spodumene, amblygonite, and rare phosphates. Columbite-tantalite formed during the crystallisation of the pegmatites, followed by intense alkali metasomatism, i. e. widespread growth of albite and white mica. The major part of the cassiterite mineralisation is, however, concentrated in zones associated with intense phyllic alteration. U–Pb ages of columbite-tantalite samples vary between ∼ 975 Ma and ∼ 930 Ma. The oldest ages (975 + 8. 2 /− 8. 3 Ma and 966 + 8. 7 /− 8. 6 Ma) overlap with previous reported Rb–Sr ages of the emplacement of the pegmatites (∼ 965 Ma) and are interpreted to reflect the crystallisation of the Nb–Ta mineralisation. The youngest ages (951 ± 15 Ma to 936 ± 14 Ma) are apparently related to <b>variable</b> degrees of <b>resetting</b> by (metasomatic) post-crystallisation processes. The resetting could either be due to recrystallisation of early Nb–Ta minerals or due to the disturbance of the U–Pb isotopic signature of the Nb–Ta minerals. The 40 Ar– 39 Ar spectra of muscovite samples associated with different steps in the paragenesis of the pegmatites show a spread of apparent ages between ∼ 940 Ma and ∼ 560 Ma that reflect Late Neoproterozoic tectonothermal events. One plateau age of 592. 2 ± 0. 8 Ma is interpreted to reflect far-field effects of the East African orogeny on the Karagwe–Ankole Belt. status: publishe...|$|R
40|$|The Rabinal Granite is a peraluminous S-type {{composite}} pluton formed upon partial melting of a metasedimentary {{source region}} that fringes the southernmost North America plate in central Guatemala. It is therefore considered, {{together with the}} intruded metasedimentary sequences, {{to be part of}} the continental basement of the Maya block. This leucocratic K-feldspar-plagioclase-quartz-muscovite ± biotite granite shows increasing deformation along its southern margin, where it is cut across by the dextral, Late Cretaceous, top-tothe-NE Baja Verapaz shear zone. Although it has been recently dated at 562 - 453 Ma (isotope dilution-thermal ionization mass spectrometry), the new data presented here, including laser ablation-inductively coupled plasma-mass spectrometry (LAICP-MS) U-Pb and ⁴⁰Ar-³⁹Ar geochronology and electron-probe mineral chemistry, allow us to more precisely establish the timing of intrusion and metamorphic overprinting of the Rabinal Granite. The zircons dated by LA-ICP-MS indicate a crystallization age of 471 ⁺³/₋₅ Ma (Early Ordovician), as well as abundant inherited cores with Pan-African and Mesoproterozoic dates. Laser total fusion Ar-Ar analyses of magmatic low-silica muscovite (Si = 6. 2 - 6. 4 atoms per formula unit) indicate cooling following magmatic crystallization during the mid-late Paleozoic and <b>variable</b> extents of <b>resetting</b> of Ordovician micas during Cretaceous metamorphism and deformation. The pressure-temperature (P-T) conditions of the inferred Ordovician metamorphism that produced partial melting of the metasedimentary source of the Rabinal Granite and the ascent and crystallization of the granitic melt are uncertain, but a clockwise P-T-time path with maximum P and T of < 8 kbar and 750 °C, respectively, is proposed. A second thermal event is recognized in recrystallized high-silica muscovite (Si up to = 6. 8 atoms per formula unit) formed at peak P and T of ~ 8. 5 kbar and ~ 300 °C, respectively. This second event, dated at 70. 1 ± 0. 6 Ma by means of laser total fusion ⁴⁰Ar-³⁹Ar analyses on high-Si muscovite grains, is interpreted to be the result of subduction and accretion of the basement of the Maya block during the latest Cretaceous, likely in a transpressional tectonic regime related to the lateral collision of the Maya block with the Pacific (Farallon) -derived Caribbean arc. This finding represents the first direct evidence for latest Cretaceous subduction of the metamorphic Paleozoic basement of the Maya block, north of the Baja Verapaz shear zone. 15 page(s...|$|R
40|$|Quantitative model {{checking}} {{is concerned}} with the verification of both quantitative and qualitative properties over models incorporating quantitative information. Increases in expressivity of the models involved allow more types of systems to be analysed, but also raise the difficulty of their efficient analysis. Three years ago, the Markov automaton (MA) was introduced as a generalisation of probabilistic automata and interactive Markov chains, supporting nondeterminism, discrete probabilistic choice as well as stochastic timing (Markovian rates). Later, the tool IMCA was developed to compute time-bounded reachability probabilities, expected times and long-run averages for sets of goal states within an MA. However, an efficient formalism for modelling and generating MAs was still lacking. Additionally, the omnipresent state space explosion also threatened the analysability of these models. This thesis solves the first problem and contributes significantly to the solution of the second. First, we introduce the process-algebraic language MAPA for modelling MAs. It incorporates the use of static as well as dynamic data (such as lists), allowing systems to be modelled efficiently. A transformation of MAPA specifications to a restricted part of the language - enabled through an encoding of Markovian rates in action - allows for easy parallel composition, state space generation and syntactic optimisations (also known as reduction techniques). Second, we introduce five reduction techniques for MAPA specifications: constant elimination, expression simplification, summation elimination, dead variable reduction and confluence reduction. The first three aim to speed up state space generation by simplifying the specification, while the last two aim to speed up analysis by reductions {{in the size of the}} state space. Dead <b>variable</b> reduction <b>resets</b> data <b>variables</b> the moment their value becomes irrelevant, while confluence reduction detects and resolves spurious nondeterminism often arising in the presence of loosely coupled parallel components. Since MAs generalise labelled transition systems, discrete-time Markov chains, continuous-time Markov chains, probabilistic automata and interactive Markov chains, our techniques and results are also applicable to all these subclasses. Third, we thoroughly compare confluence reduction to the ample set variant of partial order reduction. Since partial order reduction has not yet been defined for MAs, we restrict both to the context of probabilistic automata. We precisely pinpoint the differences between the two methods on a theoretical level, resolving the long-standing uncertainty about the relation between these two concepts: when preserving branching-time properties, confluence reduction strictly subsumes partial order reduction and hence is slightly more powerful. Also, we compare the techniques in the practical setting of statistical model checking, demonstrating that the additional potential of confluence indeed may provide larger reductions (even compared to a variant of the ample set method that only preserves linear-time properties). We developed a tool called SCOOP, which contains all our techniques and is able to export to the IMCA tool. Together, these tools for the first time allow the analysis of MAs. Case studies on a handshake register, a leader election protocol, a polling system and a processor grid demonstrate the large variety of systems that can be modelled using MAPA. Experiments additionally show significant reductions by all our techniques, sometimes reducing state spaces to less than a percent of their original size. Moreover, our results enable us to provide guidelines that indicate for each technique the aspects of case studies that predict large reductions. In the end, MAPA indeed enables us to efficiently specify systems incorporating nondeterminism, discrete probabilistic choice and stochastic timing. It also allows several advanced reduction techniques to be applied rather easily, leading us to define a variety of such techniques. Our comparison of confluence reduction and partial order reduction provides several novel insights in their relation. Also, experiments show that our techniques greatly reduce the impact of the state space explosion: a major step forward in efficient quantitative verification...|$|R

