209|96|Public
25|$|Virtual reality {{also has}} {{applications}} in the physical side of occupational therapy. For stroke patients, various virtual reality technologies can help bring fine control back to different muscle groups. Therapy often includes games controlled with haptic-feedback controllers that require fine movements, such as playing piano with a <b>virtual</b> <b>hand.</b> The Wii gaming system has also been {{used in conjunction with}} virtual reality as a treatment method.|$|E
5000|$|The {{sequel to}} Event Horizon's first game, DarkSpyre, The Summoning {{replaced}} the random-generated dungeons {{of the previous}} game with pre-designed levels. The title featured numerous magic items, {{as well as a}} spell-casting system utilizing <b>virtual</b> <b>hand</b> gestures. The game also utilized [...] "runes" [...] as a way to magically gain abilities or items.|$|E
50|$|To {{accomplish}} {{these tasks}} usually the system provides the user a 3D cursor {{represented as a}} human hand whose movements correspond to {{the motion of the}} hand tracker. This <b>virtual</b> <b>hand</b> technique is rather intuitive because simulates a real-world interaction with objects but with the limit of objects that we can reach inside a reach-area.|$|E
40|$|Abstract—The human {{hands are}} complex {{articulated}} struc-tures with multiple degrees of freedom. This makes the modelling and animation {{of high quality}} flexible <b>virtual</b> <b>hands</b> extremely difficult especially for real-time interactive applications. We wish to employ <b>virtual</b> <b>hands</b> for real-time Sign Language visualisation {{for which they are}} of the utmost importance. In this paper we present our investigation into developing high quality flexible <b>virtual</b> human <b>hands.</b> Moreover, we make use of the H-Anim skeleton specification to enable the sharing of animation data between different hand models. Index Terms—sign language, hand animation, articulated fig-ure, H-Ani...|$|R
5000|$|In October 2012, the Collision Repair class began {{delivering}} <b>virtual</b> <b>hands</b> on painting, using {{state of}} the art 3D virtualization. https://tcatshelbyville.edu/programs/collision-repair-technology ...|$|R
5000|$|Uranai no Yakata: This is {{a fortune}} tellers house. The Wii Remote and Nunchuk {{are used to}} make a pair of <b>virtual</b> <b>hands</b> grasp a crystal fortune ball and hear a fortune for the day.|$|R
50|$|Virtual reality {{also has}} {{applications}} in the physical side of occupational therapy. For stroke patients, various virtual reality technologies can help bring fine control back to different muscle groups. Therapy often includes games controlled with haptic-feedback controllers that require fine movements, such as playing piano with a <b>virtual</b> <b>hand.</b> The Wii gaming system has also been {{used in conjunction with}} virtual reality as a treatment method.|$|E
50|$|There {{is another}} way to select and {{manipulate}} objects in 3D virtual spaces and that is pointing objects using a virtual-ray emanating from the <b>virtual</b> <b>hand.</b> When the ray intersects with the objects, it can be manipulated. Several variations of this technique has been made, like the aperture technique, which uses a conic pointer addressed for the user's eyes, estimated from the head location, to select distant objects. This technique also uses a hand sensor to adjust the conic pointer size.|$|E
5000|$|Bella Todd {{described}} {{the experience in}} the Guardian: 'It is a piece for an audience of two, who are given video goggles and headsets and seated in wheelchairs. Soon I was in a virtual car, joyriding through a dystopian landscape {{with a group of}} circus clowns. One of them opened a bottle in the front seat and I smelt and felt real-life vodka hit my face; later I was instructed to hold up my left hand, only to see a <b>virtual</b> <b>hand</b> on screen, hairy and stained with blood. My response to the autoteatro-style instructions was as immediate and unquestioning as that of a sleepwalker.' ...|$|E
40|$|University of Minnesota M. S. thesis. June 2013. Major: Biomedical Engineering. Advisor: Bin He. 1 {{computer}} file (PDF); v, 34 pages. The burden of stroke {{on the health}} care system at large and individual patients is profound, and current techniques for rehabilitation rely on the training and dedication of the rehabilitation specialist. Here we present an immersive, virtual reality environment for presenting feedback to subjects {{in the form of a}} set of <b>virtual</b> <b>hands.</b> By just imagining the use of the left or right hands, subjects could see movement in the <b>virtual</b> <b>hands</b> and learn to modulate their thoughts to control them. Allowing subjects task relevant motor feedback early could prove an effective means of early rehabilitation. The implications of this training are presented in 6 patients who had suffered cortical or basal ganglia stroke. Using the system described below, the subject's were able to achieve control accuracies of as high as 81 % in a binary classification task and showed progression of skill in as little as three, two-hour experimental sessions...|$|R
40|$|This paper {{deals with}} haptic realism related to Kinematic {{capabilities}} of the devices used in manipulation of virtual objects in virtual assembly environments {{and its effect on}} achieving haptic realism. Haptic realism implies realistic touch sensation. In virtual world all the operations are to be performed in the same way and with same level of accuracy as in the real world. In order to achieve realism there should be a complete mapping of real and virtual world dimensions. Experiments are conducted to know the kinematic {{capabilities of the}} device by comparing the dimensions of the object in the real and virtual world. Registered dimensions in the virtual world are found to be approximately 1. 5 times that of the real world. Dimensional variations observed were discrepancy due to exoskeleton and discrepancy due to real and <b>virtual</b> <b>hands.</b> Experiments are conducted to know the discrepancy due to exoskeleton and this discrepancy can be taken care of by either at the hardware or software level. A Mathematical model is proposed to know the discrepancy between real and <b>virtual</b> <b>hands.</b> This could not give a fixed value and can not be taken care of by calibration. Experiments are conducted to figure out how much compensation can be given to achieve haptic realism...|$|R
40|$|As {{virtual reality}} (VR) {{emerges as a}} {{mainstream}} platform, designers have started to experiment new interaction techniques to enhance the user experience. This is a challenging task because designers not only strive to provide designs with good performance but also carefully ensure not to disrupt users' immersive experience. There is a dire {{need for a new}} evaluation tool that extends beyond traditional quantitative measurements to assist designers in the design process. We propose an EEG-based experiment framework that evaluates interaction techniques in VR by measuring intentionally elicited cognitive conflict. Through the analysis of the feedback-related negativity (FRN) as well as other quantitative measurements, this framework allows designers to evaluate the effect of the variables of interest. We studied the framework by applying it to the fundamental task of 3 D object selection using direct 3 D input, i. e. tracked hand in VR. The cognitive conflict is intentionally elicited by manipulating the selection radius of the target object. Our first behavior experiment validated the framework in line with the findings of conflict-induced behavior adjustments like those reported in other classical psychology experiment paradigms. Our second EEG-based experiment examines the effect of the appearance of <b>virtual</b> <b>hands.</b> We found that the amplitude of FRN correlates with the level of realism of the <b>virtual</b> <b>hands,</b> which concurs with the Uncanny Valley theory...|$|R
50|$|Users {{need to be}} able to {{manipulate}} virtual objects. Manipulation tasks involve selecting and moving an object. Sometimes, rotation of the object is involved as well. Direct-hand manipulation is the most natural technique because manipulating physical objects with the hand is intuitive for humans. However, this is not always possible. A <b>virtual</b> <b>hand</b> that can select and re-locate virtual objects will work as well.3D widgets can be used to put controls on objects: these are usually called 3D Gizmos or Manipulators (a good example are the ones from Blender). Users can employ these to re-locate, re-scale or re-orient an object (Translate, Scale, Rotate).Other techniques include the Go-Go technique and ray casting, where a virtual ray is used to point to, and select and object. More recently there has been user interface development and research by Richard White in Kansas over the past 3 years regarding interactive surfaces & classroom interactive whiteboards, grade school students, and 3D natural user interfaces known as Edusim.|$|E
30|$|The {{manipulation}} {{rules are}} required for the interaction of <b>virtual</b> <b>hand.</b> Driven by the motion data input to the <b>virtual</b> <b>hand,</b> the <b>virtual</b> <b>hand</b> has different shapes that can be projected to two orthogonal planes, and the gestures are obtained. With the meaningful gestures and the collision detection results, the manipulation is validated and then the <b>virtual</b> <b>hand</b> manipulation rules are established.|$|E
30|$|As the {{response}} of the interaction between <b>virtual</b> <b>hand</b> and the objects in the virtual environment, collision detection is described as the intersection of geometric elements in CGA, so the <b>virtual</b> <b>hand</b> and the manipulated objects can be decomposed as geometric elements on a different level for such intersection tests. Besides, the shapes of <b>virtual</b> <b>hand</b> also correspond to various gestures. With the defined gestures and the results of collision detection, the rules of <b>virtual</b> <b>hand</b> manipulation are established.|$|E
50|$|Virtual-based Second Life {{groups such}} as Virtual Ability, <b>Virtual</b> Helping <b>Hands</b> (including Helen Keller Day in Second Life), TechSoup, and the Second Life Bar Association to provide training, {{technical}} assistance, and virtual referrals.|$|R
30|$|In {{the virtual}} environment, the human {{operator}} can drive the <b>virtual</b> robot <b>hand</b> {{with the data}} glove and perform dynamic manipulation of objects. Scenes of the virtual environment can be obtained as color images of 256 × 256 pixels by window capture instead of using a camera.|$|R
2500|$|Max, The Virtual Guidedog, {{developed}} by <b>Virtual</b> Helping <b>Hands,</b> offers a <b>virtual</b> guide dog object {{that can be}} [...] "worn" [...] by a user's avatar. The guidedog provides a number of functions such as navigation and querying the environment through a chat-like interface. Feedback is provided using synthetic speech.|$|R
40|$|This {{research}} {{promoted the}} topic concerning the interaction technique using <b>virtual</b> <b>hand</b> in Augmented Reality environment (AR). The kind of interaction researched uses tracker library that use marker {{to know the}} human gesture. This research results in <b>virtual</b> <b>hand</b> design {{that can be used}} to interact with virtual object. With the kind of interaction i. e. grab the virtual object and drop that grabbed virtual object to the place desired and the design of <b>virtual</b> <b>hand</b> that similar to the real,. So that it is expected <b>virtual</b> <b>hand</b> could be another alternative on how we interact in world of augmented reality. Kata kunci : Augmented Reality, <b>Virtual</b> <b>Hand,</b> Natural Gestur...|$|E
40|$|International audienceHow {{do people}} {{appropriate}} their <b>virtual</b> <b>hand</b> representation when interacting in virtual environments? In order {{to answer this}} question, we conducted an experiment studying the sense of embodiment when interacting with three different <b>virtual</b> <b>hand</b> representations, each one providing a different degree of visual realism but keeping the same control mechanism. The main experimental task was a Pick-and-Place task in which participants had to grasp a virtual cube and place it to an indicated position while avoiding an obstacle (brick, barbed wire or fire). An additional task was considered in which participants had to perform a potentially dangerous operation towards their virtual hand: place their <b>virtual</b> <b>hand</b> close to a virtual spinning saw. Both qualitative measures and questionnaire data were gathered {{in order to assess}} the sense of agency and ownership towards each <b>virtual</b> <b>hand.</b> Results show that the sense of agency is stronger for less realistic virtual hands which also provide less mismatch between the participant's actions and the animation of the <b>virtual</b> <b>hand.</b> In contrast, the sense of ownership is increased for the human <b>virtual</b> <b>hand</b> which provides a direct mapping between the degrees of freedom of the real and <b>virtual</b> <b>hand...</b>|$|E
30|$|Besides rubber hand {{illusion}} and its revised version, <b>virtual</b> <b>hand</b> illusion {{is another}} way to induce body perception illusion. In the experiment of <b>virtual</b> <b>hand</b> illusion, participants sit in front of a screen where a virtual 3 D image of the <b>virtual</b> <b>hand</b> would be presented while having tactile stimulation on their real hidden hand. It is indicated that the way of inducing <b>virtual</b> <b>hand</b> illusion can achieve the same effect as what rubber hand illusion did. In other words, imposing the same tactile stimulation on both the <b>virtual</b> <b>hand</b> on the screen and the real hand which is hidden from view can let the participants feel the similar experience to that under rubber hand illusion condition (Ma & Hommel, 2013; Zhang & Chen, 2016). Experiment showed that by simply manipulating the temporal delay between participants’ real movement and the movement of the <b>virtual</b> <b>hand</b> on the screen, a <b>virtual</b> <b>hand</b> illusion can be induced {{even in the absence of}} tactile stimulation (Sanchez-Vives et al., 2010). Slater et al. found that there were reliable correlations between the impression of hand ownership and hand-related electromyography (EMG) activation, suggesting a connection between perceived ownership and action control (Slater, Perez-Marcos, Ehrsson, & Sanchez-Vives, 2008).|$|E
40|$|Recent neurophysiological {{experiments}} {{have shown that}} the visual stimuli that trigger a particular kind of neurons located in the ventral premotor cortex of monkeys and humans are very selective. These mirror neurons are activated when the hand of another individual interacts with an objects but are not activated when the actions, identical in purpose, are made by manipulated mechanical tools. A Human Frontiers Science Program project is investigating which are the parameters of the external stimuli that mirror neurons visually extract and match on their movement related activity. The planned neurophysiological experiments will require the presentation of digital stimuli of different kinds, including video sequences showing meaningful actions made by human hands, synthetic reproductions of the same actions made by realistic <b>virtual</b> <b>hands,</b> as well as variations of the same actions by controlled modifications of hand geometry and/or action kinematics. This paper presents th [...] ...|$|R
40|$|The {{problem of}} robotic and virtual {{interaction}} with physical objects {{has been the}} subject of research for many years in both the robotic manipulation and haptics communities. Both communities have focused much attention on human touch-based perception and manipulation, modelling contact between real or <b>virtual</b> <b>hands</b> and objects, or mechanism design. However, as a whole, these problems have not yet been addressed from a unified perspective. This edited book is the outcome of a well-attended workshop which brought together leading scholars from various branches of the robotics, virtual-reality, and human studies communities during the 2004 IEEE International Conference on Robotics and Automation. It covers some of the most challenging problems on the forefront of today’s research on physical interaction with real and virtual objects, with special emphasis on modelling contacts between objects, grasp planning algorithms, haptic perception, and advanced design of hands, devices and interfaces...|$|R
40|$|A {{goal of the}} SKILLS {{project is}} to develop Virtual Reality (VR) -based {{training}} simulators for different application domains, {{one of which is}} juggling. Within this context the value of multimodal VR environments for skill acquisition is investigated. In this study, we investigated whether it was necessary to render the sounds of virtual balls hitting <b>virtual</b> <b>hands</b> within the juggling training simulator. First, we recorded sounds at the jugglers’ ears and found the sound of ball hitting hands to be audible. Second, we asked 24 jugglers to juggle under normal conditions (Audible) or while listening to pink noise intended to mask the juggling sounds (Inaudible). We found that although the jugglers themselves reported no difference in their juggling across these two conditions, external juggling experts rated rhythmic stability worse in the Inaudible condition than in the Audible condition. This result suggests that auditory information should be rendered in the VR juggling training simulator...|$|R
30|$|<b>Virtual</b> <b>hand</b> usually simulates human hands by {{mapping the}} actual shapes of hands to virtual {{environment}} and modeling the hand-object manipulations in human machine interactions. For describing <b>virtual</b> <b>hand</b> and its manipulation in a unified framework, a method based on conformal geometric algebra (CGA) is proposed {{to solve the}} problems of <b>virtual</b> <b>hand</b> modeling and interaction in this paper. With the vertex blending based on CGA, the artifacts on the finger joints are improved, which enhances the realism of the <b>virtual</b> <b>hand</b> model. With the same tool, the collision detection between the <b>virtual</b> <b>hand</b> and the manipulated objects is implemented. The gesture of grasp, pinch, and hold are recognized, and the corresponding manipulation rules are established by CGA calculation. To test these three typical manipulations, the manipulated objects are imported and the manipulation effectiveness is evaluated.|$|E
30|$|Return {{the state}} of the <b>virtual</b> <b>hand.</b>|$|E
30|$|Input the <b>virtual</b> <b>hand</b> and the {{manipulated}} object.|$|E
40|$|Background: This study {{examined}} perceptual-motor coordination with an apparatus that simulated a situation representative of endoscopic surgery. Methods: Participants were trained with one {{arrangement of the}} apparatus, then tested with an alternative arrangement in which either {{the positions of the}} camera, the surgeon, or the objects in the surgical field were altered. Results: Results showed that changes of either the camera 2 ̆ 7 s position or the surgeon 2 ̆ 7 s position disrupted performance. However, when the camera and surgeon positions were changed together, skilled performance was maintained. Conclusions: This suggests that skill depends on a consistent mapping between the <b>virtual</b> <b>hands</b> and eyes, but not on the particular visual or motor orientations. The results suggest that movements of the camera during surgery can disrupt coordinated action. Also, in the design of training simulators, the mapping between camera and instruments may be more important than the static appearance of the displays or the topology of the movements...|$|R
40|$|Based on a {{distributed}} architecture for real-time collection and broadcast of haptic information to multiple participants, heterogeneous haptic devices (the PHANToM and the CyberGrasp) {{were used in}} an experiment to test the performance accuracy and sense of presence of participants engaged in a task involving mutual touch. In the experiment, the hands of CyberGrasp users were modeled for the computer to which the PHANToM was connected. PHANToM users were requested to touch the <b>virtual</b> <b>hands</b> of CyberGrasp users to transmit randomly ordered {{letters of the alphabet}} using a pre-set coding system. Performance accuracy achieved by a small sample of participants was less than optimal in the strict sense: accurate detection of intended location and frequency of touch combined ranged from. 27 to. 42. However, participants accurately detected the intended location of touch in 92 % of the cases. Accuracy may be positively related to pairwise sense of co-presence and negatively related to mean force, force variability, and task completion time. 1...|$|R
40|$|Purpose: To {{develop a}} system for refined motor control of {{artificial}} hands based on multiple electromyographic (EMG) recordings, allowing multiple patterns of hand movements. Methods: Five subjects with traumatic below-elbow amputations and 1 subject with a congenital below-elbow failure of formation performed 10 imaginary movements with their phantom hand while surface electrodes recorded the EMG data. In a training phase a data glove with 18 degrees of freedom was used for positional recording of movements in the contralateral healthy hand. These movements were performed {{at the same time}} as the imaginary movements in the phantom hand. An artificial neural network (ANN) then could be trained to associate the specific EMG patterns recorded from the amputation stump with the analogous specific hand movements synchronously performed in the healthy hand. The ability of the ANN to predict the 10 imaginary movements off line, when they were reflected in a <b>virtual</b> computer <b>hand,</b> was assessed and calculated. Results: After the ANN was trained the subjects were able to perform and control 10 hand movements in the <b>virtual</b> computer <b>hand.</b> The subjects showed a median performance of 5 types of movement with a high correlation with the movement pattern of the data glove. The subjects seemed to relearn to execute motor commands rapidly that had been learned before the accident, independent of how old the injury was. The subject with congenital below-elbow failure of formation was able to perform and control several hand movements in the computer hand that cannot be performed in a myoelectric prosthesis (eg, opposition of the thumb). Conclusions: With the combined use of an ANN and a data glove, acting in concert in a training phase, amputees rapidly can learn to execute several imaginary movements in a <b>virtual</b> computerized <b>hand,</b> this opens promising possibilities for motor control of future hand prostheses...|$|R
30|$|In pick-and-place, the {{operator}} pinched the object {{by the two}} fingers of the <b>virtual</b> <b>hand,</b> lifted it up, and carried it to the goal position. In pushing, {{the operator}} pushed the object by {{the tips of the}} two fingers of the <b>virtual</b> <b>hand</b> and carried it to the goal position.|$|E
40|$|We {{have studied}} {{manipulation}} performance in virtual environments using {{two types of}} controllers: <b>virtual</b> <b>hand</b> control and 3 D mouse-cursor control. These manipulation methods were tested under monoscopic and stereoscopic viewing conditions. Participants were asked to discriminate, grasp, pitch, roll and position virtual objects. Both speed and accuracy of manipulation tasks were meas-ured. <b>Virtual</b> <b>hand</b> control proved to be significantly faster and more accurate than 3 D mouse-cursor control. <b>Virtual</b> <b>hand</b> control affected more head movement which was available in both conditions. Further, it was shown that the speed and accuracy of manipulations strongly improve under stereo-scopic viewing conditions...|$|E
40|$|We {{present results}} of a formal {{evaluation}} of three direct manipulation interaction techniques for picking and positioning objects in VEs: the "classical" <b>virtual</b> <b>hand,</b> raycasting, and the Go-Go techniques. Our goal was to assess and compare the two most basic metaphors for selection and manipulation in VEs: virtual pointer and <b>virtual</b> <b>hand.</b> The main variables of interest were distance to and size of objects, interaction technique, and visual feedback. The results of the studies suggest that within the user-centered coordinate system, virtual pointing is an essentially twodimensional metaphor, while the <b>virtual</b> <b>hand</b> is a threedimensional metaphor. We also found that for some applications the "classical" <b>virtual</b> <b>hand</b> technique appears to be obsolete and may {{be replaced by the}} ray or Go-Go techniques without a reduction in user performance. The paper reports these and other experimental results and discusses their implications for the design of VEs. Keywords: virtual manipulation, virtua [...] ...|$|E
40|$|VR-CHEM is a {{prototype}} for a virtual reality molecular modelling {{program with a}} modern 3 D user interface. In this thesis, the author discusses the research behind {{the development of the}} prototype, provides {{a detailed description of the}} program and its features, and reports on the user tests. The research includes reviewing previous programs of a similar category that have appeared in studies in the literature. Some of these are related to chemistry and molecular modelling while others focus on 3 D input techniques. Consequently, the prototype contributes by exploring the design of the user interface and how it can affect productivity in this category of programs. The prototype is subjected to a pilot user test to evaluate what further developments are required. Based on this, the thesis proposes that 3 D interfaces, while capable of several unique tasks, are yet to overcome some significant drawbacks such as limitations in accuracy and precision. It also suggests that virtual reality can aid in spatial understanding but <b>virtual</b> <b>hands</b> and controllers are far inferior to real hands for even basic tasks due to a lack of tactile feedback...|$|R
30|$|From users’ {{feedback}} in the interviews, {{we learned}} users {{are prone to}} be distracted by the <b>virtual</b> and actual <b>hands</b> in the FI setup. As a result, the user finds it difficult to explore in the depth direction, leading to less efficient trajectories.|$|R
40|$|This paper {{presents}} a novel input device design for capturing gestures. The system {{is based on}} commodity components and combines accelerometers, gyroscopes and bend sensors. It is a low-power, low-cost hand device, characterized by extreme wearability thanks to wireless communication support and small form-factor. It {{can be used as}} a stand-alone platform or combined with other wireless sensor nodes in a body area network. The system has been tested as input interface for moving a <b>virtual</b> threedimensional <b>hand</b> in real-time. 1...|$|R
