80|251|Public
5000|$|Step {{one can be}} seen a {{simplified}} version of Personalized PageRank. Step two sums up the <b>vector</b> <b>similarity</b> of each iteration. Both, matrix and local representation, compute the same similarity score. CoSimRank {{can also be used}} to compute the similarity of sets of nodes, by modifying [...]|$|E
50|$|Distributional {{semantics}} {{favor the}} use of linear algebra as computational tool and representational framework. The basic approach is to collect distributional information in high-dimensional vectors, and to define distributional/semantic similarity in terms of <b>vector</b> <b>similarity.</b> Different kinds of similarities can be extracted depending on which type of distributional information is used to collect the vectors: topical similarities can be extracted by populating the vectors with information on which text regions the linguistic items occur in; paradigmatic similarities can be extracted by populating the vectors with information on which other linguistic items the items co-occur with. Note that the latter type of vectors {{can also be used}} to extract syntagmatic similarities by looking at the individual vector components.|$|E
40|$|The main aim of {{this study}} is to present a novel method based on multi-criteria {{decision}} making for bipolar neutrosophic sets. Therefore, Jaccard <b>vector</b> <b>similarity</b> and weighted Jaccard <b>vector</b> <b>similarity</b> measure is defined to develop the bipolar neutrosophic decision making method. In addition, the method is applied to a numerical example in order to confirm the practicality and accuracy of the proposed method...|$|E
3000|$|..., {{built with}} the same {{threshold}} values but different <b>vectors</b> and <b>similarity</b> measures, {{and a number of}} single-species graphs G [...]...|$|R
40|$|This {{paper is}} {{centered}} on using chemical reaction as a computational metaphor for simultaneously solving problems. An artificial chemical reactor that can simultaneously solve instances of three unrelated problems was created. The reactor is a distributed stochastic algorithm that simulates a chemical universe wherein the molecular species are being represented either by a human genomic contig panel, a Hamiltonian cycle, or an aircraft landing schedule. The chemical universe is governed by reactions that can alter genomic sequences, re-order Hamiltonian cycles, or reschedule an aircraft landing program. Molecular masses were considered as measures of goodness of solutions, and represented radiation hybrid (RH) <b>vector</b> <b>similarities,</b> costs of Hamiltonian cycles, and penalty costs for landing an aircraft before and after target landing times. This method, tested by solving in tandem with deterministic algorithms, {{has been shown to}} find quality solutions in finding the minima RH <b>vector</b> <b>similarities</b> of genomic data, minima costs in Hamiltonian cycles of the traveling salesman, and minima costs for landing aircrafts before or after target landing times. Comment: 6 pages, appeared in R. P. Saldaña (ed.) Proceedings (CDROM) of the 6 th Philippine Computing Science Congress (PCSC 2006), Ateneo De Manila University, Loyola Heights, Quezon City, Philippines, 28 - 29 March 2006, pp 48 - 53 (ISSN 1908 - 1146...|$|R
3000|$|... (k)) are {{concatenated}} {{to generate}} the k-th early fused <b>similarity</b> <b>vector,</b> denoted as X^(k)=[X^(k)_ 1,...,X^(k)_N^ 2]^T.|$|R
40|$|The cosine {{similarity}} {{measure is}} widely used in big data analysis to compare vectors. In this article {{a new set of}} <b>vector</b> <b>similarity</b> measures are proposed. New <b>vector</b> <b>similarity</b> measures are based on a multiplication-free operator which requires only additions and sign operations. A vector 'product' using the multiplication-free operator is also defined. The new vector product induces the ℓ 1 -norm. As a result, new cosine measure-like similarity measures are normalized by the ℓ 1 -norms of the vectors. They can be computed using the MapReduce framework. Simulation examples are presented. © 2014 IEEE...|$|E
40|$|Abstract The aim of {{this paper}} is to compare {{different}} methods for automatic ex-traction of semantic similarity measures from corpora. The semantic similarity measure is proven to be very useful for many tasks in natural language processing like information retrieval, information extraction, machine translation etc. Additionally, one of the main problems in natural language processing is data sparseness since no language sample is large enough to seize all possible language combinations. In our research we experiment with four different measures of association with context and eight different measures of <b>vector</b> <b>similarity.</b> The results show that the Jensen-Shannon divergence and L 1 and L 2 norm outperform other measures of <b>vector</b> <b>similarity</b> regardless of the measure of associ-ation with context used. Maximum likelihood estimate and t-test show better results than other measures of association with context...|$|E
40|$|Abstract—Wireless Capsule Endoscopy (WCE) {{generates a}} large number of images in one {{examination}} of a patient. It is very laborious and time-consuming to detect the WCE video, and so limits the wider application of WCE. Color similarity measurement is the key technique of color image segmentation and recognition, as well as the premise of bleeding detection in WCE images. This paper deduces two color <b>vector</b> <b>similarity</b> coefficients to measure the color similarity degree in RGB color space, and based on which, a novel method of intelligent bleeding detection in WCE image is implemented. The novel algorithm is implemented in RGB color space, and is featured with simple computation and practicability. The experiments showed that the bleeding regions in WCE images can be correctly extracted, and the sensitivity and specificity of this algorithm were 90 % and 97 % respectively. Index Terms—color similarity measurement, color <b>vector,</b> <b>similarity</b> coefficient, bleeding detection I...|$|E
30|$|A word {{similarity}} computing method can {{be taken}} as a function. Consequently, we have the following definition of a similarity model for our paper, which inputs could be concept vectors or concepts (without their <b>vectors).</b> A <b>similarity</b> model could be annotation-based or algorithm-based.|$|R
40|$|International audienceIn this paper, {{we propose}} a Web page {{archiving}} system that combines state-of-the-art comparison methods {{based on the}} source codes of Web pages, with computer vision techniques. To detect whether successive versions of a Web page are similar or not, our system is based on: (1) a combination of structural and visual comparison methods embedded in a statistical discriminative model, (2) a visual similarity measure designed for Web pages that improves change detection, (3) a supervised feature selection method adapted to Web archiving. We train a Support Vector Machine model with <b>vectors</b> of <b>similarity</b> scores between successive versions of pages. The trained model then determines whether two versions, defined by their <b>vector</b> of <b>similarity</b> scores, are similar or not. Experiments on real archives validate our approach...|$|R
40|$|Lacking {{standardized}} extrinsic evaluation {{methods for}} vector representations of words, the NLP community has {{relied heavily on}} word similarity tasks {{as a proxy for}} intrinsic evaluation of word <b>vectors.</b> Word <b>similarity</b> evaluation, which correlates the distance between vectors and human judgments of semantic similarity is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word <b>vectors</b> on word <b>similarity</b> datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods. Comment: The First Workshop on Evaluating Vector Space Representations for NL...|$|R
40|$|This thesis {{summarizes}} the information retrieval theory, the relational model basic {{and focuses on}} the data indexing in relational database systems. The thesis focuses on multimedia data searching. It includes description of automatic multimedia data content extraction and multimedia data indexing. Practical part discusses design and solution implementation for improving query effectivity for multidimensional <b>vector</b> <b>similarity</b> which describes multimedia data. Thesis final part discusses experiments with this solution...|$|E
40|$|This paper {{explores the}} {{segmentation}} of tutorial dialogue into cohesive topics. A latent semantic space was created using conversations from human to human tutoring transcripts, allowing cohesion between utterances {{to be measured}} using <b>vector</b> <b>similarity.</b> Previous cohesionbased segmentation methods that focus on expository monologue are reapplied to these dialogues to create benchmarks for performance. A novel moving window technique using orthonormal bases of semantic vectors significantly outperforms these benchmarks on this dialogue segmentation task. ...|$|E
30|$|Another {{method to}} search in BESS is concept-based {{subjective}} concept-directed vertical search, or focused search. It is realized by matching of {{concepts of the}} individual user profile with concepts of other users when retrieving through the subjective search engine. This operation forms the target search document space for the query by finding users with similar interests and reaching their subjective index repositories. After the search document space is determined relevant documents are retrieved by comparing query and document <b>vector</b> <b>similarity.</b>|$|E
40|$|Content-based image {{retrieval}} systems use low-level {{features like}} {{color and texture}} for image representation. Given these representations as feature <b>vectors,</b> <b>similarity</b> between images is measured by computing distances in the feature space. Unfortunately, these low-level features cannot always capture the high-level concept of similarity in human perception. Relevance feedback tries to improve the performance by allowing iterative retrievals where the feedback information from the user is incorporated into the database search. We present a weighted distance approach that uses standard deviations of the features both for the whole database and also among the images selected as relevant by the user. These weights are used to iteratively refine the effects of different features in the database search. Retrieval performance is evaluated using average precision and progress computed on a database of approximately 10, 000 images and an average performance improvement of 19 % is obtained after [...] ...|$|R
40|$|In this paper, {{we focus}} on the problem of {{unsupervised}} clustering which allows automatic setting of optimal clusters number. We present a generalization of the competitive agglomeration clustering algorithm firstly introduced in [1]. This generalization is inspired by the regularization theory and suggests a new schema for using various cluster validity criteria continuously proposed in the literature. As a consequence of this generalization, we introduce new objective clustering functions, and present their associated optimal solutions. We present an application of this competitive clustering schema to color image segmentation in order to perform partial queries in the context of image retrieval by content. In this case, each pixel is represented by the color distribution in its vicinity. Clustering algorithm has to incorporate an appropriate distance measure to compare feature <b>vectors</b> <b>similarity.</b> 1. Introduction Our work was motivated by requirements and constraints in the context o [...] ...|$|R
40|$|In {{this paper}} we will present new results in image {{database}} retrieval, {{which is a}} developing field with growing interest. In case of the images in a database many features can be considered like color, shape, texture. These can be considered to find similar images in the database. The investigated procedure is to compose <b>similarity</b> <b>vectors</b> from the <b>similarity</b> values according to these features. Then some measurement is applied to calculate the norm of the <b>similarity</b> <b>vectors</b> (what is between the query image and the other images in the database). Such a norm can be the weighted Euclidean one (applied in Oracle 9 i). Our new approach is based on neighborhood sequences as a new method to calculate the norm, and we will show that for some purposes it allows more flexibility for the user in composing the queries than the classic norm-computing methods. 1...|$|R
40|$|This paper {{presents}} {{the application of}} context <b>vector</b> <b>similarity</b> {{for the purpose of}} word sense discrimination during query translation. The random indexing vector space method is used to accumulate the context vectors. Pair wise similarity of the context vectors of ambiguous terms with that of anchor terms indicated the possible correct translation of a query term. Two retrieval experiments were conducted using the discriminated queries and weighted maximally expanded queries. The discriminated queries show a substantial increase in retrieval performance...|$|E
40|$|We {{present results}} from {{improving}} vector space based extraction summarizers. The summarizer uses Random Indexing and Page Rank to extract those sentences whose importance are ranked highest for a document, based on <b>vector</b> <b>similarity.</b> Originally the summarizer used only word vectors {{based on the}} words in the document to be summarized. By using a larger word space model the performance of the summarizer was improved. Along with the performance, robustness was improved as random seeds did not affect the summarizer as much as before, making for more predictable results from the summarizer. ...|$|E
40|$|We {{introduce}} {{an approach}} to question answering in the biomedical domain that utilises similarity matching of question/answer pairs in a document, or a set of background documents, to select the best answer to a multiple-choice question. We explored a range of possible similarity matching methods, ranging from simple word overlap, to dependency graph matching, to feature-based <b>vector</b> <b>similarity</b> models that incorporate lexical, syntactic and/or semantic features. We found that while these methods performed reasonably well on a small training set, they did not generalise well to the final test data. 13 page(s...|$|E
40|$|This study {{presents}} {{a simple and}} novel approach to scheduling parallel and distributed applications on parallel and distributed systems. The approach is based on <b>similarity</b> of feature <b>vectors</b> used in information retrieval. The modules making up an application and processors making up a system are treated as objects characterized {{by a number of}} metrics {{that can be used to}} determine the level of association or affinity between them. The module-module and module-hardware affinities computed from feature vectors are first used for application clustering. The results are eventually translated to agglomerative metrics that represent module-processor mapping. The feature vectors for modules and hardware (processors and devices) are constructed so that similarities are computable. The module clustering and module-processor mapping are based on the computed <b>vector</b> <b>similarities.</b> Preliminary investigation shows the feasibility of the approach as applied to load balancing, regarding sub-optimal mapping of parallel applications to parallel systems...|$|R
40|$|Abstract. Ontology mapping {{has been}} applied widely {{in the field of}} {{semantic}} web. In this paper a new algorithm of ontology mapping were achieved. First, the new algorithms of calculating four individual similarities (concept name, property, instance and structure) between two concepts were mentioned. Secondly, the <b>similarity</b> <b>vectors</b> consisting of four weighted individual similarities were built, and the weights are the linear function of harmony and reliability, and the linear function can measure the importance of individual similarities. Here, each of ontology concept pairs was represented by a <b>similarity</b> <b>vector.</b> Lastly, Support Vector Machine (SVM) was used to accomplish mapping discovery by training the <b>similarity</b> <b>vectors.</b> Experimental results showed that, in our method, precision, recall and f-measure of ontology mapping discovery reached 95 %, 93. 5 % and 94. 24 %, respectively. Our method outperformed other existing methods. Introduction: In this paper, our study mainly is to discover the mapping [1] between concepts belonging to the different ontologies respectively. The proposed algorith...|$|R
40|$|Significant {{time and}} effort {{has been devoted to}} finding feature {{representations}} of images in databases in order to enable content-based image retrieval (CBIR). Relevance feedback is a mechanism for improving retrieval precision over time by allowing the user to implicitly communicate to the system which of these features are relevant and which are not. We propose a relevance feedback retrieval system that, for each retrieval iteration, learns a decision tree to uncover a common thread between all images marked as relevant. This tree is then used as a model for inferring which of the unseen images the user would most likely desire. We evaluate our approach within the domain of HRCT images of the lung. 1. Introduction In CBIR, a query is characterized by a feature vector which is then used by the retrieval mechanism to retrieve images from the database that have similar feature <b>vectors.</b> <b>Similarity</b> to the query is computed using either a default or user-defined similarity metric. The mo [...] ...|$|R
40|$|Entailment is {{a logical}} {{relationship}} in which the truth of a proposition implies the truth of another proposition. The ability to detect entailment has applications in IR, QA, and many other areas. This study uses the vector space model to explore the relationship between cohesion and entailment. A Latent Semantic Analysis space is used to measure cohesion between propositions using <b>vector</b> <b>similarity.</b> We present perhaps the first vector space model of entailment. The critical element of the model is the orthonormal basis, which we propose is a geometric construction for inference...|$|E
40|$|This thesis {{deals with}} {{problems}} of recommender systems and their usage in web applications. There are three main data mining techniques summarized and individual approaches for recommendation. Main {{part of this}} thesis is a suggestion and an implementation of web applications for recommending dishes from restaurants. Algorithm for food recommending is designed and implemented in this paper. The algorithm deals {{with the problem of}} frequently changing items. The algorithm utilizes hybrid filtering technique which is based on content and knowledge. This filtering technique uses cosine <b>vector</b> <b>similarity</b> for computation...|$|E
40|$|Collaborative {{filtering}} as {{a classical}} method of information retrieval {{is widely used}} in helping people to deal with information overload. In this paper, we in-troduce the concept of local user similarity and global user similarity, based on surprisal-based <b>vector</b> <b>similarity</b> {{and the application of}} the concept of maximin distance in graph theory. Surprisal-based <b>vector</b> <b>similarity</b> expresses the rela-tionship between any two users based on the quantities of information (called surprisal) contained in their ratings, which is based on the intuition that less common ratings for a specific item tend to provide more discriminative infor-mation than the most common ratings. Furthermore, traditional methods of computing user similarity can not work if two users have not rated any identical item. To solve this problem, the global user similarity is introduced to define two users being similar if they can be connected through their locally similar neighbors. A weighted user graph is first constructed by using local similarity of any two users as the weight of the edge connecting them. Then the global similarity can be calculated as the maximin distance of any two nodes in the graph. Based on both of Local User Similarity and Global User Similarity, we develop a collaborative filtering framework called LS&GS. An empirical study using the MovieLens dataset shows that the proposed framework outperforms other state-of-the-art collaborative filtering algorithms...|$|E
40|$|Person re-identification is {{particularly}} challenging due to significant appearance changes across separate camera views. In order to re-identify people, a representative human signature should effectively handle differences in illumination, pose and camera parameters. While general appearance-based methods are modelled in Euclidean spaces, {{it has been}} argued that some applications in image and video analysis are better modelled via non-Euclidean manifold geometry. To this end, recent approaches represent images as covariance matri-ces, and interpret such matrices as points on Riemannian manifolds. As direct classification on such manifolds can be difficult, in this paper we propose to represent each manifold point as a <b>vector</b> of <b>similarities</b> to class representers, via a recently introduced form of Bregman matrix divergence known as the Stein divergence. This is followed by using a discriminative mapping of <b>similarity</b> <b>vectors</b> for final classification. The use of <b>similarity</b> <b>vectors</b> is in contrast to the traditional approach of embedding manifolds into tangent spaces, which can suffer from representing the manifold structure inaccurately. Comparative evaluations on benchmark ETHZ and iLIDS datasets for the person re-identification task show that the proposed approach obtains better performance than recent tech...|$|R
3000|$|... is the {{interaction}} index between criteria i and j (as defined in [30]), and z is the <b>vector</b> of pairwise <b>similarities</b> {{obtained from the}} comparison of email features for any pair of scam messages.|$|R
40|$|Minwise hashing is the {{standard}} technique {{in the context of}} search and databases for efficiently estimating set (e. g., high-dimensional 0 / 1 <b>vector)</b> <b>similarities.</b> Recently, b-bit minwise hashing was proposed which significantly improves upon the original minwise hashing in practice by storing only the lowest b bits of each hashed value, as opposed to using 64 bits. b-bit hashing is particularly effective in applications which mainly concern sets of high similarities (e. g., the resemblance > 0. 5). However, there are other important applications in which not just pairs of high similarities matter. For example, many learning algorithms require all pairwise similarities and it is expected that {{only a small fraction of}} the pairs are similar. Furthermore, many applications care more about containment (e. g., how much one object is contained by another object) than the resemblance. In this paper, we show that the estimators for minwise hashing and b-bit minwise hashing used in the current practice can be systematically improved and the improvements are most significant for set pairs of low resemblance and high containment...|$|R
30|$|The distractor {{candidates}} are collected from several sources, including the synonyms of the co-occurrence words {{with the same}} part-of-speech as the target word in the reading passage, and the sibling and hyponym words of the target word in a lexical taxonomy. These {{candidates are}} filtered and the cosine similarity is further used for calculating the word <b>vector</b> <b>similarity</b> between the correct answer and every distractor candidate (Susanti et al. 2015). The three candidates with the lowest similarity are chosen for the low-level distractors, and the three candidates with the highest similarity are chosen as the high-level distractors.|$|E
30|$|Initially, our dataset {{contained}} 30 malicious apps {{but during}} feature <b>vector</b> <b>similarity</b> scoring using the Manhattan Distance metric, we discovered {{the existence of}} two malicious apps that consistently yielded a Manhattan Distance of 0.0 with multiple benign apps. In other words, the two malicious apps are identical to multiple benign apps. These ambiguous apps adversely affect the two-class identification process employed in this research. Therefore, we chose to remove them from our dataset for further study. For this research, {{the removal of the}} two apps resulted in a dataset of 30 benign apps and 28 malicious apps.|$|E
40|$|This paper proposes an {{efficient}} method {{to extract the}} most important and non-redundant sentence segments based on sum of similarity. The new characteristics of our method are listed as follows: 1) we use preprocessing to delete the additional information (comma parenthesis) that won't turn up in the summarization; 2) redesign the <b>vector</b> <b>similarity</b> between a pair of sentences by using sum of similarity; 3) in order to maximize topic diversity, we use a strikingly different redundancy reducing ways other than MMR [...] Experimental results show that our approach compares favorably with some other summary systems. 1...|$|E
40|$|Abstract—Coping with {{nonlinear}} distortions in fingerprint matching is {{a challenging}} task. This paper proposes a novel method, a fuzzy feature match (FFM) {{based on a}} local triangle feature set to match the deformed fingerprints. The fingerprint {{is represented by the}} fuzzy feature set: the local triangle feature set. The similarity between the fuzzy feature set is used to characterize the similarity between fingerprints. A fuzzy similarity measure for two triangles is introduced and extended to construct a <b>similarity</b> <b>vector</b> including the triangle-level similarities for all triangles in two fingerprints. Accordingly, a <b>similarity</b> <b>vector</b> pair is defined to illustrate the similarities between two fingerprints. The FFM method maps the <b>similarity</b> <b>vector</b> pair to a normalized value which quantifies the overall image to image similarity. The pro-posed algorithm has been evaluated with NIST 24 and FVC 2004 fingerprint databases. Experimental results confirm that the proposed FFM based on the local triangle feature set is a reliable and effective algorithm for fingerprint matching with nonlinear distortions. Index Terms—Distortion, fingerprint recognition, matching, minutia, similarity measure. I...|$|R
40|$|Deep neural {{networks}} (DNN) {{have been successfully}} applied to music classification including music tagging. However, there are several open questions regarding the training, evaluation, and analysis of DNNs. In this article, we investigate specific aspects of {{neural networks}}, the effects of noisy labels, to deepen our understanding of their properties. We analyse and (re-) validate a large music tagging dataset to investigate the reliability of training and evaluation. Using a trained network, we compute label <b>vector</b> <b>similarities</b> which is compared to groundtruth similarity. The results highlight several important aspects of music tagging and neural networks. We show that networks can be effective despite relatively large error rates in groundtruth datasets, while conjecturing that label noise can {{be the cause of}} varying tag-wise performance differences. Lastly, the analysis of our trained network provides valuable insight into the relationships between music tags. These results highlight the benefit of using data-driven methods to address automatic music tagging. Comment: The section that overlapped with arXiv: 1709. 01922 is completely removed since the earlier version. This is the camera-ready versio...|$|R
40|$|Efficient {{watermarking}} techniques guarantee inaudibility and robustness against signal degradation. Spread spectrum watermarking technique {{makes it}} harder for unauthorized adversary to detect the position of the embedded watermark in the carrier file, because the watermark bits are spread in the carrier medium. Unfortunately, there is a high possibility that synchronization of the watermark bits and carrier bits will go out of phase. This will lead to watermark detection problem in the carrier bit sequence. In this paper, we propose a vector space projections approach on spread spectrum audio watermarking technique, in order to presents both the watermark bits and carrier bits as <b>vectors.</b> <b>Similarities</b> of watermark <b>vector</b> to a carrier vector are resolve by the normalized dot product of the cosine of angle between them for embedding. After embedding and extraction by the technique, signal processing methods in the form of attacks were applied. Our approach proved robust when compared with other audio watermarking techniques. This technique gives good results and was found to be robust on performance test...|$|R
