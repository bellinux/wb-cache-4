56|381|Public
50|$|VPN: the view-plane normal - {{a normal}} to the <b>view</b> <b>plane.</b>|$|E
50|$|VUV: the view-up vector - {{the vector}} on the <b>view</b> <b>plane</b> that {{indicates}} the upward direction.|$|E
50|$|VRP: {{the viewing}} {{reference}} point - a point {{located on the}} <b>view</b> <b>plane,</b> and {{the origin of the}} VRC.|$|E
50|$|Roof {{plans are}} orthographic projections, {{but they are}} not {{sections}} as their <b>viewing</b> <b>plane</b> is outside of the object.|$|R
2500|$|When the {{distance}} between the slits and the <b>viewing</b> <b>plane</b> is , the spacing of the fringes is equal to [...] and is the same as above: ...|$|R
5000|$|... #Caption: Linear {{perspective}} of a cube (left) and reverse perspective (right). The <b>viewing</b> <b>plane</b> is shown in blue, with the projection point where the red lines meet.|$|R
50|$|For 3D {{modeling}} {{in computer}} graphics, panning means moving {{parallel to the}} current <b>view</b> <b>plane.</b> In other words, the camera moves perpendicular to the direction it is pointed.|$|E
50|$|Families {{in nearby}} Eastern Passage {{have a deep}} {{attachment}} which is especially felt by the many descendants of the families who were once residents of Devils Island and who now live in Eastern Passage and share a living connection {{to the history of}} this island as well as a vivid <b>view</b> <b>plane</b> of this iconic structure against sky and sea. Many Eastern Passage residents and descendants of Devils Island long to see a lighted tower on the island again.|$|E
5000|$|The TMT {{project is}} a {{response}} to a recommendation in 2000 from the US National Academy of Sciences that a thirty-meter telescope be the top priority and be built within the decade. Urgency in construction is due to the competitive nature of science with the European-Extremely Large Telescope also under construction. Mauna Kea's summit is the most sacred of all the mountains in Hawaii to many, but not all, Native Hawaiian people. Hawaiian cultural practitioners cite impacts to indigenous cultural practice, while recreational users have argued that construction harms the scenic <b>view</b> <b>plane.</b> Some environmentalists are concerned that irreparable ecological damage may be done by construction, although this has been disputed by other environmental advocates. All three groups are represented among the petitioners opposing the TMT. According to the State of Hawaii law HAR 13-5-30, the eight key criteria must be met before construction be allowed on conservation lands in Hawaii. Among other criteria, the development may not [...] "cause substantial adverse impact to existing natural resources within the surrounding area, community, or region,” and the [...] "existing physical and environmental aspects of the land must be preserved or improved upon." ...|$|E
2500|$|The figure {{shows the}} {{geometry}} for a far-field <b>viewing</b> <b>plane.</b> It is {{seen that the}} relative paths of the light travelling from the two points sources to a given point in the <b>viewing</b> <b>plane</b> varies with the angle θ, so that their relative phases also vary. [...] When the path difference is equal to an integral number of wavelengths, the two waves add together to give a maximum in the brightness, whereas when the path difference is equal to half a wavelength, or {{one and a half}} etc., then the two waves cancel, and the intensity is at a minimum. [...] The angular spacing of the fringes, , is then given by ...|$|R
40|$|Background Magnetic {{resonance}} imaging {{has become the}} method of choice to diagnose meniscal and ligament lesions of the knee. New approaches to the surgical treatment of partial anterior cruciate ligament tears led us to try to distinguish the two bundles of that ligament with MRI and to evaluate inter-observer agreement in diVerent <b>viewing</b> <b>planes.</b> Methods Images of 50 right and 50 left ligament-intact knees were examined in the coronal and axial <b>viewing</b> <b>planes.</b> Each sequence was independently read by a radiologist and a medical student to note, in each <b>viewing</b> <b>plane,</b> {{the number of images}} in which the two-bundle structure of the ligament was clearly seen. Cohen’s Kappa coeYcients were used to determine inter-observer agreements. Results The percentage of sequences in which the two bundles were distinguished by the radiologist and the student on one image at least were 82 versus 73 % in the coronal plane and 90 versus 93 % in the axial plane, respectively. The average number of successive images with clear bundles was higher in the axial (2. 7 vs. 1. 7) than in the coronal plane (2. 2 vs. 1). There was a poor interobserver agreement in the coronal plane (k = 0. 176) but anintermediate agreement in the axial plane (k = 0. 385 or 0. 522 depending on number pooling). Conclusions The axial <b>viewing</b> <b>plane</b> seems more favourable to distinguish the two bundles of the anterior cruciate ligament with the best possible reproducibility. An obliqueaxial plane is deemed to insure a clearer diagnosis of partial tears...|$|R
50|$|These slices {{can either}} be aligned with the volume and {{rendered}} {{at an angle to}} the viewer, or aligned with the <b>viewing</b> <b>plane</b> and sampled from unaligned slices through the volume. Graphics hardware support for 3D textures is needed for the second technique.|$|R
40|$|With {{increasing}} {{information density}} and complexity, computer displays may become visually cluttered, adversely affecting overall usability. Text labels can significantly add to visual clutter in graphical user interfaces, but are generally kept legible through specific label placement algorithms that seek visual separation of labels {{and other objects}} in the 2 D <b>view</b> <b>plane.</b> This work studies an alternative approach: can overlapping labels be visually segregated by distributing them in stereoscopic depth? The {{fact that we have}} two forward-looking eyes yields stereoscopic disparity: each eye has a slightly different perspective on objects in the visual field. Disparity is used for depth perception by the human visual system, and is therefore also provided by stereoscopic 3 D displays to produce a sense of depth. This work has shown that a stereoscopic label placement algorithm yields user performance comparable with existing algorithms that separate labels in the <b>view</b> <b>plane.</b> At the same time, such stereoscopic label placement is subjectively rated significantly less disturbing than traditional methods. Furthermore, it does not allow for potentially ambiguous spatial relationships between labels and background objects inherent to labels separated in the <b>view</b> <b>plane.</b> These findings are important for display systems where disturbance, distraction and ambiguity of the overlay can negatively impact safety and efficiency of the system, including the reference application of this work: an augmented vision system for Air Traffic Control towers...|$|E
40|$|This paper {{describes}} a new algorithm for geometric displacement mapping. Its key {{idea is that}} all occluded solutions for an eye ray lie in two-dimensional manifolds perpendicular to the underlying surface to which the height map is applied. The manifold depends only on the eye position and surface geometry, {{and not on the}} height field. A simple stepping algorithm, moving along the surface within a manifold renders a curve of pixels to the <b>view</b> <b>plane,</b> which re-duces height map rendering to a set of one-dimensional computa-tions that can be done in parallel. The curves on the <b>view</b> <b>plane</b> for two specific underlying manifolds, a plane and a sphere, are straight lines. In this paper we focus on the specific geometry of simple un-derlying surfaces for which the geometry is more intuitive and the sampling of the rendered image direct...|$|E
40|$|We {{describe}} a view-management component for interactive 3 D user interfaces. By view management, we mean maintaining visual {{constraints on the}} projections of objects on the <b>view</b> <b>plane,</b> such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible. We introduce algorithms that use upright rectangular extents to represent on the <b>view</b> <b>plane</b> a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3 D objects, {{as well as the}} unoccupied space in which objects can be placed t...|$|E
40|$|An {{electronic}} absolute Cartesian autocollimator {{performs the}} same basic optical function as does a conventional all-optical or a conventional electronic autocollimator but differs {{in the nature of}} its optical target {{and the manner in which}} the position of the image of the target is measured. The term absolute in the name of this apparatus reflects the nature of the position measurement, which, unlike in a conventional electronic autocollimator, is based absolutely on the position of the image rather than on an assumed proportionality between the position and the levels of processed analog electronic signals. The term Cartesian in the name of this apparatus reflects the nature of its optical target. Figure 1 depicts the electronic functional blocks of an electronic absolute Cartesian autocollimator along with its basic optical layout, which is the same as that of a conventional autocollimator. Referring first to the optical layout and functions only, this or any autocollimator is used to measure the compound angular deviation of a flat datum mirror with respect to the optical axis of the autocollimator itself. The optical components include an illuminated target, a beam splitter, an objective or collimating lens, and a viewer or detector (described in more detail below) at a <b>viewing</b> <b>plane.</b> The target and the <b>viewing</b> <b>planes</b> are focal planes of the lens. Target light reflected by the datum mirror is imaged on the <b>viewing</b> <b>plane</b> at unit magnification by the collimating lens. If the normal to the datum mirror is parallel to the optical axis of the autocollimator, then the target image is centered on the <b>viewing</b> <b>plane.</b> Any angular deviation of the normal from the optical axis manifests itself as a lateral displacement of the target image from the center. The magnitude of the displacement is proportional to the focal length and to the magnitude (assumed to be small) of the angular deviation. The direction of the displacement is perpendicular to the axis about which the mirror is slightly tilted. Hence, one can determine the amount and direction of tilt from the coordinates of the target image on the <b>viewing</b> <b>plane...</b>|$|R
5000|$|Reverse perspective, {{also called}} inverse perspective, {{inverted}} perspective, divergent perspective, or Byzantine perspective, {{is a form}} of perspective drawing in which the objects depicted in a scene are placed between the projective point and the <b>viewing</b> <b>plane.</b> This has the visual effect that objects farther away from the <b>viewing</b> <b>plane</b> are drawn as larger, and closer objects are drawn as smaller, in contrast to the more conventional linear perspective for which closer objects appear larger. Lines that are parallel in three-dimensional space are drawn as diverging against the horizon, rather than converging as they do in linear perspective. Technically, the vanishing points are placed outside the painting with the illusion that they are [...] "in front of" [...] the painting.|$|R
50|$|Conceptually, {{the idea}} is to {{transform}} the perspective viewing volume into the orthogonal viewing volume. The perspective viewing volume is a frustum, that is, a truncated pyramid. The orthographic viewing volume is a rectangular box, where both the near and far <b>viewing</b> <b>planes</b> are parallel to the image plane.|$|R
40|$|This {{thesis is}} {{available}} online through Linköping University Electronic Press: www. ep. liu. se With increasing information density and complexity, computer displays may become visually cluttered, adversely affecting overall usability. Text labels can significantly add to visual clutter in graphical user interfaces, but are generally kept legible through specific label placement algorithms that seek visual separation of labels {{and other objects}} in the 2 D <b>view</b> <b>plane.</b> This work studies an alternative approach: can overlapping labels be visually segregated by distributing them in stereoscopic depth? The {{fact that we have}} two forward-looking eyes yields stereoscopic disparity: each eye has a slightly different perspective on objects in the visual field. Disparity is used for depth perception by the human visual system, and is therefore also provided by stereoscopic 3 D displays to produce a sense of depth. This work has shown that a stereoscopic label placement algorithm yields user performance comparable with existing algorithms that separate labels in the <b>view</b> <b>plane...</b>|$|E
40|$|We give an efficient, {{randomized}} {{hidden surface}} removal algorithm for scenes containing intersecting faces. Randomization is assumed only in the algorithm {{and not in the}} input. The algorithm is quasi-output sensitive in the following sense. Project all boundary edges as well as the edges formed by the intersections of scene faces onto the <b>view</b> <b>plane.</b> This gives rise to several junctions in the <b>view</b> <b>plane,</b> visible as well as invisible. Let us define the degree deg(q) of a junction q as the number of scene faces that give rise to q. Define l(q), the obstruction level of q, to be the number of faces in the scene that obscure q with respect to the view point. Thus l(q) = 0 iff q is visible. Then the expected time spent by the algorithm on a junction q is inversely proportional to (1 + l(q)) deg(q) − 1. Thus the work done on a junction decreases very fast as its obstruction level w. r. t. the view point increases...|$|E
40|$|We {{consider}} a surface (semitransparent or opaque) in space, viewed by orthogonal projection to a <b>view</b> <b>plane</b> that is rotating uniformly about an unknown axis (equivalently, a surface rotating about an unknown axis and viewed by orthogonal projection to a fixed <b>view</b> <b>plane).</b> We consider profiles of this surface (also known as apparent contours, occluding contours, and outlines), {{and we do}} not track marked points or curves nor assume that a correspondence problem has been solved. We show that, provided the angular speed is known, the location of the axis, and hence the surface, can be recovered from measurements on the profiles over an interval of time. If the angular speed is unknown, then there is a one-parameter family of solutions similar to the bas-relief ambiguity. The results are obtained by use of frontier points on the surface, which can also be viewed as points of epipolar tangency. Results of a numerical experiment showed that the performance was best with larger extents of rotation or when the axis was nearly perpendicular to the view direction...|$|E
50|$|Multiphoton {{excitation}} {{is a way}} {{of focusing}} the <b>viewing</b> <b>plane</b> of the microscope by taking advantage of the phenomenon where two simultaneous low energy photons are absorbed by a fluorescent moiety which normally absorbs one photon with double their individual energy: say two NIR photons (800 nm) to excite a UV dye (400 nm).|$|R
50|$|F and N {{here are}} the {{distances}} of the far and near <b>viewing</b> <b>planes,</b> respectively. The resulting four vector will be a vector where the homogeneous variable is not 1. Homogenizing the vector, or multiplying it by the inverse of the homogeneous variable such that the homogeneous variable becomes unitary, gives us our resulting 2-D location in the x and y coordinates.|$|R
5000|$|Zhengyou Zhang, [...] "Flexible Camera Calibration by <b>Viewing</b> a <b>Plane</b> from Unknown Orientations", ICCV 1999.|$|R
40|$|Interactive {{tridimensional}} {{visualization of}} terrain models {{can be found}} in Geographical Information Systems and computer games. Both applications share the need for high performance algorithms that are tailored to produce textured images in interactive time, say at least 5 frames per second. In this paper we propose and analyze the extension of a well known ray casting algorithm to the case where the <b>view</b> <b>plane</b> is not vertical. Three efficient algorithms are presented and compared. Experimental results are shown and conclusions are made...|$|E
40|$|What is a {{nonlinear}} projection? � Anything that's not {{a linear}} projection. � They {{are known as}} linear projections because the projectors are straight lines � A linear system is defined by system which satisfies the following properties � Superposition – f(x + y) = f(x) + f(y) � Homogeneity – f(kx) = k*f(x) Pinhole camera review Recall the pinhole camera algorithm This technique casts a ray in a direction defined by the camera through the <b>view</b> <b>plane</b> into the scene to create the projection � d = x u + y v –dw v...|$|E
40|$|On the {{one hand}} normal mapping is a common {{technique}} to improve normal interpolation of low tesselated triangle meshes for a realistic lighting. On the other hand today’s graphics hardware allows texturing of <b>view</b> <b>plane</b> aligned point primitives. In this paper we illustrate how to use textured points together with normal mapping to increase surfel splatting quality, especially when using larger splats on lower level of detail. In combination with a silhouette refinement {{this results in a}} significant decimation of needed surfels with small visual disadvantages only. Furthermore, we explain how to create a normal map for points within a point hierarchy...|$|E
40|$|We have {{previously}} reported a standardized 10 -step sequence of monoplane (transverse plane) transesophageal two-dimensional echocardiographic views and a standardized 7 -step vertical plane examination, both suitable for expeditious intraoperative {{use by the}} beginning practitioner. A multiplane transesophageal examination involves transverse <b>plane</b> <b>views,</b> vertical <b>plane</b> <b>views</b> and the remaining "in-between" oblique <b>plane</b> <b>views.</b> This report describes a sequence of specific oblique views {{to be used as}} a framework for the completion of a multiplane transesophageal examination. Each of these steps is illustrated with a two-dimensional echocardiographic image, a matching diagram and a schematic representation of the corresponding axis of interrogation. This description of oblique plane imaging, therefore, completes the components of a multiplane transesophageal examination...|$|R
40|$|We {{present a}} new method for solving the {{hidden surface removal}} (HSR) problem in {{parallel}} on a crew pram {{using a combination of}} new observations and known methods for solving related geometric problems. The new algorithm obtains the bounds of O (log 2 n) time and O (n long n + 1) processors, where n is the amount of given endpoints of the input and l is the number of intersections in the <b>viewing</b> <b>plane.</b> In contrast to most known algorithms solving the HSR problem in object space, this method is able to process input scenes where penetrations of line segments and polygonal areas are allowed. The ability to process such an input has enormous advantage for integrating three dimensional curves c ∩ IR 3, which have been approximated by segments. Since a penetration of two polygonal areas will be detected during run time, this algorithm could also be used for testing given scenes for intersections of polygonal areas. In this case the algorithm is able to solve the hidden line removal problem in the same bounds. For practical purposes this algorithm allows also non convex (but simple and planar) polygons as input. You do not have to have triangulated scenes. According to the constraints of object space HSR algorithms, we will construct the visibility graph of the given scene as a planar graph in the <b>viewing</b> <b>plane.</b> Although the number of processors depends on the number of intersections in the <b>viewing</b> <b>plane,</b> in most given scenes this method will work like an output sensitive algorithm. Typical examples like "one big rectangle is covering all intersections" will be detected and these intersections are not computed...|$|R
50|$|The term cabinet {{projection}} (sometimes cabinet perspective) {{stems from}} {{its use in}} illustrations by the furniture industry. Like cavalier perspective, one face of the projected object {{is parallel to the}} <b>viewing</b> <b>plane,</b> and the third axis is projected as going off at an angle (typically 63.4°). Unlike cavalier projection, where the third axis keeps its length, with cabinet projection the length of the receding lines is cut in half.|$|R
40|$|In this article, {{we present}} a method for {{rendering}} dynamic scenes featuring translucent procedural volumetric detail with all-frequency soft shadows being cast from objects residing inside the view frustum. Our approach {{is based on an}} approximation of physically correct shadows from distant Gaussian area light sources positioned behind the <b>view</b> <b>plane,</b> using iterative convolution. We present a theoretical and empirical analysis of this model and propose an efficient class of convolution kernels which provide high quality at interactive frame rates. Our GPU-based implementation supports arbitrary volumetric detail maps, requires no precomputation, and therefore allows for real-time modification of all rendering parameters...|$|E
40|$|With {{the advent}} of sketch-based methods for shape construction, there's a new degree of power {{available}} in the rapid creation of approximate shapes. Sketch [Zeleznik, 1996] showed how a gesture-based modeler {{could be used to}} simplify conventional CSG-like shape creation. Teddy [Igarashi, 1999] extended this to more free-form models, getting much of its power from its "inflation" operation (which converted a simple closed curve in the plane into a 3 D shape whose silhouette, from the current point of view, was that curve on the <b>view</b> <b>plane)</b> and from an elegant collection of gestures for attaching additional parts to a shape, cutting a shape, and deforming it...|$|E
40|$|We {{develop a}} novel method {{to study the}} gas phase {{features}} in a bubbly Taylor–Couette flow when bubbles are arranged as elevated toroidal strings. The flow is recorded in the front <b>view</b> <b>plane</b> with a highspeed camera for a Reynolds number of 1500 and a global void fraction of 0. 14 %. An image processing algorithm is developed to discriminate bubbles accumulated in clouds near the inner cylinder (cloud bubbles) from bubbles trapped in the bulk flow by vortices (swirl bubbles). The analysis of the preferential positions, azimuthal velocities, and equivalent void fraction of clouds and swirl bubbles separately provides a new insight into {{the dynamics of the}} bubble’s entrapment...|$|E
40|$|Realistic {{images of}} faces of {{individual}} {{people have been}} generated by computer by texture mapping {{a photograph of the}} individual onto a wireframe model of their head. In previous work each individual had to be measured in order to build this model. We use a generic head model to describe the shape of a human head. This model gives us “world knowledge ” of the shape of a head, to create the new images we distorted it to fit the face in the photograph, creating a new, specific, head model. This specific model can be distorted further to change the expression before it is projected onto a <b>viewing</b> <b>plane.</b> We then map the texture from the photograph of the person, one triangle of the model at a time, onto the <b>viewing</b> <b>plane</b> to create a novel view of the individual. Texture from more than one image may be used, by selecting the most appropriate image for each triangle of the model. The generic → specific transformation is implemented by making each edge o...|$|R
50|$|The term cabinet {{projection}} (sometimes cabinet perspective) {{stems from}} {{its use in}} illustrations by the furniture industry. Like cavalier perspective, one face of the projected object {{is parallel to the}} <b>viewing</b> <b>plane,</b> and the third axis is projected as going off in an angle (typically 30° or 45° or arctan(2) = 63.4°). Unlike cavalier projection, where the third axis keeps its length, with cabinet projection the length of the receding lines is cut in half.|$|R
40|$|Considerations of the {{geometry}} appropriate to {{observations of the}} zodiacal light made {{from out of the}} ecliptic plane yield the general inversion of the brightness integral. The brightness per unit volume of interplanetary space can thus be determined in the immediate neighborhood of the spacecraft in directions confined to a unique <b>viewing</b> <b>plane</b> which depends upon the spacecraft's trajectory. The implementation of this technique guarantees the maximum information content of optical observations made from future deep-space probes including the 'Out-of-Ecliptic' mission scheduled for launch in 1983...|$|R
