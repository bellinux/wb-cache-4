38|295|Public
5000|$|... 1993. [...] "Abner Shimony Selected Papers, Search for a Naturalistic World <b>View,</b> <b>Volume</b> 1, Scientific Method and Epistemology" ...|$|E
5000|$|... 1993. [...] "Abner Shimony Selected Papers, Search for a Naturalistic World <b>View,</b> <b>Volume</b> 2, Natural Science and Metaphysics" ...|$|E
5000|$|Biomedical data {{visualization}} in 2D and 3D: multiplanar reformation, ortho slice view, multi slice <b>view,</b> <b>volume</b> rendering, X-ray rendering, maximum intensity projection ...|$|E
50|$|Conceptually, {{the idea}} is to {{transform}} the perspective <b>viewing</b> <b>volume</b> into the orthogonal <b>viewing</b> <b>volume.</b> The perspective <b>viewing</b> <b>volume</b> is a frustum, that is, a truncated pyramid. The orthographic <b>viewing</b> <b>volume</b> is a rectangular box, where both the near and far viewing planes are parallel to the image plane.|$|R
40|$|While {{conventional}} asymptotic transformations for mapping {{a visual}} scene onto a stereo <b>viewing</b> <b>volume</b> allow a single, specific scene-distance {{to be fixed}} at the screen location, the present piecewise linear approach allows creative partitioning of the depth <b>viewing</b> <b>volume</b> and affords the freedom to place depth-cueing emphasis wherever desired. Attention {{is given to the}} results of an experiment with the novel system which attempted to ascertain the effective region of stereopsis cueing. A practical <b>viewing</b> <b>volume</b> falls between - 25 and + 60 percent of the viewer-to-screen distance. The data indicate that increased viewer-to-CRT distances furnish increasing usable depth...|$|R
40|$|The {{number of}} polygons in {{realistic}} architectural models is {{many more than}} can be rendered at interactive frame rates. Typically, however, due to occlusion by opaque surfaces (e. g., walls), only small fractions of such models are visible from most viewpoints. This fact is used in many popular methods for preprocessing visibility information which assume a scene model subdivided into convex cells connected through convex portals. These methods need to establish which cells or parts thereof are visible to a generalized observer located within each cell. The geometry of this information is termed a `visibility volume' and its computation is usually quite complex. Conservative approximations of <b>viewing</b> <b>volumes,</b> however, are simpler and less expensive to compute. In this paper we present techniques and algorithms which permit the computation of conservative <b>viewing</b> <b>volumes</b> incrementally. In particular, we describe an algorithm for computing the <b>viewing</b> <b>volumes</b> for a given cell through a seq [...] ...|$|R
50|$|A line-clipping method {{consists}} of various parts. Tests are conducted {{on a given}} line segment {{to find out whether}} it lies outside the <b>view</b> <b>volume.</b> Afterwards, intersection calculations are carried out with one or more clipping boundaries.|$|E
5000|$|Whitaker, G. B. (1827). The History of Paris {{from the}} {{earliest}} period to the present day: containing a description of its antiquities, public buildings, civil, religious, scientific, and commercial institutions (3 volumes). London: G. B. Whitaker. <b>View</b> <b>volume</b> 2 at Google Books.|$|E
50|$|It is {{possible}} to eliminate unneeded data from going through the rendering pipeline to cut out extraneous work (called <b>view</b> <b>volume</b> clipping and backface culling). After the vertex engine is done working with the geometry, all the 2D calculated data {{is sent to the}} pixel engine for further processing such as texturing and fragment shading.|$|E
50|$|More {{advanced}} techniques use data {{structures to}} cull out objects which are either outside the <b>viewing</b> <b>volume</b> or are occluded by other objects. The most common data structures are binary space partitions, octrees, and cell and portal culling.|$|R
40|$|We {{introduce}} a flexible projection framework that {{is capable of}} modeling a wide variety of linear, nonlinear, and hand-tailored artistic projections with a single camera. This framework introduces a unified geometry for all of these types of projections using the concept of a flexible <b>viewing</b> <b>volume.</b> With a parametric representation of the <b>viewing</b> <b>volume,</b> we obtain the ability to create curvy volumes, curvy near and far clipping surfaces, and curvy projectors. Through a description of the framework’s geometry, we illustrate its capabilities to recreate existing projections and reveal new projection variations. Further, we apply two techniques for rendering the framework’s projections: ray casting, and a limited GPU based scanline algorithm that achieves real-time results...|$|R
2500|$|Harold Bloom echoes Arnold's self-characterization in his {{introduction}} (as series editor) to the Modern Critical <b>Views</b> <b>volume</b> on Arnold: [...] "Arnold got into his poetry what Tennyson and Browning scarcely needed (but absorbed anyway), the main march of mind of his time." [...] Of his poetry, Bloom says, ...|$|R
50|$|Clipping, in {{the context}} of {{computer}} graphics, is a method to selectively enable or disable rendering operations within a defined region of interest. Mathematically, clipping can be described using the terminology of constructive geometry. A rendering algorithm only draws pixels in the intersection between the clip region and the scene model. Lines and surfaces outside the <b>view</b> <b>volume</b> (aka. frustum) are removed.|$|E
5000|$|Testing {{against a}} {{bounding}} volume is typically {{much faster than}} testing against the object itself, because of the bounding volume's simpler geometry. This is because an 'object' is typically composed of polygons or data structures that are reduced to polygonal approximations. In either case, it is computationally wasteful to test each polygon against the <b>view</b> <b>volume</b> if the object is not visible. (Onscreen objects must be 'clipped' to the screen, regardless of whether their surfaces are actually visible.) ...|$|E
50|$|The Hogel {{processing}} unit (HPU) {{is a highly}} parallel homogeneous computation device dedicated to rendering hogels for a holographic light-field display and encompasses the 3D scene conversion into hogels, the 2D post processing filters on hogels for spatial and color corrections and framebuffer management tasks. Hogels are similar to a sub-aperture image in a plenoptic radiance image, in that a hogel represents both the direction and intensity of light within a frustum from a given point on the light-field display plane. The resulting projected light-field is full parallax, allowing the viewer a perspective correct visualization within the light-field display <b>view</b> <b>volume.</b>|$|E
40|$|Abstract. In-Situ Particle Monitors (ISPMs) {{are widely}} used today to monitor {{particles}} within and downstream of semiconductor processing equipment, for they can detect particles under vacuum conditions on real-time basis. However, ISPMs have some limitations. Firstly, since {{only a small fraction}} of the flow passes through the laser beam, the counting efficiency of ISPMs is quite low. Also, particles are not uniformly distributed in low pressure flows and this results in the poor correlation between ISPM counts and the actual particle concentration. Secondly, the intensity of the laser light in the <b>viewing</b> <b>volume</b> is not uniform, but instead has a profile that is typically Gaussian. This leads to a broad distribution of scattered intensities for particles of a given size. Therefore, ISPMs cannot offer accurate sizing resolution. In this experimental study, we improved ISPM performance by using aerodynamic lenses to focus particles through the center of the illuminated optical <b>viewing</b> <b>volume.</b> Aerodynamic lenses consist of a series of circular orifices in a cylindrical tube. The flow converges and then diverges as it flows through these orifices. Because of their inertia, particles do not perfectly follow the flow stream lines. Particles smaller than a critical size concentrate along the centerline of the flow, while particles larger than this critical size are lost by inertial deposition. Therefore, it is possible to design a lens system that focuses all the particles through the <b>viewing</b> <b>volume.</b> We foun...|$|R
5000|$|Rear <b>View</b> Mirror, <b>Volume</b> 2 2004 (recorded 1977-80*) *album credits erroneously state 1976-79 ...|$|R
40|$|Real-world, 3 -D, {{pictorial}} displays incorporating true depth cues via stereopsis techniques offer {{a potential}} means of displaying complex {{information in a}} natural way to prevent loss of situational awareness and provide increases in pilot/vehicle performance in advanced flight display concepts. Optimal use of stereopsis requires {{an understanding of the}} depth <b>viewing</b> <b>volume</b> available to the display designer. Suggested guidelines are presented for the depth <b>viewing</b> <b>volume</b> from an empirical determination of the effective region of stereopsis cueing (at several viewer-CRT screen distances) for a time multiplexed stereopsis display system. The results provide the display designer with information that will allow more effective placement of depth information to enable the full exploitation of stereopsis cueing. Increasing viewer-CRT screen distances provides increasing amounts of usable depth, but with decreasing fields-of-view. A stereopsis hardware system that permits an increased viewer-screen distance by incorporating larger screen sizes or collimation optics to maintain the field-of-view at required levels would provide a much larger stereo depth-viewing volume...|$|R
5000|$|The 3D {{projection}} step {{transforms the}} <b>view</b> <b>volume</b> into a cube with the corner point coordinates (-1, -1, -1) and (1, 1, 1); Occasionally other target volumes are also used. This step is called projection, {{even though it}} transforms a volume into another volume, since the resulting Z coordinates are not stored in the image, but are only used in Z-buffering in the later rastering step. In a perspective illustration, a central projection is used. To {{limit the number of}} displayed objects, two additional clipping planes are used; The visual volume is therefore a truncated pyramid (frustum). The parallel or orthogonal projection is used, for example, for technical representations because it has the advantage that all parallels in the object space are also parallel in the image space, and the surfaces and volumes are the same size regardless of the distance from the viewer. Maps use, for example, an orthogonal projection (so-called orthophoto), but oblique images of a landscape cannot be used in this way - although they can technically be rendered, they seem so distorted that we cannot make any use of them.The formula for calculating a perspective mapping matrix is: ...|$|E
40|$|Recall: 3 D viewing set upProjection Transformation � <b>View</b> <b>volume</b> {{can have}} {{different}} shapes � Different types of projection: parallel, perspective, etc � Control <b>view</b> <b>volume</b> parameters Projection type: perspective, orthographic, etc. Field {{of view and}} aspect ratio Near and far clipping planesPerspective Projection � Similar to real world � object foreshortening: Objects appear larger if closer to cameraPerspective Projection � Need...|$|E
40|$|This paper {{describes}} a new algorithm for <b>view</b> <b>volume</b> culling. During an interactive walkthrough of a 3 D scene, {{at any moment}} {{a large proportion of}} objects will be outside of the <b>view</b> <b>volume.</b> Frame-to-frame coherence implies that the sets of objects that are completely outside, completely inside, or intersecting the boundary of the <b>view</b> <b>volume,</b> will change slowly over time. This coherence is exploited to develop an algorithm that quickly identifies these three sets of objects, and partitions those completely outside into subsets which are probabilistically sampled according to their distance from the <b>view</b> <b>volume.</b> A statistical object representation scheme is used to classify objects into the various sets. The algorithm is implemented {{in the context of a}} Binary Space Partition tree, and preliminary investigation of the algorithm on two scenes with more than 11, 000 polygons, suggests that it is approximately twice as fast as the hierarchical bounding box approach to culling, and that only about 14 % of the total frame-polygons are passed through the viewing pipeline during the course of a walkthrough...|$|E
50|$|Among these {{features}} are mapping between screen- and world-coordinates, generation of texture mipmaps, drawing of quadric surfaces, NURBS, tessellation of polygonal primitives, interpretation of OpenGL error codes, an extended range of transformation routines {{for setting up}} <b>viewing</b> <b>volumes</b> and simple positioning of the camera, generally in more human-friendly terms than the routines presented by OpenGL. It also provides additional primitives for use in OpenGL applications, including spheres, cylinders and disks.|$|R
50|$|The main {{interface}} window is simple to navigate. From here, the original video file {{and all of}} the composition/movie settings can be <b>viewed,</b> <b>volume</b> settings can be adjusted, individual tracks can be soloed or muted and the soundtrack can be rearranged. This is also where the compose button which can be found, which is where all of the already mentioned compositional settings are set before the soundtrack is composed.|$|R
40|$|The {{task of the}} {{rendering}} process is to display the primitives used to represent the 3 D volumetric scene onto a 2 D screen. Rendering is composed of a viewing process which {{is the subject of}} this paper, and the shading process. The projection process determines, for each screen pixel, which objects are seen y the sight ray cast from this pixel into the scene. The viewing algorithm is heavily dependent on the e e display primitives used to represent the volume and whether volume rendering or surface rendering ar mployed. Conventional viewing algorithms and graphics engines can be utilized to display geometric d primitives, typically employing surface rendering. However, when volume primitives are displayed irectly, a special <b>volume</b> <b>viewing</b> algorithm should be employed. This algorithm should capture the conp tents of the voxels on the surface as well as the inside of the volumetric object being visualized. This aper surveys and compares previous work in the field of direct <b>volume</b> <b>viewing.</b> We describe methods c for accelerating <b>volume</b> <b>viewing,</b> parallel <b>volume</b> <b>viewing</b> algorithms, and special purpose hardware. We onclude by surveying recent method for rendering intermixed volume-and-surface datasets and algorithms for <b>volume</b> <b>viewing</b> of various types of irregular grids...|$|R
30|$|Development {{of a new}} spherical volume {{rendering}} (SVR) {{technique that}} can generate a complete and camera-invariant <b>view</b> (<b>volume</b> map) of the entire structure.|$|E
40|$|We {{present a}} new method for {{displaying}} time varying volumetric data. The {{core of the}} algorithm is an integration through time producing a single <b>view</b> <b>volume</b> that captures the essence of multiple time steps in a sequence. The resulting <b>view</b> <b>volume</b> then can be viewed with traditional raycasting techniques. With different time integration functions, we can generate several kinds of resulting chronovolumes, which illustrate differing types of time varying features to the user. By utilizing graphics hardware and texture memory, the integration through time can be sped up, allowing the user interactive control over the temporal transfer function and exploration of the data...|$|E
40|$|Abstract: We {{propose a}} two phase hybrid {{reflection}} rendering method based on approximating the reflected rays {{with a set}} of simple cameras modeled as continuous 3 -ray cameras. In the first, &quot;backward&quot;, phase, the <b>view</b> <b>volume</b> of each simple camera is intersected with a hierarchical subdivision of the scene to find the geometry it encompasses. In the second, &quot;forward&quot;, phase the geometry is projected with the simple camera. Since the shape and topology of reflected triangles is complex, point based rendering is adopted to reconstruct the reflection. The hybrid method is efficient since it combines advantages of backward and forward techniques: there are two orders of magnitude fewer simple cameras than reflected rays, the hierarchical scene subdivision implements fast <b>view</b> <b>volume</b> culling for each of the simple cameras, and the reflection piece corresponding to each simple camera is computed efficiently in feed-forward fashion. 1...|$|E
5000|$|... #Caption: The Bimaran casket, {{illustrated}} by Charles Masson: <b>view</b> in <b>volume,</b> flattened <b>view</b> {{of a half}} portion of the casket, and bottom.|$|R
40|$|Semi-spherical screen VR {{system is}} introduced, which is under {{development}} {{as the main}} part of a distributed multi-user VR environment. With this system users can obtain more immersive stereo VR view than other conventional system. The imple-mentation details are described, and the rendering method used in this system is discussed. In this method, whole view is rendered by dividing <b>view</b> <b>volumes</b> and processed by inverse distortion cor-rection, which cancel image distortion caused by projection onto spherical screen...|$|R
5000|$|<b>Viewing</b> chamber <b>volume</b> requirements: 0.3 ml (most models) or 0.1 ml (NS500, {{although}} larger volumes must {{be loaded}} into the fluidics system if sample is not directly injected) ...|$|R
40|$|NPSNET-IV. 7 J has {{a limited}} {{capability}} to display up to 10 Dismounted Infantry (DI) icons {{due to the}} enormous number of rendered polygons and computational load required. In order {{to provide a more}} realistic training scenario for the user, NPSNET-IV. 8. 1 requires the capability to display at least 150 independent DI entities in real-time. Requirements were satisfied by creating low-level and medium level-of-detail autonomous and controlled DI icons and their associated motion algorithms to Support existing DI models in NPSNET- IV. 8. 1. Additionally, <b>view</b> <b>volume</b> culling techniques were employed to reduce polygon flow through the graphics pipeline and enhance system speed. This research enhances the capabilities of the NPSNET-IV. 8. 1 to be able to display 150 DI icons. The inclusion of level of detail models composed of fewer polygons than an existing high level model, together with <b>view</b> <b>volume</b> culling ensure NPSNET-IV. 8. 1 can satisfy sponsor requirements of displaying 150 DI icons and still maintain a frame rate of 10 - 15 frames per second. NANAU. S. Navy (U. S. N.) authors...|$|E
40|$|Some recent {{developments}} {{in the area of}} multi-scale viewing have concerned the creation of such views from magnification factors as input. We present an algorithm in which magnification factors are used directly to control the creation of a multiscale view in a 3 D based distortion viewing. Our algorithm relies on the basic geometry of a perspective <b>view</b> <b>volume,</b> the properties of which provide a single step conversion between magnification and transformation. ...|$|E
40|$|We {{introduce}} a probability model of surface visibilities in densely cluttered 3 D scenes. The model assumes the scene {{consists of a}} large number of small surfaces distributed randomly in a 3 D <b>view</b> <b>volume.</b> An example is the leaves and/or branches on a tree. We derive probabilities for surface visibility, instantaneous image velocity under egomotion, and binocular half–occlusions in these scenes. The probabilies depend on parameters such as scene depth, object size, 3 D density, observer speed, and binocular baseline. We verify the correctness of our models using computer graphics simulations and discuss possible applications of this model to various problems in computer vision...|$|E
40|$|Diasporas', {{as used in}} {{the title}} of this volume, refers to a {{multitude}} of groups and communities with widely differing histories, identities and current locations. This book brings together essays on theatre by people of African descent in North America, Cuba, Italy, the UK, Israel and Tasmania. Several chapters present overviews of particular national contexts, others offer insights into play texts or specific performances. Offering a mix of academic and practitioner's points of <b>views,</b> <b>Volume</b> 8 in the African Theatre series analyses and celebrates various aspects of African diasporic theatre worldwide...|$|R
40|$|This paper {{introduces}} {{the idea of}} using real mirrors in combination with rear-projection systems for the purpose of interacting with and navigating through the displayed information. Subsequently a derived application is described. For this, we use a hand-held planar mirror and address two fundamental problems of applying head tracking with rear-projection planes: the limited <b>viewing</b> <b>volume</b> of these environments and their incapability of simultaneously supporting multiple observers. Furthermore, we describe the possibility of combining a reflective pad with a transparent one, thus introducing a complementary tool for interaction and navigation...|$|R
50|$|A New <b>View</b> of London <b>Volume</b> I. London, 1708.|$|R
