191|657|Public
5000|$|Vision Processing Unit - other {{processors}} {{aimed at}} accelerating <b>visual</b> <b>sensing</b> tasks ...|$|E
50|$|Zhiha He {{from the}} University of Missouri, Columbia, MO was named Fellow of the Institute of Electrical and Electronics Engineers (IEEE) in 2015 for {{contributions}} to video communication and <b>visual</b> <b>sensing</b> technologies.|$|E
50|$|According to Richard Felder from North Carolina State University and Linda Silverman {{from the}} Institute {{for the study}} of Advanced Development, most {{engineering}} students are <b>visual,</b> <b>sensing,</b> inductive, and active learners. Also, many of the creative students are global learners. However, most engineering education is auditory, intuitive, deductive, passive and sequential.|$|E
40|$|There {{have been}} many {{implementations}} of virtual reality, using audio and <b>visual</b> <b>senses.</b> However, implementations of mixed reality (MR) have thus far only dealt with the <b>visual</b> <b>sense.</b> We have developed an MR system that merges real and virtual worlds in both the audio and <b>visual</b> <b>senses,</b> wherein the geometric consistency of the audio sense was fully coordinated with the <b>visual</b> <b>sense.</b> We tried two approaches for merging real and virtual worlds in the audio sense, using open-air and closed-air headphones...|$|R
2500|$|Eye-consciousness — seeing apprehended by the <b>visual</b> <b>sense</b> organs ...|$|R
25|$|This passage {{emphasizes}} {{the association of}} the <b>visual</b> <b>sense</b> with spatial orientation. The image of the speaker is placed in a room. The importance of the <b>visual</b> <b>sense</b> {{in the art of}} memory would seem to lead naturally to the importance of a spatial context, given that our sight and depth-perception naturally position images seen within space.|$|R
50|$|The <b>visual</b> <b>sensing</b> {{system can}} be based on {{anything}} from the traditional camera, sonar, and laser to the new technology radio frequency identification (RFID), which transmits radio signals to a tag on an object that emits back an identification code. All four methods aim for three procedures—sensation, estimation, and matching.|$|E
50|$|In this specialty, {{computer}} engineers {{focus on}} developing <b>visual</b> <b>sensing</b> technology to sense an environment, representation of an environment, and {{manipulation of the}} environment. The gathered three-dimensional information is then implemented to perform a variety of tasks. These include, improved human modeling, image communication, and human-computer interfaces, as well as devices such as special-purpose cameras with versatile vision sensors.|$|E
40|$|In vision-guided robotic operations, {{vision is}} used for extracting {{necessary}} information for achieving the task. Since <b>visual</b> <b>sensing</b> is usually performed with limited resources, <b>visual</b> <b>sensing</b> strategies should be planned so that only necessary information is obtained efficiently. This paper describes a method of systematically generating <b>visual</b> <b>sensing</b> strategies based on knowledge of the task to be performed. The generation of the appropriate <b>visual</b> <b>sensing</b> strategy entails knowing what information to extract, where to get it, {{and how to get}} it. This is facilitated by the knowledge of the task, which describes what objects are involved in the operation, and how they are assembled. Our method has been implemented using a laser range finder as the sensor. Experimental results show the feasibility of the method, and point out the importance of task-oriented evaluation of <b>visual</b> <b>sensing</b> strategies. 1 Introduction In vision-guided robotic operations, <b>visual</b> <b>sensing</b> strategies should be [...] ...|$|E
30|$|The <b>visual</b> <b>sense</b> {{of mystery}} in the TCPG is {{heightened}} by the complexity of its spaces.|$|R
2500|$|Perhaps {{the most}} {{important}} principle of the art is {{the dominance of the}} <b>visual</b> <b>sense</b> in combination with the orientation of 'seen' objects within space. This principle is reflected in the early Dialexis fragment on memory, and is found throughout later texts on the art. Mary Carruthers, in a review of Hugh of St. Victor's Didascalion, emphasizes the importance of the <b>visual</b> <b>sense</b> as follows: ...|$|R
5000|$|... #Caption: Images {{taken with}} a lower shutter speed evoke a <b>visual</b> <b>sense</b> of movement. Exposure time 3 seconds.|$|R
40|$|This paper {{describes}} {{a method of}} systematically generating <b>visual</b> <b>sensing</b> strategies based on knowledge of the assembly task to be performed. Since <b>visual</b> <b>sensing</b> is usually performed with limited resources, <b>visual</b> <b>sensing</b> strategies should be planned so that only necessary information is obtained efficiently. The generation of the appropriate <b>visual</b> <b>sensing</b> strategy entails knowing what information to extract, where to get it, {{and how to get}} it. This is facilitated by the knowledge of the task, which describes what objects are involved in the operation, and how they are assembled. In th...|$|E
40|$|It is {{generally}} very difficult, if not impossible, for a robot to perform fine manipulation tasks {{without the benefit}} of some form of sensory feedback during actual task execution. As a result, robot sensing strategy planning is an important component in assembly task planning. This report describes a method of systematically generating <b>visual</b> <b>sensing</b> strategies based on knowledge of the task to be performed. Since <b>visual</b> <b>sensing</b> is usually performed with limited resources, <b>visual</b> <b>sensing</b> strategies should be planned so that only necessary information is obtained efficiently. The generation of the appropriate <b>visual</b> <b>sensing</b> strategy entails knowing what information to extract, where to get it, and how to get it. This is facilitated by the knowledge of the task, which describes what objects are involved in the operation, and how they are assembled. In the proposed method, using the task analysis based on face contact relations betwee...|$|E
40|$|In {{this paper}} we present {{preliminary}} work on integrating <b>visual</b> <b>sensing</b> {{with the more}} traditional sensing modalities for marine locations. We have deployed <b>visual</b> <b>sensing</b> {{at one of the}} Smart Coast WSN sites in Ireland and have built a software platform for gathering and synchronizing all sensed data. We describe how the analysis of a range of different sensor modalities can reinforce readings from a given noisy, unreliable sensor...|$|E
40|$|We {{introduce}} a new task, <b>visual</b> <b>sense</b> disambiguation for verbs: given an image and a verb, assign the correct sense of the verb, i. e., the one that describes the action depicted in the image. Just as textual word sense disambiguation is useful {{for a wide range}} of NLP tasks, <b>visual</b> <b>sense</b> disambiguation can be useful for multimodal tasks such as image retrieval, image description, and text illustration. We introduce VerSe, a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels. We propose an unsupervised algorithm based on Lesk which performs <b>visual</b> <b>sense</b> disambiguation using textual, visual, or multimodal embeddings. We find that textual embeddings perform well when gold-standard textual annotations (object labels and image descriptions) are available, while multimodal embeddings perform well on unannotated images. We also verify our findings by using the textual and multimodal embeddings as features in a supervised setting and analyse the performance of <b>visual</b> <b>sense</b> disambiguation task. VerSe is made publicly available and can be downloaded at: [URL] 11 pages, NAACL-HLT 201...|$|R
5000|$|In {{his book}} on ataxia, Frenkel states: [...] "The <b>visual</b> <b>sense</b> is the {{greatest}} supporting factor in the treatment".|$|R
50|$|Tadpoles of bronzed frogs detect food {{based on}} {{chemical}} cues and not visually, indicating that chemical perception predominates <b>visual</b> <b>senses</b> in R. temporalis tadpoles.|$|R
40|$|In {{order to}} realize an {{autonomous}} nuclear power plant, the sensing system performing daily inspection {{inside of the}} plant is indispensable. For the sensing system, reliability and flexibility are demanded to cope with risky and complicated environments. In order to build such sensing system, wehave been emphasizing the importance of cooperation and active control of various sensing functions, {{and the role of}} <b>visual</b> <b>sensing</b> as the most significant sensing function. In this paper, what criteria must be satisfied by the <b>visual</b> <b>sensing</b> system is first described and the active vision system equipped with foveated image acquisition capability is introduced as the practical implementation. Furthermore we propose the attention control mechanism to realize effective and reliable <b>visual</b> <b>sensing</b> by the foveated active vision system. I...|$|E
40|$|This paper {{focuses on}} flood-region {{detection}} using monitoring images. However, adverse weather affects {{the outcome of}} image segmentation methods. In this paper, we present an experimental comparison of an outdoor <b>visual</b> <b>sensing</b> system using region-growing methods with two different growing rules—namely, GrowCut and RegGro. For each growing rule, several tests on adverse weather and lens-stained scenes were performed, taking into account and analyzing different weather conditions with the outdoor <b>visual</b> <b>sensing</b> system. The influence of several weather conditions was analyzed, highlighting their effect on the outdoor <b>visual</b> <b>sensing</b> system with different growing rules. Furthermore, experimental errors and uncertainties obtained with the growing rules were compared. The segmentation accuracy of flood regions yielded by the GrowCut, RegGro, and hybrid methods was 75 %, 85 %, and 87. 7 %, respectively...|$|E
40|$|This paper {{documents}} research {{exploring the}} issues in using <b>visual</b> <b>sensing</b> to control an underwater robot. The local nature of <b>visual</b> <b>sensing</b> makes it useful for underwater situations in which the exact global location of the robot is not accurately known. In these situations, the relative measurement obtained from <b>visual</b> <b>sensing</b> {{can be used for}} precise control of the vehicle. To perform <b>visual</b> <b>sensing</b> in an underwater environment, several challenges must be overcome. In addition to standard vision processing issues, extreme lighting variations and marine snow make <b>visual</b> <b>sensing</b> problematic. To handle these di culties, an approach using Laplacian of Gaussian sign correlation hardware has been chosen which o ers robust real-time optical ow and stereo disparity measurements. This approach has been applied to several representative underwater vehicle tasks including: 1) object following, 2) station keeping, 3) mosaicking, and 4) navigation. Local sensing One of the largest problems in achieving automatic control of underwater vehicles is adequate sensing capability. Unlike land and space regimes for which GPS is available, there is no global position measurement system in place underwater. 1 Long baseline and other sonic beacon systems can be used in many situations; however, these systems cover limited areas and provide inadequate accuracies and update rates for many applications. A promising alternative to such systems is control from local sensors. Using local sensors for control o ers several advantages. The accuracy and bandwidth achievable with local sensors is much greater. In addition, the accuracies of many local sensors increase with proximity. Local sensors are carried on board and are self-contained. They do not rely upon proper functioning of external system...|$|E
50|$|The {{researchers}} {{concluded that}} the 'light on' subjects exhibited a sensory shift characterized by a drop in <b>visual</b> <b>sense</b> {{and an increase in}} tactile sense.|$|R
5000|$|David Rooney of Variety {{called it}} [...] "a {{promising}} debut from a filmmaker with a strongly developed <b>visual</b> <b>sense</b> {{and a solid}} grasp of character-driven comedy".|$|R
30|$|Humans {{receive a}} great deal of {{information}} through the five senses and receive about 70 % by the <b>visual</b> <b>sense.</b> Video techniques based on the <b>visual</b> <b>sense</b> are extending to television broadcasting, machine, and image processing applications as well as to other applications. Video techniques have developed rapidly with the realization of HDTV broadcasting. Recently, we have tried to reproduce high-resolution three-dimensional stereoscopic images in a way that allows a direct watching of microscopic imaging. Because of the perception of depth, the 3 D stereoscopic imaging has various applications [1 – 3].|$|R
40|$|<b>Visual</b> <b>sensing,</b> such as vision based localization, nav-igation, tracking, {{are crucial}} for {{intelligent}} robots, which have shown great advantage in many robotic applications. However, {{the market is}} still in lack of a powerful <b>visual</b> <b>sensing</b> platform to deal {{with most of the}} visual processing tasks. In this paper we introduce a powerful and efficient platform, Guidance, which is composed of one processor and multiple (up to five) stereo sensing units. Basic visual tasks including visual odometry, obstacle avoidance, depth generation, are given as built-in functions. Additionally, {{with the aid of a}} well documented SDK, Guidance is extremely flexible for users to develop other applications, such as autonomous navigation, SLAM, tracking. 1...|$|E
40|$|With the {{increasing}} climatic extremes, {{the frequency and}} severity of urban flood events have intensified worldwide. In this study, image-based automated monitoring of flood formation and analyses of water level fluctuation were proposed as value-added intelligent sensing applications to turn a passive monitoring camera into a visual sensor. Combined with the proposed <b>visual</b> <b>sensing</b> method, traditional hydrological monitoring cameras {{have the ability to}} sense and analyze the local situation of flood events. This can solve the current problem that image-based flood monitoring heavily relies on continuous manned monitoring. Conventional sensing networks can only offer one-dimensional physical parameters measured by gauge sensors, whereas visual sensors can acquire dynamic image information of monitored sites and provide disaster prevention agencies with actual field information for decision-making to relieve flood hazards. The <b>visual</b> <b>sensing</b> method established in this study provides spatiotemporal information {{that can be used for}} automated remote analysis for monitoring urban floods. This paper focuses on the determination of flood formation based on image-processing techniques. The experimental results suggest that the <b>visual</b> <b>sensing</b> approach may be a reliable way for determining the water fluctuation and measuring its elevation and flood intrusion with respect to real-world coordinates. The performance of the proposed method has been confirmed; it has the capability to monitor and analyze the flood status, and therefore, it can serve as an active flood warning system...|$|E
40|$|Due {{to energy}} and {{throughput}} constraints of <b>visual</b> <b>sensing</b> nodes, in-node energy conservation {{is one of}} the prime concerns in visual sensor networks (VSNs) with wireless transceiving capability. To cope with these constraints, the energy efficiency of a VSN for a given level of reliability can be enhanced by reconfiguring its nodes dynamically to achieve optimal configurations. In this paper, a unified framework for node classification and dynamic self-reconfiguration in VSNs is proposed. The proposed framework incorporates quality-of-information (QoI) awareness using peak signal-to-noise ratio-based representative metric to support a diverse range of applications. First, for a given application, the proposed framework provides a feasible solution for the classification of <b>visual</b> <b>sensing</b> nodes based on their field-of-view by exploiting the heterogeneity of the targeted QoI within the sensing region. Second, with the dynamic realization of QoI, a strategy is devised for selecting suitable configurations of <b>visual</b> <b>sensing</b> nodes to reduce redundant visual content prior to transmission without sacrificing the expected information retrieval reliability. The robustness of the proposed framework is evaluated under various scenarios by considering: 1) target QoI thresholds; 2) degree of heterogeneity; and 3) compression schemes. From the simulation results, it is observed that for the second degree of heterogeneity in targeted QoI, the unified framework outperforms its existing counterparts and results in up to 72...|$|E
40|$|The paper {{designed}} the light illumination of the totally enclosed target lane {{and found out}} the <b>visual</b> <b>sense</b> factors influencing the illumination system. Then, the specific index is determined and the shooting score tests and shooters’ <b>visual</b> <b>senses</b> are counted in the designed light illumination system. The contrastive data analysis of natural light and light of target lane, also with optical aiming and mechanical aiming are implemented. The weight value of the light evaluation is calculated and the design is proved to be reasonable as analyzed objectively. </p...|$|R
40|$|According to the COLREGs, every Officer On Watch (OOW) shall at {{all times}} {{maintain}} a proper look-out. Among all the available means of look-out, sight is the most basic and commonly used one. OOWs always {{need to make a}} full appraisal of the situation and of the risk of collision by sight, including estimating the range and bearing. Unlike the ranging equipment (such as Radar), OOWs, who are keeping a look-out by sight, cannot estimate the distance precisely according to their <b>visual</b> <b>sense.</b> In this paper, an experiment is carried out for finding the difference between the actual distance and the estimating one by OOWs with different onboard experience. The data are analyzed by using methods of statistics, Anomaly Detection, and Two-Step clustering for researching the human element of the <b>visual</b> <b>sense</b> during look-out. The factors, which can affect the <b>visual</b> <b>sense,</b> are revealed. Conclusions are drawn for improving the look-out and enhancing the safety of the navigation...|$|R
5000|$|Jennifer Dunning, {{writing of}} the piece [...] "/Asunder" [...] in the New York Times, {{describes}} Yin Mei as having [...] "a striking <b>visual</b> <b>sense</b> and an authoritative way with social and literary themes." ...|$|R
30|$|In summary, {{a visual}} sensor has been {{developed}} for the sensitive and selective detection of Hg 2 + using unmodified Au@Ag NPs. The changes of solution color and the peak intensities were readily obtained in the Hg 2 + detection. The merits of the <b>visual</b> <b>sensing</b> lie in its simplicity, speed and convenience.|$|E
40|$|Omnidirectional vision sensors, {{which have}} been {{proposed}} in 1970, are recently studied in Computer Vision and Multimedia research. This paper discusses features of previously developed omnidirectional vision sensors and their problems in the design. Further, this paper proposes designs of low-cost and compact omnidirectional vision sensors and their new applications. The author considers utilization of omnidirectional vision sensor will be a key issue in Computer Vision and Multimedia applications. Keywords: Omnidirectional vision sensor, Omnidirectional Image, Visual surveillance, Multimedia, Robot vision. 1 Introduction Physical agents living in complex environments, such as humans and animals, need two types of <b>visual</b> <b>sensing</b> abilities. One is to gaze particular objects with a precise but small retina, {{the other is to}} look around the environment with a wide but coarse retina. Both <b>visual</b> <b>sensing</b> mechanisms are required for realizing robust and flexible visual behaviors. Especiall [...] ...|$|E
40|$|The {{presence}} of raindrop induced image distortion {{has a significant}} {{negative impact on the}} performance {{of a wide range of}} all-weather <b>visual</b> <b>sensing</b> applications including within the increasingly import contexts of visual surveillance and vehicle autonomy. A key part of this problem is robust raindrop detection such that the potential for performance degradation in effected image regions can be identified. Here we address the problem of raindrop detection in colour video imagery using an extended feature descriptor comprising localised shape, saliency and texture information isolated from the overall scene context. This is verified within a bag of visual words feature encoding framework using Support Vector Machine and Random Forest classification to achieve notable 86 % detection accuracy with minimal false positives compared to prior work. Our approach is evaluated under a range of environmental conditions typical of all-weather automotive <b>visual</b> <b>sensing</b> applications...|$|E
6000|$|And now the Spirits of the Mind [...] Are {{busy with}} poor Peter Bell; [...] Upon {{the rights of}} <b>visual</b> <b>sense</b> [...] Usurping, with a {{prevalence}} [...] More terrible than magic spell. [101] [...] 920 ...|$|R
50|$|Ornithomimus had {{legs that}} seem clearly suited for rapid locomotion, with the tibia about 20% {{longer than the}} femur. The large eye sockets suggest a keen <b>visual</b> <b>sense,</b> and also suggest the {{possibility}} that they were nocturnal.|$|R
25|$|The McGurk {{effect was}} greatly reduced when {{attention}} was diverted to a tactile task (touching something). Touch is a sensory perception like vision and audition, therefore increasing attention to touch decreases {{the attention to}} auditory and <b>visual</b> <b>senses.</b>|$|R
