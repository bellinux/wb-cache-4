1696|3144|Public
25|$|Properties of {{the virtual}} machine, like guest OS, {{processor}}, processor features, video mode, video RAM, code cache, IDE controller reads and writes, Ethernet reads and writes, <b>video</b> <b>frame</b> rate and command line options {{can no longer be}} viewed.|$|E
25|$|With severe {{resource}} limitations {{including a}} mere 128 bytes of RAM and no <b>video</b> <b>frame</b> buffer, the 2600 {{is considered to}} be a difficult machine to program. However, tools such as emulators, the batari Basic language, and a wealth of documentation, exist to assist the homebrewer.|$|E
25|$|The Atari 2600 is {{generally}} considered to be a very demanding programming environment, with a mere 128 bytes of RAM available without additional hardware, and no <b>video</b> <b>frame</b> buffer at all. The programmer must prepare each line of video output one at a time as it is being sent to the television. The only sprite capabilities the 2600 offers are one-dimensional 1-bit and 8-bit patterns; creating a two-dimensional object requires changing the pattern between each line of video.|$|E
30|$|CS has {{the linear}} {{monotone}} change characteristics. With {{the increase of}} distortion jitter number of <b>video</b> <b>frames,</b> the quality of mobile multimedia crowd service will be seriously reduced. This {{is due to the}} intense competition between the <b>video</b> <b>frames</b> of the multimedia distortion. Therefore, the mobile multimedia receiver can select the crowd service rejection probability and a sequence of <b>video</b> <b>frames</b> according to the competition state of the <b>video</b> <b>frames.</b>|$|R
3000|$|... is the {{transmission}} delay of <b>video</b> <b>frames.</b> The robustness could be analyzed {{based on the}} encode pixels and <b>video</b> <b>frames.</b>|$|R
30|$|Color histograms of <b>video</b> <b>frames</b> are calculated.|$|R
2500|$|During transmission, single byte errors can be {{replaced}} by a white space which can appear {{at the beginning of the}} program. More byte errors during EIA-608 transmission can affect the screen momentarily, by defaulting to a real-time mode such as the [...] "roll up" [...] style, type random letters on screen, and then revert to normal. Uncorrectable byte errors within the teletext page header will cause whole captions to be dropped. EIA-608, due to using only two characters per <b>video</b> <b>frame,</b> sends these captions ahead of time storing them in a second buffer awaiting a command to display them; Teletext sends these in real-time.|$|E
2500|$|... 7 October 2010 {{marked the}} release of Chromium 8.0, {{seven and a half}} weeks after that of Chromium 7. The initial release in this series was version 8.0.549.0. The {{development}} of Chromium 8.0 focused on improved integration into Chrome OS and improved cloud features. These include background web applications, host remoting (allowing users centrally to control features and settings on other computers) and cloud printing. On 12 January 2011 versions of Chrome and Chromium prior to version 8.0.552.237 were identified by US-CERT as [...] "contain multiple memory corruption vulnerabilities. These vulnerabilities include a stack corruption vulnerability in the PDF renderer component, two memory corruption vulnerabilities in the Vorbis decoder and a <b>video</b> <b>frame</b> size error resulting in a bad memory access... By convincing a user to view a specially crafted HTML document, PDF file, or video file, an attacker can cause the application to crash or possibly execute arbitrary code." [...] This vulnerability was publicized after Chrome version 8.0.552.237 was released fixing these problems, to alert users to upgrade versions as soon as possible.|$|E
2500|$|In January 2014, {{scientists}} from Northwest Normal University in Lanzhou, China, published {{the results of}} recordings made in July 2012 of the optical spectrum of what {{was thought to be}} natural ball lightning made by chance during the study of ordinary cloud–ground lightning on the Tibetan Plateau. At a distance of , a total of 1.64 seconds of digital video of the ball lightning and its spectrum was made, from the formation of the ball lightning after the ordinary lightning struck the ground, up to the optical decay of the phenomenon. [...] Additional video was recorded by a high-speed (3000 frames/sec) camera, which captured only the last 0.78 seconds of the event, due to its limited recording capacity. [...] Both cameras were equipped with slitless spectrographs. [...] The researchers detected emission lines of neutral atomic silicon, calcium, iron, nitrogen and oxygen—in contrast with mainly ionized nitrogen emission lines in the spectrum of the parent lightning. The ball lightning traveled horizontally across the <b>video</b> <b>frame</b> at an average speed equivalent of [...] It had a diameter of [...] and covered a distance of about [...] within those 1.64 s.|$|E
50|$|<b>Video</b> <b>frames</b> are {{typically}} identified using SMPTE time code.|$|R
40|$|Extraction and {{recognition}} of text present in video has become a very popular research area in the last decade. Generally, text present in <b>video</b> <b>frames</b> is of different size, orientation, style, etc. with complex backgrounds, noise, low resolution and contrast. These factors make the automatic text extraction {{and recognition}} in <b>video</b> <b>frames</b> a challenging task. A large number of techniques have been proposed by various researchers {{in the recent past}} to address the problem. This paper presents a review of various state-of-the-art techniques proposed towards different stages (e. g. detection, localization, extraction, etc.) of text information processing in <b>video</b> <b>frames.</b> Looking at the growing popularity and the recent developments in the processing of text in <b>video</b> <b>frames,</b> this review imparts details of current trends and potential directions for further research activities to assist researchers. Griffith Sciences, Griffith School of EngineeringFull Tex...|$|R
30|$|Note {{that the}} query {{results of the}} {{filtering}} functions obtained from the database consist {{of a set of}} FOVs. To return <b>video</b> <b>frames</b> or <b>video</b> segments to the application side, we use FFMPEG library to extract <b>video</b> <b>frames</b> or <b>video</b> segments based on the query results from the video contents stored in the file system.|$|R
2500|$|Kinematics is {{the study}} of the motion of an entire animal or parts of its body. It is {{typically}} accomplished by placing visual markers at particular anatomical locations on the animal and then recording video of its movement. The video is often captured from multiple angles, with frame rates exceeding 2000 frames per second when capturing high speed movement. [...] The location of each marker is determined for each <b>video</b> <b>frame,</b> and data from multiple views is integrated to give positions of each point through time. [...] Computers are sometimes used to track the markers, although this task must often be performed manually. The kinematic data can be used to determine fundamental motion attributes such as velocity, acceleration, joint angles, and the sequencing and timing of kinematic events. These fundamental attributes can be used to quantify various higher level attributes, such as the physical abilities of the animal (e.g., its maximum running speed, how steep a slope it can climb), neural control of locomotion, gait, and responses to environmental variation. These, in turn, can aid in formulation of hypotheses about the animal or locomotion in general.|$|E
50|$|Stepping {{through the}} <b>video</b> <b>frame</b> by frame can cause {{artifacts}} with XvMC.|$|E
5000|$|Multimedia: 3GP, MP4, MP3, WAV, MIDI, <b>Video</b> <b>Frame</b> Rate 30fps, FM Radio ...|$|E
40|$|Absnuct-For entropy-coded MPEG 2 <b>video</b> <b>frames,</b> a {{transmission}} error {{will not only}} affect the underlying codeword but also may affect subsequent codewords, resulting in a great degradation of the received <b>video</b> <b>frames.</b> In this study, transmission errors in MPEG 2 <b>video</b> <b>frames</b> are first detected and located by the error detection scheme proposed by Shyu and Leou 161, and then the corrupted blocks are concealed by the proposed hybrid error concealment scheme. Based {{on the condition of}} a corrupted block, a corrupted block in an intra-coded I frame is concealed by either the spatial error concealment algorithm in H. 26 L Test Model Long-Term Number 9 (TML- 9) or the best neighborhood matching (BNM) algorithm followed by the proposed modified spatial anisotropic diflusion (SD) algorithm. A corrupted block in an inter-eoded P or B frame Is concealed by tbe proposed motion-eompensated BNM algorithm. Based on the simulation results obtained in this study, the proposed scheme can recover high-quality MPEG 2 <b>video</b> <b>frames</b> from the corresponding corrupted <b>video</b> <b>frames</b> up to a bit error rate of 0. 5 %. Keyworddybrid error concealment; fransmisswn error; MPEG 2 video frnnsmission; besf neighborhood mafrhing (BNW I...|$|R
50|$|Digital Video (DV) {{adopts a}} similar method by {{compressing}} <b>video</b> <b>frames</b> individually.|$|R
40|$|Cosegmentation has {{achieved}} great success in exploiting inter-image segmentation consistency to segment {{a group of}} images simultaneously. To enforces non-local temporal coherence across all the frames by high-order object-level appearance/semantic correspondence with a compensation to the short-time window motion coherence cue, this paper cosegments the <b>video</b> <b>frames</b> together with a novel inter-frame segmentation consistency term. A direct application of existing cosegmentation algorithms to <b>video</b> <b>frames</b> encoun-ters the following challenges: the high correlation of adjacent frames which makes the segmentation ambiguous {{and a large number}} of <b>video</b> <b>frames</b> which makes the computation ex-pensive. To tackle them, we formulate the cosegmentation in a transductive learning framework to iteratively learn the inter-frame consistency term from all the <b>video</b> <b>frames.</b> The proposed algorithm is evaluated on the standard SegTrack dataset and promising results are obtained. Index Terms — video co-segmentation, transductive co-segmentation, greedy transductive inference, parametric min-cut, figure-ground video segmentation 1...|$|R
5000|$|... #Caption: Surveillance <b>video</b> <b>frame</b> of Pollard {{in the act}} of {{stealing}} classified documents ...|$|E
50|$|Standards for {{the digital}} <b>video</b> <b>frame</b> raster include Rec. 601 for standard-definition {{television}} and Rec. 709 for high-definition television.|$|E
5000|$|Split Frame Rendering (SFR): One GPU {{renders the}} top half of each <b>video</b> <b>frame,</b> the other does the bottom. (i.e. plane division) ...|$|E
5000|$|I‑frames are {{the least}} {{compressible}} but don't require other <b>video</b> <b>frames</b> to decode.|$|R
30|$|This {{user-friendly}} interface permits {{the user to}} embed chosen arrays in chosen <b>video</b> <b>frames.</b>|$|R
40|$|Abstract — In the Transmission of Videos over channel, <b>Video</b> <b>frames</b> are {{corrupted}} by {{salt and pepper}} noise (Impulse Noise), due to faulty communication systems. The objective {{of this paper is}} to implement a better filtering technique that makes the noisy <b>video</b> <b>frames</b> to noise free <b>video</b> <b>frames.</b> Median filters are the best known non-linear digital filters based on order statistics to solve the present problem. Median filters are known for their capability to remove salt and pepper noise and preserves the shape. The noise detection process to discriminate between uncorrupted pixels and the corrupted pixels prior to applying non-linear filtering is highly desirable to protect the signal details of uncorrupted pixels. In this paper we proposed A Modified Decision Based Unsymmetrical Trimmed Median filter (MDBUTM) algorithm for the restoration of gray scale, and color <b>video</b> <b>frames</b> that are highly corrupted by salt and pepper noise. This proposed algorithm shows better results than the Standard Media...|$|R
5000|$|This {{solution}} {{could not}} be used for television. To store a full <b>video</b> <b>frame</b> and display it twice requires a frame buffer—electronic memory (RAM—sufficient to store a <b>video</b> <b>frame).</b> This method did not become feasible until the late 1980s. In addition, avoiding on-screen interference patterns caused by studio lighting and the limits of vacuum tube technology required that CRTs for TV be scanned at AC line frequency. (This was 60 Hz in the US, 50 Hz Europe.) ...|$|E
5000|$|The {{system had}} two {{recording}} modes: standard mode (STD), and a long-play mode (LP) which sacrificed recording quality for extra capacity. In STD mode both {{recording and playback}} heads are used, writing both fields of each interlaced <b>video</b> <b>frame.</b> In long-play mode only a single head is used to record a single field from each <b>video</b> <b>frame,</b> with each field being read twice on playback, in a [...] "skip field" [...] technique. The heads scanned the tape in a helical scan fashion ...|$|E
50|$|Modern day Digital Light Processing (DLP) {{projectors}} commonly use color wheels {{to generate}} color images, typically running at {{a multiple of}} the <b>video</b> <b>frame</b> rate.|$|E
40|$|Abstract—This paper {{presents}} an evaluation for a wavelet-based digital watermarking technique used in estimating {{the quality of}} video sequences transmitted over Additive White Gaussian Noise (AWGN) channel {{in terms of a}} classical objective metric, such as Peak Signal-to-Noise Ratio (PSNR) without the need of the original video. In this method, a watermark is embedded into the Discrete Wavelet Transform (DWT) domain of the original <b>video</b> <b>frames</b> using a quantization method. The degradation of the extracted watermark can be used to estimate the video quality in terms of PSNR with good accuracy. We calculated PSNR for <b>video</b> <b>frames</b> contaminated with AWGN and compared the values with those estimated using the Watermarking-DWT based approach. It is found that the calculated and estimated quality measures of the <b>video</b> <b>frames</b> are highly correlated, suggesting that this method can provide a good quality measure for <b>video</b> <b>frames</b> transmitted over AWGN channel without the need of the original video...|$|R
3000|$|The motion {{estimation}} {{based on the}} bilinear transformation between two successive <b>video</b> <b>frames</b> of dimension [...]...|$|R
3000|$|... in Figure 15. PMOS_SSIM and PMOS_PSNR {{are adopted}} to {{evaluate}} image {{quality of the}} reconstructed <b>video</b> <b>frames.</b>|$|R
5000|$|... #Caption: Underwater <b>video</b> <b>frame</b> of the {{sea floor}} in the western Baltic covered with dead or dying crabs, fish and clams killed by oxygen {{depletion}} ...|$|E
5000|$|... bit 7: This [...] "sampling {{frequency}} scaling flag", if set, {{indicates that the}} sample rate is multiplied by 1/1.001 to match NTSC <b>video</b> <b>frame</b> rates.|$|E
5000|$|... #Caption: This <b>video</b> <b>frame</b> shows S-II-S/D {{after it}} failed in final structuraltests. These {{excerpts}} from Saturn V Quarterly Film Reports show SA-500D coming together for testing.|$|E
3000|$|... of all <b>video</b> <b>frames</b> in poet-verses train corpus and {{illustrated}} its variations along the principal eigenvector [...]...|$|R
40|$|The task of {{registering}} <b>video</b> <b>frames</b> with {{a static}} {{model is a}} common problem in many computer vision domains. The standard approach to registration involves finding point correspondences between the video and the model and using those correspondences to numerically determine registration transforms. Current methods locate video-to-model point correspondences by assembling a set of reference images to represent the model and then detecting and matching invariant local image features between the <b>video</b> <b>frames</b> and the set of reference images. These methods work well when all <b>video</b> <b>frames</b> can be guaranteed to contain {{a sufficient number of}} distinctive visual features. However, as we demonstrate, these methods are prone to severe misregistration errors in domains where many <b>video</b> <b>frames</b> lack distinctive image features. To overcome these errors, we introduce a concept of local distinctiveness which allows us to find model matches for nearly all video features, regardless of their distinctiveness on a global scale. We present results from the American football domain—where many <b>video</b> <b>frames</b> lack distinctive image features—which show a drastic improvement in registration accuracy over current methods. In addition, we introduce a simple, empirical stability test that allows our method to be fully automated. Finally, we present a registration dataset from the American football domain we hope {{can be used as a}} benchmarking tool for registration methods. 1...|$|R
40|$|The growing {{importance}} of mobile networks has stimulated active research into how multimedia {{information can be}} distributed over a slow and reliable network. In this paper, we propose a multi-channel approach for transmitting <b>video</b> <b>frames</b> over a GSM network. A number of strategies are proposed for channel allocation so that sufficient bandwidth can be provided to satisfy the timing requirements of the <b>video</b> <b>frames.</b> Their performance evaluated using simulation. ...|$|R
