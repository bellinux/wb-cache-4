26|445|Public
40|$|Abstract—Various {{case studies}} in {{different}} application domains {{have shown the}} great potential of <b>visual</b> <b>parameter</b> space analysis to support validating and using simulation models. In order to guide and systematize research endeavors in this area, we provide a conceptual framework for <b>visual</b> <b>parameter</b> space analysis problems. The framework is based on our own experience and a structured analysis of the visualization literature. It contains three major components: (1) a data flow model that helps to abstractly describe <b>visual</b> <b>parameter</b> space analysis problems independent of their application domain; (2) a set of four navigation strategies of how parameter space analysis can be supported by visualization tools; and (3) a characterization of six analysis tasks. Based on our framework, we analyze and classify the current body of literature, and identify three open research gaps in <b>visual</b> <b>parameter</b> space analysis. The framework and its discussion are meant to support visualization designers and researchers in characterizing parameter space analysis problems and to guide their design and evaluation processes. Index Terms—Parameter space analysis, input-output model, simulation, task characterization, literature analysis. ...|$|E
3000|$|... {{denote the}} mean and {{variance}} of the r th <b>visual</b> <b>parameter</b> in the time interval [1, T], respectively, and [...]...|$|E
3000|$|... {{denote the}} mean and {{variance}} of the r th estimated <b>visual</b> <b>parameter</b> in [1, T], respectively. For the quantification of the visual estimation accuracy, the models were trained using an 80 % of the training database, while the remaining 20 % was used for testing.|$|E
30|$|The {{measurable}} criteria {{consider the}} subjective impression of quality. We have performed the following objective evaluations. The similarity {{of the real}} sequence and the animated sequence is described by directly comparing the <b>visual</b> <b>parameters</b> of the animated sequence with the real parameters extracted from the original video. We use the cross-correlation of the two <b>visual</b> <b>parameters</b> as the measure of similarity. The <b>visual</b> <b>parameters</b> are the size of open mouth and the texture parameter.|$|R
40|$|This {{paper will}} discuss the {{exploration}} of key musical and <b>visual</b> <b>parameters</b> {{with the aim of}} enhancing the appreciation of Abstracted Animation [1] with varying degrees of animacy. A series of animations were created in response to multiple, wordless, sung, close variations of a song. Carefully delineated, <b>visual</b> <b>parameters</b> and a mapping of visual to audio relationships of the animations afforded insights into key audio-visual intersections and suggested future directions...|$|R
30|$|Emotion {{expressions}} {{sometimes are}} mixed with the utterance expression in spontaneous face-to-face communication, which makes difficulties for emotion recognition. This article introduces the methods of reducing the utterance influences in <b>visual</b> <b>parameters</b> for the audio-visual-based emotion recognition. The audio and visual channels are first combined under a Multistream Hidden Markov Model (MHMM). Then, the utterance reduction is finished by finding the residual between the real <b>visual</b> <b>parameters</b> and the outputs of the utterance related <b>visual</b> <b>parameters.</b> This article introduces the Fused Hidden Markov Model Inversion method which is trained in the neutral expressed audio-visual corpus to solve the problem. To reduce the computing complexity the inversion model is further simplified to a Gaussian Mixture Model (GMM) mapping. Compared with traditional bimodal emotion recognition methods (e.g., SVM, CART, Boosting), the utterance reduction method can give better results of emotion recognition. The experiments also show the effectiveness of our emotion recognition system when it was used in a live environment.|$|R
40|$|Abstract- In this demo, {{we present}} a {{technique}} for synthesizing the mouth movement from acoustic speech information. The algorithm maps the audio parameter set to the <b>visual</b> <b>parameter</b> set using the Gaussian Mixture Model and the Hidden Markov Model. With this technique, we can create smooth and realistic lip movements...|$|E
40|$|High quality speech-to-lips conversion, {{investigated}} in this work, renders realistic lips movement (video) consistent with input speech (audio) without knowing its linguistic content. Instead of memoryless framebased conversion, we adopt maximum likelihood {{estimation of the}} <b>visual</b> <b>parameter</b> trajectories using an audio-visual joint Gaussian Mixture Model (GMM). We propose a minimum converted trajectory error approach (MCTE) to further refine the converted visual parameters. First, we reduce the conversion error by training the joint audio-visual GMM with weighted audio and visual likelihood. Then MCTE uses the generalized probabilistic descent algorithm to minimize a conversion error of the <b>visual</b> <b>parameter</b> trajectories defined on the optimal Gaussian kernel sequence according to the input speech. We demonstrate {{the effectiveness of the}} proposed methods using the LIPS 2009 Visual Speech Synthesis Challenge dataset, without knowing the linguistic (phonetic) content of the input speech. Index Terms: visual speech synthesis, speech-to-lips conversion, minimum conversion error, minimum generation erro...|$|E
30|$|Even though VA is {{the only}} <b>visual</b> <b>parameter</b> {{currently}} measured by FRSC (Agunloye 1990 b), {{it was noted that}} it was not checked in most cases before drivers were issued their driver’s license. Seven out of the 400 drivers (1.8 %) were VI, similar to the findings by Effiong (Effiong 1993) (1.6 %), Erikitola (Erikitola 1998) (1.7 %) and Abraham (Abraham 2007) (1.7 %).|$|E
3000|$|... are {{composed}} {{of the values of}} a set of animation parameters of the Candide- 3 face model. Thus, the <b>visual</b> <b>parameters</b> estimated from speech can be used to animate this face model.|$|R
40|$|For {{the purpose}} of coinciding the virtual {{environment}} displayed by See-Through Head-Mounted Display（STHMD） with the actual environment, a new calibration method is proposed. The method is for the calibration {{of the difference between}} supposed position and actual，position of the user's viewpoint, which is mainly caused by the individual difference of inter-pupil distance. This method is efficient to minimize the error concerning distance. The efficiency is evaluated by sensitivity analysis of <b>visual</b> <b>parameters.</b> It is also confirmed by experiment in which <b>visual</b> <b>parameters</b> of STHNID are calibrated individually, and the calibrated virtual environment matched the actual environment...|$|R
40|$|Abstract:- In this paper, transform-based {{watermarking}} scheme {{based on}} human visual system is proposed. Human <b>visual</b> <b>parameters</b> are useful to ensure the imperceptibility of watermark image. Not only human <b>visual</b> <b>parameters,</b> such as contrast sensitivity, texture degree but also statistical characteristics are involved to select the optimal coefficients region. The performance of proposed algorithm is evaluated by the experiments of imperceptibility and correctness of watermark. According to some experimental results, contrast sensitivity function is superior in smooth image. On the other hand, statistical characteristics provide good results in rough images. Consequently, how to select the parameters considering image attribute is key problem in effective watermarking...|$|R
3000|$|We {{can find}} HMM-inversion-based method {{is better than}} GMM-based method. Using the HMM state or {{the center of the}} visual {{clusters}} as the outputs of visual parameters, the HMM inversion can simulate the detailed facial deformation while speaking. In our previous study, we even use it for the system of speech-driven facial animation [68]. In our study, it gets the better utterance reduction results than GMM-based method which may give an over-smoothing <b>visual</b> <b>parameter</b> outputs. The results of [...] "neutral" [...] and [...] "fear" [...] in GMM-based method are even worse than that without utterance reduction method. The results confirm the report in [75], which proved the over-smoothing problem while using GMM for conversion problems.|$|E
40|$|In {{this paper}} we present two {{different}} methods for mapping auditory, telephone quality speech to <b>visual</b> <b>parameter</b> trajectories, specifying {{the movements of}} an animated synthetic face. In the first method, Hidden Markov Models (HMMs) where used to obtain phoneme strings and time labels. These where then transformed by rules into parameter trajectories for visual speech synthesis. In the second method, Artificial Neural Networks (ANNs) were trained to directly map acoustic parameters to synthesis parameters. Speaker independent HMMs were trained on a phonetically transcribed telephone speech database. Different underlying units of speech were modelled by the HMMs, such as monophones, diphones, triphones, and visemes. The ANNs were trained on male, female, and mixed speakers. The HM...|$|E
40|$|Recent HCI {{research}} {{has looked at}} conveying emotions through non-visual modalities, such as vibrotactile and thermal feedback. However, emotion is primarily conveyed through visual signals, and so this research aims to support the design of emotional visual feedback. We adapt and extend {{the design of the}} "pulsing amoeba" [29], and measure the emotion conveyed through the abstract visual designs. It is a first step towards more holistic multimodal affective feedback combining visual, auditory and tactile stimuli. An online survey garnered valence and arousal ratings of 32 stimuli that varied in colour, contour, pulse size and pulse speed. The results support previous research but also provide new findings and highlight the effects of each individual <b>visual</b> <b>parameter</b> on perceived emotion. We present a mapping of all stimulus combinations onto the common two-dimensional valence-arousal model of emotion...|$|E
40|$|We {{show how}} to {{visually}} control acoustic speech synthesis by modelling the dependency between <b>visual</b> and acoustic <b>parameters</b> within the Hidden-Semi-Markov-Model (HSMM) based speech synthesis framework. A joint audio-visual model is trained with 3 D facial marker trajectories as visual features. Since the dependencies of acoustic features on visual features are only present for certain phones, we implemented a model where dependencies are estimated {{for a set}} of vowels only. A subjective evaluation consisting of a vowel identification task showed that we can transform some vowel trajectories in a phonetically meaningful way by controlling the <b>visual</b> <b>parameters</b> in PCA space. These <b>visual</b> <b>parameters</b> can also be interpreted as fundamental visual speech motion components, which leads to an intuitive control model. Index Terms: audio-visual speech synthesis, HMM-based speech synthesis, controllabilit...|$|R
30|$|In this paper, we {{exploit the}} {{non-linear}} relation between a speech source {{and its associated}} lip video {{as a source of}} extra information to propose an improved audio-visual speech source separation (AVSS) algorithm. The audio-visual association is modeled using a neural associator which estimates the <b>visual</b> lip <b>parameters</b> from a temporal context of acoustic observation frames. We define an objective function based on mean square error (MSE) measure between estimated and target <b>visual</b> <b>parameters.</b>|$|R
40|$|Orthogonal {{information}} {{present in}} the video signal associated with the audio helps in improving the accuracy of a speech recognition system. Audio-visual speech recognition involves extraction of both the audio as well as visual features from the input signal. Extraction of <b>visual</b> <b>parameters</b> {{is done by the}} recognition of speech dependent features from the video sequence. This paper uses geometrical features to describe the lip shapes. Curve-based Active Shape Models are used to extract the geometry. These geometrically represented <b>visual</b> <b>parameters</b> are used along with the audio cepstral features to perform an audio-visual classification. It is shown that the bimodal system presented here gives an improvement in the classification results over classification using only the audio features...|$|R
40|$|Abstract- In this paper, {{traditional}} methods of edge detection and their problems are discussed. After that high performance method for edge detection that is fuzzy logic based image processing is used for accurate and noise free edge detection and Cellular Learning Automata (CLA) is used for enhance the previously-detected edges {{with the help of}} the repeatable and neighborhood-considering nature of CLA. In the end, we compare it with popular methods such as Sobel, Canny and Prewitts with enhanced by CLA. Edge is most important <b>visual</b> <b>parameter</b> so if the edges in an image can be identified accurately, all of the objects can be located and basic properties such as area, perimeter, and shape can be measured. In this paper, all the algorithms and result are prepared in MATLAB...|$|E
40|$|ICSLP 1998 : the 5 th International Conference on Spoken Language Processing, November 30 - December 4, 1998, Sydney, Australia. This paper {{proposes a}} method to re-estimate output visual {{parameters}} for speech-to-lip movement synthesis using audio-visual Hidden Markov Models (HMMs) under the Expectation-Maximization(EM) algorithm. In the conventional methods for speech-to-lip movement synthesis, there is a synthesis method estimating a <b>visual</b> <b>parameter</b> sequence through the Viterbi alignment of an input acoustic speech signal using audio HMMs. However, the HMM-Viterbi method involves a substantial problem that incorrect HMM state alignment may output incorrect visual parameters. The problem in the HMM-Viterbi method {{is caused by the}} deterministic synthesis process to assign a single HMM state for an input audio frame. The proposed method avoids the deterministic process by re-estimating non-deterministic visual parameters while maximizing the likelihood of the audio-visual observation sequence under the EM algorithm...|$|E
40|$|Many {{activities}} in computer graphics {{can be regarded}} as experiments on virtual objects or models. In the process of experimentation the existing models are gradually improved and new model categories emerge. The Virtual Laboratory (VLAB) is a software environment designed to support model development by facilitating the manipulation of models and providing mechanisms for retrieving and storing large numbers (e. g., thousands) of them. This thesis describes a number of VLAB extensions I designed and implemented as part of my master’s research. As a result of these extension, the models in VLAB can be shared between many users who may work at different geographical locations. Alternate views of databases can be maintained, allowing users to access objects in different orders. A <b>visual</b> <b>parameter</b> editor was implemented, providing intuitive mechanism for external control of parameters used in experimentations through user configurable graphical user interfaces. The overall performance and portability of VLAB was improved, and various customization mechanisms for adjusting visual appearances of VLAB applications made available...|$|E
40|$|This paper {{demonstrates}} {{the use of}} <b>visual</b> <b>parameters</b> extracted from video for automatic recognition of phoneme strings. Encouraged by previous works utilizing ”visually clean” data we investigate their efficiency in non-ideal conditions which are introduced by meeting audio-visual data employed in our experiments. ...|$|R
40|$|BACKGROUND: Restrictions {{placed on}} the working hours of doctors {{over the past decade}} have {{resulted}} in substantial changes to the training and assessment of orthopaedic surgical residents. Many who are responsible for training the surgeons of the future have become concerned that this reduced clinical exposure is having a detrimental impact on technical skill acquisition. Consequently, {{there is a need for}} surgical educators to develop more objective methods for assessing surgical skill. The primary aim of this study was to determine whether a novel set of <b>visual</b> <b>parameters</b> assessing visuospatial ability, fine motor dexterity, and gaze control could objectively discriminate among various levels of arthroscopic experience. The secondary aim was to evaluate the correlations between these new parameters and previously established technical skill assessment methods. METHODS: Twenty-seven subjects were divided into a novice group (n = 7), a resident group (n = 15), and an expert group (n = 5) on the basis of arthroscopic experience. All subjects performed a diagnostic knee arthroscopy task on a simulator. Their performance was assessed with use of novel simple <b>visual</b> <b>parameters</b> that included the prevalence of instrument loss, triangulation time, and prevalence of lookdowns. Performance was also evaluated with use of previously validated technical skill assessment methods (a global rating scale and motion analysis). RESULTS: A significant difference in performance among the groups was demonstrated with use of all three novel <b>visual</b> <b>parameters,</b> the global rating scale, and motion analysis (p < 0. 05). There were strong and highly significant correlations (p < 0. 0001) between each of the novel parameters and the previously validated skill assessment methods. CONCLUSIONS: This study demonstrates the construct validity of three novel <b>visual</b> <b>parameters</b> for objectively assessing arthroscopic performance. These parameters are simple, can be used easily in the operating room, and are strongly correlated with current validated methods of technical skill assessment...|$|R
40|$|International audienceBACKGROUND AND PURPOSE: No {{practical}} tool {{has been}} reported in the literature to evaluate the quality of cerebral TR- 3 D-CE-MRA techniques. Our study assessed a large list of parameters used to propose a quality-evaluation scheme for TR- 3 D-CE-MRA. MATERIALS AND METHODS: A large list of <b>visual</b> and quantitative <b>parameters</b> used to study the quality of images was collected from the literature and evaluated in 19 healthy patients and 11 patients with arteriovenous shunts who had undergone both CENTRA keyhole TR- 3 D-CE-MRA at 3 T and CCA. Several observers evaluated the <b>visual</b> <b>parameters,</b> such as the diagnostic confidence index, artifacts, maximum vascular signal intensity, arterial-to-venous separation, and visibility of 17 arteries and 7 veins; and quantitative parameters, such as maximum arterial SI, arteriovenous transit time, arteriovenous contrast curve, and ADW. A statistical analysis was used to determine interobserver reproducibility of the <b>visual</b> <b>parameters,</b> to calculate the sensitivity of TR- 3 D-CE-MRA for detecting each vessel (with CCA as standard of reference), and to compare the results of the visual and quantitative evaluations. RESULTS: Diagnostic confidence index, artifacts, arterial-to-venous separation, and 4 vessels-the PICA, ophthalmic and occipital arteries, and the ISS-demonstrated high reproducibility and sensitivity. The ADW was the most reliable dynamic quantitative parameter and was correlated with arterial-to-venous separation. CONCLUSIONS: The image quality of TR- 3 D-CE-MRA can be effectively evaluated with a scheme of 1 quantitative and 7 <b>visual</b> <b>parameters...</b>|$|R
40|$|Until a {{few years}} ago there has been little {{development}} of image processing systems in the minerals industry, mainly owing to the computational demands of image recognition and interpretation. Due to rapid progress in computer technology at present however, image processing systems are increasingly being investigated in order to solve complex process engineering problems. In this investigation videographic analysis of the hydrocyclone underflow discharge is applied primarily as a diagnostic tool to assess the performance of the classifier. The discharge spray angle is a <b>visual</b> <b>parameter</b> which immediately identifies normal (umbrella profile discharge) or anomalous (roping discharge) operation, and can be directly interpreted to characterise internal flow properties such as pressure drop. Also, this information can be applied to estimate the feed stream particle concentration and ultimately the hydrocyclone's cut size and can serve as a basis for considerable improvement in the on-line control of mineral processing circuits as a whole. Articl...|$|E
40|$|The rising {{quantity}} {{and complexity of}} data creates a need to design and optimize data processing pipelines - the set of data processing steps, parameters and algorithms that perform operations on the data. Visualization can support this process but, {{although there are many}} examples of systems for <b>visual</b> <b>parameter</b> analysis, there remains a need to systematically assess users' requirements and match those requirements to exemplar visualization methods. This article presents a new characterization of the requirements for pipeline design and optimization. This characterization is based on both {{a review of the literature}} and first-hand assessment of eight application case studies. We also match these requirements with exemplar functionality provided by existing visualization tools. Thus, we provide end-users and visualization developers with a way of identifying functionality that addresses data processing problems in an application. We also identify seven future challenges for visualization research that are not met by the capabilities of today's systems...|$|E
40|$|International audienceThis paper {{presents}} a quantitative and {{comprehensive study of}} the lip movements of a given speaker in different speech/nonspeech contexts, with a particular focus on silences i. e., when no sound is produced by the speaker. The aim is to characterize the relationship between "lip activity" and "speech activity" and then to use visual speech information as a voice activity detector VAD. To this aim, an original audiovisual corpus was recorded with two speakers involved in a face-to-face spontaneous dialog, although being in separate rooms. Each speaker communicated with the other using a microphone, a camera, a screen, and headphones. This system was used to capture separate audio stimuli for each speaker and to synchronously monitor the speaker's lip movements. A comprehensive analysis was carried out on the lip shapes and lip movements in either silence or nonsilence i. e., speech+nonspeech audible events. A single <b>visual</b> <b>parameter,</b> defined to characterize the lip movements, was shown to be efficient {{for the detection of}} silence sections. This results in a visual VAD {{that can be used in}} any kind of environment noise, including intricate and highly nonstationary noises, e. g., multiple and/or moving noise sources or competing speech signals...|$|E
50|$|The {{original}} PANOSE System {{was developed}} in 1985 by Benjamin Bauermeister. In 1988, it was published by Van Nostrand Reinhold Company Inc. under the title A Manual of Comparative Typography: The PANOSE System. This initial version of the PANOSE system consisted of seven classification categories and was based on subjective <b>visual</b> <b>parameters.</b>|$|R
30|$|To summarize, {{this study}} {{demonstrates}} a rapid and reliable method for BW estimation. Given {{the lack of}} reliable methods for practitioners to estimate patient BW based on <b>visual</b> <b>parameters</b> or physical exam, BW estimation based on CT dose modulation may have potential use in clinical radiology and polytrauma patients. Certainly, further studies are required.|$|R
3000|$|... (i.e., the {{selected}} left-right phonetic {{context of the}} frame). Because of this the distance between a frame of audio data and a state will change according to its phonetic context in the target utterance. This optimises the mapping from audio to <b>visual</b> <b>parameters</b> according to {{the selected}} units. If we have a sequence of [...]...|$|R
40|$|Comparing {{multiple}} {{variables to}} select those that effectively characterize complex entities {{is important in}} a wide variety of domains – geodemographics for example. Identifying variables that correlate is a common practice to remove redundancy, but correlation varies across space, with scale and over time, and the frequently used global statistics hide potentially important differentiating local variation. For more comprehensive and robust insights into multivariate relations, these local correlations need to be assessed through various means of defining locality. We explore the geography of this issue, and use novel interactive visualization to identify interdependencies in multivariate data sets to support geographically informed multivariate analysis. We offer terminology for considering scale and locality, visual techniques for establishing the effects of scale on correlation and a theoretical framework through which variation in geographic correlation with scale and locality are addressed explicitly. Prototype software demonstrates how these contributions act together. These techniques enable multiple variables and their geographic characteristics to be considered concurrently as we extend <b>visual</b> <b>parameter</b> space analysis (vPSA) to the spatial domain. We find variable correlations to be sensitive to scale and geography to varying degrees in the context of energy-based geodemographics. This sensitivity depends upon the calculation of locality as well as the geographical and statistical structure of the variable...|$|E
40|$|The paper aims {{to reflect}} on the {{potential}} today offered by modeling both traditional and digital, for the analysis and design of the urban landscape, focusing especially the issue coming from the subtitle: Innovations in design languages and project procedures. In a perspective that takes into account the new languages in the design procedures, the technical approach needs to be associated with research in the field of urban design and the new related dimensions. This line of inquiry is aimed at integration of the morphological component, achieved through modeling, first through the "time" to finally reach an appropriate balance between the <b>visual</b> <b>parameter</b> with other sensory spheres. If the link quality / quantity is crucial for the design at all scales, these modeling tools capable of highlighting, for example, the relationship between physical and visual space with urban soundscape, i. e., then become necessary. The development of such integrated models, supported often by the experimental use of “Game Engines”, is implemented through a multi-dimensional practice of the survey, which takes into consideration not only the shape of objects and contexts, but also an expanded sensory range not limited to the visual. The applications of these modeling experiments concerning both the design of urban spaces and the representation of the historical memory of the city, finally serves also as a tourist activity...|$|E
30|$|SES {{score is}} a <b>visual</b> <b>parameter</b> for {{assessing}} the tolerance of seedlings under salinity stress. The lower the SES score (1 or 3), the higher the tolerance, whereas a higher SES score (7 or 9) suggests sensitivity. An SES score of 5 indicates moderate tolerance. Significant negative correlations were observed for SES score {{with all the other}} four parameters (SL, RL, FWsht, and DWsht) (Table 1). This is obvious because seedlings can be scored with low SES only if they have attained higher shoot length and high vigor, which means long root length, although higher SES scores could be given to plants with poor vigor and growth. This suggests that all four growth-related parameters directly relate to visually based SES scores. The negative associations observed between SES score and plant growth attributes clearly demonstrate the significance and the detrimental effects of high Na+ accumulation in plant tissue under saline conditions. The most common salt injury symptoms in rice are leaf tip burning, early senescence, and complete necrosis, particularly among sensitive varieties such as IR 29. The detrimental effects of salt stress on the growth and yield of rice genotypes are well documented in several earlier reports (Flowers and Yeo 1981, 1995; Gregorio and Senadhira 1993; Ashraf et al. 1999; Ismail et al. 2007; Munns and Tester 2008; Ding et al. 2010; Singh et al. 2010; Bimpong et al. 2016). The mostly positive significant correlations among SL, RL, FWsht, and DWsht suggest that these traits ultimately contribute to seedling-stage salinity tolerance.|$|E
30|$|Some {{approaches}} not requiring an {{a priori}} classification of the audio-visual {{data for the}} training stage have been also proposed in the literature. These techniques {{have the advantage of}} avoiding the propagation of possible errors during the classification stage. For instance, in [22] a shared Gaussian process latent variable model (SGPLVM) is introduced to perform a mapping between facial motion and speech signal. A shared latent space is computed by maximizing the joint likelihood function of the audio and visual data sets, using Gaussian kernels. During the synthesis stage, intermediate latent points are obtained from the audio data, and then used to predict the corresponding visual data by means of the Gaussian process mapping. Visual data is represented in terms of active appearance models (AAMs) [23], trained with shape and texture data provided by a set of annotated prototype face images. Given a set of AAM parameters estimated from audio data, novel frames of the animation are generated by first reconstructing the shape and texture separately and then warping the texture to the shape. The limitation of this approach {{is that it does not}} actually animate 3 D head models, but rather the resulting animation consists of a sequence of synthetic face images generated from the AAM. Another approach not requiring a priori classification, was introduced in the late 90 ’s by Massaro and colleagues in [24], based on their text-to-speech driven talking head Baldi[25]. An artificial neural network (ANN) is trained with an only audio database and the associated <b>visual</b> <b>parameters,</b> where the <b>visual</b> <b>parameters</b> are not extracted from real videos but are computed from the corresponding audio transcriptions. Given a novel audio signal, the ANN produces as an output the set of estimated <b>visual</b> <b>parameters</b> to control Baldi’s animation. This approach is constrained to using Baldi’s movements as <b>visual</b> <b>parameters,</b> computed from the corresponding text transcriptions, not allowing the use of visual data from other speakers. The disadvantage of the technique in [25] is that it is only capable of animating Baldi 3 D head model.|$|R
40|$|Based on {{six years}} of {{continuous}} measurements, we have analysed in detail the occupancy, thermal and <b>visual</b> <b>parameters</b> influencing blind usage behaviour. This paper begins by presenting {{some of the key}} findings from these analyses. Informed by other developments in the literature, we go on to propose an approach for a comprehensive stochastic model for simulating blind usage...|$|R
40|$|Many {{multimedia}} {{applications and}} entertainment industry products like games, cartoons and film dubbing require speech driven face animation and audio-video synchronization. Only Automatic Speech Recognition system (ASR) {{does not give}} good results in noisy environment. Audio Visual Speech Recognition system plays vital role in such harsh environment as it uses both – audio and visual – information. In this paper, we have proposed a novel approach with enhanced performance over traditional methods that have been reported so far. Our algorithm works on the bases of acoustic and <b>visual</b> <b>parameters</b> to achieve better results. We have tested our system for English language using MFCC and LPC parameters of the speech. Lip parameters like lip width, lip height etc are extracted from the video and these both acoustic and <b>visual</b> <b>parameters</b> are used to train neural network. Our system is giving almost cent percent response against vowels...|$|R
