5|252|Public
50|$|A <b>Validation</b> <b>rule</b> is a {{criterion}} or constraint {{used in the}} process of data validation, carried out after the data has been encoded onto an input medium and involves a data vet or validation program. This is distinct from formal verification, where the operation of a program is determined to be that which was intended, and that meets the purpose. The <b>Validation</b> <b>rule</b> or check system still used by many major software manufacturers was designed by an employee at Microsoft some time between 1997 and 1999.|$|E
40|$|This paper {{introduces}} an algebraic {{model of}} the protection mechanism that control access to information objects in computer systems. The model is a capability-based one, i. e. it uses the notion of access capability to define a hierarchy of access privileges which {{is the basis of}} the control mechanism. The notion {{of the state of the}} protection system is introduced as a mapping of the set of subjects of the system into the set of capabilities. The state transition rules are defined and the problem of correctness is seen as a state-transition <b>validation</b> <b>rule...</b>|$|E
40|$|Input {{validation}} in web applications {{represents an}} important part of their functionality. With proper validation we ensure that provided input data is in accordance with technical constraints, defined by the developer and with business-related constraints. In web development frameworks, validation logic is coupled with program code. If one <b>validation</b> <b>rule</b> is changed, application needs to be recompiled and redeployed. In this thesis we developed a system for input validation based on business rules management system. Validation rules are stored in central repository, separated from implementation of web applications. Thus, we have achieved a simple and transparent way of declaring validation logic in the form of declarative business rules as well as simplifed applications maintenance in case of changes in validation logic...|$|E
5000|$|Data {{validation}} is {{the application}} of <b>validation</b> <b>rules</b> to the data. For electronic CRFs the <b>validation</b> <b>rules</b> may be applied in real time {{at the point of}} entry. Offline validation may still be required (e.g. for cross checks between data types) ...|$|R
5000|$|The CSLA [...]NET {{framework}} {{provides a}} rules engine that supports <b>validation</b> <b>rules,</b> business rules, and authorization rules. These rules {{are attached to}} object instances or properties, and are automatically invoked by CSLA [...]NET when necessary. <b>Validation</b> <b>rules</b> may be implemented using the CSLA [...]NET rule engine, or {{through the use of}} the DataAnnotations feature of Microsoft [...]NET.|$|R
5000|$|New {{and changed}} data {{should be tested}} against <b>validation</b> <b>rules</b> ...|$|R
40|$|Complex {{languages}} {{built on}} XML require support for validation that {{goes far beyond}} what can be provided using traditional schema languages like XML Schema. In this paper {{we look at the}} experience we made as part of the FpML Validation Working Group, which is specifying standard validation rules for the Financial Products Markup Language (FpML), a cross-industry standard for financial derivatives trades. The issues surfaced in here are likely to be of interest to any organisation or standards body dealing with data that exhibits complex structure or semantics. We discuss the process for <b>validation</b> <b>rule</b> gathering that the working group followed, and the issues it has been facing. We then look at the two reference implementation languages, Schematron and CLIX, that we used to formally express the validation rules, and some of the requirements we found: validation of the documents against external data sources like relational databases, expressiveness requirements, and complex data type support...|$|E
40|$|Over {{the last}} decade, human facial {{expressions}} recognition (FER) {{has emerged as}} an important research area. Several factors make FER a challenging research problem. These include varying light conditions in training and test images; need for automatic and accurate face detection before feature extraction; and high similarity among different expressions that makes it difﬁcult to distinguish these expressions with a high accuracy. This work implements a hierarchical linear discriminant analysis-based facial expressions recognition (HL-FER) system to tackle these problems. Unlike the previous systems, the HL-FER uses a pre-processing step to eliminate light effects, incorporates a new automatic face detection scheme, employs methods to extract both global and local features, and utilizes a HL-FER to overcome the problem of high similarity among different expressions. Unlike most of the previous works that were evaluated using a single dataset, {{the performance of the}} HL-FER is assessed using three publicly available datasets under three different experimental settings: n-fold cross validation based on subjects for each dataset separately; n-fold cross <b>validation</b> <b>rule</b> based on datasets; and, ﬁnally, a last set of experiments to assess the effectiveness of each module of the HL-FER separately. Weighted average recognition accuracy of 98. 7 % across three different datasets, using three classifiers, indicates the success of employing the HL-FER for human FER...|$|E
5000|$|Data <b>validation</b> <b>rules</b> may be defined, {{designed}} and deployed, for example: ...|$|R
5000|$|Frank {{is used as}} a user when adding <b>validation</b> <b>rules</b> to a Microsoft Access {{data entry}} box.|$|R
5000|$|StrategyEdit section which {{describes}} the <b>validation</b> <b>rules</b> {{to be applied}} - typically these will be cross field validations ...|$|R
40|$|Abstract: The {{introduction}} of a new logical structure of data <b>validation</b> <b>rules,</b> presented in this paper, aims at the enrichment and further refinement of the domain-based rules classification framework, adopted as a design standard by Eurostat. The new scheme introduces three types of generic <b>validation</b> <b>rules</b> using Boolean Algebra expressions, achieving a level of detail, which supports the direct translation of the rules into relational algebra. These expressions that can be constructed and evaluated at run-time, using a logical schema of generic data domains, maintained in a relational repository of rules. This approach, already been elaborated and tested vertically in all statistical sub-systems of the National Statistical Service of Greece, is illustrated here, using a list of <b>validation</b> <b>rules</b> from various classification cells of our design...|$|R
40|$|Abstract. There {{are many}} tools {{suitable}} to model systems and to generate software code from system models, but these tools {{do not support}} data validation. Available data validation tools are domain specific and require manual definition of data <b>validation</b> <b>rules.</b> Thus, the lack of the tool supporting both system modelling and automated generation of data <b>validation</b> <b>rules</b> from system models is obvious. The paper discusses the use of system models represented by UML in automation of data validation. The method to derive rules from UML models is presented. A fragment of UML model and derivation of rules represented in the model is demonstrated. The tool supporting proposed method is introduced {{and the process of}} automated data <b>validation</b> <b>rules</b> generation from UML models is presented. 1...|$|R
50|$|<b>Validation</b> <b>Rules</b> are {{electronic}} checks {{defined in}} advance which ensure the completeness and {{consistency of the}} clinical trial data.|$|R
30|$|Finally, we also cite NCL Inspector [25], {{which is}} an NCL code {{reviewing}} tool that allows the author to define new rules to be validated. These rules can be defined using the Java programming language or using XSLT documents. This tool, however, does not validate NCL code; it just validates the defined rules. This work does not focus on supporting specific <b>validation</b> <b>rules</b> defined by users, but in establishing general <b>validation</b> <b>rules</b> to NCL documents.|$|R
30|$|With the {{distribution}} database schema ready, {{the data from}} production can be loaded into geodatabank by using a set of mapping and <b>validation</b> <b>rules.</b>|$|R
40|$|The {{introduction}} of a new logical structure of data <b>validation</b> <b>rules,</b> presented in this paper, aims at the enrichment and further refinement of the domainbased rules classification framework, adopted as a design standard by Eurostat. The new scheme introduces three types of generic <b>validation</b> <b>rules</b> using Boolean Algebra expressions, achieving a level of detail, which supports the direct translation of the rules into relational algebra. These expressions that can be constructed and evaluated at run-time, using a logical schema of generic data domains, maintained in a relational repository of rules. This approach, already been elaborated and tested vertically in all statistical sub-systems of the National Statistical Service of Greece, is illustrated here, using a list of <b>validation</b> <b>rules</b> from various classification cells of our design. Keywords: Data Quality, Statistical Data Validation, Data Editing, Boolean Algebra 1...|$|R
5000|$|... metadata-driven {{components}} {{which can}} bind to metadata formats such as XML schema, allowing a shared client-server data model with extensible types, <b>validation</b> <b>rules</b> and editing constraints ...|$|R
40|$|Microsoft Visio 2013 Business Process Diagramming and Validation {{provides}} a comprehensive and practical tutorial including example code and demonstrations for creating <b>validation</b> <b>rules,</b> writing ShapeSheet formulae, and much more. If {{you are a}} Microsoft Visio 2013 Professional Edition power user or developer who wants to get to grips with both the essential features of Visio 2013 and the <b>validation</b> <b>rules</b> in this edition, then this book is for you. A working knowledge of Microsoft Visio and optionally. NET for the add-on code is required, though previous knowledge of business process diagrammin...|$|R
40|$|In the {{framework}} of data editing we consider a non-parametric approach based on Information Retrieval to be used both for missing data imputation and data validation. In particular, an incremental procedure based on the iterative use of recursive partitioning methods and a suitable Incremental Imputation Algorithm is proposed. A simulation study and real world applications for non-linear structures are shown to describe the advantages and the good performance with respect to standard approaches. Furthermore, we introduce a novel approach for the automatic derivation of <b>validation</b> <b>rules</b> through recursive partitioning, named TreeVal. It derives {{a specific set of}} <b>validation</b> <b>rules</b> using recursive partitioning on a learning sample and then it checks for the intrinsic occurrence of possible errors in a validation sample using the previously identified <b>validation</b> <b>rules.</b> For this approach results of a simulation study and an application on census data are presented. The work summarizes the research efforts performed by the authors within the researc...|$|R
40|$|The {{goal of this}} {{research}} is to develop a 3 D topological structure with validation functionality and a conversion function. In order to reach this goal, the main requirements for a 3 D topological structure are set, as well as ten <b>validation</b> <b>rules.</b> The designed 3 D topological structure represents a full space partition of volumes. A prototype of this structure has been implemented in Oracle Spatial (11 g). The <b>validation</b> <b>rules</b> have been translated into validation tests, which have been implemented on the structure. Finally, for each primitive (node, edge, face and volume) a geometry operation has been implemented on the structure...|$|R
40|$|Consistent {{validation}} of Statistical Data {{is an essential}} prerequisite of any process aiming at ensuring the quality and homogeneity of Statistical Information, especially when data are acquired from distributed heterogeneous sources. The ad-hoc, filter-like solutions currently used are not sufficient to ensure a systematic and consistent validation process. The present paper provides a framework for the formal definition and classification of <b>validation</b> <b>rules,</b> {{which in turn is}} used as the theoretical basis for the definition of abstract data structures, able to hold the rules declarations. Thus, the construction of an abstract information model that stores and handles <b>validation</b> <b>rules</b> as metadata becomes feasible. This leads to a declarative formalisation of rules, as opposed to the usual procedural (algorithmic) approach. Furthermore, based on these concepts, the paper describes the implementation of this data model in the form of distributed globally accessible repositories (i. e. databases) of <b>validation</b> <b>rules.</b> Suitable <b>validation</b> engines can then access these repositories to consistently validate data sets, even in ad-hoc cases. Keywords: Data Validation, Metadata, Outliers, Data Quality, Editing, Distributed Metadata Repositorie...|$|R
50|$|The {{range of}} data values or data quality in an {{operational}} system may exceed {{the expectations of}} designers at the time <b>validation</b> and transformation <b>rules</b> are specified. Data profiling of a source during data analysis can identify the data conditions that must be managed by transform rules specifications, leading to an amendment of <b>validation</b> <b>rules</b> explicitly and implicitly implemented in the ETL process.|$|R
5000|$|Extensive {{reliance}} on terminal [...] "screen painting" [...] (i.e. [...] "mocking"-up a CRT data-entry screen) {{was used to}} assist system definition. e.g. to define a components database attributes (name, length, alpha(numeric), <b>validation</b> <b>rules,</b> etc., and for defining report layouts).|$|R
40|$|We present PEAR (Protocol Extendable AnalyzeR), a tool automat-ing the two static {{analyses}} for authentication protocols presented in [7, 8]. These analyses {{are based on}} a tagging scheme that de-scribes how message components contribute in achieving authen-tication. The tool provides a tag inference procedure that allows users to analyze untagged protocol specications. When a proto-col is successfully validated, tags give users precise information on how and why authentication is guaranteed. Notably, the tool re-ceives in input both the protocol specication and the <b>validation</b> <b>rules.</b> Both <b>validation</b> and tag inference are parametric with re-spect to the <b>validation</b> <b>rules,</b> thus allowing users to easily imple-ment new rules/analyses with no need of modifying the underlying procedures. 1...|$|R
50|$|A {{commitment}} to SQL code containing inner joins assumes NULL join columns {{will not be}} introduced by future changes, including vendor updates, design changes and bulk processing outside of the application's data <b>validation</b> <b>rules</b> such as data conversions, migrations, bulk imports and merges.|$|R
5000|$|Integrity or <b>validation</b> <b>rules,</b> {{also known}} as constraints, {{restrict}} the set of facts and the transitions between the permitted sets of facts to those that are considered useful. In terms of data quality, integrity rules are used to guarantee {{the quality of the}} facts.|$|R
40|$|Part 1 : Internet Crime InvestigationsInternational audienceExperiments {{with the}} Foxy P 2 P network have {{demonstrated}} that the first uploader of a file can be identified when search queries are submitted to all the network nodes during initial file sharing. However, in real Foxy networks, file search queries are not transmitted to the entire Foxy network and this process may not identify the first uploader. This paper presents a set of <b>validation</b> <b>rules</b> that validate the observed first uploader. The <b>validation</b> <b>rules</b> define the seeder curve that consistently describes the number of uploaders over time. Analysis of four scenarios shows improved accuracy at detecting the first uploader and that, in situations with insufficient competition for file content, the first uploader may not be identified precisely...|$|R
50|$|Data {{validation}} {{is intended}} to provide certain well-defined guarantees for fitness, accuracy, and consistency for any of various kinds of user input into an application or automated system. Data <b>validation</b> <b>rules</b> can be defined and designed using any of various methodologies, and be deployed in any of various contexts.|$|R
40|$|Abstract: This paper {{presents}} an approach for using model checking for process models and workflows. The approach {{allows it to}} graphically state semantic <b>validation</b> <b>rules</b> on the levelofprocesses. This enables the process modeler to use model checking techniques to get higher quality models for the further software development process. ...|$|R
40|$|Data <b>validation</b> <b>rules</b> {{constitute}} the constraints that data input and processing must adhere to {{in addition to}} the structural constraints imposed by a data model. Web modeling tools do not make all types of data validation explicit in their models, hampering full code generation and model expressivity. Web application frameworks do not offer a consistent interface for data validation. In this paper, we present a solution for the integration of declarative data <b>validation</b> <b>rules</b> with user interface models in the domain of web applications, unifying syntax, mechanisms for error handling, and semantics of validation checks, and covering value well-formedness, data invariants, input assertions, and action assertions. We have implemented the approach in WebDSL, a domain-specific language for the definition of web applications. Software TechnologyElectrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|Abstract: This paper {{presents}} {{a new approach}} to Automatic GUI Test Case Gen-eration and Validation: a use case-guided technique to reduce the effort required in GUI modeling and test coverage analysis. The test case generation process is initially guided by use cases describing the GUI behavior, recorded as a series of interactions with the application widgets (e. g. widgets being clicked, data input, etc.) These use cases (modeled as a set of initial test cases) are annotated by the tester to signal in-teresting variations in widget values (ranges, valid or invalid values) and <b>validation</b> <b>rules</b> with expected results. Annotations and <b>validation</b> <b>rules</b> allow this approach to automatically generate new test cases and expected results, easily expanding the test coverage. Also, the process allows narrowing the GUI model testing to precisely the set of widgets, interactions, and values the tester is interested in...|$|R
40|$|Abstract: Domain-specific {{languages}} and model-driven development are two promising approaches for tackling {{the complexity of}} software systems development. However, creating domain-specific modeling languages is a complex and lengthy task which makes the creation of DSMLs only feasible in large and complex projects. To alleviate this difficulty, we developed OPM/D, a visual meta-modeling language for the definition of domain-specific modeling languages. Languages in OPM/D are defined via a static structural meta-model of the language {{and a set of}} <b>validation</b> <b>rules</b> that define the non-structural constraints of the language. The language editor is created on-the-fly through interpretation of the static structure and <b>validation</b> <b>rules,</b> minimizing the time between language definition and its use. Our approach has been applied to define a subset of the OPM modeling language, and a prototype tools is being developed using the Eclipse platform and technologies. ...|$|R
40|$|Data validation, i. e. {{the process}} of {{detecting}} erroneous, or probably erroneous, values within statistical data sets, is normally done {{through the application of}} <b>validation</b> <b>rules,</b> such as ranges or deterministic correlations among variables. The presented research work on the contrary, is based on the concept of probabilistic validation treatment, based on the conditional probability distribution of multi-dimensional random variables. After the probability space is defined, the conditional probabilities are calculated from historical data and/or prior information, and an estimation of the underlying probability structure is derived. Thus, a generic and consistent validation treatment is feasible, which can then be applied without the overhead of expressing and formally declaring variable-specific <b>validation</b> <b>rules.</b> Obviously, due to the significant number of dimensions as well as the important volume of statistical data, this process can demanding in terms of both computing and dat...|$|R
50|$|Consistency is a {{very general}} term, which demands that the data must meet all <b>validation</b> <b>rules.</b> In the {{previous}} example, the validation is a requirement that A + B = 100. All <b>validation</b> <b>rules</b> must be checked to ensure consistency. Assume that a transaction attempts to subtract 10 from A without altering B. Because consistency is checked after each transaction, {{it is known that}} A + B = 100 before the transaction begins. If the transaction removes 10 from A successfully, atomicity will be achieved. However, a validation check will show that A + B = 90, which is inconsistent with the rules of the database. The entire transaction must be cancelled and the affected rows rolled back to their pre-transaction state. If there had been other constraints, triggers, or cascades, every single change operation would have been checked {{in the same way as}} above before the transaction was committed.|$|R
40|$|This paper {{describes}} a validation architecture used within Intermountain Health Care’s Clinical Knowledge Repository (CKR). The architecture provides additional functionality that complements XML Schema validation, producing user-friendly error messages and enabling <b>validation</b> <b>rules</b> reuse. The <b>validation</b> architecture helps document authors fix their own errors. As a result, less than 1 % of all {{documents in the}} CKR are considered invalid...|$|R
