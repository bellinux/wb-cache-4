4354|911|Public
50|$|MARS (like {{recursive}} partitioning) does automatic <b>variable</b> <b>selection</b> (meaning {{it includes}} important {{variables in the}} model and excludes unimportant ones). However, {{bear in mind that}} <b>variable</b> <b>selection</b> is not a clean problem and there is usually some arbitrariness in the selection, especially in the presence of collinearity and 'concurvity'.|$|E
5000|$|... the Cp cannot handle complex {{collections}} of models {{as in the}} <b>variable</b> <b>selection</b> (or feature selection) problem.|$|E
5000|$|The {{distribution}} has heavy {{tails and}} a sharp peak at [...] and, because of this, it has applications in <b>variable</b> <b>selection.</b>|$|E
30|$|As {{evident in}} the above literatures, several {{researchers}} have done extensive work {{on the performance of}} various discriminant and classification functions under skewed or non normal distributions. However, not much attention has been focused on studying and evaluating the performance of these classifiers using three populations under skewed distribution considering different sampling ratios, under different centroid separators and under varying <b>variable</b> <b>selections.</b> This study therefore seeks to investigate the performance of a single classifier (i.e the QDF) under skewed distribution considering different <b>variable</b> <b>selections,</b> varying sampling ratios and varying centroid separators considering three groups/populations.|$|R
30|$|Planning defines how the {{experiment}} should be conducted. It {{is divided into}} six steps: Context <b>Selection,</b> <b>Variables</b> <b>selection,</b> Hypothesis formulation, Selection of subjects, Experiment design, Instrumentation and Validity evaluation. These steps are described in details in next subsections.|$|R
3000|$|... 3 <b>Selection</b> of {{response}} <b>variables</b> <b>Selection</b> {{of response}} <b>variables</b> {{should be done}} properly so that it provides useful information about the process. Typically in a well-testing problem, a number of points sampled on the pressure derivative plots or a useful transformation of them may be considered as the response variables.|$|R
50|$|Spike-and-slab {{regression}} is a Bayesian <b>variable</b> <b>selection</b> {{technique that}} is particularly useful {{when the number of}} possible predictors is larger than the number of observations.|$|E
5000|$|Principal {{steps of}} QSAR/QSPR {{including}} (i) Selection of Data set and extraction of structural/empirical descriptors (ii) <b>variable</b> <b>selection,</b> (iii) model construction and (iv) validation evaluation. [...] " ...|$|E
50|$|In statistics, the g-prior is an {{objective}} prior for the regression coefficients of a multiple regression. It {{was introduced by}} Arnold Zellner.It is a key tool in Bayes and empirical Bayes <b>variable</b> <b>selection.</b>|$|E
40|$|This article evaluates, in {{quantitative}} terms, Mathematical courses {{offered by}} the Undergraduate Distance Education Center of Rio de Janeiro (CEDERJ), consortium between the state government, city halls and several universities. For this purpose, we used the Envelopment Data Analysis (DEA) tool, which measures the relative efficiency between different Decision Making Units (DMUs), combined with a method of <b>variables</b> <b>selection...</b>|$|R
40|$|We {{present a}} model of Bayesian network for {{continuous}} variables, where densities and conditional densities are estimated with B-spline MoPs. We use a novel approach to directly obtain conditional densities estimation using B-spline properties. In particular we implement naive Bayes and wrapper <b>variables</b> <b>selection.</b> Finally we apply our techniques {{to the problem of}} predicting neurons morphological variables from electrophysiological ones...|$|R
40|$|We {{develop a}} nonparametric test, based on kernel smoothers, {{in order to}} decide whether some {{covariates}} could be suppressed in a multidimensional nonparametric regression study. We give the asymptotic distribution of the statistic involved in our test, under a general dependence assumption on the sample that allows for application to time series prediction. Curse of dimensionality Kernel regression <b>Variables</b> <b>selection</b> test [beta]-mixing...|$|R
5000|$|Structured {{sparsity}} regularization extends and generalizes the <b>variable</b> <b>selection</b> {{problem that}} characterizes sparsity regularization. Consider the above regularized empirical risk minimization {{problem with a}} general kernel and associated feature map [...] with [...]|$|E
50|$|Embedded {{methods have}} been {{recently}} proposed that try to combine the advantages of both previous methods. A learning algorithm takes advantage of its own <b>variable</b> <b>selection</b> process and performs feature selection and classification simultaneously.|$|E
5000|$|For {{achieving}} a better compression/time ratio, a heuristic for <b>variable</b> <b>selection</b> is desirable. For this purpose, Cotton defines the [...] "additivity" [...] of a resolution step (with antecedents [...] and [...] and resolvent [...] ): ...|$|E
40|$|The paper aims {{to improve}} the {{methodology}} of measuring efficiency of Latvian banks. Efficiency scores were calculated with application of non-parametric frontier technique Data Envelopment Analysis (DEA). Input-oriented DEA model under Variable Returns to Scale (VRS) assumption was used. Potential model variables were selected based on the intermediation and profitability approach. Fourteen alternative models with different inputs-outputs combinations were developed for the research purposes. To substantiate the <b>variables</b> <b>selection</b> for DEA model the received data was processed, using such methods, as correlation analysis, linear regression analysis, analysis of mean values, and two-samples Kolmogorov-Smirnov test. The research results assisted the authors in providing general recommendations about the <b>variables</b> <b>selection</b> for DEA application in the Latvian banking sector. The present research contributes to the existing analytical data on bank performance in Latvia. The empirical findings provide a background for further studies, in particular, the efficiency of Latvian banks could be analysed in the extended time period...|$|R
40|$|Semiconductor {{manufacturers}} are forced by market demand to continually deliver lower cost and faster devices. This results in complex industrial processes that, with continuous evolution, aim to improve quality and reduce costs. Plasma etching processes {{have been identified}} as a critical part of the production of semiconductor devices. It is therefore important to have good control over plasma etching but this is a challenging task due to the complex physics involved. Optical Emission Spectroscopy (OES) measurements can be collected non-intrusively during wafer processing and are being used more and more in semiconductor manufacturing as they provide real time plasma chemical information. However, the use of OES measurements is challenging due to its complexity, high dimension and the presence of many redundant variables. The development of advanced analysis algorithms for virtual metrology, anomaly detection and <b>variables</b> <b>selection</b> is fundamental in order to effectively use OES measurements in a production process. This thesis focuses on computational intelligence techniques for OES data analysis in semiconductor manufacturing presenting both theoretical results and industrial application studies. To begin with, a spectrum alignment algorithm is developed to align OES measurements from different sensors. Then supervised <b>variables</b> <b>selection</b> algorithms are developed. These are defined as improved versions of the LASSO estimator with the view to selecting a more stable set of variables and better prediction performance in virtual metrology applications. After this, the focus of the thesis moves to the unsupervised <b>variables</b> <b>selection</b> problem. The Forward Selection Component Analysis (FSCA) algorithm is improved with the introduction of computationally efficient implementations and different refinement procedures. Nonlinear extensions of FSCA are also proposed. Finally, the fundamental topic of anomaly detection is investigated and an unsupervised <b>variables</b> <b>selection</b> algorithm tailored to anomaly detection is developed. In addition, it is shown how OES data can be effectively used for semi-supervised anomaly detection in a semiconductor manufacturing process. The developed algorithms open up opportunities for the effective use of OES data for advanced process control. All the developed methodologies require minimal user intervention and provide easy to interpret models. This makes them practical for engineers to use during production for process monitoring and for in-line detection and diagnosis of process issues, thereby resulting in an overall improvement in production performance...|$|R
30|$|A <b>selection</b> <b>variable</b> (i.e., {{optimization}} variable) in [13] {{was defined}} {{based on an}} antenna basis. When n D[*]>[*] 1, a similar definition of a <b>selection</b> <b>variable</b> will result in binary nonlinear optimization problems. This is clearly not favorable from a practical viewpoint. As shown later in this section, binary linear optimization could be obtained by defining a <b>selection</b> <b>variable</b> based on a subset basis.|$|R
5000|$|In machine {{learning}} and statistics, feature selection, {{also known as}} <b>variable</b> <b>selection,</b> attribute selection or variable subset selection, {{is the process of}} selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons: ...|$|E
50|$|A {{different}} {{approach is to}} consider union of groups for <b>variable</b> <b>selection.</b> This approach captures the modeling situation where variables can be selected {{as long as they}} belong at least to one group with positive coefficients. This modeling perspective implies that we want to preserve group structure.|$|E
50|$|The {{standard}} GWAS regression can {{be improved}} on using penalized regression methods like the LASSO/ridge regression. (Penalized regression {{can be interpreted as}} placing informative priors on how many genetic variants are expected to affect a trait, and the distribution of their effect sizes; Bayesian counterparts exist for LASSO/ridge, and other priors have been suggested & used. They can perform better in some circumstances.) A multi-dataset, multi-method study found that of 15 different methods compared across four datasets, minimum redundancy maximum relevance was the best performing method. Furthermore, <b>variable</b> <b>selection</b> methods tended to outperform other methods. <b>Variable</b> <b>selection</b> methods do not use all the available genomic variants present in a dataset, but attempt to select an optimal subset of variants to use. This leads to less overfitting but more bias (see bias-variance tradeoff).|$|E
40|$|This work {{deals with}} {{prediction}} of anopheles number using environmental and climate <b>variables.</b> The <b>variables</b> <b>selection</b> is performed by GLMM (Generalized linear mixed model) {{combined with the}} Lasso method and simple cross validation. Selected variables are debiased while the prediction is generated by simple GLMM. Finally, the results reveal to be qualitatively better, at selection, the prediction point of view than those obtained by the reference method...|$|R
40|$|The aim of {{this work}} is to {{establish}} a relationship between schistosomiasis prevalence and social-environmental variables, in the state of Minas Gerais, Brazil, through multiple linear regression. The final regression model was established, after a <b>variables</b> <b>selection</b> phase, with a set of spatial variables which contains the summer minimum temperature, human development index, and vegetation type variables. Based on this model, a schistosomiasis risk map was built for Minas Gerais...|$|R
40|$|Problems, arising during global {{reconstruction}} of dynamical models from time series, and {{prospects of the}} further development of empiric modeling methods are discussed. We present a specialized approach aimed at modeling the systems being under arbitrary regular driving. Original “technical tricks ” are also described: a technique for dynamical <b>variables</b> <b>selection</b> and a model structure optimization routine. All the approaches are illustrated with examples of reconstructing etalon equations and modeling real-world (radiophysical) objects. 1...|$|R
5000|$|With {{any amount}} of noise in the {{dependent}} variable and with high dimensional multicollinear independent variables, {{there is no reason}} to believe that the selected variables will have a high probability of being the actual underlying causal variables. This problem is not unique to LARS, as it is a general problem with <b>variable</b> <b>selection</b> approaches that seek to find underlying deterministic components. Yet, because LARS is based upon an iterative refitting of the residuals, it would appear to be especially sensitive to the effects of noise. This problem is discussed in detail by Weisberg in the discussion section of the Efron et al. (2004) Annals of Statistics article. Weisberg provides an empirical example based upon re-analysis of data originally used to validate LARS that the <b>variable</b> <b>selection</b> appears to have problems with highly correlated variables.|$|E
5000|$|Different {{aspects of}} {{validation}} of QSAR models that need attention includes methods of selection of training set compounds, setting training set size {{and impact of}} <b>variable</b> <b>selection</b> [...] for training set models for determining the quality of prediction. Development of novel validation parameters for judging quality of QSAR models is also important.|$|E
50|$|Ensuring {{results are}} {{repeatable}} by employing {{a wide array}} of statistical methodologies. <b>Variable</b> <b>selection,</b> validation testing, simulation, control groups and other techniques together help to distinguish true effects from chance events. A champion/challenger framework ensures that the best mathematical models are deployed always. In addition, performance is enhanced by the ability to analyze huge datasets and to retain historical learning.|$|E
40|$|Abstract. The {{purpose of}} this study is to {{identify}} the Hierarchical Wavelet Neural Networks (HWNN) and select important input features for each sub-wavelet neural network automatically. Based on the predefined instruction/operator sets, a HWNN is created and evolved using tree-structure based Extended Compact Genetic Programming (ECGP), and the parameters are optimized by Differential Evolution (DE) algorithm. This framework also allows input <b>variables</b> <b>selection.</b> Empirical results on benchmark time-series approximation problems indicate that theproposedmethodiseffectiveandefficient. ...|$|R
40|$|This paper {{deals with}} {{prediction}} of anopheles number using environmental and climate <b>variables.</b> The <b>variables</b> <b>selection</b> is performed by an automatic machine learning method %don't {{get what you}} mean % %ok% based on Lasso and stratified two levels cross validation. Selected variables are debiased while the predictionis generated by simple GLM (Generalized linear model). Finally, the results reveal to be qualitatively better, at selection, the prediction,and the CPU time point of view than those obtained by B-GLM method...|$|R
40|$|Abstract. This paper {{proposes a}} Flexible Neural Tree (FNT) model for {{informative}} gene selection and gene expression profiles classification. Based on the pre-defined instruction/operator sets, a flexible neural tree {{model can be}} created and evolved. This framework allows input <b>variables</b> <b>selection,</b> over-layer connections and different activation functions for the various nodes involved. The FNT structure is developed using the Extended Compact Genetic Programming and the free parameters embedded in the neural tree are optimized by particle swarm optimization algorithm. Empirical results on two well-known cancer datasets shows competitive results with existing methods. ...|$|R
5000|$|In {{statistical}} modeling, {{a training}} set {{is used to}} fit a model {{that can be used}} to predict a [...] "response value" [...] from one or more [...] "predictors." [...] The fitting can include both <b>variable</b> <b>selection</b> and parameter estimation. Statistical models used for prediction are often called regression models, of which linear regression and logistic regression are two examples.|$|E
5000|$|Shrinkage (also {{known as}} regularization), that fits the full model with all {{predictors}} using an algorithm that shrinks the estimated coefficients towards zero, which can significantly reduce their variance. Depending on {{what type of}} shrinkage is performed, some of the coefficients may be shrunk to zero itself. As such, some shrinkage methods - like the lasso - de facto perform <b>variable</b> <b>selection.</b>|$|E
50|$|Richard John Samworth (born May 1978) is Professor of Statistics in the Statistical Laboratory, University of Cambridge, and a Teaching Fellow of St John's College, Cambridge. He {{was educated}} at St John's College, Cambridge. Currently, his main {{research}} interests are in nonparametric and high-dimensional statistics. Particular topics include shape-constrained density and other nonparametric function estimation problems, nonparametric classification, clustering and regression, the bootstrap and high-dimensional <b>variable</b> <b>selection</b> problems.|$|E
40|$|Abstract: Given a {{population}} described by p explanatory and one dependent categorical variables, {{we assume that}} the dependent variable defines a partition of the population into g groups. Discriminant Analysis studies the relation between the p explanatory variables and the dependent variable finding the subset of variables that has the most predictive power. Generally, in categorical discriminant analysis, the a priori probabilities associated to the g groups are assumed known. In this paper we summarise some suitable approaches under the hypothesis of unknown group a priori probabilities and we propose a new <b>variables</b> <b>selection</b> algorithm...|$|R
30|$|Note {{that for}} the US case we use as our <b>selection</b> <b>variable</b> ‘degree of {{research}} done before buying an investing product’, following Bertaut and Starr-McCluer (2000). Due to privacy concerns the FRB does not release publicly the information {{we would need to}} construct the <b>selection</b> <b>variables</b> we use in the Spanish case.|$|R
30|$|An {{enormous}} deal {{of study}} has been made since Fisher’s (1936) original work on discriminant analysis {{as well as several}} other researchers tackling similar problem. Some estimation methods have been proposed and some sampling properties derived. However, there is little investigation done on large sample properties of these functions. Also a considerable number of studies had been carried out on discriminant analysis but not much is done on the effect or the performance of the QDF under correlated and uncorrelated data with varying sample size ratios, different <b>variable</b> <b>selections</b> and with different centroid separators for three populations.|$|R
