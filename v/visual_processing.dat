4392|1845|Public
5|$|Autism is a {{disorder}} of neural development characterized by {{impaired social interaction}} and communication, and by restricted and repetitive behaviour. Typically there is particular difficulty acquiring expressive communication skills. People with autism {{have been found to}} have strong <b>visual</b> <b>processing</b> skills, making them good candidates for an AAC approach.|$|E
5|$|Most of the {{enlargement}} of the primate brain comes from a massive expansion of the cerebral cortex, especially the prefrontal cortex and {{the parts of the}} cortex involved in vision. The <b>visual</b> <b>processing</b> network of primates includes at least 30 distinguishable brain areas, with a complex web of interconnections. It has been estimated that <b>visual</b> <b>processing</b> areas occupy {{more than half of the}} total surface of the primate neocortex. The prefrontal cortex carries out functions that include planning, working memory, motivation, attention, and executive control. It takes up a much larger proportion of the brain for primates than for other species, and an especially large fraction of the human brain.|$|E
5|$|Two {{groups of}} {{invertebrates}} have notably complex brains: arthropods (insects, crustaceans, arachnids, and others), and cephalopods (octopuses, squids, and similar molluscs). The brains of arthropods and cephalopods arise from twin parallel nerve cords that extend {{through the body}} of the animal. Arthropods have a central brain, the supraesophageal ganglion, with three divisions and large optical lobes behind each eye for <b>visual</b> <b>processing.</b> Cephalopods such as the octopus and squid have the largest brains of any invertebrates.|$|E
40|$|Abstract—Visual {{awareness}} {{is an important}} function in mind model CAM (Consciousness And Memory). In this paper，we construct a visual awareness component from two respects, namely, objective processing and spatial processing. The Conditional Random fields based Feature Binding computational model (CRFB) is applied to <b>visual</b> objective <b>processing.</b> For <b>visual</b> spatial <b>processing,</b> we explore three important kinds of relationships between objects that can be queried: topology, distance, and direction. The details of object processing and spatial processing are presented. Keywords-visual awareness; CAM; visual objective processing; <b>visual</b> spatial <b>processing</b> I...|$|R
50|$|The VSI is {{a measure}} of <b>visual</b> spatial <b>processing.</b>|$|R
5000|$|... #Article: The Role of Serotonin in <b>Visual</b> Orientation <b>Processing</b> ...|$|R
5|$|The {{auditory}} nerve was also large, suggesting good hearing, {{which may have}} been useful for auditory communication and spatial awareness. The nerve had a well-developed vestibular component as well, which implies a good sense of balance and coordination. In contrast, the nerves and brain structures associated with eyesight were smaller and undeveloped. The midbrain tectum, responsible for <b>visual</b> <b>processing</b> in reptiles, was very small in Tarbosaurus, as were the optic nerve and the oculomotor nerve, which controls eye movement. Unlike Tyrannosaurus, which had forward-facing eyes that provided some degree of binocular vision, Tarbosaurus had a narrower skull more typical of other tyrannosaurids in which the eyes faced primarily sideways. All of this suggests that Tarbosaurus relied more on its senses of smell and hearing than on its eyesight.|$|E
25|$|Anthony Zee, {{theoretical}} {{physicist and}} Pulitzer Prize nominee. Known for applying {{quantum field theory}} to such problems as RNA folding and <b>visual</b> <b>processing.</b>|$|E
25|$|Irritation of {{cortical}} centers {{responsible for}} <b>visual</b> <b>processing</b> (e.g., seizure activity). The irritation {{of the primary}} visual cortex causes simple elementary visual hallucinations.|$|E
40|$| versus inwards) {{operating}} {{at a higher}} level of <b>visual</b> motion <b>processing.</b> In|$|R
30|$|With the {{development}} of information technology, especially {{the development}} of <b>visual</b> image <b>processing</b> technology, more and more automated production links, such as injection molding production, use visualization to realize automatic detection of injection molds, but the traditional integrated environment for <b>visual</b> image <b>processing</b> exists in real time. In order to solve this problem, this paper proposes a <b>visual</b> image <b>processing</b> integrated development environment model based on the Moore nearest neighbor model. The model runs on the visual platform, and the residuals to be detected are highlighted by the Moore nearest neighbor model. In order to solve the error caused by the image shift, the model introduces the support vector machine as the classification method, and the model {{is used for the}} simulation experiment. The average accuracy of cavity residual detection in the model is 85.71 %, and the average time of residual detection is 0.910  s. The results show that the model solves the real-time and accuracy problems of the traditional <b>visual</b> image <b>processing</b> model.|$|R
3000|$|Attention—Cambridge Neuropsychological Test Automated Battery (CANTAB) Rapid <b>Visual</b> Information <b>Processing</b> (Robbins et al. 1994) [...]...|$|R
25|$|<b>Visual</b> <b>processing</b> (Gv) is {{the ability}} to perceive, analyze, synthesize, and think with visual patterns, {{including}} the ability to store and recall visual representations.|$|E
25|$|About {{two-thirds of}} the Drosophila brain is {{dedicated}} to <b>visual</b> <b>processing.</b> Although the spatial resolution of their vision is significantly worse than that of humans, their temporal resolution is around 10 times better.|$|E
25|$|The {{major problem}} in visual {{perception}} is that what people see {{is not simply a}} translation of retinal stimuli (i.e., the image on the retina). Thus people interested in perception have long struggled to explain what <b>visual</b> <b>processing</b> does to create what is actually seen.|$|E
40|$|We {{aimed at}} {{investigating}} whether on-line and delayed <b>visual</b> pattern <b>processing</b> activated different areas in human prefrontal and parietal cortex. For this purpose we measured the regional {{cerebral blood flow}} (rCBF) during simultaneous and successive <b>visual</b> matrix <b>processing</b> in 10 right-handed subjects. Delayed matching to sample activated predominantly left hemispheric ventrolateral prefrontal cortex, Broca's area {{and parts of the}} parietal cortex. In contrast, visuospatial matrix rotation showed activation of the right dorsolateral prefrontal cortex and parietal lobe. The present results suggest a hemispheric dissociation of fronto-parietal circuits with a left dominance for <b>visual</b> pattern <b>processing</b> like storage and a right dominance for visuospatial processing. (C) 2002 Elsevier Science B. V. All rights reserved...|$|R
5000|$|Turvey, M. T. (1977). Contrasting orientations to {{the theory}} of <b>visual</b> {{information}} <b>processing.</b> Psychological Review, 84, 67-88.|$|R
40|$|International audienceThe brain areas {{involved}} in <b>visual</b> word <b>processing</b> rapidly become lateralized {{to the left}} cerebral hemisphere. It is often assumed this is because, {{in the vast majority}} of people, cortical structures underlying language production are lateralized to the left hemisphere. An alternative hypothesis, however, might be that the early stages of <b>visual</b> word <b>processing</b> are lateralized to the left hemisphere because of intrinsic hemispheric differences in <b>processing</b> low-level <b>visual</b> information as required for distinguishing fine-grained visual forms such as letters. If the alternative hypothesis was correct, we would expect posterior occipito-temporal processing stages still to be lateralized to the left hemisphere for participants with right hemisphere dominance for the frontal lobe processes involved in language production. By analyzing event-related potentials of native readers of French with either left hemisphere or right hemisphere dominance for language production (determined using a verb generation task), we were able to show that the posterior occipito-temporal {{areas involved}} in <b>visual</b> word <b>processing</b> are lateralized to the same hemisphere as language production. This finding could suggest top-down influences in the development of posterior <b>visual</b> word <b>processing</b> areas...|$|R
25|$|The {{ability to}} {{understand}} which factors in <b>visual</b> <b>processing</b> determine whether a contralesional event, occurring on the half of the patient's brain or body opposing {{the site of a}} lesion, is observed or eliminated can provide crucial insights connecting to the mechanisms of attention and operation.|$|E
25|$|Impairments in {{multiple}} aspects of cognition, including attention, learning, memory, <b>visual</b> <b>processing,</b> and sleep {{have been found}} in regular MDMA users. The magnitude of these impairments is correlated with lifetime MDMA usage and are partially reversible with abstinence. Several forms of memory are impaired by chronic ecstasy use; however, the effect sizes for memory impairments in ecstasy users are generally small overall. MDMA use is also associated with increased impulsivity and depression.|$|E
25|$|Human {{echolocation}} is {{a learned}} ability {{for humans to}} sense their environment from echoes. This ability is used by some blind people to navigate their environment and sense their surroundings in detail. Studies in 2010 and 2011 using {{functional magnetic resonance imaging}} techniques have shown that parts of the brain associated with <b>visual</b> <b>processing</b> are adapted for the new skill of echolocation. Studies with blind patients, for example, suggest that the click-echoes heard by these patients were processed by brain regions devoted to vision rather than audition.|$|E
40|$|The article {{describes}} a symbolic approach to <b>visual</b> information <b>processing,</b> and sets out four principles {{that appear to}} govern the design of complex symbolic information processing systems. A computational theory of early <b>visual</b> information <b>processing</b> is presented, which extends to {{about the level of}} figure-ground separation. It includes a process-oriented theory of texture vision. Most of the theory has been implemented, and examples are shown of the analysis of several natural images. This replaces Memos 324 and 334...|$|R
40|$|The aim of {{this study}} was to examine the {{difference}} of <b>visual</b> information <b>processing</b> in KYT (Kiken Yochi Training) among novice, expert and non-licensed persons. In KYT tasks, participants were required to search for a potentially dangerous part using a static image under driving situations. The location of fixation point and the time series change of eye gaze were measured using an eye camera. In order to detect the difference of <b>visual</b> information <b>processing</b> among three groups above, an important area that the participants must pay attention to with the highest priority was set for each static image. The time until the eye gaze fixates to the important area, and the ratio of the fixation time to the total search time were detected. Using these measures, the difference of <b>visual</b> information <b>processing</b> among three groups was clarified. Moreover, for novice and non-licensed participants, it was also explored whether a lecture related to KYT would improve the efficiency of <b>visual</b> information <b>processing.</b> The time until the eye gaze fixates to the important area was longer for the non-licensed participants than for the experienced participants. The learning effect by means of a KYT lecture was also observed...|$|R
25|$|The {{visual cortex}} {{is the part}} of the {{cerebral}} cortex in the posterior part of the brain responsible for <b>processing</b> <b>visual</b> stimuli, called the occipital lobe. The central 10° of field (approximately the extension of the macula) is represented by at least 60% of the visual cortex. Many of these neurons are believed to be involved directly in <b>visual</b> acuity <b>processing.</b>|$|R
25|$|Those with {{damage to}} {{regions of the}} brain {{involved}} in <b>visual</b> <b>processing,</b> such as the occipital lobes, may develop amnesia. The episodic content of autobiographical memories is predominantly encoded {{in the form of}} visual images. If the ability to generate visual images is compromised or lost then access to specific details of the past held in episodic images is lost as well. When life events or episodic memories are encoded in the brain, they are in the form of pictures or visual images. They develop amnesia since they can no longer bring these visual images of the past to mind.|$|E
25|$|Ultra-rapid visual {{categorization}} {{is a model}} proposing {{an automatic}} feedforward mechanism that forms high-level object representations in parallel without focused attention. In this model, the mechanism cannot be sped up by training. Evidence for a feedforward mechanism {{can be found in}} studies that have shown that many neurons are already highly selective {{at the beginning of a}} visual response, thus suggesting that feedback mechanisms are not required for response selectivity to increase. Furthermore, recent fMRI and ERP studies have shown that masked visual stimuli that participants do not consciously perceive can significantly modulate activity in the motor system, thus suggesting somewhat sophisticated <b>visual</b> <b>processing.</b>|$|E
25|$|Extinction {{frequency}} {{may also}} be affected by reporting method, due to how <b>visual</b> <b>processing</b> interacts {{with the rest of}} the brain. Reported extinctions drop when the patient is told to relay the extinction news by a method other than speaking – particularly, the use of the eyes in a feedback loop (looking to the side if there is a single stimuli, and up if there are two). This may work by eliminating the need for hemispheric transfer of information – routing the response to the motor cortex instead of the visual cortex – and so shortening the path and response time, a conclusion supported by increased detection of isolated contralesional stimuli under such circumstances.|$|E
40|$|<b>Visual</b> {{information}} <b>processing</b> is {{a rather}} mature and rich field. An impressive amount of image and video processing algorithms are currently available for {{a great variety of}} applications. There is also a considerable number of software packages, either commercial products or academic shareware, ranging from dedicated systems aiming at solving very specific problems, to open environments designed for research and development purposes. However, {{there is a lack of}} a common framework that integrates all prior efforts and developments in the field, and at the same time provides added-value features that support and in essence realize a `processing service' for networked visual information systems. This report discusses <b>visual</b> information <b>processing</b> services and presents a distributed, autonomous, cooperating agent architecture, which has been used to design and implement DIPE, a novel environment for the support of such services. An Environment Supporting <b>Visual</b> Information <b>Processing</b> Se [...] ...|$|R
30|$|A {{medium to}} large effect (Cohen’s d with Hedges’ {{correction}} = 0.62) was noted in rapid <b>visual</b> information <b>processing,</b> with HCs significantly surpassing {{the performance of}} FEM patients.|$|R
50|$|In {{the late}} 1970s, Harris {{developed}} {{a mathematical model}} of <b>visual</b> information <b>processing</b> which {{formed the basis for}} two articles in the journal Perception and Psychophysics (1979, 1984).|$|R
25|$|Along {{with the}} ventral pathway being {{important}} for <b>visual</b> <b>processing,</b> {{it is also}} important for processing auditory information. During processing, when sound waves enter the ear, this information is transduced into an unanalyzed auditory object. The subcortical auditory pathway then relays the information to the auditory cortex in the dorsal superior temporal gyrus (dSTG). In the dSTG, it is then divided into its constituent phones. These phones are then recognized into phonetic words in the superior temporal sulcus (STS). The information first enters the ventral stream at the posterior middle temporal gyrus and the posterior inferior temporal sulcus p(MTG+ITS). Here the auditory words are converted into semantic words. These words are then converted into a semantic phrase by the first combinatorial net at the anterior portion of the middle temporal lobe and then converted into a syntactic noun phrase at the second combinatorial net. This second combinatorial net is location in the anterior inferior temporal gyrus.|$|E
25|$|Evans & Treisman (2005) {{proposed}} a hypothesis that humans rapidly detect disjunctive sets of unbound features of target categories {{in a parallel}} manner, and then use these features to discriminate between scenes that do or do not contain the target without necessarily fully identifying it. An {{example of such a}} feature would be outstretched wings {{that can be used to}} tell whether or not a bird is in a picture, even before the system has identified an object as a bird. Evans & Treisman propose that natural scene perception involves a first pass through the <b>visual</b> <b>processing</b> hierarchy up to the nodes in a visual identification network, and then optional revisiting of earlier levels for more detailed analysis. During the 'first pass' stage, the system forms a global representation of the natural scene that includes the layout of global boundaries and potential objects. During the 'revisiting' stage, focused attention is employed to select local objects of interest in a serial manner, and then bind their features to their representations.|$|E
25|$|The time {{resolution}} needed depends on brain processing time for various events. An {{example of the}} broad range here is given by the <b>visual</b> <b>processing</b> system. What the eye sees is registered on the photoreceptors of the retina within a millisecond or so. These signals get to the primary visual cortex via the thalamus in tens of milliseconds. Neuronal activity related to the act of seeing lasts for more than 100ms. A fast reaction, such as swerving to avoid a car crash, takes around 200ms. By about half-a-second, awareness and reflection of the incident sets in. Remembering a similar event may take a few seconds, and emotional or physiological changes such as fear arousal may last minutes or hours. Learned changes, such as recognizing faces or scenes, may last days, months, or years. Most fMRI experiments study brain processes lasting a few seconds, with the study conducted over some tens of minutes. Subjects may move their heads during that time, and this head motion needs to be corrected for. So does drift in the baseline signal over time. Boredom and learning may modify both subject behavior and cognitive processes.|$|E
50|$|Frederic Dufaux {{from the}} Telecom ParisTech, Paris, France was named Fellow of the Institute of Electrical and Electronics Engineers (IEEE) in 2016 for {{contributions}} to <b>visual</b> information <b>processing</b> and coding.|$|R
5000|$|Low-level <b>processing</b> (<b>visual</b> and {{auditory}} acuity, gender, age, mood, …) ...|$|R
40|$|In this lecture I give {{a survey}} of joint works of Hitoshi Arai and Shinobu Arai. The main purpose of our study is to {{construct}} mathematical models of <b>visual</b> information <b>processing</b> in the brain, and to give applications to image processing. On the past few decades, several studies have been made on mathematical models of <b>visual</b> information <b>processing</b> in the human brain. Our new models are constructed by using simple pinwheel framelets ([4]) and pinwheel framelets ([6]), which are {{a new class of}} the so-called framelets 1. Before going to the main body of this lecture, we review (simple) pinwheel framelets...|$|R
