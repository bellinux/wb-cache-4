133|128|Public
25|$|The {{underlying}} {{rules for}} IBANs {{is that the}} account-servicing financial institution should issue an IBAN, as {{there are a number}} of areas where different IBANs could be generated from the same account and branch numbers that would satisfy the generic IBAN <b>validation</b> <b>rules.</b> In particular cases where 00 is a valid check digit, 97 will not be a valid check digit, likewise, if 01 is a valid check digit, 98 will not be a valid check digit, similarly with 02 and 99.|$|E
5000|$|Data {{validation}} is {{the application}} of <b>validation</b> <b>rules</b> to the data. For electronic CRFs the <b>validation</b> <b>rules</b> may be applied in real time {{at the point of}} entry. Offline validation may still be required (e.g. for cross checks between data types) ...|$|E
5000|$|New {{and changed}} data {{should be tested}} against <b>validation</b> <b>rules</b> ...|$|E
50|$|A <b>Validation</b> <b>rule</b> is a {{criterion}} or constraint {{used in the}} process of data validation, carried out after the data has been encoded onto an input medium and involves a data vet or validation program. This is distinct from formal verification, where the operation of a program is determined to be that which was intended, and that meets the purpose. The <b>Validation</b> <b>rule</b> or check system still used by many major software manufacturers was designed by an employee at Microsoft some time between 1997 and 1999.|$|R
40|$|This paper {{introduces}} an algebraic {{model of}} the protection mechanism that control access to information objects in computer systems. The model is a capability-based one, i. e. it uses the notion of access capability to define a hierarchy of access privileges which {{is the basis of}} the control mechanism. The notion {{of the state of the}} protection system is introduced as a mapping of the set of subjects of the system into the set of capabilities. The state transition rules are defined and the problem of correctness is seen as a state-transition <b>validation</b> <b>rule...</b>|$|R
5000|$|Tamil Nadu General Sales Tax (Turnover and Assessment) <b>Rules</b> <b>Validation</b> Act, 1959 ...|$|R
5000|$|Data <b>validation</b> <b>rules</b> may be defined, {{designed}} and deployed, for example: ...|$|E
5000|$|The CSLA [...]NET {{framework}} {{provides a}} rules engine that supports <b>validation</b> <b>rules,</b> business rules, and authorization rules. These rules {{are attached to}} object instances or properties, and are automatically invoked by CSLA [...]NET when necessary. <b>Validation</b> <b>rules</b> may be implemented using the CSLA [...]NET rule engine, or {{through the use of}} the DataAnnotations feature of Microsoft [...]NET.|$|E
5000|$|Frank {{is used as}} a user when adding <b>validation</b> <b>rules</b> to a Microsoft Access {{data entry}} box.|$|E
25|$|Ryan Holiday {{described}} {{our cultural}} values as dependent on <b>validation,</b> entitled, and <b>ruled</b> by our emotions, {{a form of}} egotism.|$|R
50|$|Anemic domain {{model is}} the use of a {{software}} domain model where the domain objects contain little or no business logic (<b>validations,</b> calculations, business <b>rules</b> etc.).|$|R
40|$|Part 5 : NETWORK FORENSICSInternational audienceRules {{have been}} {{specified}} for identifying first seeders in the Foxy peer-to-peer (P 2 P) network. However, these rules {{have not been}} validated due to difficulties in repeating download scenarios. This paper describes a <b>rule</b> <b>validation</b> scheme that uses a network simulation environment. The Type I and Type II error rates of Foxy network monitoring rules over 100 simulation experiments covering ten scenarios are measured and analyzed. The error rates reflect {{the limitations of the}} monitoring rules and demonstrate the importance of using network simulations for <b>rule</b> <b>validation...</b>|$|R
5000|$|StrategyEdit section which {{describes}} the <b>validation</b> <b>rules</b> {{to be applied}} - typically these will be cross field validations ...|$|E
50|$|<b>Validation</b> <b>Rules</b> are {{electronic}} checks {{defined in}} advance which ensure the completeness and {{consistency of the}} clinical trial data.|$|E
5000|$|... metadata-driven {{components}} {{which can}} bind to metadata formats such as XML schema, allowing a shared client-server data model with extensible types, <b>validation</b> <b>rules</b> and editing constraints ...|$|E
40|$|Complete {{support of}} {{user-defined}} features in a design-by-features system requires that feature classes {{created by the}} user become full-privileged members of the feature collection of the system. That is, they can be created, deleted and manipulated; they can have relationships to other features (and these relationships themselves can be defined too); they can be validated by <b>validation</b> constraints or <b>rules</b> (and new <b>validation</b> constraints and <b>rules</b> can be defined); their geometry can be {{anything that can be}} described in the underlying geometric modeling system. Clearly, to create a feature definition mechanism that covers all these facilities completely is...|$|R
40|$|IFIP Advances in Information and Communication Technology, v. 383 has title: Advances in Digital Forensics VIIIRules {{have been}} {{specified}} for identifying first seeders in the Foxy peer-to-peer (P 2 P) network. However, these rules {{have not been}} validated due to difficulties in repeating download scenarios. This paper describes a <b>rule</b> <b>validation</b> scheme that uses a network simulation environment. The Type I and Type II error rates of Foxy network monitoring rules over 100 simulation experiments covering ten scenarios are measured and analyzed. The error rates reflect {{the limitations of the}} monitoring rules and demonstrate the importance of using network simulations for <b>rule</b> <b>validation...</b>|$|R
50|$|WeDo Technologies’ RAID {{integrates}} {{information from}} different platforms {{to a central}} database, applies business <b>validation</b> and comparison <b>rules</b> to detect errors and inconsistencies, produces and manages alarms, provides full-problem lifecycle management, and offers different kinds of KPI’s and reports.|$|R
5000|$|Extensive {{reliance}} on terminal [...] "screen painting" [...] (i.e. [...] "mocking"-up a CRT data-entry screen) {{was used to}} assist system definition. e.g. to define a components database attributes (name, length, alpha(numeric), <b>validation</b> <b>rules,</b> etc., and for defining report layouts).|$|E
50|$|A {{commitment}} to SQL code containing inner joins assumes NULL join columns {{will not be}} introduced by future changes, including vendor updates, design changes and bulk processing outside of the application's data <b>validation</b> <b>rules</b> such as data conversions, migrations, bulk imports and merges.|$|E
5000|$|Integrity or <b>validation</b> <b>rules,</b> {{also known}} as constraints, {{restrict}} the set of facts and the transitions between the permitted sets of facts to those that are considered useful. In terms of data quality, integrity rules are used to guarantee {{the quality of the}} facts.|$|E
50|$|The {{first rule}} holds that a {{double-stranded}} DNA molecule globally has percentage base pair equality: %A = %T and %G = %C. The rigorous <b>validation</b> of the <b>rule</b> constitutes {{the basis of}} Watson-Crick pairs in the DNA double helix model.|$|R
40|$|This paper {{addresses}} the industrial visualization tools used when validating vehicle configuration rules. The configuration rules are logic expressions allowing vehicle configurations to be built, {{as well as}} which components these configurations should consist of. Valid configuration rules are permitting vehicle configurations according to the specialists’ expectations. Those vehicles also {{have to have the}} correct components assigned. Currently, the <b>validation</b> of configuration <b>rules</b> partially relies on time-consuming manual inspection and calculations. Our aim is to find a demonstrator facilitating the <b>validation</b> of configuration <b>rules.</b> Our approach is to visualize the calculation results together with the original configuration rules. In doing so, it should also be possible to use one single user interface instead of the multiple used today. The work presented in this paper is rooted in user studies at three automotive companies including both interviews and observations. The identified typical rule queries when using the industrial visualization tools and the identified difficulties informed the development of a demonstrator. This paper describes the conducted usability tests of the demonstrator, showing how the users found the <b>validation</b> of configuration <b>rules</b> less error-prone, more time-efficient and easier to learn. </p...|$|R
40|$|Over {{the last}} decade, human facial {{expressions}} recognition (FER) {{has emerged as}} an important research area. Several factors make FER a challenging research problem. These include varying light conditions in training and test images; need for automatic and accurate face detection before feature extraction; and high similarity among different expressions that makes it difﬁcult to distinguish these expressions with a high accuracy. This work implements a hierarchical linear discriminant analysis-based facial expressions recognition (HL-FER) system to tackle these problems. Unlike the previous systems, the HL-FER uses a pre-processing step to eliminate light effects, incorporates a new automatic face detection scheme, employs methods to extract both global and local features, and utilizes a HL-FER to overcome the problem of high similarity among different expressions. Unlike most of the previous works that were evaluated using a single dataset, {{the performance of the}} HL-FER is assessed using three publicly available datasets under three different experimental settings: n-fold cross validation based on subjects for each dataset separately; n-fold cross <b>validation</b> <b>rule</b> based on datasets; and, ﬁnally, a last set of experiments to assess the effectiveness of each module of the HL-FER separately. Weighted average recognition accuracy of 98. 7 % across three different datasets, using three classifiers, indicates the success of employing the HL-FER for human FER...|$|R
50|$|Data {{validation}} {{is intended}} to provide certain well-defined guarantees for fitness, accuracy, and consistency for any of various kinds of user input into an application or automated system. Data <b>validation</b> <b>rules</b> can be defined and designed using any of various methodologies, and be deployed in any of various contexts.|$|E
50|$|Consistency is a {{very general}} term, which demands that the data must meet all <b>validation</b> <b>rules.</b> In the {{previous}} example, the validation is a requirement that A + B = 100. All <b>validation</b> <b>rules</b> must be checked to ensure consistency. Assume that a transaction attempts to subtract 10 from A without altering B. Because consistency is checked after each transaction, {{it is known that}} A + B = 100 before the transaction begins. If the transaction removes 10 from A successfully, atomicity will be achieved. However, a validation check will show that A + B = 90, which is inconsistent with the rules of the database. The entire transaction must be cancelled and the affected rows rolled back to their pre-transaction state. If there had been other constraints, triggers, or cascades, every single change operation would have been checked {{in the same way as}} above before the transaction was committed.|$|E
50|$|The {{range of}} data values or data quality in an {{operational}} system may exceed {{the expectations of}} designers at the time validation and transformation rules are specified. Data profiling of a source during data analysis can identify the data conditions that must be managed by transform rules specifications, leading to an amendment of <b>validation</b> <b>rules</b> explicitly and implicitly implemented in the ETL process.|$|E
50|$|The {{small size}} and {{simplicity}} of the example allow simple <b>validation</b> of the <b>rules</b> of interpretation. But the method will be more valuable when the data set is large and complex.Other methods suitable {{for this type of}} data are available. Procrustes analysis is compared to the MFA in.|$|R
40|$|Abstract. Due {{to their}} simple and {{intuitive}} manner rules {{are often used}} {{for the implementation of}} intelligent systems. Besides general methods for the verification and <b>validation</b> of <b>rule</b> systems there exists only little research on the evaluation of their robustness with respect to faulty user inputs or partially incorrect rules. This paper introduces a gray box approach for testing the robustness of rule systems, thus including a preceding analysis of the utilized inputs and the application of background knowledge. The practicability of the approach is demonstrated by a case study. ...|$|R
40|$|ITC/USA 2015 Conference Proceedings / The Fifty-First Annual International Telemetering Conference and Technical Exhibition / October 26 - 29, 2015 / Bally's Hotel & Convention Center, Las Vegas, NVThis paper {{describes}} a method in which users realize {{the benefits of}} a standards-based method for capturing and evaluating verification and <b>validation</b> (V&V) <b>rules</b> within and across metadata instance documents. The method uses a natural language based syntax for the T&E metadata V&V rule set in order to abstract the highly technical rule languages to a domain-specific syntax. As a result, the domain expert can easily specify, validate and manage the specification and <b>validation</b> of the <b>rules</b> themselves. Our approach is very flexible in that under the hood, the method automatically translates rules to a host of target rule languages. We validated our method in a multi-vendor scenario involving Metadata Description Language (MDL) and Instrumentation Hardware Abstraction Language (IHAL) instance documents, user constraints, and domain constraints. The rules are captured in natural language, and used to perform V&V within a single metadata instance document and across multiple metadata instance documents...|$|R
5000|$|This is {{a feature}} of iDempiere that extends {{the concept of a}} data {{dictionary}} to an [...] "Active Data Dictionary" [...] that lets it manage entities, <b>validation</b> <b>rules,</b> windows, formats, and other customizations of the application without new JAVA code. So iDempiere can be seen not only as an ERP but also as a platform to build database driven applications.|$|E
50|$|In {{the context}} of {{software}} or information modeling, a happy path is a default scenario featuring no exceptional or error conditions, and comprises nothing if everything goes as expected. For example, the happy path for a function validating credit card numbers would be where none of the <b>validation</b> <b>rules</b> raise an error, thus letting execution continue successfully to the end, generating a positive response.|$|E
50|$|Plessey also {{pioneered the}} {{gathering}} and consolidation of accounting information {{from around the}} world using in-house software. Each of their 140 management reporting entities used HP125s with DIVAT (data input, validation and transmission) software. Nearly 450 <b>validation</b> <b>rules</b> ensured accuracy within and between various reports.The data were then transmitted to Ilford where a HP3000 used Fortran software for consolidation and reporting - also on HP125s.|$|E
40|$|Expert {{systems and}} {{deductive}} database systems are knowledge-based systems {{which have the}} capability of stor-ing andproeessing datartndknowledge rules, and performing logical deductions. The knowledge bases of these systems are usually assumed to be eonsisten ~ that is, no contradic-tions are deduced by the system. This assumption is not realistic since, in real-world applications, a knowledge base can contain {{a large number of}} rules which pose problems in terms of their ecmsistency. Thus, an automatic knowledge validation procedure is necessary for building a reliable knowledge-based system. In this paper, we present a knowl-edge validation technique based on the resolution principle to detect inconsistencies of a knowledge base. In this work, we formally define the eoneept of rule base inconsistency and show its relationship with the concept of unsatisfiability in formal logic. We also define completeness of a rule val-idation algorithm and show that our <b>rule</b> <b>validation</b> method is complete {{in the sense that it}} can not only identify all the input facts casing the system to deduee contradictions but also determine the specific subset of rules involved in the deductions of the contradictions. Based on this information, knowledge base designers can then make proper corrections to their knowledge base designs. A <b>rule</b> <b>validation</b> system using the proposed technique has been implemented in Prolog. Key Words: knowledge-based systems, knowledge base design, logical deduction, resolution principle and theorem proving, <b>rule</b> <b>validation.</b> ...|$|R
40|$|This {{document}} contains {{both the}} <b>validation</b> and business <b>rules,</b> which will {{form part of}} the validation checks that {{will need to be}} undertaken by third party software suppliers to support CIS Agent Authorisations. 11 / 10 / 2006 Page 3 Version 1. 0 HM Revenue & Customs Business Rules for Agent Authorisation (CIS...|$|R
40|$|Within the {{questionnaire}} design process {{there is a}} great deal of information that is used to realise the goal of producing a questionnaire and subsequently discarded. This data includes structural, contextual and semantic information as well as <b>validation</b> and navigation <b>rules.</b> However, much of this data can be used as metadata further on in th...|$|R
