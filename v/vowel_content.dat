2|5|Public
5000|$|White {{noise is}} created vocally through the initial {{consonant}} [...] "sh" [...] and variations such as s, z, wh, p, t, h, ct, d, ch, th, k, and sw. Ring modulation is imitated through rapidly changing the <b>vowel</b> <b>content,</b> percussive envelopes through [...] "lip pops," [...] "tongue clicks," [...] "snap fingers," [...] and [...] "flutter lips", and filtering through muting such as by covering the mouth with one's hand, sounding the consonant [...] "M" [...] through tightly closed lips, or singing through clenched teeth.|$|E
40|$|Intonation can be {{perceived}} in whispered speech despite {{the absence of}} the fundamental frequency. In the past, acoustic correlates of pitch in whisper have been sought in <b>vowel</b> <b>content,</b> but, recently, studies of normal speech demonstrated correlates of intonation in consonants as well. This study examined how consonants may contribute to the coding of intonation in whispered relative to normal speech. The acoustic characteristics of whispered, voiceless fricatives /s/ and /f/, produced at different pitch targets (low, mid, high), were investigated and compared to corresponding normal speech productions to assess if whisper contained secondary or compensatory pitch correlates. Furthermore, listener sensitivity to fricative cues to pitch in whisper was established, also relative to normal speech. Consistent with recent studies, acoustic correlates of whispered and normal speech fricatives systematically varied with pitch target. Comparable findings across speech modes showed that acoustic correlates were secondary. Discrimination of vowel-fricative-vowel stimuli was less accurate and slower in whispered than normal speech, which is attributed to differences in acoustic cues available. Perception of fricatives presented without their vowel contexts, however, revealed comparable processing speeds and response accuracies between speech modes, supporting the finding that within fricatives, acoustic correlates of pitch are similar across speech modes...|$|E
40|$|The frame/content {{theory of}} {{evolution}} of speech production This target article {{is concerned with the}} evolution of speech production as action. The question is, how did we evolve the capacity to do what we do with the speech production apparatus when we speak? There will be little concern with the evolution of the conceptual structure that underlies speech actions. Instead, the focus will be on a capability typically taken for granted in current linguistic theory and cognitive science: How do we explain our remarkable capacity for making the serially organized complexes of movements that constitute speech? The basic thesis is quite simple. Human speech differs from vocal communication of other mammals in that we alone superimpose a continual rhythmic alternation between an open and closed mouth (a frame) on the sound production process. The likelihood that this cyclicity, associated with the syllable, evolved from ingestive cyclicities (e. g., chewing) is indicated by the fact that much of the new development of the brain for speech purposes occurred in and around Brocaâ€™s area, in a frontal perisylvian region basic to the control of ingestive movements in mammals. An evolutionary route from ingestive cyclicities to speech is suggested by the existence of a putative intermediate form present in many other higher primates, namely, visuofacial communicative cyclicities such as lipsmacks, tonguesmacks, and teeth chatters. The modification of the frontal perisylvian region leading to syllable production presumably made its other ingestion-related capabilities available for use in modulation of the basic cycle in the form of different consonants and <b>vowels</b> (<b>content).</b> Mor...|$|R
40|$|Computer {{assisted}} {{language learning}} (CAPT) {{has been shown}} to be effective for learning non-natives pronunciation details of a new language. No automatic pronunciation evaluation system exists for non-native Norwegian. We present initial experiments on the Norwegian quantity contrast between short and long vowels. A database of native and non-native speakers was recorded for training and test respectively. We have used a set of acoustic-phonetic features and combined them in a classifier based on linear discriminant analysis (LDA). The resulting classification rate was 92. 3 % compared with a human rating. As expected, vowel duration was the most important feature, whereas <b>vowel</b> spectral <b>content</b> contributed insignificantly. The achieved classification rate is promising with respect to making a useful Norwegian CAPT for quantity. 1...|$|R
40|$|Thesis (Ph. D.) [...] University of Washington, 2015 Stance [...] {{attitudes and}} {{opinions}} about the topic of discussion [...] has been investigated textually in conversation- and discourse analysis and in computational models, but little work has focused on its acoustic-phonetic properties. It is a challenging problem, given the complexity of stance and the many other types of meaning that must share the same acoustic channels, {{all of which are}} overlaid on the lexical and syntactic material of the message. With the goal of identifying automatically-extractable, acoustically-measurable correlates of stance-taking, this dissertation presents a new audio corpus of stance-dense interaction and three phonetic experiments which find signals of stance in prosodic measures of pitch, intensity, and duration. The ATAROS corpus contains pairs of speakers engaged in collaborative conversational tasks designed to elicit frequent changes in stance at varying levels of involvement. Interactions are transcribed, time-aligned to the audio, and manually annotated for stance strength (none, weak, moderate, strong), polarity (positive, negative, neutral), and stance type (e. g., opinion-offering and soliciting, (dis) agreement, persuasion, rapport-building, etc.). In the first experiment, combinations of pitch and intensity contours are shown to differentiate four discourse functions within a small sample of instances of the word `yeah' that contribute to negative stances. In the second experiment, vowel duration and intensity separate six common stance-act types in over 2200 `yeahs,' changes in pitch and intensity correlate with stance strength, and all three measures are involved in signaling positive stance. The third and largest experiment examines over 32, 000 stressed <b>vowels</b> in <b>content</b> words spoken by 40 speakers and finds that pitch and intensity increase with stance strength, longer vowel duration is the primary signal of positive polarity, and a combination of these measures helps distinguish several notable stance-act types, including: agreement in general, weak-positive agreement, rapport-building agreement, reluctance to accept a stance, stance-softening, and backchannels. These results, and the corpus itself, contribute to the study and understanding of the acoustic-phonetic properties of the social and attitudinal messages conveyed in natural speech, information which may be of use to future work in theoretical, experimental, and computational linguistics...|$|R
40|$|Pattern {{recognition}} algorithms {{are becoming}} increasingly used in functional neuroimaging. These algorithms exploit information contained in temporal, spatial, or spatio-temporal patterns of independent variables (features) to detect subtle but reliable differences between brain responses to external stimuli or internal brain states. When applied {{to the analysis of}} electroencephalography (EEG) or magnetoencephalography (MEG) data, a choice needs to be made on how the input features to the algorithm are obtained from the signal amplitudes measured at the various channels. In this article, we consider six types of pattern analyses deriving from the combination of three types of feature selection in the temporal domain (predefined windows, shifting window, whole trial) with two approaches to handle the channel dimension (channel wise, multi-channel). We combined these different types of analyses with a Gaussian Naive Bayes classifier and analyzed a multi-subject EEG data set from a study aimed at understanding the task dependence of the cortical mechanisms for encoding speaker's identity and speech <b>content</b> (<b>vowels)</b> from short speech utterances (Bonte, Valente, & Formisano, 2009). Outcomes of the analyses showed that different grouping of available features helps highlighting complementary (i. e. temporal, topographic) aspects of information content in the data. A shifting window/multi-channel approach proved especially valuable in tracing both the early build up of neural information reflecting speaker or vowel identity and the late and task-dependent maintenance of relevant information reflecting the performance of a working memory task. Because it exploits the high temporal resolution of EEG (and MEG), such a shifting window approach with sequential multi-channel classifications seems the most appropriate choice for tracing the temporal profile of neural information processing...|$|R
40|$|Language {{learning}} strategy (LLS) research {{has provided a}} large body of evidence for the effectiveness of strategy-based instruction (SBI), though the evidence is very limited for pronunciation strategy instruction. For both general and pronunciation LLSs, most research has focused on identifying the strategies used by successful learners. When strategy instruction has been investigated, in most cases the strategies that were taught were not directly linked to specific tasks, learners were not observed using the strategies, and measures of strategy effectiveness often were holistic and did not reveal improvements in specific pronunciation features. The goal {{of this study was to}} extend our understanding of the role of strategy use in L 2 (second language) pronunciation learning by investigating the effectiveness of training future international teaching assistants (ITAs) to critically listen to, transcribe, mark corrections (annotate), and orally rehearse English suprasegmental features in their own speech. The suprasegmental features investigated were message unit boundaries, primary phrase stress, intonation, <b>vowel</b> reduction in <b>content</b> and function words, linking, word stress, and multiword construction stress. Fifteen graduate-level learners of English (14 Mandarin speakers, 1 Korean speaker) from an intact English as a Second Language (ESL) pronunciation class at a Midwestern university were solicited to participate in a repeated-measures design, in which the independent variables were 3 levels of self-monitoring (listening only [L], listening + transcription [LT], and listening + transcription + annotation [LTA]) and rehearsal (R). The strategies were examined in the following combinations: LR-LR-LR, LT-RRR, and LTA-RRR. The dependent variable was the change in suprasegmental accuracy following self-monitoring and rehearsal. Speech data resulting from strategy use were gathered at the beginning and end of a 16 -week semester in order to determine the extent to which strategy use corresponded to improved suprasegmental accuracy. Key findings include the following: (a) All participants made meaningful improvements in suprasegmental accuracy for at least some of the targets following self-monitoring; (b) the LT-RRR combination was most effective for lower proficiency learners and LTA-RRR was most effective for higher proficiency learners; (c) starting proficiency and size of accuracy gains following self-monitoring were negatively correlated; (d) self-monitoring had differential effects on accuracy for the suprasegmental features, with message units, linking, and function words showing the greatest improvement;(e) and observation of individual task performance provided useful insights into how effectively adult L 2 learners utilize self-monitoring strategies. Implications for language teaching and learning, limitations of the study, and future research opportunities are explored...|$|R

