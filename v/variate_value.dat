1|23|Public
40|$|We {{introduce}} {{a new form of}} regression that has many applications to environmental studies. For a sequence composed of key variates with prototypic value chi, this form differs from the estimation of a location parameter-based curve, mu(chi), a scale parameter-based curve, sigma(chi), or other currently used types of regression. Instead of estimating a curve location, scale, or alpha-quantile parameter, it assumes that there are two or more population subgroups; for example, consisting of unsensitized and sensitized individuals, respectively. Although within each subgroup the relationships mu(chi) or sigma(chi) {{may or may not be}} horizontal, these relationships are not deemed to be of primary importance. Instead, the mixing parameter P that indexes the proportions of the two subgroups is treated as being related to the key <b>variate</b> <b>value</b> chi. In the sense that its goal is the estimation of a proportion, the new procedure resembles logit regression. But, in terms of the continuous spectrum of values attained by the response variate, the means used to attain its goal are dissimilar from those of logit regression. Specifically, group membership is not known directly but is determined from a proxy continuous variate whose values overlap between groups. Examples are given with simulated and natural data where this new form of regression is applied. We believe that conditional switching regression is a particularly valuable research tool when chemical level chi of an induced asthma attack or birthweight chi measured in a study of the biomarker cotinine's effect on pregnancy outcomes determines whether an attack or a negative outcome occurs. (ABSTRACT TRUNCATED AT 250 WORDS...|$|E
25|$|This is {{an analog}} {{of the mean}} {{difference}} - {{the average of the}} differences of all the possible pairs of <b>variate</b> <b>values,</b> taken regardless of sign. The mean difference differs from the mean and standard deviation because it is dependent on the spread of the <b>variate</b> <b>values</b> among themselves and not on the deviations from some central value.|$|R
40|$|Graduation date: 1981 The {{problem of}} {{predicting}} <b>variate</b> <b>values</b> for all individual units in a finite population {{based on a}} sample {{of some of the}} units is Investigated. Two prediction problems are considered: the one-stage prediction problem where the unit <b>variate</b> <b>values</b> are directly observable for each sample unit, and the two-stage prediction problem where the sample units are subsampled in order to estimate the <b>variate</b> <b>values</b> for these units. A superpopulation model-based approach is suggested to both of these problems. In both cases, a model framework is developed, predictors given for the unit <b>variate</b> <b>values,</b> and the error variances of these predictors are derived. An important aspect in the sample design of these prediction problems is the allocation of the sample. An algorithm Is developed for selecting specific units to be sampled. This selection procedure is used for the one-stage problem to derive a method for determining the number of units to sample to achieve a prespecified precision on the predictors. For the two-stage problem, a method is developed for selecting both the number of units and subunits to achieve a fixed sample size while minimizing the error variances of the predictors. An example is given that illustrates some of the theoretical results. Finally, the applicability of this approach is discussed and areas for further investigation are indicated...|$|R
40|$|Modelling {{discrete}} variate time series {{is the most}} challenging and, as yet, least well developed of all areas of research in time series. The fact that <b>variate</b> <b>values</b> are integer renders most traditional representations of dependence either impossible or impractical. In {{the last two decades}} {{there have been a number}} of imaginative attempts to develop...|$|R
40|$|Amira and Mazloum (1993) studied  the Complete Fourth Power Exponential  (CFPE) {{distribution}} and its geometric and statistical properties against the normal distribution.   Maturi and Elsayigh (2009) investigated {{the relationship between}} <b>variate</b> <b>values</b> and ranks in samples   from the CFPE distribution. In this paper a bivariate version of the CFPE  distribution (BCFPE) is introduced {{and some of its}} properties are explored. <br /...|$|R
40|$|In this paper, a {{new class}} of Length-biased beta {{distribution}} of first kind is introduced. A Lengthbiased beta distribution of first kind is a particular case of the weighted beta distribution of first kind, taking the weights as the <b>variate</b> <b>values</b> has been defined. The characterizing properties of the model are derived. The estimates of parameters of Length-biased beta distribution of first kind are obtained by using method of moments. Also, a test for detecting the length-baisedness is conducted...|$|R
40|$|In this paper, we {{introduce}} {{and study}} the size-biased form of Kumaraswamy distribution. The Kumaraswamy distribution which has drawn considerable attention in hydrology and related areas was proposed by Kumarswamy. The new distribution is derived under sizebiased probability of sampling taking the weights as the <b>variate</b> <b>values.</b> Various distributional and characterizing properties of the model are studied. The methods of maximum likelihood and matching quantiles estimation are employed to estimate {{the parameters of the}} proposed model. Finally, we apply the proposed model to simulated and real data sets. </p...|$|R
40|$|Overfitting {{arises when}} model {{components}} are evaluated against the wrong reference distribution. Most modeling algorithms iteratively {{find the best}} of several components and then test whether this component {{is good enough to}} add to the model. We show that for independently distributed random variables, the reference distribution for any one variable underestimates the reference distribution for the the highest-valued variable # thus <b>variate</b> <b>values</b> will appear significant when they are not, and model components will be added when they should not be added. We relate this problem to the well-known statistical theory of multiple comparisons or simultaneous inference...|$|R
40|$|In {{this paper}} we derive {{the effect of}} {{aggregation}} on common statistics when the geographic areas used in the analysis are equivalent to randomly formed groups of individuals. Simple rules of aggregation are provided for use when analysis of such groups is performed. The expectations of common statistics such as means, variances, regression and correlation coefficients, are not affected by aggregation. However, the variation of these statistics is affected, mainly {{as a result of}} changes in the number of groups. This variation is related solely to random fluctuations associated with the generation of <b>variate</b> <b>values.</b> Weighting by the group population sizes is shown to be important in the calculation of statistics. Generally, unweighted statistics have larger variation than the corresponding weighted version, and the variation depends not only on the number of groups but also on the distribution of group population sizes. Methods for conducting statistical analysis of aggregate data in this situation are described and statistical inferences based on unweighted statistics are shown to be invalid. ...|$|R
40|$|Introduction: Innumerably many {{textbooks}} in Statistics explicitly {{mention that}} one of the weaknesses (or properties) of median (a well known measure of central tendency) {{is that it is not}} computed by incorporating all sample observations. That is so because if the sample 1 2 (,, [...] .,) n x x x x =, where the <b>variate</b> <b>values</b> are ordered such that 1 2 [...] . n x x x # # # then 1 () () / 2; int((1) / 2). k n k median x x x k n + - = + = + Here int(.) is the integer value of (.). For example int(10 # (n+ 1) / 2 < 11) = 10. This formula, although queer and expressed in a little roundabout way, applies uniformly when n is odd or even. Evidently, () median x is not obtained by incorporating all the values of x, and so the alleged weakness of the median as a measure of central tendency. 2. The Median Minimizes the Absolute Norm of Deviations: It is a commonplace knowledge in Statistics that the statistic x (the arithmetic mean of x) minimizes the (squared) Euclidean norm of deviations o...|$|R
40|$|The paper {{demonstrates}} the interchangeability of the ratio and product methods of estimation in sample surveys through translating the unbiased estimator {{of the population}} total of the auxiliary variate (or the study <b>variate).</b> The <b>values</b> of the translation parameters minimizing the mean squared error are obtained. The allowable departures from this optimum, which still ensure {{a reduction in the}} mean squared error, as compared to the traditional case, are indicated. Â© 1983, Taylor & Francis Group, LLC. All rights reserved...|$|R
40|$|Since {{the widely}} using of the {{weighted}} distribution in many fields {{of real life}} such various areas including medicine, ecology, reliability, and so on, then we try to shed light and record our contribution in this field thru the research. In this paper, {{a new class of}} size-biased Generalized Rayleigh distribution is defined. A size-biased Generalized Rayleigh distribution, a particular case of weighted Generalized Rayleigh distribution, taking the weights as the <b>variate</b> <b>values</b> has been defined. The power and logarithmic moments of this family is defined. Some important theorems of SBGRD has been derived and studied. A new moment estimation method of parameters of SBGRD using its characterization is presented. In brief, this paper consists of presentation of general review of important properties of the new distribution. Bayes estimators of Size biased Generalized Rayleigh distribution (SBGRD), that stems from an extension of Jeffery’s prior (Al-Kutubi [13]) with a new loss function (Al-Bayyati [12]). We are proposing four different types of estimator. Under squared error loss function, there are two estimators formed by using Jaffrey prior and an extension of Jaffrey’s prior. The two remaining estimators are derived using the same Jeffrey’s prior and extension of Jeffrey’s prior under a new loss function. These methods are compared by using mean square error through simulation study with varying sample sizes...|$|R
40|$|We present novel general, simple, exact, and closed-form {{expressions}} for {{the probability}} density function (PDF) and {{cumulative distribution function}} (CDF) of {{the ratio of the}} products of two independent α-μ variates, where all <b>variates</b> have identical <b>values</b> of alpha parameter. Obtained results are applied in analysis of multihop wireless communication systems in different fading transmission environments. The proposed theoretical analysis is also complemented by various graphically presented numerical results...|$|R
40|$|This paper {{proposes a}} robust {{algorithm}} to detect colon polyps and cancerous lesions in virtual colonoscopy and present {{them to the}} user by automatically guiding the virtual camera. The detection algorithm uses Gaussian filters to construct the Hessian matrix, which represents the second order derivatives of a vector <b>variate</b> scalar <b>valued</b> function. Based on the sign and scale of the eigenvalues of the Hessian matrix, blob like lesions can be selected on a given scale. In the visualization stage the camera is moved along the colon centerline with its speed and viewing direction adopted {{to the results of}} detection. The camera path and the viewing direction are described by Kochanek-Bartels splines. The velocity along the path is also governed by a C² continuous function. The resulting fly through is smooth and physically plausible, and it is guaranteed that the user can see all regions of interest and spends sufficient time looking at each of them...|$|R
40|$|We {{consider}} {{the issue of}} performing accurate small-sample testing inference in beta regression models, which are useful for modeling continuous <b>variates</b> that assume <b>values</b> in $(0, 1) $, such as rates and proportions. We derive the Bartlett correction to the likelihood ratio test statistic and also consider a bootstrap Bartlett correction. Using Monte Carlo simulations we compare the finite sample performances of the two corrected tests {{to that of the}} standard likelihood ratio test and also to its variant that employs Skovgaard's adjustment; the latter is already available in the literature. The numerical evidence favors the corrected tests we propose. We also present an empirical application. Comment: 27 pages, 5 tables, 3 figure...|$|R
40|$|IT {{sometimes}} {{happens that}} correlational analysis {{can best be}} handled by rank methods, since the <b>variate</b> <b>values</b> are expressed as ranks. In other cases, rank methods may be pre-ferred by the investigator because of peculiarities in the bi-variate distributions involved. In such cases the Spearman Rho Coefficient has been used quite commonly despite some difficulties with tests of significance. The Kendall Tau Coeffi-cient possesses a frequency distribution which is easy to calcu-late for small n and which approaches normality quite rapidly (i). However, the computation of the tau coefficient from its definition is tedious. A method which {{can be carried out}} easily and quickly for small numbers of cases, say fifty or less, is, therefore, set forth in this paper. The tau coefficient is quite simply defined. Assume ranks assigned to a set of individuals on two tests as follows: Consider each possible pair of ranks in the A array. Assign a plus one to every pair which is in normal order and a minus one to each pair which is in inverted order. Do the same for the B array. Then for corresponding pairs of the two arrays, the plus or minus one values are multiplied together and the products are summed. The result, in this case 7, is denoted by S. The total number of pairs in each array must be the combi-nations of the n ranks taken two at a time, in this case (6) _ at PENNSYLVANIA STATE UNIV on May 12, 2016 epm. sagepub. comDownloaded from 70 IHAROLD F. BRIGHT I S. Then the tau coefficient is defined as Kendall points out that an easier procedure is to rank one array in normal order so that the two arrays previously given would appear as follows: Then since array A can contribute only positive pairs to the value of S, it is only necessary to consider the contributions of the pairs in array B. It is clear that S- P &horbar; ~ and (&dquo;) ~= P + ~ where P is the number of positive pairs in B, and ~ is the number of negative pairs. A simple algebraic transforma-tion then gives the result It is evident that the ~ score can be arrived at by counting the number of transpositions necessary to put array B into normal order. In the present example the array is in the order 2 4 1 5 3 6. It takes two transpositions to put the i in its proper initial position so that the array become...|$|R
40|$|Importance {{sampling}} is {{a widely}} applied method for variance reduction in simulations. However, a naive application of importance sampling {{can result in a}} great increase in variance. This article surveys and extends some recently developed methods that eliminate this problem: defensive mixture sampling of Hesterberg (1995) and multiple importance sampling of Veach (1997). The method we propose is to importance sample from a mixture density using as control variates the ratios of mixture components to the mixture density. The resulting method is never much worse than importance sampling from the best of the mixture components, and can be much better. Finally, {{it is well known that}} importance sampling can be nearly perfect for nonnegative integrands. We show that this is also true for integrands taking both positive and negative values, using a modification of multiple importance sampling. KEY WORDS: bidirectional path sampling, control <b>variates,</b> Monte Carlo, <b>value</b> at risk, variance redu [...] ...|$|R
40|$|Isotopic dating {{is subject}} to {{uncertainties}} arising from counting statistics and experimental errors. These uncertainties are additive when an isotopic age difference is calculated. If large, they can lead to no significant age difference by “classical” statistics. In many cases, relative ages are known because of stratigraphic order or other clues. Such information {{can be used to}} establish a Bayes estimate of age difference which will include prior knowledge of age order. Age measurement errors are assumed to be log-normal and a noninformative but constrained bivariate prior for two true ages in known order is adopted. True-age ratio is distributed as a truncated log-normal <b>variate.</b> Its expected <b>value</b> gives an age-ratio estimate, and its variance provides credible intervals. Bayesian estimates of ages are different and in correct order even if measured ages are identical or reversed in order. For example, age measurements on two samples might both yield 100 ka with coefficients of variation of 0. 2. Bayesian estimates are 22. 7 ka for age difference with a 75 % credible interval of [4. 4, 43. 7] ka...|$|R
30|$|RDH [10, 11, 12], {{intentions}} to inscribe secret information and slightly <b>variates</b> pixel <b>value</b> of original media to entirely recover from marked image. RDH [10, 11, 13, 14, 15] has been distributed into three realms, i.e., spatial domain, frequency domain and compressed domain [16]. The spatial domain has been further categorized into LSB substitution [13], histogram shifting (HS) [11, 17, 18, 19, 20], and difference expansion (DE) [10, 13, 14, 16, 21, 22]. In 2003, Tian et al. [10] premeditated the variance among two neighbor pair of pixels to embed the information. Thodi and Rodriguez [16] adapted an expansion technique based on error prediction to envisage the pixel value. In 2006, Ni et al. [17] presented the histogram shifting method {{to get high}} PSNR, but low EC. In 2009, Tai et al. [19] proposed the binary tree structure to discover the peak point {{that is used to}} write data. Kim et al. [11] employed center pixel as the reference pixel to reckon the difference of sub-image conversely, but center pixel cannot be used to hide the information. Luo et al. [13] enhanced Kim’s technique, but the reference pixel does not hold the data. In 2010, Li et al. [18] designed the variation among neighbor pixel based on HS. In 2011, Hong and Chen [23] divided the host image into two blocks, but data write in only the smooth block region. In 2013, Huang et al. [24] enhanced EC to amend Hong and Chen techniques. Lin et al. [25] planned multilevel hiding procedure to obtain greater EC and better PSNR. Tsai et al. [26] combined HS and predictive coding (PC) to examine the elementary pixel in every block of residual image, but needed more space to record extra informations.|$|R
40|$|Phonon Monte Carlo (PMC) is a {{versatile}} stochasic technique for solving the Boltzmann transport equation for phonons. It is particularly {{well suited for}} analyzing thermal transport in structures that have real-space roughness or are too large to simulate directly using atomistic techniques. PMC hinges on the generation and use of random <b>variates</b> [...] specific <b>values</b> of the random variables that correspond to physical observables [...] {{in a way that}} accurately and efficiently captures the appropriate distribution functions. We present the relative merits of the inversion and rejection techniques for generating random variates on several examples relevant in thermal transport: drawing phonons from a thermal distribution and with full or isotropic dispersion, randomizing outgoing momentum upon diffuse boundary scattering, implementing contacts (boundary and internal), and conserving energy in the simulation. We also identify common themes in phonon generation and scattering that are helpful for reusing code in the simulation (generating thermal-phonon attributes vs internal contacts; diffuse surface scattering vs boundary contacts). We hope these examples will inform the reader about the mechanics of random-variate generation and how to choose a good approach for whatever problem is at hand, and aid in the more widespread use of PMC for thermal transport simulation. Comment: This is a book chapter to appear in "Nanophononics," ed. Z. Aksamija, Pan Stanford Publishing, 201...|$|R
40|$|Abstract: Genetic {{diversity}} {{is very important}} factor for any hybridization program aiming at genetic improvement of yield especially in self pollinated crops. It is widely accepted that information about germplasm diversity and genetic relatedness among elite breeding material is a fundamental element in plant breeding. Genetic variation was evaluated in twenty five grasspea genotypes for thirteen morphological traits through analysis of variance and multivariate analysis. The analysis of variance for all the characters revealed highly significant variation among the genotypes of grasspea except for days to 50 % fruiting. BD 3618, BD 3620 and BD 3621 were found as superior {{on the basis of}} the mean performance of desirable traits. The genotypes were grouped into five clusters through principal component analysis (PCA). A total of 84. 58 % variation was observed from PCA against first thirteen eigen values while only first two values were responsible for 53. 25 % variation. The inter cluster distance was maximum (17. 038) between the cluster V and III and the lowest (2. 6) distance between IV and I. Considering cluster mean, the genotypes of cluster III might be selected as a suitable parent for future hybridization programme. The highest inter genotypic distance was observed between BD 3621 -BD 3686 that have separate cluster. Contribution of characters towards divergence obtained from canonical <b>variate</b> analysis. The <b>value</b> of Vector I and Vector II revealed that both Vectors had positive values for Plant height, Pods/Plant and Seed Yield / Plant indicating the highest contribution of these traits towards the divergence among 25 genotypes of grasspea genotypes. Key words: Genetic diversity Grasspea Multivariate analysi...|$|R
40|$|This thesis studies two Markov {{processes}} {{describing the}} evolution of a system of many interacting random components. These processes undergo an absorbing-state phase transition, i. e., as one <b>variates</b> the parameter <b>values,</b> the process exhibits a transition from a convergence regime to one of the absorbing-states to an active regime. In Chapter 2 we study Activated Random Walk, which is an interacting particle system where the particles can be of two types and their number is conserved. Firstly, we provide a new lower bound for the critical density on Z {{as a function of the}} jump distribution and of the sleeping rate and we prove that the critical density is not a constant function of the jump distribution. Secondly, we prove that on Zd in the case of biased jump distribution the critical density is strictly less than one, provided that the sleeping rate is small enough. This answers a question that has been asked by Dickman, Rolla, Sidoravicius [9, 28] in the case of biased jump distribution. Our results have been presented in [33]. In Chapter 3 we study a class of probabilistic cellular automata which are related by a natural coupling to a special type of oriented percolation model. Firstly, we consider the process on a finite torus of size n, which is ergodic for any parameter value. By employing dynamic-renormalization techniques, we prove that the average absorption time grows exponentially (resp. logarithmically) with n when the model on Z is in the active (resp. absorbing) regime. This answers a question that has been asked by Toom [37]. Secondly, we study how the neighbourhood of the model affects the critical probability for the process on Z. We provide a lower bound for the critical probability as a function of the neighbourhood and we show that our estimates are sharp by comparing them with our numerical estimates. Our results have been presented in [34, 35]...|$|R
40|$|Abstract Background Infection of the CNS is {{considered}} to be the major cause of encephalitis and more than 100 different pathogens have been recognized as causative agents. Despite being identified worldwide as an important public health concern, studies on encephalitis are very few and often focus on particular types (with respect to causative agents) of encephalitis (e. g. West Nile, Japanese, etc.). Moreover, a number of other infectious and non-infectious conditions present with similar symptoms, and distinguishing encephalitis from other disguising conditions continues to a challenging task. Methods We used canonical correlation analysis (CCA) to assess associations between set of exposure variable and set of symptom and diagnostic variables in human encephalitis. Data consists of 208 confirmed cases of encephalitis from a prospective multicenter study conducted in the United Kingdom. We used a covariance matrix based on Gini's measure of similarity and used permutation based approaches to test significance of canonical variates. Results Results show that weak pair-wise correlation exists between the risk factor (exposure and demographic) and symptom/laboratory variables. However, the first canonical variate from CCA revealed strong multivariate correlation (ρ = 0. 71, se = 0. 03, p = 0. 013) between the two sets. We found a moderate correlation (ρ = 0. 54, se = 0. 02) between the variables in the second canonical <b>variate,</b> however, the <b>value</b> is not statistically significant (p = 0. 68). Our results also show that a very small amount of the variation in the symptom sets is explained by the exposure variables. This indicates that host factors, rather than environmental factors might be important towards understanding the etiology of encephalitis and facilitate early diagnosis and treatment of encephalitis patients. Conclusions There is no standard laboratory diagnostic strategy for investigation of encephalitis and even experienced physicians are often uncertain about the cause, appropriate therapy and prognosis of encephalitis. Exploration of human encephalitis data using advanced multivariate statistical modelling approaches that can capture the inherent complexity in the data is, therefore, crucial in understanding the causes of human encephalitis. Moreover, application of multivariate exploratory techniques will generate clinically important hypotheses and offer useful insight into the number and nature of variables worthy of further consideration in a confirmatory statistical analysis. </p...|$|R

