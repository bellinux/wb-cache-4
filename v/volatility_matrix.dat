62|20|Public
40|$|This {{document}} {{describes how}} to create a volatility cube object. The volatility cube object is an object that takes as input a yield curve, cap <b>volatility</b> <b>matrix,</b> swaption <b>volatility</b> <b>matrix,</b> and, possibly, eurodollar future option (EDFO) prices, and is able to compute a swaption volatility for any give...|$|E
3000|$|... +), we {{suppose that}} a(t,X(t))=σ(t,X(t))σ(t,X(t))⊤ with a Lipschitz {{continuous}} <b>volatility</b> <b>matrix</b> σ such that a [...]...|$|E
3000|$|..., Theorem 20) In case of (FTVP), we {{suppose that}} a(t,X(t))=σ(t,X(t))σ(t,X(t))⊤ with a Lipschitz {{continuous}} <b>volatility</b> <b>matrix</b> σ.|$|E
40|$|The Wishart Autoregressive (WAR) {{process is}} a multivariate process of {{stochastic}} positive definite matrices. The WAR is proposed in this paper as a dynamic model for stochastic <b>volatility</b> <b>matrices.</b> It yields simple nonlinear forecasts at any horizon and has factor representation, which separates white noise directions from those that contain all information about the past. For illustration, the WAR is applied to a sequence of intraday realized <b>volatility</b> covolatility <b>matrices.</b> Stochastic <b>Volatility,</b> Car Process, Factor Analysis, Reduced Rank, Realized Volatility...|$|R
40|$|The Wishart Autoregressive (WAR) {{process is}} a dynamic model for time series of multivariate {{stochastic}} volatility. The WAR naturally accommodates the positivity and symmetry of <b>volatility</b> <b>matrices</b> and provides closed-form non-linear forecasts. The estimation of the WAR is straighforward, as it relies on standard methods such as the Method of Moments and Maximum Likelihood. For illustration, the WAR is applied to a sequence of intraday realized volatility-covolatility matrices from the Toronto Stock MarketÂ (TSX). Stochastic volatility Car process Autoregressive gamma process Factor analysis Reduced rank Realized volatility...|$|R
40|$|In this paper, {{we study}} the {{asymptotic}} {{behavior of the}} sequential empirical process and the sequential empirical copula process, both constructed from residuals of multivariate stochastic volatility models. Applications {{for the detection of}} structural changes and specification tests of the distribution of innovations are discussed. It is also shown that if the stochastic <b>volatility</b> <b>matrices</b> are diagonal, which is the case if the univariate time series are estimated separately instead of being jointly estimated, then the empirical copula process behaves as if the innovations were observed; a remarkable property. As a by-product, one also obtains the asymptotic behavior of rank-based measures of dependence applied to residuals of these time series models...|$|R
40|$|Large matrix {{estimation}} gains {{an increasing}} attention in recent years. This paper investigates the high dimensional statistical problem where a p-dimensional diffusion process is observed with measurement errors at n distinct time points, and {{our goal is}} to estimate the <b>volatility</b> <b>matrix</b> of the diffusion process. We establish the minimax theory for estimating large sparse volatility matrices under matrix spectral norm as both n and p go to infinity. The theory shows that the optimal convergence rate depends on n and p through n− 1 / 4 √ log p and a <b>volatility</b> <b>matrix</b> estimator is explicitly constructed to achieve the optimal convergence rate. Key words and phrases: Diffusion process, large matrix estimation, measurement error, minimax lower bound, optimal convergence rate, small n and large p, thresholding, <b>volatility</b> <b>matrix</b> estimator...|$|E
3000|$|..., is a n×m <b>volatility</b> <b>matrix</b> and Λ is a n×m×k tensor. The tensor product, Λ√(Y_t), {{contracts}} {{the resulting}} tensor {{so that the}} last dimension is dropped.|$|E
40|$|We {{propose a}} locally {{stationary}} linear {{model for the}} evolution of high-dimensional financial returns, where the time-varying <b>volatility</b> <b>matrix</b> is modelled as a piecewise constant function of time. We introduce a new wavelet-based technique for estimating the <b>volatility</b> <b>matrix,</b> which 10 combines four ingredients: a Haar wavelet decomposition, variance stabilization of the Haar coefficients via the Fisz transform prior to thresholding, a bias correction, and extra time-domain thresholding, soft or hard. Under the assumption of sparsity, we demonstrate the interval-wise consistency of the proposed estimators of the <b>volatility</b> <b>matrix</b> and its inverse in the operator norm, with rates which adapt to the features of the target matrix. We also propose a version of 15 the estimators based on the polarization identity, which permits a more precise derivation of the thresholds. We discuss the practicalities of the algorithm, including parameter selection and how to perform it online. A simulation study shows the benefits of the method, which is illustrated using a stock index portfolio...|$|E
3000|$|... are {{diagonal}} <b>matrices.</b> <b>Volatility</b> {{is modeled}} without {{interaction between the}} assets to simplify the model.|$|R
40|$|This paper {{examines}} {{the dynamics of}} the asymptotic long rate in three classes of term structure models. It shows that, in a frictionless and arbitrage-free market, the asymptotic long rate is a non-decreasing process. This gives an alternative proof of the same result of Dybvig et al. (Dybvig, P. H., Ingersol, Jr., J. E., Ross, S. A., 1996. Journal of Business 69, 1 - 25). It proves that the asymptotic long rate in factor models with state variables having non-singular diffusion <b>volatility</b> <b>matrices</b> is a deterministic function of time t. This paper also discusses a class of models in which bond prices have closed-form formulas and the asymptotic long rate is a constant. (C) 1999 Elsevier Science B. V. All rights reserved...|$|R
40|$|In this paper, {{we apply}} tools from the random matrix theory (RMT) to {{estimates}} of correlations across volatility of various {{assets in the}} S&P 500. The volatility inputs are estimated by modeling price fluctuations as GARCH(1, 1) process. The corresponding correlation matrix is constructed. It is found that the distribution of {{a significant number of}} eigenvalues of the <b>volatility</b> correlation <b>matrix</b> matches with the analytical result from the RMT. Furthermore, the empirical estimates of short and long-range correlations among eigenvalues, which are within the RMT bounds, match with the analytical results for Gaussian Orthogonal ensemble (GOE) of the RMT. To understand the information content of the largest eigenvectors, we estimate the contribution of GICS industry groups in each eigenvector. In comparison with eigenvectors of correlation matrix for price fluctuations, only few of the largest eigenvectors of <b>volatility</b> correlation <b>matrix</b> are dominated by a single industry group. We also study correlations among `volatility return' and get similar results. Comment: 17 pages, 14 figure...|$|R
40|$|An {{aggregated}} {{method of}} nonparametric estimators based on time-domain and state-domain estimators is proposed and studied. To attenuate {{the curse of}} dimensionality, we propose a factor modeling strategy. We first investigate the asymptotic behavior of nonparametric estimators of the <b>volatility</b> <b>matrix</b> in the time domain and in the state domain. Asymptotic normality is separately established for nonparametric estimators in the time domain and state domain. These two estimators are asymptotically independent. Hence, they can be combined, through a dynamic weighting scheme, to improve the efficiency of <b>volatility</b> <b>matrix</b> estimation. The optimal dynamic weights are derived, and it is shown that the aggregated estimator uniformly dominates <b>volatility</b> <b>matrix</b> estimators using time-domain or state-domain smoothing alone. A simulation study, based on an essentially affine model for the term structure, is conducted, and it demonstrates convincingly that the newly proposed procedure outperforms both time- and state-domain estimators. Empirical studies further endorse the advantages of our aggregated method. Comment: 46 pages, 11 PostScript figure...|$|E
30|$|We {{consider}} a strictly pathwise setting for Delta hedging exotic options, based on Föllmer’s pathwise Itô calculus. Price trajectories are d-dimensional continuous functions whose pathwise quadratic variations and covariations {{are determined by}} a given local <b>volatility</b> <b>matrix.</b> The existence of Delta hedging strategies in this pathwise setting is established via existence results for recursive schemes of parabolic Cauchy problems and via the existence of functional Cauchy problems on path space. Our main results establish the nonexistence of pathwise arbitrage opportunities in classes of strategies containing these Delta hedging strategies and under relatively mild conditions on the local <b>volatility</b> <b>matrix.</b>|$|E
40|$|Portfolio {{allocation}} with gross-exposure constraint is {{an effective}} method {{to increase the efficiency}} and stability of portfolios selection among a vast pool of assets, as demonstrated by Fan, Zhang, and Yu. The required high-dimensional <b>volatility</b> <b>matrix</b> can be estimated by using high-frequency financial data. This enables us to better adapt to the local volatilities and local correlations among a vast number of assets and to increase significantly the sample size for estimating the <b>volatility</b> <b>matrix.</b> This article studies the <b>volatility</b> <b>matrix</b> estimation using high-dimensional, high-frequency data from the perspective of portfolio selection. Specifically, we propose the use of "pairwise-refresh time" and "all-refresh time" methods based on the concept of "refresh time" proposed by Barndorff-Nielsen, Hansen, Lunde, and Shephard for the estimation of vast covariance matrix and compare their merits in the portfolio selection. We establish the concentration inequalities of the estimates, which guarantee desirable properties of the estimated <b>volatility</b> <b>matrix</b> in vast asset allocation with gross-exposure constraints. Extensive numerical studies are made via carefully designed simulations. Comparing with the methods based on low-frequency daily data, our methods can capture the most recent trend of the time varying volatility and correlation, hence provide more accurate guidance for the portfolio allocation in the next time period. The advantage of using high-frequency data is significant in our simulation and empirical studies, which consist of 50 simulated assets and 30 constituent stocks of Dow Jones Industrial Average index...|$|E
40|$|We {{examine the}} issue of {{sensitivity}} with respect to model parameters for the problem of utility maximization from final wealth in an incomplete Samuelson model and mainly, but not exclusively, for utility functions of positive power-type. The method consists in moving the parameters through change of measure, which we call a weak perturbation, decoupling the usual wealth equation from the varying parameters. By rewriting the maximization problem {{in terms of a}} convex-analytical support function of a weakly-compact set, crucially leveraging on the work of Backhoff and Fontbona (SIFIN 2016), the previous formulation let us prove the Hadamard directional differentiability of the value function w. r. t. the drift and interest rate parameters, as well as for <b>volatility</b> <b>matrices</b> under a stability condition on their Kernel, and derive explicit expressions for the directional derivatives. We contrast our proposed weak perturbations against what we call strong perturbations, where the wealth equation is directly influenced by the changing parameters. Contrary to conventional wisdom, we find that both points of view generally yield different sensitivities unless e. g. if initial parameters and their perturbations are deterministic. Comment: Improves and highlights the discussion on weak vs strong perturbation...|$|R
40|$|We {{present a}} new matrix-logarithm {{model of the}} {{realized}} covariance matrix of stock returns. The model uses latent factors which are functions of both lagged volatility and returns. The model has several advantages: it is parsimonious; {{it does not require}} imposing parameter restrictions; and, it results in a positive-definite covariance matrix. We apply the model to the covariance matrix of size-sorted stock returns and find that two factors are sufficient to capture most of the dynamics. We also introduce a new method to track an index using our model of the realized <b>volatility</b> covariance <b>matrix.</b> Econometric and statistical methods; Financial markets...|$|R
40|$|A new multivariate {{stochastic}} volatility estimation {{procedure for}} financial time series is proposed. A Wishart autoregressive process is {{considered for the}} <b>volatility</b> precision covariance <b>matrix,</b> for the estimation of which a two step procedure is adopted. The first step is the conditional inference on the autoregressive parameters and the second step is the unconditional inference, based on a Newton-Raphson iterative algorithm. The proposed methodology, which is mostly Bayesian, is suitable for medium dimensional data and it bridges the gap between closed-form estimation and simulation-based estimation algorithms. An example, consisting of foreign exchange rates data, illustrates the proposed methodology. Comment: 29 pages, 3 figures, 2 table...|$|R
40|$|Financial {{practices}} {{often need}} to estimate an integrated <b>volatility</b> <b>matrix</b> {{of a large}} number of assets using noisy high-frequency financial data. This estimation problem is a challenging one for four reasons: (1) high-frequency financial data are discrete observations of the underlying assets ’ price processes; (2) due to market micro-structure noise, high-frequency data are observed with measurement errors; (3) different assets are traded at different time points, which is the so-called non-synchronization phenomenon in high-frequency financial data; (4) the number of assets may be comparable to or even exceed the observations, and hence many existing estimators of small size volatility matrices become inconsistent when the size of the matrix is close to or larger than the sample size. In this dissertation, we focus on large <b>volatility</b> <b>matrix</b> inference for high-frequency financial data, which can be summarized in three aspects. On the methodological aspect, we propose a new threshold MSRVM estimator of large <b>volatility</b> <b>matrix.</b> This estimator can deal with all the four challenges, and is consistent when both sample size and matrix size go to infinity...|$|E
40|$|We derive {{necessary}} and sufficient {{conditions for the}} positive definiteness of the predicted <b>volatility</b> <b>matrix</b> in a bivariate autoregressive volatility specification. These nonlinear inequality restrictions have strong implications in terms of causality between volatilities and covolatilities. Copyright, Oxford University Press. ...|$|E
30|$|The {{family of}} models studied {{in this paper}} {{is an example of}} models where a pricing theory can be {{obtained}} using the results of Londoño (2008, 2004). In general, the theory developed in Londoño (2008) does not impose conditions on the eigenvalues of the <b>volatility</b> <b>matrix.</b> Traditional models usually require that the eigenvalues of the <b>volatility</b> <b>matrix</b> remain away from 0 (see Karatzas and Shreve 1998). The methodology developed in this paper gives sufficient conditions for the existence of non-explosive solutions and market completeness for the family of models proposed (see Remark  1). In general do not impose conditions on the eigenvalues of the volatility of the price process. The main results in this paper are complementary to results in  Londoño (2008) since it gives us concrete examples beyond standard models of financial markets.|$|E
40|$|The {{forecasting}} of variance-covariance matrices is {{an important}} issue. In recent years an increasing body of literature has focused on multivariate models to forecast this quantity. This paper develops a nonparametric technique for generating multivariate volatility forecasts from a weighted average of historical volatility and a broader set of macroeconomic variables. As opposed to traditional techniques where the weights solely decay {{as a function of}} time, this approach employs a kernel weighting scheme where historical periods exhibiting the most similar conditions to the time at which the forecast if formed attract the greatest weight. It is found that the proposed method leads to superior forecasts, with macroeconomic information playing an important role. Nonparametric, variance-covariance <b>matrix,</b> <b>volatility</b> forecasting, multivariate...|$|R
40|$|The {{purpose of}} this paper is to propose a {{time-varying}} vector autoregressive model (TV-VAR) for forecasting multivariate time series. The model is casted into a state-space form that allows flexible description and analysis. The <b>volatility</b> covariance <b>matrix</b> of the time series is modelled via inverted Wishart and singular multivariate beta distributions allowing a fully conjugate Bayesian inference. Model assessment and model comparison are performed via the log-posterior function, sequential Bayes factors, the mean of squared standardized forecast errors, the mean of absolute forecast errors (known also as mean absolute deviation), and the mean forecast error. Bayes factors are also used in order to choose the autoregressive (AR) order of the model. Multi-step forecasting is discussed in detail and a flexible formula is proposed to approximate the forecast function. Two examples, consisting of bivariate data of IBM and Microsoft shares and of a 30 -dimensional asset selection problem, illustrate the methods. For the IBM and Microsoft data we discuss model performance and multi-step forecasting in some detail. For the basket of 30 assets we discuss sequential portfolio allocation; for both data sets our empirical findings suggest that the TV-VAR models outperform the widely used vector AR models. ...|$|R
40|$|Summary. This paper {{studies the}} {{determinants}} growth rate volatility, {{focusing on the}} effect of teh level of GDP, structural change {{and the size of the}} economy. First we provide a graphical analysis based on nonparametric techniques, then a quantitative analysis which follows the distribution dynamics approach. Growth volatility appears to (i) decrease with per capita GDP, (ii) increase with the share of the agricultural sector on GDP and, (iii) decrease with the size of the economy, measured by a combination of total GDP and trade openness. However, we show that the explanatory power of per capita GDP tends to vanish when we control for the size of the economy. Key words: growth <b>volatility,</b> Markov transition <b>matrix,</b> structural change, nonparametric methods. ...|$|R
40|$|The {{use of a}} BEKK (Baba-Engle-Kraft-Kroner) {{model is}} {{proposed}} to estimate the volatility {{of a set of}} financial historical series with a view to the selection of a stock portfolio. An individual element on the diagonal of the <b>volatility</b> <b>matrix</b> is estimated by applying the model to the series of log returns both of the share i to which it refers and of the market index. An extra-diagonal element is instead estimated by using in the model the covariances between the series of log returns of the two shares i and j to which the element of the <b>volatility</b> <b>matrix</b> corresponds. The procedure proposed for the estimation of volatility was applied to the series of monthly stock log returns of 150 shares of major value traded on the Italian market between 1 January 1975 and 31 August 2011 and the Markowitz portfolio is simulated...|$|E
40|$|Stochastic {{processes}} {{are often used}} to model complex scientific problems in fields ranging from biology and finance to engineering and physical science. This paper investigates rate-optimal estimation of the <b>volatility</b> <b>matrix</b> of a high-dimensional Itô process observed with measurement errors at discrete time points. The minimax rate of convergence is established for estimating sparse volatility matrices. By combining the multi-scale and threshold approaches we construct a <b>volatility</b> <b>matrix</b> estimator to achieve the optimal convergence rate. The minimax lower bound is derived by considering a subclass of Itô processes for which the minimax lower bound is obtained through a novel equivalent model of covariance matrix estimation for independent but nonidentically distributed observations and through a delicate construction of the least favorable parameters. In addition, a simulation {{study was conducted to}} test the finite sample performance of the optimal estimator, and the simulation results were found to support the established asymptotic theory. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|This paper {{extends to}} the multiasset {{framework}} the closed-form solution for options withstochastic volatility derived in Heston (1993) and Ball and Roma (1994). This extensionintroduces a risk premium in the return equation and considers Wishart dynamics for theprocess of the stochastic <b>volatility</b> <b>matrix,</b> which is the multiasset analogue of the model ofCox, Ingersoll, and Ross (1985). This approach is used to extend Merton’s model (Merton(1974)) for corporate default to a framework with stochastic liability, stochastic volatilityand several firms. ...|$|E
40|$|In this work, the {{possibilities}} of graphite furnace atomic absorption spectrometry for the direct determination of Au in solid samples (NIST SRM 2709 San Joaquin soil, CANMET PTC- 1 a Co-Ni sulfide concentrate and a vitamin complex) have been evaluated. The diverse <b>volatilities</b> of the <b>matrixes</b> resulted in different degrees of difficulty for Au determination. In this regard, {{the analysis of the}} less volatile samples (CANMET PTC- 1 a and NIST SRM 2709) was particularly challenging. For these materials, the addition of Na 2 CO 3 solutions, in order to carry out a microfusion directly in the graphite furnace, was found to be essential to liberate structurally bound gold at a moderate temperature (2000 - 2100 degrees C). In all the cases investigated, accurate results were obtained by means of external calibration using aqueous standards. Every determination required approximate to 15 - 20 minutes. Precision ranged between 12 and 14 % relative standard deviation. A detection limit of 15 ng g(- 1) was calculated...|$|R
40|$|Graphite filter atomizers (GFA) for {{electrothermal}} {{atomic absorption}} spectrometry (ETAAS) show substantial advantages over commonly employed electrothermal vaporizers and atomizers, tube and platform furnaces, for direct determination of high and medium <b>volatility</b> elements in <b>matrices</b> associated with strong spectral and chemical interferences. Two factors provide lower limits of detection and shorter determination cycles with the GFA: the vaporization area in the GFA is separated from the absorption volume by a porous graphite partition; the sample is distributed over a large surface of a collector in the vaporization area. These factors convert the GFA into an efficient chemical reactor. The research concerning the GFA concept, technique and analytical methodology, carried out mainly in the author's laboratory in Russia and South Africa, is reviewed. Examples of analytical applications of the GFA in AAS for analysis of organic liquids and slurries, bio-samples and food products are given. Future prospects for the GFA are discussed in connection with analyses by fast multi-element AAS...|$|R
40|$|Analyte-matrix adducts are {{normally}} absent under typical matrix assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI TOF MS) conditions. Interestingly, though, {{in the analysis}} of several types of organic compounds synthesized in our laboratory, analyte-matrix adduct ion peaks were always recorded when common MALDI matrices such as 4 -hydroxy-alpha-cyanocinnamic acid (CHCA) were used. These compounds are mainly those with a benzene- 1, 3, 5 -tricarboxamide (BTA) or urea moiety, which are important building blocks to make new functional supramolecular materials. The possible mechanism of the adduct formation was investigated. A shared feature of the compounds studied is that they can form intermolecular hydrogen bonding with matrices like CHCA. The intermolecular hydrogen bonding will make the association between analyte ions and matrix molecules stronger. As a result, the analyte ions and matrix molecules in MALDI clusters will become more difficult to be separated from each other. Furthermore, it was found that analyte ions were mainly adducted with matrix salts, which is probably due to the much lower volatility of the salts compared with that of their corresponding matrix acids. It seems that the analyte-matrix adduct formation for our compounds are caused by the incomplete evaporation of matrix molecules from the MALDI clusters because of the combined effects of enhanced intermolecular interaction between analyte-matrix and of the low <b>volatility</b> of <b>matrix</b> salts. Based on these findings, strategies to suppress the analyte-matrix adduction are briefly discussed. In return, the positive results of using these strategies support the proposed mechanism of the analyte-matrix adduct formation...|$|R
40|$|We {{introduce}} {{and evaluate}} mixed-frequency multivariate GARCH models for forecasting low-frequency (weekly or monthly) multivariate volatility based on high-frequency intra-day returns (at five-minute intervals) {{and on the}} overnight returns. The low-frequency conditional <b>volatility</b> <b>matrix</b> is modelled as a weighted sum of an intra-day and an overnight component, driven by the intra-day and the overnight returns, respectively. The components are specified as multivariate GARCH (1, 1) models of the BEKK type, adapted to the mixed-frequency data setting. For the intra-day component, the squared high-frequency returns enter the GARCH model through a parametrically specified mixed-data sampling (MIDAS) weight function or through {{the sum of the}} intra-day realized volatilities. For the overnight component, the squared overnight returns enter the model with equal weights. Alternatively, the low-frequency conditional <b>volatility</b> <b>matrix</b> may be modelled as a single-component BEKK-GARCH model where the overnight returns and the high-frequency returns enter through the weekly realized volatility (defined as the unweighted sum of squares of overnight and high-frequency returns), or where the overnight returns are simply ignored. All model variants may further be extended by allowing for a non-parametrically estimated slowly-varying long-run <b>volatility</b> <b>matrix.</b> The proposed models are evaluated using five-minute and overnight return data on four DJIA stocks (AXP, GE, HD, and IBM) from January 1988 to November 2014. The focus is on forecasting weekly volatilities (defined as the low frequency). The mixed-frequency GARCH models are found to systematically dominate the low-frequency GARCH model in terms of in-sample fit and out-of-sample forecasting accuracy. They also exhibit much lower low-frequency volatility persistence than the low-frequency GARCH model. Among the mixed-frequency models, the low-frequency persistence estimates decrease as the data frequency increases from daily to five-minute frequency, and as overnight returns are included. That is, ignoring the available high-frequency information leads to spuriously high volatility persistence. Among the other findings are that the single-component model variants perform worse than the two-component variants; that the overnight volatility component exhibits more persistence than the intra-day component; and that MIDAS weighting performs better than not weighting at all (i. e., than realized volatility). status: publishe...|$|E
40|$|High-frequency data {{observed}} on {{the prices of}} financial assets are commonly modeled by diffusion processes with micro-structure noise, and realized volatility-based methods are often used to estimate integrated volatility. For problems involving {{a large number of}} assets, the estimation objects we face are volatility matrices of large size. The existing volatility estimators work well for a small number of assets but perform poorly when the number of assets is very large. In fact, they are inconsistent when both the number, $p$, of the assets and the average sample size, $n$, of the price data on the $p$ assets go to infinity. This paper proposes a new type of estimators for the integrated <b>volatility</b> <b>matrix</b> and establishes asymptotic theory for the proposed estimators in the framework that allows both $n$ and $p$ to approach to infinity. The theory shows that the proposed estimators achieve high convergence rates under a sparsity assumption on the integrated <b>volatility</b> <b>matrix.</b> The numerical studies demonstrate that the proposed estimators perform well for large $p$ and complex price and volatility models. The proposed method is applied to real high-frequency financial data. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|We {{consider}} an investor {{in a financial}} market consisting of a riskless bond and several risky assets. The price processes of the risky assets are geometric Brownian motions where either the drifts are modeled as random variables assuming a constant <b>volatility</b> <b>matrix</b> or the <b>volatility</b> <b>matrix</b> is considered random and drifts {{are assumed to be}} constant. The investor is only able to observe the asset prices but not all the model parameters and hence information is only partial. A Bayesian approach is used with known prior distributions for the random model parameters. We assume that the investor can only trade at discrete-time points which are multiples of h> 0 and investigate the loss in expected utility of terminal wealth which {{is due to the fact}} that the investor cannot trade and observe continuously. It turns out that in general a discretization gap appears, i. e., for h→ 0 the expected utility of the h-investor does not converge to the expected utility of the continuous investor. This is in contrast to results under full information in [L. C. G. Rogers, Finance Stoch., 5 (2001), pp. 131 – 154]. We also present simple asymptotically optimal portfolio strategies for the discrete-time problem. Our results are illustrated by some numerical examples...|$|E
40|$|The {{main purpose}} of this thesis is to develop methodological and {{practical}} improvements on robust portfolio optimization procedures. Firstly, the thesis discusses the drawbacks of classical mean-variance optimization models, and examines robust portfolio optimization procedures with CVaR and worst-case CVaR risk models by providing a clear presentation of derivation of robust optimization models from a basic VaR model. For practical purposes, the thesis introduces an open source software interface called “RobustRisk”, which is developed for producing empirical evidence for the robust portfolio optimization models. The software, which performs Monte-Carlo simulation and out-of-sample performance for the portfolio optimization, is introduced by using a hypothetical portfolio data from selected emerging markets. In addition, the performance of robust portfolio optimization procedures are discussed by providing empirical evidence in the crisis period from advanced markets. Empirical results show that robust optimization with worst-case CVaR model outperforms the nominal CVaR model in the crisis period. The empirical results encourage us to construct a forward-looking stress test procedure based on robust portfolio optimization under regime switches. For this purpose, the Markov chain process is embedded into robust optimization procedure in order to stress regime transition matrix. In addition, assets returns, <b>volatilities,</b> correlation <b>matrix</b> and covariance matrix can be stressed under pre-defined scenario expectations. An application is provided with a hypothetical portfolio representing an internationally diversified portfolio. The CVaR efficient frontier and corresponding optimized portfolio weights are achieved under regime switch scenarios. The research suggests that stressed-CVaR optimization provides a robust and forward-looking stress test procedure {{to comply with the}} regulatory requirements stated in Basel II and CRD regulations...|$|R
40|$|This report {{describes}} {{significant results}} from an on-going, collaborative effort {{to enable the}} use of inexpensive metallic alloys as interconnects in planar solid oxide fuel cells (SOFCs) {{through the use of}} advanced coating technologies. Arcomac Surface Engineering, LLC, under the leadership of Dr. Vladimir Gorokhovsky, is investigating filtered-arc and filtered-arc plasma-assisted hybrid coating deposition technologies to promote oxidation resistance, eliminate Cr volatility, and stabilize the electrical conductivity of both standard and specialty steel alloys of interest for SOFC metallic interconnect (IC) applications. Arcomac has successfully developed technologies and processes to deposit coatings with excellent adhesion, which have demonstrated a substantial increase in high temperature oxidation resistance, stabilization of low Area Specific Resistance values and significantly decrease Cr <b>volatility.</b> An extensive <b>matrix</b> of deposition processes, coating compositions and architectures was evaluated. Technical performance of coated and uncoated sample coupons during exposures to SOFC interconnect-relevant conditions is discussed, and promising future directions are considered. Cost analyses have been prepared based on assessment of plasma processing parameters, which demonstrate the feasibility of the proposed surface engineering process for SOFC metallic IC applications...|$|R
40|$|This {{thesis is}} focused on the {{financial}} model for interest rates called the LIBOR Market Model. In the appendixes, we provide the necessary mathematical theory. In the inner chapters, firstly, we define the main interest rates and financial instruments concerning with the interest rate models, then, we set the LIBOR market model, demonstrate its existence, derive the dynamics of forward LIBOR rates and justify the pricing of caps according to the Black’s formula. Then, we also present the Swap Market Model, which models the forward swap rates instead of the LIBOR ones. Even this model is justified by a theoretical demonstration and the resulting formula to price the swaptions coincides with the Black’s one. However, the two models are not compatible from a theoretical point. Therefore, we derive various analytical approximating formulae to price the swaptions in the LIBOR market model and we explain how to perform a Monte Carlo simulation. Finally, we present the calibration of the LIBOR market model to the markets of both caps and swaptions, together with various examples of application to the historical correlation matrix and the cascade calibration of the forward <b>volatilities</b> to the <b>matrix</b> of implied swaption volatilities provided by the market. ...|$|R
