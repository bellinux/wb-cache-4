2343|6431|Public
5|$|When meeting {{another of}} the same species, P. fimbriata does not stalk but {{displays}} by moving quickly and smoothly, and displays at 4 to 27centimetres away. It raises its legs, its body sways from side to side, and the palps are lowered below the chelicerae ("fangs"). This {{is very different from}} the stalking it uses when encountering another salticid of a different species, despite receiving the same <b>visual</b> <b>stimulus,</b> the sight of the other's large anterior-median eyes. Although P. fimbriata is influenced by pheromones much more than is usual among salticids, visual cues alone are enough to start displays and distinguish members of the same species from other salticids, even if neither partner moves. The spindly, fringed legs of Portia species may identify members of the same species, as well as concealing these spiders from other salticid species.|$|E
25|$|Visual display: Visual {{displays}} {{provide the}} <b>visual</b> <b>stimulus</b> to the user.|$|E
25|$|In {{patients}} with Alzheimer's disease (AD), {{there is a}} smaller McGurk effect exhibited than in normals. Often a reduced size of the corpus callosum produces a hemisphere disconnection process. Less influence on <b>visual</b> <b>stimulus</b> is seen in {{patients with}} AD, which is {{a reason for the}} lowered McGurk effect.|$|E
40|$|The {{influence}} of noncontact plant cues is investigated on {{the likelihood that}} individual conifer seedlings will be found by walking adults of the pine weevil, Hylobius abietis, in the field. Traps with solely odour or solely <b>visual</b> <b>stimuli</b> catch significantly more weevils than stimulus-free traps, and traps with the combination of odour and <b>visual</b> <b>stimuli</b> catch more weevils than traps with odour or <b>visual</b> <b>stimuli</b> alone. There is essentially an additive effect between odour and <b>visual</b> <b>stimuli.</b> The reactions to odour and <b>visual</b> <b>stimuli</b> are similar for three phases of the pine weevil's life cycle associated with three ages of clear-cuttings (i. e. sites where all trees have been harvested). <b>Visual</b> <b>stimuli</b> appear {{to be at least}} as important as odour for the pine weevil in finding an undamaged conifer seedling...|$|R
40|$|Coordinating spatial {{perception}} between body {{space and}} its external surrounding space {{is essential to}} adapt behaviors to objects, especially when they are noxious. Such coherent multisensory representation of the body extended into external space is conceptualized by the notion of peripersonal reference frame, mapping the portion of space in which somatic and extra-somatic inputs interact closely. Studies on crossmodal interactions between nociception and vision have been scarce. Here we investigated how the perception of <b>visual</b> <b>stimuli,</b> especially those surrounding the body, can be impacted by a nociceptive and potentially harmful stimulus inflicted on a particular body part. In two temporal order judgment tasks, participants judged which of two lateralized <b>visual</b> <b>stimuli,</b> presented either near or far from the body, had been presented first. <b>Visual</b> <b>stimuli</b> were preceded by nociceptive stimuli, either applied unilaterally (on one single hand) or bilaterally (on both hands simultaneously). In Experiment 1 participants' hands were always placed next to the <b>visual</b> <b>stimuli</b> presented near the trunk, while in Experiment 2 they could also be placed next to the <b>visual</b> <b>stimuli</b> presented far from the trunk. In Experiment 1, the presence of unilateral nociceptive stimuli prioritized the perception of <b>visual</b> <b>stimuli</b> presented in {{the same side of}} space as the stimulated hand, with a significantly larger effect when <b>visual</b> <b>stimuli</b> were presented near the body than when presented farther away. Experiment 2 showed that these visuospatial biases were related to the spatial congruency between the hand on which nociceptive stimuli were applied and the <b>visual</b> <b>stimuli,</b> independently of the relative distance of both the stimulated hand and the <b>visual</b> <b>stimuli</b> from the trunk. Indeed, nociceptive stimuli mostly impacted the perception of the closest <b>visual</b> <b>stimuli.</b> It is hypothesized that these crossmodal interactions may rely on representations of the space directly surrounding specific body parts...|$|R
40|$|A fission {{illusion}} (also named a double—flash illusion) is {{a famous}} phenomenon of audio-visual interaction, {{in which a}} single brief flash is perceived as two flashes when presented simultaneously with two brief beeps (Shames, Kamitani, & Shimojo, 2000; 2002). The fission illusion has been investigated using relatively simple <b>visual</b> <b>stimuli</b> like single circle. Thus the illusion has not been examined by using complex <b>visual</b> <b>stimuli.</b> Markovic & Gvozdenovic (2001) reported that the processing of complex <b>visual</b> <b>stimuli</b> tends to be delayed. Therefore, the complexity of <b>visual</b> <b>stimuli</b> may affect the occurrence rate of the fission illusion, since this illusion is generated in the process that copes with <b>visual</b> and auditory <b>stimuli</b> in a short time. The present study examined the differences in illusory occurrence rates by manipulating the complexity of <b>visual</b> <b>stimuli.</b> We used the patterns proposed by Garner & Clement (1963) to control the complexity. The results indicated {{that it was more}} difficult to induce the fission illusion by using complex <b>visual</b> <b>stimuli</b> than it was by using simple stimuli. Thus, the present study suggested that the occurrence rate of the fission illusion differed depending on the perceptual efficiency in the coding process of <b>visual</b> <b>stimuli.</b> This study was supported by Grant-in-Aid for Specifically Promoted Research (No. 19001004) ...|$|R
25|$|Adult blow flies are {{occasional}} pollinators, being {{attracted to}} flowers with strong odors resembling rotting meat, {{such as the}} American pawpaw or dead horse arum. Little doubt remains that these flies use nectar {{as a source of}} carbohydrates to fuel flight, but just how and when this happens is unknown. One study showed the <b>visual</b> <b>stimulus</b> a blow fly receives from its compound eyes is responsible for causing its legs to extend from its flight position and allow it to land on any surface.|$|E
25|$|When {{a street}} is {{wide enough to}} {{accommodate}} several vehicles traveling side-by-side, it is usual for traffic to organize itself into lanes, that is, parallel corridors of traffic. Some roads have one lane for each direction of travel and others have multiple lanes for each direction. Most countries apply pavement markings to clearly indicate the limits of each lane and the direction of travel {{that it must be}} used for. In other countries lanes have no markings at all and drivers follow them mostly by intuition rather than <b>visual</b> <b>stimulus.</b>|$|E
25|$|As {{noticed in}} A. pallipes {{foraging}} habits, the eusocial wasps also receive <b>visual</b> <b>stimulus</b> that communicates {{to the individual}} to join the larger group. It was observed through bait stations that those stations with wasp models were better dominated by wasps when ant competition was also present. Furthermore, scientists have proved that even without their olfactory senses, {{the mere presence of}} wasp models was enough for recruitment. Thus it is clear that though A. pallipes communicate through the release of trail pheromones, they can also be recruited through simple visual stimuli.|$|E
40|$|People {{obtain a}} lot of {{information}} from visual and auditory sensation on daily life. Regarding the effect of <b>visual</b> <b>stimuli</b> on perception of auditory stimuli, studies of phonological perception and sound localization have been made in great numbers. This study examined the effect of <b>visual</b> <b>stimuli</b> on perception in loudness and pitch of auditory stimuli. We used the image of figures whose size or brightness was changed as <b>visual</b> <b>stimuli,</b> and the sound of pure tone whose loudness or pitch was changed as auditory <b>stimuli.</b> Those <b>visual</b> and auditory <b>stimuli</b> were combined independently to make four types of audio-visual multisensory stimuli for psychophysical experiments. In the experiments, participants judged change in loudness or pitch of auditory stimuli, while they judged the direction of size change or the kind of a presented figure in <b>visual</b> <b>stimuli.</b> Therefore they cannot neglect <b>visual</b> <b>stimuli</b> while they judged auditory stimuli. As a result, perception in loudness and pitch were promoted significantly around their difference limen, when the image was getting bigger or brighter, compared with the case in which the image had no changes. This indicates that perception in loudness and pitch were affected by change in size and brightness of <b>visual</b> <b>stimuli...</b>|$|R
40|$|We {{investigated}} {{what effect}} visual spatial information had on auditory temporal order judgments (TOJs) and examined what effect <b>visual</b> <b>stimuli</b> {{had on the}} TOJs of sequences of pure tones in experiment 1. The auditory stimuli were sequences of four distinct pure tones. The <b>visual</b> <b>stimuli</b> consisted of two vertically aligned flashes: one flashed before the first tone and the other flashed after the last tone. Participants judged whether the temporal order of {{the second and third}} tones in auditory stimuli occurred with the higher tone being first or the lower tone being first. As a result, the proportion of responses for higher-tone-first increased when the flash of the upper LED preceded that of the lower LED, independent of the actual temporal order. Participants in experiment 2 were asked to make simultaneity judgments instead, which were also affected by <b>visual</b> <b>stimuli.</b> The auditory stimuli in experiment 3 were the same as those in experiment 1, whereas the <b>visual</b> <b>stimuli</b> consisted of two horizontally aligned flashes. Furthermore, the participants made TOJs, which were not affected by the horizontally aligned <b>visual</b> <b>stimuli.</b> We concluded that vertically aligned <b>visual</b> <b>stimuli</b> had an effect on auditory TOJs with some response bias...|$|R
40|$|On a non-spatial task, {{individuals}} respond {{more slowly}} when the {{stimulus and response}} are on different sides of space versus {{the same side of}} space (the Simon effect). Past Simon effect studies have shown that <b>visual</b> <b>stimuli</b> are represented based on external reference frames. In contrast, we have found that tactile stimuli in a Simon effect task are encoded based on a somatotopic reference frame (eg, stimuli presented to the left hand are always encoded as left regardless of the hand's location in space). In order to understand how <b>visual</b> <b>stimuli</b> are represented on or near the body, we presented individuals with a visual Simon effect experiment. Using foot pedals, participants responded to <b>visual</b> <b>stimuli</b> projected on, near, or far from the hands in three conditions—hands uncrossed, hand crossed, and no hands. In the no hands condition, <b>visual</b> <b>stimuli</b> were encoded based on an external reference frame. However, we find that as with tactile <b>stimuli,</b> <b>visual</b> <b>stimuli</b> presented on or near the hands (crossed condition) were encoded based on a somatotopic frame of reference. These novel results provide evidence that <b>visual</b> <b>stimuli</b> on or near the hands can be encoded based on body representations...|$|R
25|$|More {{recent studies}} refine this early {{qualitative}} account of multisensory integration. Alais and Burr (2004), found that following progressive degradation {{in the quality}} of a <b>visual</b> <b>stimulus,</b> participants' perception of spatial location was determined progressively more by a simultaneous auditory cue. However, they also progressively changed the temporal uncertainty of the auditory cue; eventually concluding that it is the uncertainty of individual modalities that determine to what extent information from each modality is considered when forming a percept. This conclusion is similar in some respects to the 'inverse effectiveness rule'. The extent to which multisensory integration occurs may vary according to the ambiguity of the relevant stimuli. In support of this notion, a recent study shows that weak senses such as olfaction can even modulate the perception of visual information as long as the reliability of visual signals is adequately compromised.|$|E
25|$|Ventriloquism {{has been}} used as the {{evidence}} for the modality appropriateness hypothesis. Ventriloquism describes the situation in which auditory location perception is shifted toward a visual cue. The original study describing this phenomenon was conducted by Howard and Templeton, (1966) after which several studies have replicated and built upon the conclusions they reached. In conditions in which the visual cue is unambiguous, visual capture reliably occurs. Thus to test the influence of sound on perceived location, the <b>visual</b> <b>stimulus</b> must be progressively degraded. Furthermore, given that auditory stimuli are more attuned to temporal changes, recent studies have tested the ability of temporal characteristics to influence the spatial location of visual stimuli. Some types of EVP – electronic voice phenomenon, mainly the ones using sound bubles are considered a kind of modern ventriloquism technique and is played by the use of sophisticated software, computers and sound equipment.|$|E
2500|$|A visual {{hallucination}} is [...] "the {{perception of}} an external <b>visual</b> <b>stimulus</b> where none exists". Alternatively, a visual illusion is a distortion {{of a real}} external stimulus. Visual hallucinations are separated into simple and complex.|$|E
40|$|Abstract. We {{investigated}} {{what effect}} visual spatial information had on auditory temporal order judgments (TOJs) and examined what effect <b>visual</b> <b>stimuli</b> {{had on the}} TOJs of sequences of pure tones in experiment 1. The auditory stimuli were sequences of four distinct pure tones. The <b>visual</b> <b>stimuli</b> consisted of two vertically aligned flashes: one flashed before the first tone and the other flashed after the last tone. Participants judged whether the temporal order of {{the second and third}} tones in auditory stimuli occurred with the higher tone being first or the lower tone being first. As a result, the proportion of responses for higher-tone-first increased when the flash of the upper LED preceded that of the lower LED, independent of the actual temporal order. Participants in experiment 2 were asked to make simultaneity judgments instead, which were also affected by <b>visual</b> <b>stimuli.</b> The auditory stimuli in experiment 3 were the same as those in experiment 1, whereas the <b>visual</b> <b>stimuli</b> consisted of two horizontally aligned flashes. Furthermore, the participants made TOJs, which were not affected by the horizontally aligned <b>visual</b> <b>stimuli.</b> We concluded that vertically aligned <b>visual</b> <b>stimuli</b> had an effect on auditory TOJs with some response bias...|$|R
40|$|This {{paper is}} {{concerned}} the streetscape <b>visual</b> <b>stimuli</b> making streetscape confusions as we" as the <b>visual</b> <b>stimuli</b> making trouble-free bzdirectional movements of first time visitors to an area. Pedestrian/ vehicle streets around Saitama Universiry, Japan iuere selectedfor the stucfy. One fonvard movement movie along a streetscape with 5 movies of baceiuard movements from different streetscapes were displqyed {{to a group}} of suo/ects. S uijccts u/ere asked to select the correct bacesiard movement analogous to the fonvard movement and a questionnaire was given to fill Ivith the answers for individual's selection. This method tuas repeated ry changing the fonuard and backward movies along streetscapes and the group of suo/ects. A significant variation could be identified in the backward path selection corresponding to thefonvard movement. <b>Visual</b> <b>stimuli</b> which caused streetscape confusions and the <b>visual</b> <b>stimuli</b> that helpful jor correctpath selection sere identified. The <b>visual</b> <b>stimuli</b> that made streetscape confusions and that made correct selection were significantlY similar among different planned streetscape groups...|$|R
25|$|Temporal {{synchrony}} is {{not necessary}} for the McGurk effect to be present. Subjects are still strongly influenced by auditory stimuli even when it lagged the <b>visual</b> <b>stimuli</b> by 180 milliseconds (point at which McGurk effect begins to weaken). There was less tolerance {{for the lack of}} synchrony if the auditory <b>stimuli</b> preceded the <b>visual</b> <b>stimuli.</b> In order to produce a significant weakening of the McGurk effect, the auditory stimuli had to precede the <b>visual</b> <b>stimuli</b> by 60 milliseconds, or lag by 240 milliseconds.|$|R
2500|$|It {{contains}} individually functioning lobules. [...] The lateral intraparietal sulcus (LIP) contains neurons {{that produce}} enhanced activation when attention is moved onto the stimulus or the animal saccades towards a <b>visual</b> <b>stimulus,</b> and the ventral intraparietal sulcus (VIP) where visual and somatosensory information are integrated.|$|E
2500|$|The {{experiment}} involved asking {{volunteers to}} respond to a go-signal by pressing an electronic [...] "go" [...] button as quickly as possible. In this experiment the go-signal was represented as a <b>visual</b> <b>stimulus</b> shown on a monitor (e.g. a green light as shown on the picture). The participants' reaction times (RT) were gathered at this stage, in what was described as the [...] "primary response trials".|$|E
2500|$|In {{laboratory}} studies, {{young cattle}} {{are able to}} memorize the locations of several food sources and retain this memory for at least 8hours, although this declined after 12hours. [...] Fifteen-month-old heifers learn more quickly than adult cows which have had either one or two calvings, but their longer-term memory is less stable. [...] Mature cattle perform well in spatial learning tasks {{and have a good}} long-term memory in these tests. [...] Cattle tested in a radial arm maze are able to remember the locations of high-quality food for at least 30 days. [...] Although they initially learn to avoid low-quality food, this memory diminishes over the same duration. [...] Under less artificial testing conditions, young cattle showed they were able to remember the location of feed for at least 48 days. [...] Cattle can make an association between a <b>visual</b> <b>stimulus</b> and food within 1 day – memory of this association can be retained for 1 year, despite a slight decay.|$|E
40|$|Virtual Reality (VR) {{can provide}} <b>visual</b> <b>stimuli</b> for EEG studies {{that can be}} altered in real time and can produce effects that are {{difficult}} or impossible to reproduce in a non-virtual experimental platform. As part of this experiment the Oculus Rift, a commercial-grade, low-cost, Head Mounted Display (HMD) was assessed as a <b>visual</b> <b>stimuli</b> platform for experiments recording EEG. Following, the device was used to investigate the effect of congruent <b>visual</b> <b>stimuli</b> on Event Related Desynchronisation (ERD) due to motion imagery...|$|R
50|$|Temporal {{synchrony}} is {{not necessary}} for the McGurk effect to be present. Subjects are still strongly influenced by auditory stimuli even when it lagged the <b>visual</b> <b>stimuli</b> by 180 milliseconds (point at which McGurk effect begins to weaken). There was less tolerance {{for the lack of}} synchrony if the auditory <b>stimuli</b> preceded the <b>visual</b> <b>stimuli.</b> In order to produce a significant weakening of the McGurk effect, the auditory stimuli had to precede the <b>visual</b> <b>stimuli</b> by 60 milliseconds, or lag by 240 milliseconds.|$|R
40|$|In {{this issue}} of Neuron, Busse et al. {{describe}} the population response to superimposed <b>visual</b> <b>stimuli</b> while Sit et al. examine the spatiotemporal evolution of cortical activation in response to small <b>visual</b> <b>stimuli.</b> Surprisingly, these two studies of V 1 report that a single gain control model accounts for their results...|$|R
2500|$|The {{introduction}} of stimuli which {{were hard to}} verbalize, and unlikely {{to be held in}} long-term memory, revolutionized the study of VSTM in the early [...] 1970s (Cermak, 1971; Phillips, 1974; Phillips & Baddeley, 1971). The basic experimental technique used required observers to indicate whether two matrices (Phillips, 1974; Phillips & Baddeley, 1971), or figures (Cermak, 1971), separated by a short temporal interval, were the same. The finding that observers were able to report that a change had occurred, at levels significantly above chance, indicated {{that they were able to}} encode [...] aspect of the first stimulus in a purely visual store, at least for the period until the presentation of the second stimulus. However, as the stimuli used were complex, and the nature of the change relatively uncontrolled, these experiments left open various questions, such as: (1) whether only a subset of the perceptual dimensions comprising a <b>visual</b> <b>stimulus</b> are stored (e.g., spatial frequency, luminance, or contrast); (2) whether [...] perceptual dimensions are maintained in VSTM with greater fidelity than others; and (3) the nature by which these dimensions are encoded (i.e., are perceptual dimensions encoded within separate, parallel channels, or are all perceptual dimensions stored as a single bound entity within VSTM?).|$|E
2500|$|This {{technique}} allows several (e.g., four) SSEPs to {{be recorded}} simultaneously from any given location on the scalp. Different sites of stimulation or different stimuli can be tagged with slightly different frequencies that are virtually identical to the brain, but easily separated by Fourier series analyzers. For example, when two unpatterned lights are modulated at slightly different frequencies (F1 and F2) and superimposed, multiple nonlinear cross-modulation components of frequency (mF1 ± [...] nF2) are created in the SSEP, where m and n are integers. These components allow nonlinear processing in the brain to be investigated. By frequency-tagging two superimposed gratings, spatial frequency and orientation tuning properties of the brain mechanisms that process spatial form can be isolated and studied. Stimuli of different sensory modalities can also be tagged. For example, a <b>visual</b> <b>stimulus</b> was flickered at Fv Hz and a simultaneously presented auditory tone was amplitude modulated at Fa Hz. The existence of a (2Fv + 2Fa) component in the evoked magnetic brain response demonstrated an audio-visual convergence area in the human brain, {{and the distribution of}} this response over the head allowed this brain area to be localized. More recently, frequency tagging has been extended from studies of sensory processing to studies of selective attention and of consciousness.|$|E
50|$|In {{line with}} the current literature, with control over signal {{strength}} of the stimulus and proper management of neural adaptation, it is predicted that a stronger <b>visual</b> <b>stimulus</b> (random white noise pattern) should predictably dominate perception and the weaker <b>visual</b> <b>stimulus</b> (target grayscale image) would be suppressed.|$|E
40|$|Background: Visual {{processing}} {{network is}} one of the functional networks which have been reliably identified to consistently exist in human resting brains. In our work, we focused on this network and investigated the intrinsic properties of low frequency (0. 01 – 0. 08 Hz) fluctuations (LFFs) during changes of <b>visual</b> <b>stimuli.</b> There were two main questions to be discussed in this study: intrinsic properties of LFFs regarding (1) interactions between <b>visual</b> <b>stimuli</b> and resting-state; (2) impact of repetition rate of <b>visual</b> <b>stimuli.</b> Methodology/Principal Findings: We analyzed scanning sessions that contained rest and <b>visual</b> <b>stimuli</b> in various repetition rates with a novel method. The method included three numerical approaches involving ICA (Independent Component Analyses), fALFF (fractional Amplitude of Low Frequency Fluctuation), and Coherence, to respectively investigate the modulations of visual network pattern, low frequency fluctuation power, and interregional functional connectivity during changes of <b>visual</b> <b>stimuli.</b> We discovered when resting-state was replaced by <b>visual</b> <b>stimuli,</b> more areas were involved in visual processing, and both stronger low frequency fluctuations and higher interregional functional connectivity occurred in visual network. With changes of visual repetition rate, the number of areas which were involved in visual processing, low frequency fluctuation power, and interregional functional connectivity in this network were also modulated. Conclusions/Significance: To combine the results of prior literatures and our discoveries, intrinsic properties of LFFs i...|$|R
40|$|Coordinating spatial {{perception}} between external {{space and}} body space {{is essential to}} adapt behaviors to objects, especially when these objects become noxious. Little {{is known about the}} crossmodal link between nociception/pain and vision. This study investigates how a nociceptive stimulus affects the perception of <b>visual</b> <b>stimuli</b> that are presented in the same side of space as the stimulated body part. In a temporal order judgment task, participants judged which of two <b>visual</b> <b>stimuli,</b> one applied to either side of space, had been perceived first. <b>Visual</b> <b>stimuli</b> were presented either near participants’ hands (peripersonal) or far from them (extrapersonal). Each pair of <b>visual</b> <b>stimuli</b> was preceded by either one nociceptive stimulus applied on one of the hands (unilateral) or two nociceptive stimuli, one applied to each hand at the same time (bilateral). Results show that, as compared to the bilateral condition, participants’ judgments were biased positively to the advantage of the <b>visual</b> <b>stimuli</b> having occurred in the same side of space as the hand on which the nociceptive stimulus was applied. Moreover, this effect seems larger when the <b>visual</b> <b>stimuli</b> were presented close to the hands. These results suggest that the perception of external space can be affected by the occurrence of highly significant bodily sensations such as pain, specifically when external sensory events occur in the peripersonal space...|$|R
40|$|The {{duration}} of sounds {{can affect the}} perceived {{duration of}} co-occurring <b>visual</b> <b>stimuli.</b> However, {{it is unclear whether}} this is limited to amodal processes of duration perception or affects other non-temporal qualities of visual perception. Here, we tested the hypothesis that visual sensitivity [...] rather than only the perceived duration of <b>visual</b> <b>stimuli</b> [...] can be affected by the duration of co-occurring sounds. We found that visual detection sensitivity (d') for unimodal stimuli was higher for stimuli of longer duration. Crucially, in a cross-modal condition, we replicated previous unimodal findings, observing that visual sensitivity was shaped by the duration of co-occurring sounds. When short <b>visual</b> <b>stimuli</b> (∼ 24 ms) were accompanied by sounds of matching duration, visual sensitivity was decreased relative to the unimodal visual condition. However, when the same <b>visual</b> <b>stimuli</b> were accompanied by longer auditory stimuli (∼ 60 - 96 ms), visual sensitivity was increased relative to the performance for ∼ 24 ms auditory stimuli. Across participants, this sensitivity enhancement was observed within a critical time window of ∼ 60 - 96 ms. Moreover, the amplitude of this effect correlated with visual sensitivity enhancement found for longer lasting <b>visual</b> <b>stimuli</b> across participants. Our findings show that the duration of co-occurring sounds affects visual perception; it changes visual sensitivity in a similar way as altering the (actual) duration of the <b>visual</b> <b>stimuli</b> does...|$|R
50|$|Visual display: Visual {{displays}} {{provide the}} <b>visual</b> <b>stimulus</b> to the user.|$|E
50|$|In {{neurology}} and neuroscience research, {{steady state}} visually evoked potentials (SSVEP) are signals that are natural responses to visual stimulation at specific frequencies. When the retina is excited by a <b>visual</b> <b>stimulus</b> ranging from 3.5 Hz to 75 Hz, the brain generates electrical {{activity at the}} same (or multiples of) frequency of the <b>visual</b> <b>stimulus.</b>|$|E
5000|$|One {{will be a}} {{quantitative}} analysis and interpretation question with a <b>visual</b> <b>stimulus.</b>|$|E
5000|$|Auditory stimuli {{may appear}} to last longer than <b>visual</b> <b>stimuli</b> ...|$|R
40|$|The {{circuitry}} {{and function}} of mammalian visual cortex are shaped by patterns of <b>visual</b> <b>stimuli,</b> a plasticity likely mediated by synaptic modifications. In the adult cat, asynchronous <b>visual</b> <b>stimuli</b> in two adjacent retinal regions controlled the relative spike timing of two groups of cortical neurons with high precision. This asynchronous pairing induced rapid modifications of intracortical connections and shifts in receptive fields. These changes depended on the temporal order and interval between <b>visual</b> <b>stimuli</b> {{in a manner consistent}} with spike timing–dependent synaptic plasticity. Parallel to the cortical modifications found in the cat, such asynchronous <b>visual</b> <b>stimuli</b> also induced shifts in human spatial perception. Persistent synaptic modification can be induced by repetitive pairing of pre- and postsynaptic spikes, and the direction and magnitude of the modification depend on the relative spike timing (1 – 6). Presynaptic spikin...|$|R
40|$|Are faces {{processed}} {{differently from}} other complex <b>visual</b> <b>stimuli?</b> For {{this to be}} the case, three main criteria would have to be fulfilled: (1) face recognition would exhibit functional characteristics not found in the recognition of other <b>visual</b> <b>stimuli,</b> (2) the neural machinery that mediates face recognition would be anatomically separate from the neurons mediating general object recognition, and (3) faces would be represented differently from other <b>visual</b> <b>stimuli</b> at the neural level. This paper assesses the data bearing on these criteria and discusses whether they do indeed constitute evidence for a special face processing system...|$|R
