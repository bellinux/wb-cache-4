419|2467|Public
2500|$|The College Board also {{states that}} use of the SAT in {{combination}} with high school grade point average (GPA) provides a better indicator of success in college than high school grades alone, as measured by college freshman GPA. Various studies conducted over the lifetime of the SAT show a statistically significant increase in correlation of high school grades and college freshman grades when the SAT is factored in. A large independent <b>validity</b> <b>study</b> on the SAT's ability to predict college freshman GPA was performed by the University of California. The {{results of this study}} found how well various predictor variables could explain the variance in college freshman GPA. [...] It found that independently high school GPA could explain 15.4% of the variance in college freshman GPA, SAT I (the SAT Math and Verbal sections) could explain 13.3% of the variance in college freshman GPA, and SAT II (also known as the SAT subject tests; in the UC's case specifically Writing, Mathematics IC or IIC, plus a third subject test of the student's choice) could explain 16% of the variance in college freshman GPA. [...] When high school GPA and the SAT I were combined, they explained 20.8% of the variance in college freshman GPA. When high school GPA and the SAT II were combined, they explained 22.2% of the variance in college freshman GPA. [...] When SAT I was added to the combination of high school GPA and SAT II, it added a [...]1 percentage point increase in explaining the variance in college freshman GPA for a total of 22.3%.|$|E
5000|$|Donnelly F.A., The Luscher Color Test: A <b>Validity</b> <b>Study,</b> Perceptual and Motor Skills, Volume 44, Issue 1, pp. 17 - 18 ...|$|E
5000|$|... “Narcissistic {{personality}} disorder: A <b>validity</b> <b>study</b> {{and comparison}} to borderline personality disorder.” “Journal of the American Academy of Psychoanalysis”. Vol 12(3) (1989). Cited in 45 publications.|$|E
50|$|<b>Validity</b> <b>Studies</b> {{were carried}} out with NEPSY, WISC-IV, DAS—II, WNV, WIAT—III, CMS, DKEFS, BBCS:3R, DSMD, ABAS—II, Brown ADD Scales and CCC-2.|$|R
40|$|Colleges use SAT ® I {{scores and}} the high school record to predict success in college. <b>Validity</b> <b>studies</b> are {{conducted}} to determine {{the effectiveness of these}} predictors of success in college. The relationship of the predictors to an appropriate criterion of college success is an indicator of effectiveness. The relationship between predictors—such as the SAT I and high school grades—and a criterion of college success is usually measured by computing a correlation coefficient. Correlation coefficients range in absolute value from 0 to 1, with 0 representing no association and 1 indicating a perfect association. In <b>validity</b> <b>studies,</b> correlation coefficients are sometimes called validity coefficients. Higher correlations reflect stronger associations between the predictors and the criterion. Different outcomes may serve as criteria for determining success in college. There is no one agreed upon measure of college success. Academic achievement is most frequently used as a criterion in validating admission tests. Measures of academic achievement that have served as criteria for such <b>validity</b> <b>studies</b> include course grades, grade-point averages (GPA), graduation, attrition or persistence, promotion, teacher ratings, and special awards or honors. Researchers have <b>studied</b> the <b>validity</b> of the SAT I and its predecessor, the SAT, for more than 70 years through hundreds of <b>validity</b> <b>studies</b> conducted at various colleges employing the SAT in their admission process. The overwhelming majority of these studies use the high school record (i. e., grade averages, rank) and SA...|$|R
30|$|Messick’s {{expansion}} of validity to include values and social consequences of testing practices (Messick 1989) {{is related to}} the social and political dimensions of tests, which paved the way for developing argumentative frameworks for <b>validity</b> <b>studies.</b>|$|R
50|$|There {{are five}} key {{principles}} relating to internal <b>validity</b> (<b>study</b> design) and external validity (generalizability) which rigorous impact evaluations should address: confounding factors, selection bias, spillover effects, contamination, and impact heterogeneity.|$|E
5000|$|... http://www.cicslloydbond.org==Bond, L., Smith, T., Baker, W. K., & Hattie. J. (2000). The {{certification}} {{system of the}} National Board for Professional Teaching Standards: A construct and consequential <b>validity</b> <b>study,</b> Center for Educational Research and Evaluation, University of North Carolina, Greensboro.|$|E
50|$|The CLA lacks {{instrumental}} validity {{to measure}} individual performance. This concern, however, {{may have been}} partially addressed by a 2009 test <b>validity</b> <b>study</b> organized by the Fund for the Improvement of Postsecondary Education (FIPSE). The results showed that while these tests {{should not be used}} as a basis to make institutional decisions about students as individuals (e.g., promotion or course placement), when aggregated in larger samples they can provide reliable estimates of institutional or group-level differences in performance on these tasks.|$|E
40|$|This study {{examined}} (a) {{the academic and}} nonacademic criteria used by admission personnel to determine the eligibility of undergraduate applicants with learning disabilities, (b) agreement of criteria used by institutions of varying competitiveness, and (c) {{the frequency with which}} admission personnel conduct <b>validity</b> <b>studies</b> on these criteria. A nonrandom sample consisting of 66 state universities and colleges in the Northeast was surveyed. The results suggest that the academic and nonacademic criteria employed are similar to those employed for applicants without learning disabilities, the criteria used by different types of institutions differ significantly, and admission personnel do not conduct <b>validity</b> <b>studies.</b> Further clarification and validation of admission criteria appears warranted...|$|R
40|$|This insert {{introduces}} valides, validesi, and validesu: {{a set of}} commands {{that allow}} the calculations of point and precision estimates of sensitivity and specificity obtained in <b>validity</b> <b>studies</b> with incomplete designs. Copyright 2002 by Stata Corporation. validity,sensitivity,specificity,incomplete design...|$|R
40|$|The NAEP <b>Validity</b> <b>Studies</b> Panel {{was formed}} by the American Institutes for Research under {{contract}} with the National Center for Education Statistics (NCES). This report is based on work that was jointly supported by NCES (contract ED- 01 -CO- 0026 - 005) and the National Science Foundation (grant 454755). Any opinions, findings, and conclusions or recommendations expressed in this report {{are those of the}} authors and do not necessarily reflect the views of NCES or the National Science Foundation. The NAEP <b>Validity</b> <b>Studies</b> (NVS) Panel was formed in 1995 to provide a technical review of NAEP plans and products and to identify technical concerns and promising techniques worthy of further study and research. The members of the panel have been charged with writing focused studies and issue papers on the most salient of the identified issues...|$|R
50|$|Today, the GMAT Exam {{is used by}} {{more than}} 6,000 {{graduate}} management education programs at approximately 2,100 business schools worldwide. The exam, taken more than 230,000 times per year, is designed as an objective predictor of how well a student will perform academically {{in the first year}} of a graduate business education program. Through the <b>Validity</b> <b>Study</b> Service (VSS), the GMAT has been shown by researchers to be a reliable predictor of academic performance in business school, especially when used in combination with an applicant's undergraduate grade point average. In June 2012, a new section was added to the GMAT Exam called Integrated Reasoning. This section is designed to measure a test taker's ability to evaluate information presented in new formats from multiple sources.|$|E
5000|$|The College Board also {{states that}} use of the SAT in {{combination}} with high school grade point average (GPA) provides a better indicator of success in college than high school grades alone, as measured by college freshman GPA. Various studies conducted over the lifetime of the SAT show a statistically significant increase in correlation of high school grades and college freshman grades when the SAT is factored in. A large independent <b>validity</b> <b>study</b> on the SAT's ability to predict college freshman GPA was performed by the University of California. The {{results of this study}} found how well various predictor variables could explain the variance in college freshman GPA. It found that independently high school GPA could explain 15.4% of the variance in college freshman GPA, SAT I (the SAT Math and Verbal sections) could explain 13.3% of the variance in college freshman GPA, and SAT II (also known as the SAT subject tests; in the UC's case specifically Writing, Mathematics IC or IIC, plus a third subject test of the student's choice) could explain 16% of the variance in college freshman GPA. When high school GPA and the SAT I were combined, they explained 20.8% of the variance in college freshman GPA. When high school GPA and the SAT II were combined, they explained 22.2% of the variance in college freshman GPA. When SAT I was added to the combination of high school GPA and SAT II, it added a [...]1 percentage point increase in explaining the variance in college freshman GPA for a total of 22.3%.|$|E
40|$|The {{purpose of}} this study was to develop an {{instrument}} to examine students ' behaviors in physical education classes that might have an impact on classroom management and student learning. The study was conducted in multiple phases, including instrument development, preliminary study, content <b>validity</b> <b>study,</b> and a reliability and <b>validity</b> <b>study.</b> Participants for the content <b>validity</b> <b>study</b> were 27 experts in sport pedagogy. They were sent items from the management instrument and were asked to organize them into three severity categories. Participants for the reliability and <b>validity</b> <b>study</b> were 2, 309 middle and high school students from 2 states. Statistical results indicate that an instrument that produces reliable and valid scores was developed to measure students ' perceptions of classroom management in physical education settings. Investigating students ' views in this area will lead to a better understanding of management issues and behaviors that interfere with students ' ability to learn. (Contains 3 tables and 3...|$|E
30|$|Factorial <b>validity</b> was <b>studied</b> of {{questionnaire}} {{with using}} Confirmative Factor Analysis.|$|R
30|$|To {{assessment}} of discrimination, VISA-P-Tr scores compared all groups using the Mann–Whitney-U test. SPSS ver 11.5 (SPSS Inc., Chicago, IL, USA) {{was used for}} statistical analysis. Convergent <b>validity</b> was <b>studied</b> of questionnaire with using Pearson correlation test. Factorial <b>validity</b> was <b>studied</b> of questionnaire with using Confirmative Factor Analysis. Goodness of fit indexes was also given.|$|R
40|$|Various {{questionnaire}} {{measures have}} been developed to study body attitude and affect. The Body Cathexis Scale (BCS) was the first psychometric instrument devised to measure body dissatisfaction. In this study we report the validation of the Italian version of the BCS. Reliability and <b>validity</b> <b>studies</b> were conducted on 86 female subjects with eating disorders and 404 general population subjects...|$|R
40|$|AbstractIn {{recent years}} {{employer}} branding {{has become increasingly}} important {{as a source of}} sustainable competitive advantage. Companies are trying to engender affective commitment in the best employees in a global labour market. In this study, we develop and validate a multidimensional scale to measure the strength of an employee's affective commitment to the employer brand in five separate studies. In Studies 1 and 2 the Affective Commitment to the Employer Brand (ACEB) scale was developed and tested for its structure, reliability and convergent <b>validity.</b> <b>Study</b> 3 examines additional reliability and discriminant <b>validity.</b> <b>Study</b> 4 provides evidence of external <b>validity.</b> <b>Study</b> 5 examines the scale's nomological validity showing that a positive experience with the employer brand is important in making the employee develop affective commitment towards it. The limitations of the scale and the boundary conditions of its applicability are also discussed...|$|E
40|$|This {{dissertation}} {{studies the}} antecedents {{and consequences of}} the flow experience in online retailing environments. Flow is the enjoyable and engrossing experience that people feel when acting with total involvement. A review of previous studies suggests that applying the notion of flow to understand the online consumer experience is a promising but underdeveloped field, with several conceptual and methodological issues. This dissertation attempts to contribute {{to our understanding of}} flow in online shopping in three ways. First, a three-part <b>validity</b> <b>study</b> was carried out using different approaches to construct validity and involving two sets of two flow measures: the Flow State Scale (FSS, Jackson and Marsh 1996) and the Internet Flow Scale (IFS). The first study related flow to behavioral criteria in online shopping. The second conducted a traditional construct <b>validity</b> <b>study</b> in which we developed and tested a “nomological network ” of relationships between flow measures and other logically-related constructs. This study also included a Multitrait-Multimethod <b>validity</b> <b>study.</b> The third was ai...|$|E
30|$|CTQ was {{developed}} by Bernstein et al. (1997) to evaluate the existence and type of childhood traumas. Its reliability and <b>validity</b> <b>study</b> in Turkish {{was carried out by}} Şar et al. (2012).|$|E
40|$|Many past {{educational}} <b>validity</b> <b>studies</b> {{of business}} gaming simulation, and more specifi-cally total enterprise simulation, have been inconclusive. Studies {{have focused on}} the weaknesses of business gaming simulation; which is often regarded as an educational medium that has limitations regarding learning effectiveness. However, no {{attempts have been made to}} provide guidelines for assessing educational validity in terms of building, implementing, and validating business gaming simulation. Accordingly, this study has combined literature on learning, simulation design, and research methods to formulate a methodology to assess the educational validity of total enterprise simula-tion; the concepts of which can be applied more broadly to business gaming simulation. The authors propose that the framework introduced within this article can provide a foundation for future educational <b>validity</b> <b>studies</b> that can assist simulation designers to implement valuable and powerful simulation learning media in the future. Keywords business gaming, business gaming simulation, business simulation, construct validity...|$|R
40|$|Initial {{evidence}} for {{the reliability and validity}} of a scale to assess social perception, the Social Perception Behavior Rating Scale (SPBRS), is presented. In addition to a reliability <b>study,</b> three <b>validity</b> <b>studies</b> are reported: (a) a study of difference in group means between mildly handicapped and nonhandicapped children; (b) a study correlating SPBRS scores with three other social behavior scales-the Hahnemann Elementary Schoo...|$|R
2500|$|Landy (2005) {{claimed that}} the few [...] <b>validity</b> <b>studies</b> {{conducted}} on EI have shown that it adds {{little or nothing to}} the explanation or prediction of some common outcomes (most notably academic and work success). Landy suggested that the reason why some studies have found a small increase in predictive validity is a methodological fallacy, namely, that alternative explanations have not been completely considered: ...|$|R
40|$|Background: The {{assessment}} of food intake is challenging {{and prone to}} errors; it is therefore important to consider {{the reliability and validity}} of the assessment methods. Objective: The aim of this study was to analyze the reproducibility and validity of a developed food-frequency questionnaire (FFQ) for use among adolescents. Design: In total, 58 students (aged 13 – 14) from four different schools in the southern part of Norway participated in the reproducibility study of filling out the FFQ 4 weeks apart. In addition, 93 students participated in the relative <b>validity</b> <b>study</b> where the FFQ was compared to 2 × 24 -hour dietary recalls, while 92 students participated in the absolute <b>validity</b> <b>study</b> where the intakes of fatty acids and vitamin D from the FFQ were compared to fatty acids and 25 -hydroxy-vitamin D 3 in whole blood. Results: The median Spearman correlation coefficient for all nutrients in the test–retest reliability study was 0. 57. The median Spearman correlation for all nutrients in the relative <b>validity</b> <b>study</b> was 0. 26, while the correlations coefficients were low in the absolute <b>validity</b> <b>study</b> with n- 3 fatty acid coefficients ranging from 0. 05 to 0. 25, and absent for vitamin D (r= 0. 000). Conclusion: The test–retest reproducibility was considered good, the relative validity was considered poor to good, and the absolute validity was considered poor. However, the results are comparable to other studies among adolescents...|$|E
40|$|Published {{version of}} an article from the journal: Food & Nutrition Research. Also {{available}} from the publisher: [URL] The assessment of food intake is challenging and prone to errors; it is therefore important to consider {{the reliability and validity}} of the assessment methods. Objective: The aim of this study was to analyze the reproducibility and validity of a developed food-frequency questionnaire (FFQ) for use among adolescents. Design: In total, 58 students (aged 13 14) from four different schools in the southern part of Norway participated in the reproducibility study of filling out the FFQ 4 weeks apart. In addition, 93 students participated in the relative <b>validity</b> <b>study</b> where the FFQ was compared to 2 24 -hour dietary recalls, while 92 students participated in the absolute <b>validity</b> <b>study</b> where the intakes of fatty acids and vitamin D from the FFQ were compared to fatty acids and 25 -hydroxy-vitamin D 3 in whole blood. Results: The median Spearman correlation coefficient for all nutrients in the test retest reliability study was 0. 57. The median Spearman correlation for all nutrients in the relative <b>validity</b> <b>study</b> was 0. 26, while the correlations coefficients were low in the absolute <b>validity</b> <b>study</b> with n- 3 fatty acid coefficients ranging from 0. 05 to 0. 25, and absent for vitamin D (r 0. 000). Conclusion: The test retest reproducibility was considered good, the relative validity was considered poor to good, and the absolute validity was considered poor. However, the results are comparable to other studies among adolescents...|$|E
30|$|The {{current study}} {{is based on}} a single case which may serve as a {{limitation}} to extrapolate findings to different settings. A <b>validity</b> <b>study</b> is being planned in the near future to establish the advantages of the new incision.|$|E
50|$|Concurrent {{validity}} {{may be used}} as {{a practical}} substitute for predictive validity. In the example above, predictive validity would be the best choice for validating an employment test, because using the employment test on existing employees may not be a strong analog for using the tests for selection. Reduced motivation and restriction of range are just two possible biasing effects for concurrent <b>validity</b> <b>studies.</b>|$|R
40|$|Objective: The {{purpose of}} this study was to {{systematically}} review the literature evaluating the reliability and validity of all available methods for measuring active and passive cervical range of motion (CROM). Methods: Electronic databases (PubMed, MEDLINE, CINAHL, EMBASE, and AMED) were searched through OVID from their inception to January 2008. Articles were selected according to a priori defined criteria. Data were extracted regarding publication details, type of study, movements and device evaluated, subject and observer characteristics, and measurement protocol including blinding and statistical analysis methods. Quality assessment was undertaken using developed criteria to assess internal validity, external validity, and statistical methods. An estimate of the level of reliability and validity was calculated and used to categorize studies as good, moderate, or poor. Results: A total of 56 articles fulfilled the selection criteria and were included in the review. Forty-six of these articles described 66 reliability studies and 21 described 21 concurrent criterion <b>validity</b> <b>studies.</b> Twelve different methods were evaluated. Although it was the intention of this review to conduct meta-analysis, this was deemed inappropriate due to studies being too heterogeneous. Most of the reliability and <b>validity</b> <b>studies</b> involved asymptomatic subjects measured by allied health professionals investigating active ROM. Devices that were deemed to have "good" reliability and validity were the CROM device, the Spin-T goniometer, and the single inclinometer. Conclusions: A considerable number of reliability and concurrent <b>validity</b> <b>studies</b> have been published for CROM. The CROM device has undergone most evaluation and has been shown to be clinimetrically sound. Further research with significantly improved methodology and reporting is warranted for all devices. (J Manipulative Physiol Ther 2010; 33 : 138 - 155) ...|$|R
40|$|U. S. Department of Education or the American Institutes for Research. The NAEP <b>Validity</b> <b>Studies</b> (NVS) Panel {{was formed}} in 1995 to provide a {{technical}} review of NAEP plans and products and to identify technical concerns and promising techniques worthy of further study and research. The members of the panel {{have been charged with}} writing focused studies and issue papers on the most salient of the identified issues...|$|R
40|$|Recently, it {{has been}} {{proposed}} that a General Factor of Personality (GFP) occupies {{the top of the}} hierarchical personality structure. We present a meta-analysis (K = 212, total N = 144, 117) on the intercorrelations among the Big Five personality factors (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) to test for the existence of a GFP. In addition, we report a multi-method <b>validity</b> <b>study</b> testing the relationship between the GFP and supervisor-rated job performance. The meta-analysis provided supporting evidence for the two meta-factors Stability and Plasticity (or α and β, respectively) and a GFP at the highest hierarchal level. The <b>validity</b> <b>study</b> indicated that the GFP has a substantive component as it is related to supervisor-rated job performance...|$|E
40|$|Objective: The Depression Interview and Structured Hamilton (DISH) is a semistructured {{interview}} {{developed for}} the Enhancing Recovery in Coronary Heart Disease (ENRICHD) study, a multicenter clinical trial of treatment for depression and low perceived social support after acute myocardial infarction. The DISH is designed to diagnose depression in medically ill patients and to assess its severity on an embedded version of Williams ’ Structured Interview Guide for the Hamilton Depression scale (SIGH-D). This article describes the development and characteristics of the DISH and presents a <b>validity</b> <b>study</b> and data on its use in ENRICHD. Methods: In the <b>validity</b> <b>study,</b> the DISH and the Structured Clinical Interview for DSM-IV (SCID) were administered in randomized order to 57 patients. Trained interviewers administered the DISH, and clinicians administered the SCID. In ENRICHD, trained research nurses administered the DISH and recorded a diagnosis. Clinicians reviewed 42 % of the interviews and recorded their own diagnosis. The Beck Depression Inventory (BDI) was administered in both studies. Results: In the <b>validity</b> <b>study,</b> the SCID diagnosis agreed with the DISH on 88 % of the interviews (weighted 0. 86). In ENRICHD, the clinicians agreed with 93 % of the research nurses’ diagnoses. The BDI and the Hamilton depression scores derived from the DISH in the two studies correlated 0. 76 (p. 0001) in the <b>validity</b> <b>study</b> and 0. 64 (p. 0001) in ENRICHD. Conclusions: These findings support {{the validity of the}} DISH as a semistructured interview to assess depression in medically ill patients. The DISH is efficient in yielding both a DSM-IV depression diagnosis and a 17 -item Hamilton depression score. Key words: depressive disorder, psychiatri...|$|E
30|$|We {{conducted}} {{the construct validity}} of the Turkish FoMOs with confirmatory factor analysis, measurement invariance (study I; n[*]=[*] 354), and concurrent <b>validity</b> (<b>study</b> II; n[*]=[*] 371). We also evaluated the reliability of the Turkish FoMOs (study III; n[*]=[*] 61) using test-retest and Cronbach alpha reliability.|$|E
30|$|In {{recent decades}} {{a growing number}} of scale <b>validity</b> <b>studies</b> have {{employed}} modern test theory such as item response theory (IRT) focusing on the issues of item fit, DIF and requirements for measurement. Rasch models (RM) is a family of IRT models for dichotomous and polytomous items [38]. The Rasch family of models include the original RM for dichotomous items [39], the generalization of this into the polytomous RM [40], and the GLLRM [28].|$|R
5000|$|... 1944: {{integration}} of the Institute within the Conservatory of music and Declamation. Development of dance studios. Recognition of official <b>validity</b> of <b>studies.</b>|$|R
30|$|One {{version of}} the EPE was used from 1998 to 2005, while another version was used from 2006 to 2011. Both {{versions}} of the EPE were identical in terms of structure and have equal numbers of corresponding items and questions. Content <b>validity</b> <b>studies,</b> correlation studies, and item analysis comparisons verify that the {{two versions of the}} EPE used in this study were equivalent in terms of test content, item difficulty and item discrimination (sees Sims and Liu 2013).|$|R
