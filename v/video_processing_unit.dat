14|10000|Public
5000|$|VPU (<b>Video</b> <b>Processing</b> <b>Unit)</b> {{multimedia}} processor supporting 1080p {{image and}} video decoding ...|$|E
5000|$|VPU (<b>Video</b> <b>Processing</b> <b>Unit)</b> {{supporting}} 1080p {{image and}} video decoding for H.264, Xvid, H.263, AVS, MPEG4, RV, and WMV ...|$|E
50|$|The i.MX31 was {{launched}} in 2005. It integrates a 532 MHz ARM1136 CPU platform (with vector floating point unit, L1 caches and 128KB L2 caches) + <b>Video</b> <b>Processing</b> <b>Unit</b> (VPU) + 3D GPU (OpenGL ES 1.1) + IPU + security block It supports mDDR-SDRAM at 133 MHz.|$|E
5000|$|PowerVR - {{developing}} {{traditional and}} ray tracing GPUs, <b>video</b> <b>processing</b> <b>units,</b> and camera/ISP IP core technologies (also see List of PowerVR products) ...|$|R
5000|$|Vision <b>processing</b> <b>units</b> are {{distinct}} from <b>video</b> <b>processing</b> <b>units</b> (which are specialised for video encoding and decoding) in their suitability for running machine vision algorithms such as convolutional neural networks, SIFT etc.|$|R
50|$|This {{technical}} capability {{is used in}} a wide range of domains including entertainment, health-care, retail, automotive, transport, home automation, flame and smoke detection, safety and security. The algorithms can be implemented as software on general purpose machines, or as hardware in specialized <b>video</b> <b>processing</b> <b>units.</b>|$|R
50|$|Freescale {{proposes a}} layered {{approach}} of software with selection of software components optimized for its chips. The i.MX board support packages (BSP), common across all i.MX nodes, consists of kernel optimization, hardware drivers and unit tests. The company {{also provides a}} portfolio of multimedia Codecs (ARM and <b>Video</b> <b>processing</b> <b>unit</b> accelerated). The i.MX solution also includes middleware with reuse of open source frameworks like multimedia framework plugins, power management, security/DRM or graphics (OpenGL/OpenVG).|$|E
50|$|The {{high-end}} {{member of}} the family, i.MX515, integrates an 800 MHz ARM Cortex A8 CPU platform (with NEON co-processor, Vector Floating Point Unit, L1 caches and 256KB L2 cache) + multi-format HD 720p decode / D1 encode hardware video codecs (VPU, <b>Video</b> <b>Processing</b> <b>Unit)</b> + Imageon 3D GPU (OpenGL ES 2.0) + 2.5D GPU (OpenVG 1.1) + IPU + security block.It especially supports DDR2 SDRAM at 200 MHz.The imx51 family was launched in 2009.|$|E
50|$|NEI {{help fund}} the first retinal implant device called Argus II Retinal Prosthesis System, {{developed}} in 2011 by Second Sight Inc. {{to treat people}} who have retinitis pigmentosa. Argus II is a camera that is mounted on eyeglasses. The image is captured through the camera and processed by the <b>video</b> <b>processing</b> <b>unit</b> that transmits electrical pulse images to the retinal prosthesis through eyeglasses. This helps people with retinitis pigmentosa potentially move around and be independent. The NEI, Department of Energy, and the National Science Foundation (NSF) provided the support {{for the development of}} Argus II.|$|E
50|$|They {{may include}} direct {{interfaces}} to take data from cameras (bypassing any off chip buffers), {{and have a}} greater emphasis on on-chip dataflow between many parallel execution units with scratchpad memory, like a manycore DSP. But, like <b>video</b> <b>processing</b> <b>units,</b> they may have a focus on low precision fixed point arithmetic for image processing.|$|R
40|$|The micro {{channels}} are important components in many micro - electro - mechanical systems (MEMS) and technical systems. They {{are used to}} transport a gas or a liquid into devices and the systems. In our work, the gas flows into a micro channel have been studied for various shapes and sizes of the micro channels. Numerical simulations of compressible flows through a micro channel are performed by solving the Navier–Stokes equations and the quasigasdynamic equations (QGD). The numerical approaches are realized on hybrid parallel computer systems. The software includes the Domain Decomposition technique, the Message Passing Interface (MPI) for organization of interprocess data exchange and an application programming interface Open Multi-Processing (OpenMP). The calculations are performed on the Heterogeneous Computer Systems with <b>Video</b> <b>Processing</b> <b>Units</b> (VPU) and the classical microprocessors - Central <b>Processing</b> <b>Unit</b> (CPU) ...|$|R
40|$|Abstract—The {{architecture}} of present <b>video</b> <b>processing</b> <b>units</b> in consumer systems is usually based on {{various forms of}} processor hardware, communicating with an off-chip SDRAM memory (see Fig. 1). Examples of these systems are currently available MPEG encoders and decoders, and high-end television systems. Due to the fast increase of required computational power of consumer systems, the data communication {{to and from the}} off-chip memory has become the bottleneck in the overall system performance (memory wall problem). This paper presents a strategy for mapping pixels into the memory for video applications such as MPEG processing, thereby minimizing the transfer overhead between memory and the processing. A novelty in our approach is that the proposed communication model considers the statistics of the application-dependent data accesses in memory. With this technique, a 26 % reduction of the memory bandwidth was obtained in an MPEG decoding system containing a 64 -bit wide memory bus. For double-data-rate SDRAM (DDR SDRAM), the proposed mapping strategy reduces the bandwidth in the system with even 50 %. This substantial performance improvement can readily be used for extending the quality or the functionality of the system...|$|R
50|$|Intel and AMD {{published}} {{a press release}} in December 2010 stating they would no longer support the LVDS LCD-panel interface in their product lines by 2013. They are promoting Embedded DisplayPort and Internal DisplayPort as their preferred solution. However, the LVDS LCD-panel interface {{has proven to be}} the lowest cost method for moving streaming video from a <b>video</b> <b>processing</b> <b>unit</b> to a LCD-panel timing controller within a TV or notebook, and in February 2012 LCD TV and notebook manufacturers continue to introduce new products using the LVDS interface.|$|E
5000|$|The i.MX {{range is}} a family of Freescale Semiconductor (now part of NXP) {{proprietary}} microcontrollers for multimedia applications based on the ARM architecture and focused on low-power consumption. The i.MX application processors are SoCs (System-on-Chip), that integrate many processing units into one die, like the main CPU, a <b>video</b> <b>processing</b> <b>unit</b> and a graphics processing unit for instance. The i.MX products are qualified for automotive, industrial and consumer markets. Most of them are guaranteed for a production lifetime of 10 to 15 years.Many devices use i.MX processors, such as Ford Sync, Kobo_eReader, Amazon Kindle, Sony Reader, Onyx Boox readers/tablets, SolidRun SOM's (including CuBox), some Logitech Harmony remote controls and Squeezebox radio, some Toshiba Gigabeat mp4 players. The i.MX range was previously known as the [...] "DragonBall MX" [...] family, the fifth generation of DragonBall microcontrollers. i.MX originally stood for [...] "innovative Multimedia eXtension".|$|E
40|$|A {{compressive}} {{sensing system}} for dynamic video acquisition. The system includes a video signal interface including a compressive imager configured to acquire compressive sensed video frame data from an object, a <b>video</b> <b>processing</b> <b>unit</b> including a processor and memory. The <b>video</b> <b>processing</b> <b>unit</b> is configured {{to receive the}} compressive sensed video frame data from the video signal interface. The memory comprises computer readable instructions that when executed by the processor cause the processor to generate a motion estimate from the compressive sensed video frame data and generate dynamical video frame data from the motion estimate and the compressive sensed video frame data. The dynamical video frame data may be output...|$|E
40|$|This paper {{presents}} {{the concept of}} virtual~instmmentation and its importance in test and evaluation of launch vehicle avionics. The experiences at the Vikram Sarabhai Space Centre (VSSC) with virtual instrumentation systems, highlighting the virtual instrumentation-based checkout systemsof pyro current monitoring package and <b>video</b> image <b>processing</b> <b>unit</b> are presented. The virtual instrumentation system-based checkouts present cost-effective, compact, and user-friendly human-machine interfaces for the test and evaluation of these packages. The issues of a common hardware-software platform for testing different telemetry packages and the capability of real-time virtual instruments for testing navigation, guidance, and control packages have been investigated...|$|R
40|$|In this paper, a fast sphere-tree {{generation}} {{method used}} for collision detection called Branch On-collide Sphere-trees is proposed. Using the <b>video</b> card graphic <b>processing</b> <b>unit</b> (GPU), a sphere-tree is constructed in real-time inside an animation. With this method, the core memory usage is minimized because no pre-computed data is loaded {{at any time}} during simulation life cycle. With our method, real-time conservative collision detection is achieved using the GPU, core memory is managed efficiently and the error is lowered using fast-construction sphere-tree structures. Postprint (published version...|$|R
50|$|The API enables and {{provides}} access to hardware-accelerated <b>video</b> <b>processing,</b> using hardware such as graphics <b>processing</b> <b>units</b> (GPU) to accelerate video encoding and decoding by offloading processing from the central <b>processing</b> <b>unit</b> (CPU).|$|R
40|$|Includes bibliographical {{references}} (page 79) This {{thesis is}} intended to study the techniques and {{to solve the problems}} in the application of video processing to an automatic robotic system. The contents of this thesis are: a. Description and explanation of the Video Processign Unit, how it is integrated with a television camera, television monitor, and a system controller, Apple IIe. b. Description and explanation of the Five-Axis Robot, TeachMover, and how it is operated by the system controller, the Apple IIe. c. Connection and operation of the video camera, video monitor, <b>Video</b> <b>Processing</b> <b>Unit,</b> robotic arm, and the controller, the Apple IIe. d. Flow chart of controlling program for Apple IIe. e. Presentation of experimental program. The basic components of the system are an Apple IIe personal computer with two serial ports, a <b>Video</b> <b>Processing</b> <b>Unit,</b> television camera, television monitor, lighting system, and the TeachMover Robotic Arm. The system was designed and developed and software programs written. Test results indicated the system operated properly and its performance satisfied the design objectives...|$|E
40|$|An EPLD based {{low cost}} {{transient}} recorder of video signal bandwidth (sampling rate 25 MHz) is presented. It {{will be used}} to record and replay video signals. These video signals are handled by a <b>video</b> <b>processing</b> <b>unit,</b> under construction in a VHDL environment. The main advantage of this self made transient recorder is that the video signal processing device, which is under construction in a VHDL environment, uses the same analog components as the EPLD based transient recorder. Thus, the video signal passes the same signal path and components in the final device. This results in a simulation environment very close to the final system environment. 1 Introduction The speed of EPLDs available now allow applications in the area of video signal processing. As in the present case, the integration density of current EPLDs allows one to build a low cost transient recorder with few EPLDs {{and a handful of other}} components. The relative low effort in realizing a prototype caused by using EPLDs jus [...] ...|$|E
40|$|The {{organization}} of video data-bases according to semantic content of data, {{is a key}} point in multimedia technologies. In fact, this would allow algorithms such as indexing and retrieval to work more efficiently. As an attempt to extract semantic information, efforts have been devoted in segmenting the video in shots and for each shot trying to extract informations such as representative video frame, etc. As a video sequence is constructed from a 2 -D projection of a 3 -D scene, processing video information only has shown its limitations especially in solving problems such as object identification or object tracking. Further not all information is contained in the video signal and more {{can be achieved by}} analyzing the audio signal as well. Information can be obtained from the audio signal either to confirm the results obtained by a <b>video</b> <b>processing</b> <b>unit</b> or to acquire information that cannot be extracted from video (such as presence of music). This paper presents a technique which combines video and audio information for classification and indexing purposes...|$|E
40|$|Classical vehicle {{tracking}} approaches for highway scenarios use a Kalman [...] filter {{with a single}} dynamic model optimised to a single driving manoeuvre. In contrast, the Interacting Multiple Model (IMM) filter allows for several parallel models which are combined to a weighted estimate. Choosing models for different driving modes such as constant speed, acceleration and strong acceleration changes permits the object state estimation to be optimised for highly dynamic driving manoeuvres. The paper describes the analysis of Stop&Go situations and the systematic parametrisation of the IMM method based on these statistics. The evaluation of the IMM approach is presented based on real sensor measurements of two laserscanners, a radar and a <b>video</b> image <b>processing</b> <b>unit.</b> The performance of the lateral estimation of the IMM is shown based on simulations...|$|R
40|$|The {{authors have}} {{developed}} a new measurement system which consisted of an ultrasonic velocity profile monitor and a <b>video</b> data <b>processing</b> <b>unit</b> in order to clarify its multi-dimensional flow characteristics in bubbly flow and to offer a data base to validate numerical codes for multi-dimensional two-phase flow, ill this paper, the proposed measurement system was applied for fully developed bubbly countercurrent flows in a vertical rectangular channel. At first, both bubble and water velocity profiles and void fraction profiles in the channel were investigated statistically, Next, turbulence intensity in the channel {{was defined as a}} standard deviation of velocity fluctuation in a continuous liquid phase, and the two-phase multiplier profile of turbulence intensity in bubbly countercurrent flows was clarified. [n addition, the distribution parameter and drift velocity used in the drift flux model were calculated directly from these profiles. 1...|$|R
40|$|Camera {{positioning}} {{units for}} surveillance applications are often mounted on mobile supports or vehicles. In such circumstances, {{the motion of}} the supporting base affects the camera field of view, thus making the task of pointing and tracking a specific target problematic, especially when using low cost devices that are usually not equipped with rapid actuators and fast <b>video</b> <b>processing</b> <b>units.</b> Visual tracking capabilities can be improved if the camera field of view is preliminarily stabilized against the movements of the base. Although some cameras available on the market are already equipped with an optical image stabilization (OIS) system, implemented either in the camera lenses or in the image sensor, these are usually too expensive to be installed on low-end positioning devices. A cheaper approach to image stabilization consists of stabilizing the camera motion using the motors of the positioning unit and the inertial measurements provided by a low-cost MEMS Inertial Measurement Unit (IMU). This paper explores the feasibility of applying such image stabilization system to a low cost pan-tilt-zoom (PTZ) camera positioning unit driven by hybrid stepper motors (HSMs), in order to aid the task of pointing and tracking of a specific target on the camera image plane. In the proposed solution, a two-level cascaded control structure, consisting of inner inertial stabilizing control loop and an outer visual servoing control loop, is used to control the PTZ unit. Several tests are carried out on a real device mounted on a moving table actuated by a 6 degrees-of-freedom pneumatic hexapod. Realistic motions are recreated by using the data recordings taken aboard of a patrolling ship...|$|R
40|$|Most of the retinal {{prostheses}} use a head-fixed {{camera and}} a <b>video</b> <b>processing</b> <b>unit.</b> Some studies proposed various image processing methods to improve visual perception for patients. However, previous studies only focused on using spatial information. The present study proposes a spatiotemporal pixelization method mimicking fixational eye movements to generate stimulation images for artificial retina arrays by combining spatial and temporal information. Input images were sampled with a resolution that was four {{times higher than the}} number of pixel arrays. We subsampled this image and generated four different phosphene images. We then evaluated the recognition scores of characters by sequentially presenting phosphene images with varying pixel array sizes (6 × 6, 8 × 8 and 10 × 10) and stimulus frame rates (10 Hz, 15 Hz, 20 Hz, 30 Hz, and 60 Hz). The proposed method showed the highest recognition score at a stimulus frame rate of approximately 20 Hz. The method also significantly improved the recognition score for complex characters. This method provides a new way to increase practical resolution over restricted spatial resolution by merging the higher resolution image into high-frame time slots...|$|E
40|$|One {{of the key}} {{requirements}} for mobile devices is to provide high-performance computing at lower power consumption. The processors used in these devices provide specific hardware resources to handle computationally intensive video processing and interactive graphical applications. Moreover, processors designed for low-power applications may introduce limitations on the availability and usage of resources, which present additional challenges to the system designers. Owing to the specific design of the JZ 47 x series of mobile application processors, a hybrid software-hardware implementation scheme for H. 264 /AVC encoder is proposed in this work. The proposed scheme distributes the encoding tasks among hardware and software modules. A series of optimization techniques are developed {{to speed up the}} memory access and data transferring among memories. Moreover, an efficient data reusage design is proposed for the deblock filter <b>video</b> <b>processing</b> <b>unit</b> to reduce the memory accesses. Furthermore, fine grained macroblock (MB) level parallelism is effectively exploited and a pipelined approach is proposed for efficient utilization of hardware processing cores. Finally, based on parallelism in the proposed design, encoding tasks are distributed between two processing cores. Experiments show that the hybrid encoder is 12 times faster than a highly optimized sequential encoder due to proposed techniques...|$|E
40|$|In {{this paper}} two {{improvements}} {{to speed up}} collision detection are described. Firstly, a method called oncollide sphere-tree, OCST for short, is presented. This approach works by detecting collisions among models with arbitrary geometry using the <b>video</b> card’s Graphics <b>Processing</b> <b>Units,</b> GPU. While candidate parts of colliding objects are being detected, the OCST is constructed for collision evaluation in parallel, at the same time. Thus, the OCST is created in real–time. Secondly, we have tested two kinds of triangulated representation models for the same original–objects. We have evaluated triangle–soup and triangle–strip models {{to speed up the}} algorithm response when computing collisions. The method has been described, implemented and tested for the two kinds of triangulated models, and the obtained results are shown. Postprint (published version...|$|R
40|$|The aim of {{the project}} was to extend an {{existing}} <b>video</b> <b>processing</b> library to harness the parallel computing power from multi-core processors and modern graphics <b>processing</b> <b>units</b> (GPUs). A heterogeneous scheduler is required to dis-tribute different processing stages in a <b>video</b> <b>processing</b> pipeline to CPU cores and the GPU properly. Various image processing stages need to be implemented in CUDA – a GPU-oriented parallel computing architecture. Work Completed Two different schedulers were implemented and integrated into the existing <b>video</b> <b>processing</b> library. GPU-enabled processing pipelines on 2 D separable convolu-tion and winner-take-all stereo correspondence were implemented in CUDA and incorporated with the new schedulers. Evaluations on the schedulers and the GPU algorithms were carried out to examine the performance of the enhanced library...|$|R
40|$|Graphic <b>Processing</b> <b>Units</b> have {{during the}} recent years evolved into {{inexpensive}} high-performance many-core computing units. Earlier being accessible only by graphic APIs, new hardware architectures and programming tools {{have made it possible}} to program these devices using arbitrary data types and standard languages like C. This thesis investigates the development process and performance of image and <b>video</b> <b>processing</b> algorithms on graphic <b>processing</b> <b>units,</b> regardless of vendors. The tool used for programming the graphic <b>processing</b> <b>units</b> is OpenCL, a rela- tively new specification for heterogenous computing. Two image algorithms are investigated, bilateral filter and histogram. In addition, an attempt have been tried to make a template-based solution for generation and auto-optimalization of device code, but this approach seemed to have some shortcomings to be usable enough at this time...|$|R
40|$|Fiber {{tracking}} is {{a technique}} that, based on a diffusion tensor magnetic resonance imaging dataset, locates the fiber bundles in the human brain. Because it is a computationally expensive process, the interactivity of current fiber tracking tools is limited. We propose a new approach, which we termed real-time interactive fiber tracking, which aims at providing a rich and intuitive environment for the neuroradiologist. In this approach, fiber tracking is executed automatically every time the user acts upon the application. Particularly, when the volume of interest from which fiber trajectories are calculated is moved on the screen, fiber tracking is executed, even while it is being moved. We present our fiber tracking tool, which implements the real-time fiber tracking concept by using the <b>video</b> card’s graphics <b>processing</b> <b>units</b> to execute the fiber tracking algorithm. Results show that real-time interactive fiber tracking is feasible on computers equipped with common, low-cost video cards...|$|R
40|$|This paper {{presents}} Phase Only Correlation (POC) {{methods in}} hierarchical search motion estimation for high resolution digital <b>video</b> using Graphical <b>Processing</b> <b>Unit</b> (GPU). Using the POC function, one can estimate the translational displacement {{as well as}} the degree of similarity between two image blocks from the location and height of the correlation peak, respectively[1]. Motion Estimation is a process for defining object movement in digital video sequences. Motion Estimation is a system used in some field such as image <b>processing,</b> image analysis, <b>video</b> coding, and computer vision. A POC based hierarchical search is a high cost algorithm results in long processing time, thus the system developed in this paper proceed POC function in Graphical <b>Processing</b> <b>Unit</b> using parallel threading technology. The evaluation counts processing time speed of the methods using Graphical <b>Processing</b> <b>Unit</b> in high definition video with 1280 x 720 pixel resolution. The results show that the methods using GPU performs accelerating speed more than two times faster processing 2 layer hierarchical search in 256 x 256 POC block size than doing the same methods using CPU. Using the NVidia GeForce 9600 GT GPU, kernel execution with 256 thread per block, 9 32 -bit register per thread, and 36 bytes of memory shared for every thread block, the multiprocessor maximum occupancy is 100 %, with 768 active threads per multiprocessor, 24 Active Warps per multiprocessor, and 3 active thread blocks per multiprosessor. Key words...|$|R
40|$|Real time {{processing}} for teamwork action recognition is a challenge, due to complex computational models to achieve high system performance. Hence, this paper proposes a framework based on Graphical <b>Processing</b> <b>Units</b> (GPUs) {{to achieve a}} significant speed up {{in the performance of}} role based activity recognition of teamwork. The framework can be applied in various fields, especially athletic and military applications. Furthermore, the framework can be customized for many action recognition applications. The paper presents the stages of the framework where GPUs are the main tool for performance improvement. The speedup is achieved by performing <b>video</b> <b>processing</b> and Machine learning algorithms on GPU. <b>Video</b> <b>processing</b> and machine learning algorithms covers all computations involved in our framework. <b>Video</b> <b>processing</b> tasks on involves GPU implementation of Motion detection, segmentation and object tracking algorithms. In addition, our framework is integrated with GPUCV, a GPU version of OpenCV functions. Machine learning tasks are supported under our framework with GPU implementations of Support Vector Machine (SVM) for object classification and feature discretization, Hidden Marcov Model (HMM) for activity recognition phase, and ID 3 algorithm for role recognition of team members. The system was tested against UC-Teamwork dataset and speedup of 20 X has been achieved on NVidia 9500 GT graphics card (32 500 MHZ processors). Comment: 7 page...|$|R
5000|$|... Movidius's latest Myriad 2 chip is an {{always-on}} manycore Vision <b>processing</b> <b>unit</b> {{that can}} function on power constrained devices. It is a heterogeneous architecture, combining twelve SHAVE (Streaming Hybrid Architecture Vector Engine) 128bit VLIW SIMD processors {{connected to a}} multiported Scratchpad memory, a pair of LEON4 UltraSPARC ISA processors for control, {{and a number of}} fixed function units to accelerate specific <b>video</b> <b>processing</b> tasks (such as small Convolutions and color conversion lookups). It includes camera interface hardware, bypassing the need for external memory buffers when handling realtime image inputs. In terms of software, a Visual programming language allows workflows to be devised, and there is support for OpenCL.|$|R
40|$|This work {{presents}} {{the integration of}} several IPs to generate a system-on-chip (SoC) for digital television set-top box compliant to the SBTVD standard. Embedded consumer electronics for multimedia applications like <b>video</b> <b>processing</b> systems require large storage capacity and high bandwidth memory. Also, those systems are built from heterogeneous <b>processing</b> <b>units,</b> designed to perform specific tasks {{in order to maximize}} the overall system efficiency. A single off-chip memory is generally shared between the <b>processing</b> <b>units</b> to reduce power and save costs. The external memory access is one bottleneck when decoding high-definition video sequences in real time. In this work, a four-level memory hierarchy was designed to manage the decoded video in macroblock granularity with low latency. The use of the memory hierarchy in the system design is challenging because it impacts the system integration process and IP reuse in a collaborative design team. Practical strategies used to solve integration problems are discussed in this text. The SoC architecture was validated and is being progressively prototyped using a Xilinx Virtex- 5 FPGA board...|$|R
50|$|According to the Zii website the Zii EGG {{could provide}} 1080p video, {{and the media}} engine for <b>video</b> codec, media <b>processing</b> and 3D {{graphics}} acceleration is an array of floating-point processors (most likely some kind of digital signal processor cores) called StemCells. The ZMS-05 used in the Zii EGG has 24 of these <b>processing</b> <b>units.</b>|$|R
40|$|Abstract:- This paper {{discusses}} the programmable and dedicated approaches for real-time <b>video</b> <b>processing</b> applications. Various VLSI architecture including the design examples of both approaches are reviewed. Finally, discussions of several practical designs in real-time <b>video</b> <b>processing</b> applications are then considered in VLSI architectures to provide significant guidelines to VLSI designers for any further real-time <b>video</b> <b>processing</b> design works. Key-Words:- Dedicated, programmable, Very Large Scale Integration (VLSI) architecture, <b>video</b> <b>processing...</b>|$|R
