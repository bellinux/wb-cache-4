25|466|Public
50|$|The shear warp {{approach}} to volume rendering {{was developed by}} Cameron and Undrill, popularized by Philippe Lacroute and Marc Levoy. In this technique, the <b>viewing</b> <b>transformation</b> is transformed such that the nearest face of the volume becomes axis aligned with an off-screen image buffer with a fixed scale of voxels to pixels. The volume is then rendered into this buffer using the far more favorable memory alignment and fixed scaling and blending factors. Once all slices of the volume have been rendered, the buffer is then warped into the desired orientation and scaled in the displayed image.|$|E
50|$|In 3D {{computer}} graphics, {{the image}} plane is that {{plane in the}} world which is identified with {{the plane of the}} monitor. It is also referred to as screen space. If one makes the analogy of taking a photograph to rendering a 3D image, the surface of the film is the image plane. In this case, the <b>viewing</b> <b>transformation</b> is a projection that maps the world onto the image plane. A rectangular region of this plane, called the viewing window or viewport, maps to the monitor. This establishes the mapping between pixels on the monitor and points (or rather, rays) in the 3D world. The plane is not usually an actual geometric object in a 3D scene, but instead is usually a collection of target coordinates or dimensions that are used during the rasterization process so the final output can be displayed as intended on the physical screen.|$|E
3000|$|Apply <b>viewing</b> <b>transformation</b> X to each image—note {{that this}} leaves the image samples with floating-point values with a nominal range from 0 to 1 [...]...|$|E
50|$|Orthographic and {{perspective}} <b>viewing</b> <b>transformations.</b>|$|R
40|$|Abstract. XML is {{becoming}} the standard data exchange format. <b>View</b> or <b>transformation</b> of XML data is important and frequent operation in XML data integration and publishing. In schema-based <b>view</b> <b>transformation,</b> users define <b>view</b> schema over sources to obtain view results. This declarative approach alleviates user from writing complex scripts to perform <b>view</b> <b>transformation.</b> Current available schema formats are unable to express views with complex semantic constraints. In this paper, we introduce a semantically expressive XML data model: Object-Relationship-Attribute model for Semi-Structured data (ORA-SS), which allows users to define view schemas with rich semantic meanings. Combine with ORA-SS, we use a native XML DBMS: OrientStore to perform accurate and efficient <b>view</b> <b>transformation.</b> ...|$|R
30|$|The metric {{should have}} the ability to be {{evaluated}} on the decompressed image after it has been subjected to one or more diagnostic <b>viewing</b> <b>transformations.</b> These transformations would typically be windowing functions that are commonly used with the radiological investigation in question. Note that <b>viewing</b> <b>transformations</b> can subsume other specifics of the investigation, such as bit depth and even sample value histograms. To make this so, <b>viewing</b> <b>transformations</b> should be expressed with respect to a notional display device in which the transformed data are rendered as black when 0 and maximally bright when 1, with a prescribed display gamma function—for that, a gamma value of one-third would seem appropriate since it correlates rather well with human perception. Conversions required to map imagery at some particular bit depth to the actual diagnostic display monitor can always be folded into the formal <b>viewing</b> <b>transformations.</b>|$|R
30|$|One {{can start}} with just one {{resolution}} R and one <b>viewing</b> <b>transformation</b> for each investigation and {{it would still be}} better off than using compression ratios as a quality metric.|$|E
40|$|A new {{approach}} to indexing in model-based vision is introduced; using indexing features whose deformed instances are present {{in a number of}} different object models. Hypotheses are generated by matching an indexing feature with an image using a <b>viewing</b> <b>transformation</b> incorporating the deformation; the parameter values of the deformational component provide specific indices to those models related to the indexing feature. ...|$|E
40|$|In this article, {{we explore}} {{transformative}} interviewing {{through the lens}} of new materialism. Rather than <b>viewing</b> <b>transformation</b> through a humanist perspective that centralizes a transcendent self, we draw upon Barad’s agential realism to reconsider transformation following the ontological turn. Thinking with agential realism, we engaged two interview studies, one on biracialism and one on masculinity, to demonstrate how the materiality of our interviews (e. g., research bodies, computer programs, questionnaires) intra-acted with our participants to both facilitate and hinder our attempts at transformation. We conclude by theorizing transformation as a type of purposeful entanglement that proceeds from the material-discursive intra-actions of our inquiries...|$|E
5000|$|<b>View</b> <b>transformations</b> {{supporting}} panning and zooming, {{including both}} geometric and semantic zooming.|$|R
40|$|Recent gait {{recognition}} systems often {{suffer from}} the chal-lenges including viewing angle variation and large intra-class variations. In order to address these challenges, this paper presents a robust <b>View</b> <b>Transformation</b> Model for gait recog-nition. Based on the gait energy image, the proposed method establishes a robust <b>view</b> <b>transformation</b> model via robust principal component analysis. Partial least square is used as feature selection method. Compared with the existing methods, the proposed method finds out a shared linear corre-lated low rank subspace, which brings the advantages that the <b>view</b> <b>transformation</b> model is robust to viewing angle varia-tion, clothing and carrying condition changes. Conducted on the CASIA gait dataset, experimental {{results show that the}} proposed method outperforms the other existing methods...|$|R
3000|$|... {{contains}} at most finitely many nonzero entries. (In {{this context}} we <b>view</b> <b>transformations</b> as {{acting on the}} right, and define composition of transformations by setting [...]...|$|R
40|$|We {{present an}} {{interactive}} {{system for the}} generation of high quality triangle meshes {{that allows us to}} handle hybrid geometry (point clouds, polygons,...) as input data. In {{order to be able to}} robustly process huge data sets, we exploit graphics hardware features like the raster manager and the z-buffer for specific sub-tasks in the overall procedure. By this we significantly accelerate the stitching of mesh patches and obtain an algorithm for subsampling the data points in linear time. The target resolution and the triangle alignment in sub-regions of the resulting mesh can be controlled by adjusting the screen resolution and <b>viewing</b> <b>transformation.</b> An intuitive user interface provides a flexible tool for application dependent optimization of the mesh. 1...|$|E
40|$|We {{address the}} problem of reconstructing 3 D space in a {{projective}} framework from two views, and the problem of artificially generating novel views of the scene from two given views. We show that with the correspondences coming from four non-coplanar points in the scene and the corresponding epipoles, one can define and reconstruct (using simple linear methods) a projective invariant, referred to as projec-tive depth, that can be used later to reconstruct the projective or affine structure of the scene, or directly to generate novel views of the scene. The derivation has the advantage that the <b>viewing</b> <b>transformation</b> matrix need not be recovered in the course of computations (i. e., we compute structure without motion). ...|$|E
40|$|Max-Planck-Institute for Computer Sciences We {{present an}} {{interactive}} {{system for the}} generation of high quality triangle meshes {{that allows us to}} handle hybrid geometry (point clouds, polygons, [...] .) as input data. In {{order to be able to}} robustly process huge data sets, we exploit graphics hardware features like the raster manager and the z-buffer for specific sub-tasks in the overall procedure. By this we significantly accelerate the stitching of mesh patches and obtain an algorithm for subsampling the data points in linear time. The target resolution and the triangle alignment in sub-regions of the resulting mesh can be controlled by adjusting the screen resolution and <b>viewing</b> <b>transformation.</b> An intuitive user interface provides a flexible tool for application dependent optimization of the mesh. 1...|$|E
40|$|By {{the end of}} this quarter, {{you will}} have learnt {{techniques}} for constructing 2 -D and 3 -D obects as well as manipulating and rendering the objects using OpenGL. The outline of the course is as follows: • Introduction • Geometric primitives • Attributes of geometric primitives • Antialiasing techniques • Homogeneous coordinate system • 2 -D and 3 -D <b>viewing</b> <b>transformations</b> • Structures and hierarchical modeling • 2 -D and 3 -D <b>viewing</b> <b>transformations</b> • Input devices and interactive techniques • Visible surface detection method...|$|R
40|$|This paper {{deals with}} robust {{registration}} of object {{views in the}} presence of uncertainties and noise in depth data. Errors in registration of multiple views of a 3 D object severely affect view integration during automatic construction of object models. We derive a minimum variance estimator (MVE) for computing the <b>view</b> <b>transformation</b> parameters accurately from range data of two views of a 3 D object. The results of our experiments show that <b>view</b> <b>transformation</b> estimates obtained using MVE are significantly more accurate than those computed with an unweighted error criterion for registration. Key words: Image registration, <b>view</b> <b>transformation</b> estimation, <b>view</b> integration, automatic object modeling, 3 D free-form objects, range data. 1 Introduction An important issue in the design of 3 D object recognition systems is building models of physical objects. Object models are extensively used for synthesizing and predicting object appearances from desired viewpoints and also for recognizing th [...] ...|$|R
40|$|The {{increasing}} {{complexity of}} processes used for {{design and execution}} of critical business activities demands novel techniques and technologies. Process viewing techniques have been proposed as means to abstract from details, summarize and filter out information, and customize the visual appearance of a process to the need of particular stakeholders. However, composition of process <b>view</b> <b>transformations</b> and their provisioning to enable their usage in various scenarios is currently not discussed in research. In this paper, we present a lightweight, service-oriented approach to compose modular process <b>view</b> <b>transformation</b> functions to form complex process <b>view</b> <b>transformations</b> which can be offered as a service. We introduce a concept and an architectural framework to generate process view service compositions automatically with focus on usability. Furthermore, we discuss key aspects regarding {{the realization of the}} approach as well as different scenarios where process view se rvices and their compositions are needed...|$|R
40|$|We {{suggest an}} approachfor {{correcting}} {{several types of}} perceived geometric distortions in computer-generated and photographic images. The approach {{is based on a}} mathematical formalization of desirable properties of pictures. From a small set of simple assumptions we obtain perceptually preferable viewing transformations and show that these transformations can be decomposed into a perspective or parallel projection followed by a planar transformation. The decomposition is easily implemented and provides a convenient framework for further analysis of the image mapping. We prove that two perceptually important properties are incompatible and cannot be satisfied simultaneously. It is impossible to construct a <b>viewing</b> <b>transformation</b> such that the images of all lines are straight and the images of all spheres are exact circles. Perceptually preferable tradeoffs between these two types of distortions can depend on the content of the picture. We construct parametric families of transformations [...] ...|$|E
40|$|A simple, microcomputer-based, {{interactive}} {{graphics display}} {{system has been}} developed for the presentation of perspective views of wire frame molecular models. The display system {{is based on a}} TERAK 8510 a graphics computer system with a display unit consisting of microprocessor, television display and keyboard subsystems. The operating system includes a screen editor, file manager, PASCAL and BASIC compilers and command options for linking and executing programs. The graphics program, written in USCD PASCAL, involves the centering of the coordinate system, the transformation of centered model coordinates into homogeneous coordinates, the construction of a <b>viewing</b> <b>transformation</b> matrix to operate on the coordinates, clipping invisible points, perspective transformation and scaling to screen coordinates; commands available include ZOOM, ROTATE, RESET, and CHANGEVIEW. Data file structure was chosen to minimize the amount of disk storage space. Despite the inherent slowness of the system, its low cost and flexibility suggests general applicability...|$|E
40|$|The {{visual display}} {{transformation}} for virtual reality (VR) systems is typically {{much more complex}} than the standard <b>viewing</b> <b>transformation</b> discussed in the literature for conventional computer graphics. The process can be represented as a series of transformations, some of which contain parameters that must match the physical configuration of the system hardware and the user's body. Because of the number and complexity of the transformations, a systematic approach and a thorough understanding of the mathematical models involved is essential. This paper presents a complete model for the visual display transformation for a VR system; that is, the series of transformations used to map points from object coordinates to screen coordinates. Virtual objects are typically defined in an object-centered coordinate system (CS), but must be displayed using the screen-centered CSs of the two screens of a head-mounted display (HMD). This particular algorithm for the VR display computation allows mul [...] ...|$|E
30|$|Identify the {{diagnostic}} <b>viewing</b> <b>transformations</b> for which diagnostic acceptability {{is to be}} established. Again, this set of transformations may be expanded as new radiological investigations become better understood.|$|R
40|$|This paper {{proposes a}} high-precision, {{high-speed}} keypoint matching method using a two-stage Randomized Trees. The keypoint classification method uses the conventional Randomized Trees to enable highprecision, real-time keypoint matching. But {{the wide variety}} of <b>view</b> <b>transformations</b> for templates expressed by Randomized Trees make high-precision keypoint classification for all transformations difficult with a single Randomized Trees. To resolve this problem, proposed method classifies the template <b>view</b> <b>transformations</b> during the first stage. Then during the second stage, it classifies the keypoints using the Randomized Trees corresponding to each of the <b>view</b> <b>transformations</b> classified during the first stage. For images in which the viewpoint of the object is rotated by 70 degree, evaluation testing demonstrated that proposed method is 88. 4 % more precise than SIFT, and 63. 4 % more precise than the conventional Randomized Trees. We have also shown that the proposed method supports real-time keypoint matching at a speed of 12 fps. ...|$|R
40|$|An {{automatic}} approach for <b>view</b> <b>transformation</b> {{based on a}} single outdoor image is proposed in this paper. First, the hierarchical segmentation method is conducted to segment an outdoor image into several meaningful regions and each region is labelled as sky, ground or standing object. Then, different methods are used to estimate each region’s depth according to its label. After that, the obtained depth information is utilized {{to create a new}} view image after any rotation, translation and pitch. Finally, the image inpainting work for the missing colour region is accomplished using its neighbour’s colour. Extensive experiments show the proposed approach not only improves the accuracy of <b>view</b> <b>transformation,</b> but also performs well even for images with occlusion phenomena...|$|R
40|$|Abstract. A {{flexible}} and inexpensive remote sensing tool for albedo estimation using conventional terrestrial photography and its validation on an Alpine glacier are described. The proposed technique involves georeferencing oblique photographs to a {{digital elevation model}} (DEM), defining a mapping function between the information contained on a given pixel of the image and the corresponding cell of the DEM. This is attained by performing a perspective projection of the DEM after a <b>viewing</b> <b>transformation</b> into the camera coordinate system. Once the image is georeferenced, the reflectance values recorded by the film or digital camera are corrected for topographic and atmospheric influences and for {{the effect of the}} photographic process (lens-film-developing-scanning). Atmospheric transmittance is evaluated using the MODTRAN radiative transfer model. Diffuse and direct irradiation are estimated using a parametric solar irradiation model. The solar-ground geometry, anisotropy of reflected radiation, the effect of surrounding topography and the portion of visible sky are evaluated using terrain algorithm...|$|E
40|$|We {{describe}} {{the use of}} a analog VLSI vision sensor for fixation of a 1 D image. The sensor, which contains processing circuitry to convolve the image with a Gabor-type filter, is mounted on a mobile robot. Experimental results indicate that a simple proportional feedback loop using only two outputs of the sensor to command the velocity of the robot can successfully fixate a moving image on the sensor. I. INTRODUCTION Computer vision algorithms seek to recover information about the 3 D environment from 2 D images. In an active vision system, camera parameters such as the pan or tilt angle, focus or zoom are controllable. This ability can be exploited to simplify the recovery of 3 D information. Since the <b>viewing</b> <b>transformation</b> is known, controlling the camera parameters leads to changes in the image which can be measured and provide strong constraints on the environmental permeates to be recovered[1]. Active vision systems often include visual servo loops, where the camera images are used [...] ...|$|E
40|$|Voxel-based 2 D/ 3 D {{registration}} {{enables the}} use of preoperative 3 D CT scans during X-ray fluoroscopy guided interventions for navigation purposes. Both a high accuracy and a short computation time are crucial. The method used in this project is described. Most time consuming is the computation of a perspective projection from the 3 D data. Several volume visualisation methods to compute such a projection quickly and accurately are compared. The advantages and disadvantages of the methods in regard to our problem are discussed. Direct volume rendering methods proved to be particularly suitable and two specific methods were evaluated using a clinical dataset. Several optimisations of these methods in regard to 2 D/ 3 D registration are discussed and the results of two appropriate optimisations on the test dataset are shown. A recently proposed method based on a shear-warp factorisation of the <b>viewing</b> <b>transformation</b> combined with runlength encoded data showed the best results. Keywords: Image [...] ...|$|E
40|$|This essay {{attempts}} to <b>view</b> <b>transformations</b> in biological and social reproduction observed in Ghana {{within the context}} of the macro-economic and demographic changes taking place in a rapidly globalizing African state. Market and other power-ful forces including environmental degradation and rapid population growth, ar...|$|R
2500|$|An {{integral}} {{concept in}} this self-understanding {{is referred to}} as [...] "The Wheel of Metamorphosis." [...] Dürckheim <b>viewed</b> <b>transformation</b> not as the sudden achievement of enlightenment, but rather as a continuous and cyclical evolution, akin to the motion of a wheel. He posited three stages and five steps in each cycle: ...|$|R
3000|$|... [18] {{proposed}} a spatio-temporal silhouette volume of a walking person to encode the gait features, and then applied a <b>view</b> <b>transformation</b> model using singular value decomposition {{to obtain a}} more view-invariant feature vector. More recently, a further study on the view dependency in gait recognition was presented later by Makihara et al.|$|R
40|$|The through-the-lens {{camera control}} {{technique}} originally the exact viewing of a 3 D scene in mind. The situation {{is even worse}} for computer animation, in which a continuous proposed by M. Gleicher and A. Witkin [Comput. Graphics camera motion is often required to generate a smooth 26 (2), 1992, 331 – 340], provides a powerful user interface for {{the control of the}} virtual camera in 3 D computer graphics and animation. Their techique is based on locally inverting the nonlinear perspective <b>viewing</b> <b>transformation.</b> However, given m image control points, the Jacobian matrix is derived as a quite complex 2 m � 8 matrix; furthermore, the Jacobian matrix always has at least one redundant column since its rank can be 7 at most. For the overconstrained case of m � 4, the Lagrange equation is always singular since its 2 m � 2 m square scene change. With a poor camera model, the computer animator has to spend considerable time in fully describin...|$|E
40|$|Shear-warp {{rendering}} is a {{fast and}} efficient method for visualizing {{a volume of}} sampled data based on a factorization of the <b>viewing</b> <b>transformation</b> into a shear and a warp. In shear-warp rendering, the volume is resampled, composited and warped to obtain the final image. Many applications, however, require a mixture of polygonal and volumetric data to be rendered together in a single image. This paper describes a new approach for extending the shear-warp rendering to simultaneously handle polygonal objects. A data structure, the zlist-buffe, is presented. It is basically a multilayered z-buffer. With the zlist-buffer, an object-based scan conversion of polygons requires only a simple modification of the standard polygon scan-conversion algorithm. This paper shows how the scan conversion can be integrated with shear-warp rendering of run-length encoded volume data to obtain quality images in real time. The utility and performance of the approach using a number of test renderings is also discussed...|$|E
40|$|The through-the-lens {{camera control}} {{technique}} originally proposed by Gleicher and Witkin [12] provides a powerful user interface {{for the control}} of the virtual camera in 3 D computer graphics and animation. Their technique is based on locally inverting the nonlinear perspective <b>viewing</b> <b>transformation.</b> However, given m image control points, the Jacobian matrix is derived as a quite complex 2 m Θ 8 matrix; furthermore, the Jacobian matrix always has at least one redundant column since its rank can be 7 at most. For the overconstrained case of m 4, the Lagrange equation is always singular since its 2 m Θ 2 m square matrix has rank 7 at most. All these complications result from removing the constraint q 2 w +q 2 x +q 2 y +q 2 z = 1 for unit quaternions (q w; q x; q y; q z) 2 S 3 which represent the camera rotations. In this paper, we interpret the problem as a target tracking problem and formulate it as a constrained nonlinear inversion problem. The problem i [...] ...|$|E
40|$|Abstract. Gait {{analyses}} {{have recently}} gained attention as methods of identification of individuals {{at a distance}} from a camera. However, ap-pearance changes due to view direction changes cause difficulties for gait recognition systems. Here, we propose a method of gait recognition from various view directions using frequency-domain features and a <b>view</b> <b>transformation</b> model. We first construct a spatio-temporal silhouette volume of a walking person and then extract frequency-domain features of the volume by Fourier analysis based on gait periodicity. Next, our <b>view</b> <b>transformation</b> model is obtained with a training set of multiple persons from multiple view directions. In a recognition phase, the model transforms gallery features into the same view direction as that of an in-put feature, and so the features match each other. Experiments involving gait recognition from 24 view directions demonstrate the effectiveness of the proposed method. ...|$|R
40|$|To {{provide a}} {{complete}} {{analysis of the}} organization, its business and its needs, {{it is necessary for}} leaders to have data that help decision making. Data warehouses are designed to meet such needs; they are an analysis and data management technology. This article describes an MDA (Model Driven Architecture) process that we have used to automatically generate the multidimensional schema of data warehouse. This process uses model transformation using several standards such as Unified Modeling Language, Meta-Object Facility, Query <b>View</b> <b>Transformation,</b> Object Constraint Language, [...] . From the UML model, especially the class diagram, a multidimensional model is generated as an XML file, the transformation is carried out by the QVT (Query <b>View</b> <b>Transformation)</b> language and the OCL (Object Constraint Language) Language. To validate our approach a case study is presented {{at the end of this}} wor...|$|R
40|$|The next {{promising}} {{key issue}} of the automobile development is a self-driving technique. One of the challenges for intelligent self-driving includes a lane-detecting and lane-keeping capability for advanced driver assistance systems. This paper introduces an efficient and lane detection method designed based on top <b>view</b> image <b>transformation</b> that converts an image from a front view to a top view space. After the top <b>view</b> image <b>transformation,</b> a Hough transformation technique is integrated by using a parabolic model of a curved lane in order to estimate a parametric model of the lane in the top view space. The parameters of the parabolic model are estimated by utilizing a least-square approach. The experimental {{results show that the}} newly proposed lane detection method with the top <b>view</b> <b>transformation</b> is very effective in estimating a sharp and curved lane leading to a precise self-driving capability...|$|R
