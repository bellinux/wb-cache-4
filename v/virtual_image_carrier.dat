0|1725|Public
5000|$|... #Caption: Top: The {{formation}} of a <b>virtual</b> <b>image</b> using a diverging lens. Bottom: The {{formation of}} a <b>virtual</b> <b>image</b> using a convex mirror. In both diagrams, f is the focal point, O is the object and I is the image, shown in grey. Solid blue lines indicate light rays. It {{can be seen that}} the light rays appear to emanate from the <b>virtual</b> <b>image</b> but do not actually exist at the position of the <b>virtual</b> <b>image.</b> Thus an image cannot be seen by placing a screen at the position of the <b>virtual</b> <b>image.</b>|$|R
40|$|We {{introduce}} the <b>virtual</b> <b>image,</b> an iconic index suited for pictorial information access in a pictorial database, and a similarity retrieval approach based on <b>virtual</b> <b>images</b> to perform content-based retrieval. A <b>virtual</b> <b>image</b> represents the spatial {{information contained in}} a real image in explicit form {{by means of a}} set of spatial relations. This is useful to efficiently compute the similarity between a query and an image in the database. We also show that <b>virtual</b> <b>images</b> support real-world applications that require translation, reflection, and/or rotation invariance of image representation...|$|R
40|$|The {{influence}} of physically presented background stimuli on the perceived depth of optically overlaid, stereoscopic <b>virtual</b> <b>images</b> {{has been studied}} using headmounted stereoscopic, <b>virtual</b> <b>image</b> displays. These displays allow presentation of physically unrealizable stimulus combinations. Positioning of an opaque physical object either at the initial perceived depth of the <b>virtual</b> <b>image</b> or at a position substantially {{in front of the}} <b>virtual</b> <b>image,</b> causes the <b>virtual</b> <b>image</b> to perceptually move closer to the observer. In the case of objects positioned substantially in front of the <b>virtual</b> <b>image,</b> subjects often perceive the opaque object to become transparent. Evidence is presented that the apparent change of position caused by interposition of the physical object is not due to occlusion cues. According, it may have an alternative cause such as variation in the binocular vengeance position of the eyes caused by introduction of the physical object. This effect may complicate design of overlaid <b>virtual</b> <b>image</b> displays for near objects and appears {{to be related to the}} relative conspicuousness of the overlaid <b>virtual</b> <b>image</b> and the background. Consequently, it may be related to earlier analyses of John Foley which modeled open-loop pointing errors to stereoscopically presented points of light in terms of errors in determination of a reference point for interpretation of observed retinal disparities. Implications for the design of see-through displays for manufacturing will be discussed...|$|R
5000|$|<b>Virtual</b> <b>Image</b> Capture and <b>Virtual</b> <b>Image</b> Stitcher: Two {{software}} {{products to}} capture mult-field images and stitch them into one single and very large {{image in the}} fields of optical and electron microscopy (image stitching).|$|R
40|$|The cloud {{virtualization}} technology {{improves the}} economy of scale for data centers through server consolidation, application consolidation and resources consolidation. Virtualization allows the provider to move <b>Virtual</b> <b>Images</b> from more congested host to less-congested hosts, as required. Enterprises also get improved server reliability, which in turn increases application performance. Despite these benefits, it includes major security challenges with the portability of <b>Virtual</b> <b>Images</b> between different cloud providers. The security and integrity of <b>Virtual</b> <b>images</b> is {{the foundation for the}} overall security of the cloud. Many of the <b>Virtual</b> <b>images</b> are intended to be shared by diverse and unrelated users. Unfortunately, existing approaches to cloud security built by cloud practitioners fall short when dealing with <b>Virtual</b> <b>images.</b> Secure transmission of <b>virtual</b> <b>Images</b> can bepossible by providing authentication using Blind Authentication protocol (BAP). The proposed approach authenticates the allocation of <b>virtual</b> <b>images</b> using Blind authentication protocol. It provides provable protection against replay and client side attacks even if the keys of the user are compromised. The encryption also provides template protection, revocability and alleviates the concerns on privacy in widespread use of biometrics. Carrying out the authentication in the encrypted domain is a secure process, while the encryption key acts as an additional layer of security...|$|R
40|$|DE 102009010921 A 1 UPAB: 20100915 NOVELTY - The device (100) has a {{processing}} device (110), which is formed around a video signal (112) of a <b>virtual</b> <b>image</b> {{to produce and}} to provide a real image (102), a local information (104) of a camera and a local information (106) of the object. The <b>virtual</b> <b>image</b> has a representation of the object or object information. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following: (1) a method for providing video signal of a <b>virtual</b> <b>image</b> based on a real image of a camera; and (2) a computer program with a program code for execution of the method. USE - Device for providing video signal of a <b>virtual</b> <b>image</b> based on a real image of a camera. ADVANTAGE - The device has a {{processing device}}, which is formed around a video signal of a <b>virtual</b> <b>image</b> to produce {{and to provide a}} real image, and hence ensures cost effective and improved video signal providing device...|$|R
25|$|In optics, a <b>virtual</b> <b>image</b> is {{an image}} formed when the {{outgoing}} rays from a point on an object always diverge. The image appears to be located {{at the point of}} apparent divergence. Because the rays never really converge, a <b>virtual</b> <b>image</b> cannot be projected onto a screen. In diagrams of optical systems, virtual rays are conventionally represented by dotted lines. <b>Virtual</b> <b>images</b> are located by tracing the real rays that emerge from an optical device (lens, mirror, or some combination) backward to a perceived point of origin.|$|R
5000|$|... #Caption: A {{negative}} lens produces a demagnified <b>virtual</b> <b>image.</b>|$|R
30|$|Brightness {{differences}} in the <b>virtual</b> <b>image</b> are created from reflectivity information at each point from the laser scanner. Strong reflectivity variations within the calibration pattern result in strong contrasts in the <b>virtual</b> <b>image.</b> This is particularly useful at the transition {{between black and white}} quads of the checkerboard pattern for calibration.|$|R
40|$|DE 102009008039 A 1 UPAB: 20100729 NOVELTY - The method {{involves}} {{receiving a}} real image (RB) by a camera, and determining {{position of a}} <b>virtual</b> <b>image</b> component (VIRT) to the real image received by the camera. Color and/or texture of the real image at {{the position of the}} <b>virtual</b> <b>image</b> component is determined. Color and/or texture of the <b>virtual</b> <b>image</b> component is selected based on the color and/or texture of the real image at the position of the <b>virtual</b> <b>image</b> component. The <b>virtual</b> <b>image</b> component is compared with a part of the real image to an augmented reality image (ARB). DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following: (1) a method for manufacturing a technical component of a motor vehicle or a motor vehicle component (2) a device for manufacturing a technical component of a motor vehicle or a motor vehicle component. USE - Method for producing an augmented reality image for manufacturing a technical component of a motor vehicle (claimed) i. e. land vehicle, during road traffic. ADVANTAGE - The <b>virtual</b> <b>image</b> component is compared with the part of the real image to the augmented reality image, thus reducing the cost for producing the technical component of the motor vehicle, and hence reducing the cost for manufacturing the motor vehicle, increasing manufacturing quality of the motor vehicle, and reducing error rate during manufacturing of the motor vehicle...|$|R
30|$|The initial, {{approximate}} {{alignment of}} the point cloud with the camera image {{can be used to}} perform stereo calibration. To this end, a <b>virtual</b> <b>image</b> is computed from the point cloud. <b>Virtual</b> <b>image</b> and camera image together are then used to obtain a second transformation that refines the initial relation between the sensors.|$|R
40|$|Here {{you have}} another {{principal}} ray diagram, {{just like the one}} we constructed on the previous page, only this one is interactive. You can click on the object and drag it, and observe what happens to the image. Notice that, as we saw on the previous page, this <b>image</b> is a <b>virtual</b> <b>image.</b> To remind you of this, it is shown striped. "Virtual image" means that the light rays are not actually coming from the position where the <b>virtual</b> <b>image</b> is located ??? nothing is really there. But the light rays emerge from the lens *as if* they were coming from the <b>virtual</b> <b>image.</b> Move the object back and forth to see what happens to the image. As you can see, a concave lens always produces an upright, reduce, <b>virtual</b> <b>image.</b> CCRAA (College Cost Reduction and Accessibility Act) GrantTQE (Teacher Quality Enhancement) Gran...|$|R
40|$|Iin {{monitoring}} dynamic phenomena through {{remote sensing}} images {{it is often}} necessary to generate <b>virtual</b> <b>images</b> of the phenomenon representing its spatial reference at ideal times of observation, when no real image is available. In this contribution we propose a method based on fuzzy logic to generate <b>virtual</b> <b>images</b> {{on the basis of}} available images taken before and after the ideal date, and vague and incomplete knowledge of the phenomenon dynamics synthesised by fuzzy rules. The <b>virtual</b> <b>image</b> is generated as a non-linear coalescing of the real images based on the application of a fuzzy rule inference mechanism...|$|R
25|$|In {{some cases}} S2 is negative, {{indicating}} that the image is formed {{on the opposite side}} of the lens from where those rays are being considered. Since the diverging light rays emanating from the lens never come into focus, and those rays are not physically present at the point where they appear to form an image, this is called a <b>virtual</b> <b>image.</b> Unlike real <b>images,</b> a <b>virtual</b> <b>image</b> cannot be projected on a screen, but appears to an observer looking through the lens as if it were a real object at the location of that <b>virtual</b> <b>image.</b> Likewise, it appears to a subsequent lens as if it were an object at that location, so that second lens could again focus that light into a real image, S1 then being measured from the <b>virtual</b> <b>image</b> location behind the first lens to the second lens. This is exactly what the eye does when looking through a magnifying glass. The magnifying glass creates a (magnified) <b>virtual</b> <b>image</b> behind the magnifying glass, but those rays are then re-imaged by the lens of the eye to create a real image on the retina.|$|R
40|$|The {{influence}} of physically presented background stimuli on distance judgements to optically over-laid, stereoscopic <b>virtual</b> <b>images</b> {{has been studied}} using head-mounted stereoscopic, <b>virtual</b> <b>image</b> displays. Positioning of an opaque physical object either at the perceived depth of the <b>virtual</b> <b>image</b> or a t a position substantially in front of it, has been observed to cause the <b>virtual</b> <b>image</b> to apparently move closer to the observer. In the case of physical objects positioned substantially {{in front of the}} vir-tual image, subjects often perceive the opaque object as transparent. Evidence is presented that the apparent change of position caused by interposition o € the physical object is not influenced by the strengthening of occlusion cues but is influenced by motion of the physical objects which would attract the subjects ocular vergence. The observed effect appears {{to be associated with the}} relative conspicu-ousness of the overlaid <b>virtual</b> <b>image</b> and the background. This effect may be related to Foley's mod-els of open-loop stereoscopic pointing errors which attributed the stereoscopic distance errors to mis-judgment of a reference point for interpretation of retinal disparities. Some implications for the de-sign of see-through displays for manufaduring will also be discussed briefly...|$|R
50|$|In {{some cases}} S2 is negative, {{indicating}} that the image is formed {{on the opposite side}} of the lens from where those rays are being considered. Since the diverging light rays emanating from the lens never come into focus, and those rays are not physically present at the point where they appear to form an image, this is called a <b>virtual</b> <b>image.</b> Unlike real <b>images,</b> a <b>virtual</b> <b>image</b> cannot be projected on a screen, but appears to an observer looking through the lens as if it were a real object at the location of that <b>virtual</b> <b>image.</b> Likewise, it appears to a subsequent lens as if it were an object at that location, so that second lens could again focus that light into a real image, S1 then being measured from the <b>virtual</b> <b>image</b> location behind the first lens to the second lens. This is exactly what the eye does when looking through a magnifying glass. The magnifying glass creates a (magnified) <b>virtual</b> <b>image</b> behind the magnifying glass, but those rays are then re-imaged by the lens of the eye to create a real image on the retina.|$|R
5000|$|... ‘Natural Magic,’ a {{pamphlet}} on optics dealing with <b>virtual</b> <b>images,</b> London, 1832.|$|R
5000|$|... #Caption: <b>Virtual</b> <b>image</b> {{formation}} using {{a positive}} lens as a magnifying glass.|$|R
5000|$|... #Caption: A {{plane mirror}} showing the <b>virtual</b> <b>image</b> of an urn nearby.|$|R
40|$|Previous augmented-reality (AR) {{applications}} {{have required}} users {{to observe the}} integration of real and <b>virtual</b> <b>images</b> on a display. This study proposes a novel concept regarding AR applications. By integrating AR techniques with marker identification, virtual-image output, imaging, and image-interaction processes, this study rendered <b>virtual</b> <b>images</b> that can interact with predefined markers in a real three-dimensional (3 D) environment...|$|R
40|$|Sun-sensor {{optical system}} uses four pairs of pentaprisms to simplify {{alinement}} and reduce mechanical-stability requirements. Cross-shaped windows in field stop enhance sensitivity of signal detectors {{to changes in}} angular position. Two <b>virtual</b> <b>images</b> viewed by telescopes mark position and orientation of occulter panel. Reflector vertex, point source and corresponding <b>virtual</b> <b>image</b> are all equally spaced along a straight line...|$|R
50|$|IBM {{introduced}} IFLs on September 29, 2000. At {{the same}} time, IBM introduced a special Linux-only VM-like product, called the S/390 <b>Virtual</b> <b>Image</b> Facility for Linux, {{to cater to}} IT staff previously unfamiliar with IBM mainframes. IBM soon discovered that z/VM was not too difficult for new IT staff to learn (and worked better), so IBM discontinued S/390 <b>Virtual</b> <b>Image</b> Facility for Linux in April 2002.|$|R
5000|$|IBM Workload Deployer - a {{hardware}} appliance that {{provides access to}} IBM middleware <b>virtual</b> <b>images</b> and patterns ...|$|R
3000|$|... is the {{luminance}} of <b>virtual</b> <b>image</b> {{generated by}} processed depth. For better quality, the metric shows low values.|$|R
50|$|In most microscopes, the {{eyepiece}} is a compound lens, with one component lens {{near the front}} and one near the back of {{the eyepiece}} tube. This forms an air-separated couplet.In many designs, the <b>virtual</b> <b>image</b> comes to a focus between the two lenses of the eyepiece, the first lens bringing the real image to a focus and the second lens enabling the eye to focus on the <b>virtual</b> <b>image.</b>|$|R
40|$|We {{present in}} our paper a secure, {{flexible}} and transparent security architecture for <b>virtual</b> disk <b>images.</b> <b>Virtual</b> disk <b>images</b> are often overlooked in security concepts, {{especially in a}} grid environment where disk images {{are considered to be}} secure as long as they reside within the secured borders of the data center. However, for some applications this level of assurance is not satisfactory. In our security architecture, virtualized guests transparently benefit from integrity as well as confidentiality assurance. Traditional <b>virtual</b> disk <b>images</b> lack the ability of an efficient integrity protection mechanism. We base our concepts on trusted computing utilizing the Trusted Platform Module (TPM) to efficiently deliver integrity assurance to <b>virtual</b> disk <b>images.</b> Further, we allow a restrictive rule-set to be imposed by the <b>virtual</b> disk <b>image</b> owner, and we enable the owner to retain control over the <b>virtual</b> disk <b>image</b> throughout its life-cycle. ...|$|R
40|$|We {{propose a}} new {{approach}} for stereo vision. In human vision one percept can arise from two retinal images {{as a result of}} the process called "fusion". We try to create a <b>virtual</b> <b>image</b> from two images of stereo cameras. Optical axes of the cameras intersect on an attention point locating on the surface of an object. By Affine transformation of the respective images, the two images are transformed to a <b>virtual</b> <b>image</b> that is seen from the middle point between two cameras. By superimposing two transformed pictures, one fusion picture of <b>virtual</b> <b>image</b> is reduced. Excepting for around the attention point, two transformed pictures do not coincide with each other. Finding correspondence between two pictures, we can determine Sdimentional depth information. ...|$|R
50|$|The final image (y″) is a <b>virtual</b> <b>image,</b> {{located at}} {{infinity}} {{and is the}} same way up as the object.|$|R
40|$|We {{demonstrate}} a new 'microsphere nanoscope' that uses ordinary SiO 2 microspheres as superlenses {{to create a}} <b>virtual</b> <b>image</b> of the object in near field. The magnified <b>virtual</b> <b>image</b> greatly overcomes the diffraction limit. We are able to resolve clearly 50 -nm objects under a standard white light source in both transmission and reflection modes. The resolution achieved for white light opens a new opportunity to image viruses, DNA and molecules in real time...|$|R
50|$|These {{devices are}} sold as {{hardware}} network appliances {{and in some}} instances as <b>virtual</b> <b>images</b> that run on basic server hardware.|$|R
5000|$|... #Caption: A {{convex lens}} (...) forming a real, {{inverted}} image {{rather than the}} upright, <b>virtual</b> <b>image</b> as seen in a magnifying glass ...|$|R
40|$|In this Conference, {{the problem}} of blur in a {{panoramic}} image from a catadioptric camera is analyzed through {{the determination of the}} <b>virtual</b> <b>image.</b> This determination is done first with an approximative method, and second through the caustic approach. This leads us to a general caustic approach of panoramic image analysis, where equations of <b>virtual</b> <b>images</b> are given. Finally, we give some direct applications of our analysis, such as depth of field (blur) or image resolution...|$|R
50|$|UV/visible {{light from}} an {{integrating}} sphere (and/or other source {{such as a}} black body) is focused onto a square test target at the focal plane of a collimator (the mirrors in the diagram), such that a <b>virtual</b> <b>image</b> of the test target will be seen infinitely far away by the camera under test. The camera under test senses a real <b>image</b> of the <b>virtual</b> <b>image</b> of the target, and the sensed image is displayed on a monitor.|$|R
5000|$|Cloud {{storage can}} be used for copying <b>virtual</b> machine <b>images</b> from the cloud to {{on-premises}} locations or to import a <b>virtual</b> machine <b>image</b> from an on-premises location to the cloud image library. In addition, cloud storage {{can be used}} to move <b>virtual</b> machine <b>images</b> between user accounts or between data centers.|$|R
30|$|First, a <b>virtual</b> <b>image</b> of {{the point}} cloud has to be generated. It is {{important}} that this image shows all calibration patterns without occlusions. Thus, we demonstrate how the laser scanner’s unordered point cloud can be transformed such that it is approximately aligned with the coordinate system of the camera. From this transformed point cloud, a <b>virtual</b> <b>image</b> is generated. In this image, the pixel intensities are derived from the reflectivity data {{that is associated with}} the individual 3 -D point measurements.|$|R
30|$|The {{reprojection}} {{error is}} a common choice to evaluate {{the quality of a}} stereo calibration result. Thus, we compare the checkerboard positions in the camera image and in the <b>virtual</b> <b>image.</b> In this experiment, the system is calibrated on the first scene. Hereafter, the reprojection errors are calculated on the evaluation scene. As the <b>virtual</b> <b>image</b> is generated {{from the perspective of the}} camera, we can directly compare the 2 -D positions of the keypoints which are returned by the checkerboard detection algorithm.|$|R
