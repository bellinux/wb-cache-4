170|55|Public
5000|$|With more {{advanced}} techniques (Dudley's entropy bound and Haussler's upper bound) one can show, for example, that {{there exists a}} constant , such that any class of -indicator functions with <b>Vapnik-Chervonenkis</b> <b>dimension</b> [...] has Rademacher complexity upper-bounded by [...]|$|E
5000|$|Here [...] "simple" [...] {{means that}} the <b>Vapnik-Chervonenkis</b> <b>dimension</b> of the class [...] is small {{relative}} {{to the size of}} the sample. In other words, a sufficiently simple collection of functions behaves roughly the same on a small random sample as it does on the distribution as a whole.|$|E
50|$|In Vapnik-Chervonenkis theory, the VC {{dimension}} (for <b>Vapnik-Chervonenkis</b> <b>dimension)</b> is {{a measure}} of the capacity (complexity, expressive power, richness, or flexibility) of a space of functions that can be learned by a statistical classification algorithm. It is defined as the cardinality of the largest set of points that the algorithm can shatter. It was originally defined by Vladimir Vapnik and Alexey Chervonenkis.|$|E
40|$|AbstractAn {{investigation}} is conducted of the <b>Vapnik-Chervonenkis</b> <b>dimensions</b> (VC-dimensions) of finite automata having k letters and n states. It is shown for a fixed positive integer k (⩾ 2), that (1) the VC-dimension of DFAk(n) := {L ⊂ { 1, 2, …, k}∗ : some deterministic finite automaton with at most n states accepts L} is n + log 2 n − O(log logn) for k = 1 and (k − 1 + 0 (1)) n log 2 n for k ⩾ 2, and (2) the VC-dimension of CDFAk(n) := {L ϵ DFAk(n) : L is commutative} is n + o(n) ...|$|R
40|$|Abstract. This paper {{presents}} a brief introduction to <b>Vapnik-Chervonenkis</b> (VC) <b>dimension,</b> a quantity which characterizes {{the difficulty of}} distribution-independent learning. The paper establishes various elementary results, and discusses how to estimate the VC dimension in several examples of interest in neural network theory. ...|$|R
30|$|Support vector machine (SVM), as the representative’s kernel-based techniques, {{is a major}} {{development}} in machine learning algorithms. SVM {{is a group of}} supervised learning methods based on the statistical learning theory and the <b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> introduced by [V Vapnik and Cortes, 1995] and [Chervonenkis, 2013] that can be applied to pattern classification or non-linear regression.|$|R
5000|$|Combinatorial Geometry (with János Pach, Wiley, 1995, [...] ). This book, less {{specialized}} {{than the}} previous two, is split into two sections. The first, on packing and covering problems, includes topics such as Minkowski's theorem, sphere packing, the representation of planar graphs by tangent circles, the planar separator theorem. The second section, although primarily concerning arrangements, also includes topics from extremal graph theory, <b>Vapnik-Chervonenkis</b> <b>dimension,</b> and discrepancy theory.|$|E
5000|$|Finding a {{quadratic}} classifier for {{the original}} measurements would then become the same as finding a linear classifier based on the expanded measurement vector. This observation {{has been used in}} extending neural network models; the [...] "circular" [...] case, which corresponds to introducing only the sum of pure quadratic terms [...] with no mixed products (...) , has been proven to be the optimal compromise between extending the classifier's representation power and controlling the risk of overfitting (<b>Vapnik-Chervonenkis</b> <b>dimension).</b>|$|E
50|$|In the {{nomenclature}} of Vapnik-Chervonenkis theory, we may {{say that}} a collection S of subsets of X shatters a set B ⊆ X if every subset of B is of the form B ∩ S for some S ∈ S. Then T has the independence property if in some model M of T there is a definable family (Sa | a∈Mn) ⊆ Mk that shatters arbitrarily large finite subsets of Mk. In other words, (Sa | a∈Mn) has infinite <b>Vapnik-Chervonenkis</b> <b>dimension.</b>|$|E
40|$|Local {{receptive}} field neurons comprise such well-known and widely used unit types as radial basis function neurons and neurons with center-surround {{receptive field}}. We study the <b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> of feedforward neural networks with one hidden layer of these units. For several variants of local receptive field neurons {{we show that}} the VC dimension of these networks is superlinear...|$|R
40|$|In {{this thesis}} we are {{concerned}} with the problems of data transport in wireless networks. We study some issues related to the design and performance evaluation of multihop wireless networks, also known as ad hoc networks. The rst issue that we address is the power requirement for assuring connectivity of wireless networks. Employing some results from continuum percolation theory, we obtain a precise characterization of the critical transmission range of nodes in a wireless network such that the network is connected with probability approaching one as the number of nodes increases. We next analyze the traÆc-carrying capacity of multihop wireless networks. We show that under some noninterference models motivated by current technology, the average throughput obtained by nodes in a two-dimensional wireless network decreases as the reciprocal of the square root of the number of nodes in the network. We also show that a similar cube root law holds for three-dimensional wireless networks. In doing so, we determine the <b>Vapnik-Chervonenkis</b> <b>dimensions</b> of certain geometric sets, which may be of independent interest. We also study wireless networks in a more information-theoretic framework, which allow...|$|R
30|$|The SVM [13] is {{a popular}} small sample set {{learning}} method. It offers very good performance for pattern classification problems by minimizing the <b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> and achieving a minimal structural risk. Because of its small sample set requirement, fast learning capability, and good performance, SVM {{is a good choice}} for this method. Because each SVM classifier is trained by a subspace of the feature space, we can evaluate and penalize the features by using the corresponding SVM classifier.|$|R
40|$|A {{proof that}} a concept is learnable {{provided}} the <b>Vapnik-Chervonenkis</b> <b>dimension</b> is finite is given. The proof is more explicit than previous proofs and introduces two new parameters which allow bounds on {{the sample size}} obtained to be improved {{by a factor of}} approximately 4 log 2 (e). Keywords: learning, <b>Vapnik-Chervonenkis</b> <b>dimension,</b> Probably Approximately Correct (PAC), sample...|$|E
40|$|A {{neural network}} {{is said to}} be nonoverlapping if there is at most one edge {{outgoing}} from each node. We investigate the number of examples that a learning algorithm needs when using nonoverlapping neural networks as hypotheses. We derive bounds for this sample complexity in terms of the <b>Vapnik-Chervonenkis</b> <b>dimension.</b> In particular, we consider networks consisting of threshold, sigmoidal and linear gates. We show that the class of nonoverlapping threshold networks and the class of nonoverlapping sigmoidal networks on n inputs both have <b>Vapnik-Chervonenkis</b> <b>dimension</b> ΩΓ n log n). This bound is asymptotically tight for the class of nonoverlapping threshold networks. We also present an upper bound for this class where the constants involved are considerably smaller than in a previous calculation. Finally, we argue that the <b>Vapnik-Chervonenkis</b> <b>dimension</b> of nonoverlapping threshold or sigmoidal networks cannot become larger by allowing the nodes to compute linear functions. This she [...] ...|$|E
40|$|We {{say that}} a set system F⊆ 2 ^[n] {{shatters}} a given set S⊆ [n] if 2 ^S={F ∩ S : F ∈F}. The Sauer inequality states that in general, a set system F shatters at least |F| sets. Here we concentrate {{on the case of}} equality. A set system is called shattering-extremal if it shatters exactly |F| sets. In this paper we characterize shattering-extremal set systems of <b>Vapnik-Chervonenkis</b> <b>dimension</b> 2 in terms of their inclusion graphs, and as a corollary we answer an open question from VC 1 about leaving out elements from shattering-extremal set systems in the case of families of <b>Vapnik-Chervonenkis</b> <b>dimension</b> 2. Comment: 20 page...|$|E
40|$|We {{consider}} PAC {{learning of}} simple cooperative games, {{in which the}} coalitions are partitioned into “winning ” and “losing” coalitions. We analyze the complexity of learning a suitable concept class via its <b>Vapnik-Chervonenkis</b> (VC) <b>dimension,</b> and provide an algorithm that learns this class. Furthermore, we study constrained simple games; we demonstrate that the VC dimension can be dramatically reduced when one allows only a single minimum winning coalition (even more so when this coalition has cardinality 1), whereas other interesting constraints do not significantly lower the dimension...|$|R
40|$|For {{any family}} of {{measurable}} sets in a probability space, {{we show that}} either (i) the family has infinite <b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> or (ii) for every epsilon > 0 there is a finite partition pi such the pi-boundary of each set has measure at most epsilon. Immediate corollaries include {{the fact that a}} family with finite VC dimension has finite bracketing numbers, and satisfies uniform laws of large numbers for every ergodic process. From these corollaries, we derive analogous results for VC major and VC graph families of functions. Comment: 13 pages, no figure...|$|R
40|$|The <b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> of a {{collection}} of subsets of a set is an important combinatorial concept in settings such as discrete geometry and machine learning. In this paper we prove that the VC dimension of the family of d-dimensional cubes in R^d is (3 d+ 1) / 2. Comment: Derived from Fall 2014 Honours research project done under the supervision of Dr. Vladimir Pestov at the University of Ottawa; 4 pages; significantly simplified the constructions, removed the final two sections (which did not fit well with the rest...|$|R
40|$|Let f be {{an unknown}} multivariate density {{belonging}} to a prespecified parametric class of densities,, where k is unknown, but for all k and each has finite <b>Vapnik-Chervonenkis</b> <b>dimension.</b> Given an i. i. d. sample of size n drawn from f, we show {{that it is possible}} to select automatically, and without extra restrictions on f, an estimate with the property that. Our method is inspired by the combinatorial tools developed in Devroye and Lugosi (Combinatorial Methods in Density Estimation, Springer, New York, 2001) and it includes a wide range of density models, such as mixture models or exponential families. Multivariate density estimation <b>Vapnik-Chervonenkis</b> <b>dimension</b> Mixture densities Penalization...|$|E
40|$|In {{statistical}} learning theory, the <b>Vapnik-Chervonenkis</b> <b>dimension</b> is {{an important}} property of classifier families. With the help of this combinatoral concept {{it is possible to}} bound the error probability of a classifier, based on its performance on the training set. Convex polygon classifiers are R 2 ↦ → {+ 1, − 1 } mappings that partition the plane into 2 distinct regions such that one of the regions is a convex polygon. In this paper, the <b>Vapnik-Chervonenkis</b> <b>dimension</b> of convex n-gon classifiers is determined. Note that the label of the inner (convex) region is unrestricted which makes the problem substantially different from the well known restricted case. ...|$|E
40|$|We {{present a}} new general concentration-of-measure {{inequality}} and illustrate its power by applications in random combinatorics. The results find direct applications in some problems of learning theory. Concentration of measure, <b>Vapnik-Chervonenkis</b> <b>dimension,</b> logarithmic Sobolev inequalities, longest monotone subsequence, model selection...|$|E
40|$|Abstract. We discuss here {{empirical}} comparation between model selection methods {{based on}} Linear Genetic Programming. Two statistical methods are compared: model selection based on Empirical Risk Minimization (ERM) and model selection based on Structural Risk Minimization (SRM). For this purpose {{we have identified}} the main components which determine the capacity of some linear structures as classifiers showing an upper bound for the <b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> of classes of programs representing linear code defined by arithmetic computations and sign tests. This upper bound is used to define a fitness based on VC regularization that performs significantly better than the fitness based on empirical risk...|$|R
40|$|The <b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> {{of the set}} of half-spaces of R^d with frontiers {{parallel}} to the axes is computed exactly. It is shown that it is {{much smaller than the}} intuitive value of d. A good approximation based on the Stirling's formula proves that it is more likely of the order log_ 2 (d). This result may be used to evaluate the performance of classifiers or regressors based on dyadic partitioning of R^d for instance. Algorithms using axis-parallel cuts to partition R^d are often used to reduce the computational time of such estimators when d is large...|$|R
40|$|<b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> is a {{fundamental}} measure of the generalization capacity of learning algorithms. However, apart from a few special cases, it is hard or impossible to calculate analytically. Vapnik et al. [10] proposed a technique for estimating the VC dimension empirically. While their approach behaves well in simulations, {{it could not be}} used to bound the generalization risk of classifiers, because there were no bounds for the estimation error of the VC dimension itself. We rectify this omission, providing high probability concentration results for the proposed estimator and deriving corresponding generalization bounds. Comment: 11 page...|$|R
40|$|We prove upper bounds for {{combinatorial}} {{parameters of}} finite relational structures, {{related to the}} complexity of learning a definable set. We show that monadic second order (MSO) formulas with parameters have bounded <b>Vapnik-Chervonenkis</b> <b>dimension</b> over structures of bounded clique-width, and first-order formulas with parameters have bounded <b>Vapnik-Chervonenkis</b> <b>dimension</b> over structures of bounded local clique-width (this includes planar graphs). We also show that MSO formulas of a fixed size have bounded strong consistency dimension over MSO formulas of a fixed larger size, for labeled trees. These bounds imply positive learnability results for the PAC and equivalence query learnability of a definable set over these structures. The proofs are based on bounds for related definability problems for tree automata...|$|E
40|$|We {{show that}} the <b>Vapnik-Chervonenkis</b> <b>dimension</b> of Boolean monomials over n {{variables}} is at most n for all n 2. It follows that the VC-dimension is determined exactly and is, except for n = 1, equal to the VC-dimension of the proper subclass of monotone monomials. Keywords: Combinatorial Problems, Computational Complexity, Learnability Work supported by the ESPRIT Working Group NeuroCOLT No. 8556 1 Introduction The <b>Vapnik-Chervonenkis</b> <b>dimension</b> VC-dim(C) of a collection C of subsets of a set X {{is defined as the}} maximum cardinality of any set S ` X that is shattered by C. A set S is shattered by C if for every subset T of S there exists a C 2 C such that T = S " C. The VC-dimension of a class F of functions f : X ! f 0; 1 g is defined by identifying each element f 2 F with the set fx 2 X : f(x) = 1 g. Thus VC-dim(F) is the maximum cardinality of any set S ` X for which F induces all functions g : S ! f 0; 1 g. The <b>Vapnik-Chervonenkis</b> <b>dimension</b> gives almost tight bounds for the numb [...] ...|$|E
40|$|In {{this paper}} we {{investigate}} a parameter defined for any graph, {{known as the}} <b>Vapnik-Chervonenkis</b> <b>dimension</b> (or VC dimension). For any vertex x of a graph G, the closed neighbourhood N(x) of x is the set of all vertices of G adjacent to x, together with x. We say that a set D of vertices of G is shattered if every subset R of D can be realised as R = D ∩ N(x) for some vertex x of G. The <b>Vapnik-Chervonenkis</b> <b>dimension</b> of G is defined {{to be the largest}} cardinality of a shattered set of vertices. Our main result gives, for each positive integer d, the exact threshold function for a random graph G(n, p) to have VC dimension d...|$|E
40|$|A {{product unit}} is a formal neuron that multiplies its input values instead of summing them. Fur-thermore, it has weights acting as exponents {{instead of being}} factors. We {{investigate}} the complexity of learning for networks containing product units. We establish bounds on the <b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> {{that can be used}} to assess the generalization capabilities of these networks. In par-ticular, we show that the VC dimension for these networks is not larger than the best known bound for sigmoidal networks. For higher-order networks we derive upper bounds that are independent of the degree of these networks. We also contrast these results with lower bounds. ...|$|R
40|$|We {{establish}} {{versions of}} Descartes' rule of signs for radial basis function (RBF) neural networks. The RBF rules of signs provide tight bounds {{for the number}} of zeros of univariate networks with certain parameter restrictions. Moreover, they can be used to infer that the <b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> and pseudo-dimension of these networks are no more than linear. This contrasts with previous work showing that RBF neural networks with two and more input nodes have superlinear VC dimension. The rules give rise also to lower bounds for network sizes, thus demonstrating the relevance of network parameters for the complexity of computing with RBF neural networks...|$|R
40|$|The <b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> (also {{known as}} the trace number) and the Sauer-Shelah lemma have found {{applications}} in numerous areas including set theory, combinatorial geometry, graph theory and statistical learning theory. Estimation {{of the complexity of}} discrete structures associated with the search space of algorithms often amounts to estimating the cardinality of a simpler class which is e#ectively induced by some restrictive property of the search. In this paper we study the complexity of Boolean-function classes of finite VC-dimension which satisfy a local `smoothness' property expressed as having long runs of repeated values. As in Sauer's lemma, a bound is obtained on the cardinality of such classes...|$|R
40|$|Abstract Let f be {{an unknown}} multivariate density {{belonging}} to a prespecified parametric class of densities, Fk, where k is unknown, but Fk ae Fk+ 1 for all k and each Fk has finite <b>Vapnik-Chervonenkis</b> <b>dimension.</b> Given an i. i. d. sample of size n drawn from f, we show {{that it is possible}} to select automatically, and without extra restrictions on f, an estimate fn;^k with the property that Ef R jfn;^k Γ f jg = O(1 =pn). Our method is inspired by the combinatorial tools developed in Devroye and Lugosi [16] and it includes a wide range of density models, such as mixture models or exponential families. Index Terms [...] Multivariate density estimation, <b>Vapnik-Chervonenkis</b> <b>dimension,</b> mixture densities, penalization...|$|E
40|$|Abstract. Given any natural nmnber d, 0 2, then fd(e) > ~log~ {{which is}} not far from being optimal, if d is fi-xed and e ~ 0. Further, we prove that fl(e) = ma~c(2,[l#]- 1), and similar bounds are esta-blislhed for some special classes of range spaces of <b>Vapnik-Chervonenkis</b> <b>dimension</b> three. ...|$|E
40|$|Given any natural numberd, 0 \u 26 lt;d - 2 + \frac 2 d + 2 \leqslant lime® 0 \fracfd (e) (1 /e) log(1 /e) \leqslant d. Unknown control {{sequence}} '\leqslant' Further, we prove thatf 1 () =max(2, 1 / – 1), and similar bounds are established for some special classes of range spaces of <b>Vapnik-Chervonenkis</b> <b>dimension</b> three...|$|E
40|$|Part 16 : Multi Layer ANNInternational audienceNNIGnets is a {{freeware}} {{computer program}} {{which can be}} used for teaching, research or business applications, of Artificial Neural Networks (ANNs). This software includes presently several tools for the application and analysis of Multilayer Perceptrons (MLPs) and Radial Basis Functions (RBFs), such as stratified Cross-Validation, Learning Curves, Adjusted Rand Index, novel cost functions, and <b>Vapnik–Chervonenkis</b> (VC) <b>dimension</b> estimation, which are not usually found in other ANN software packages. NNIGnets was built following a software engineering approach which decouples operative from GUI functions, allowing an easy growth of the package. NNIGnets was tested by a variety of users, with different backgrounds and skills, who found it to be intuitive, complete and easy to use...|$|R
40|$|Abstract. The <b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> {{plays an}} {{important}} role in statistical learning the-ory. In this paper, we propose the discretized VC dimension obtained by discretizing the range of a real function class. Then, we point out that Sauer’s Lemma is valid for the discretized VC dimension. We group the real function classes having the infinite VC dimension into four categories by using the discretized VC dimension. As a byproduct, we present the equidistantly discretized VC dimension by introducing an equidistant partition to segmenting the range of a real function class. Finally, we obtain the error bounds for real function classes based on the discretized VC dimensions in the PAC-learning framework. Key words: VC dimension, statistical learning theory, error bound, real function class. ...|$|R
40|$|This report {{describes}} a recent bound on expected {{support vector machine}} (SVM) generalization error [3] and frames this work {{in the context of}} computational learning theory [1], discussing the practical value of these bounds and the value to our mathematical understanding of machine learning. The fundamentals of computational learning theory are first outlined: PAC learning is defined, an example of PAC learning is given, and the <b>Vapnik-Chervonenkis</b> (VC) <b>dimension</b> is defined and related to PAC learning. Support vector machines are then briefly described. The span of support vector concept (and the bound on expected error that is based upon it) is described. The SVM bound using span of support vectors is given for the case of separable data and the proof given in [3] is outlined. ...|$|R
