703|2297|Public
25|$|Higher-dimensional objects can be visualized {{in form of}} {{projections}} (views) {{in lower}} dimensions. In particular, 4-dimensional objects are visualized by means of projection in three dimensions. The lower-dimensional projections of higher-dimensional objects {{can be used for}} purposes of <b>virtual</b> <b>object</b> manipulation, allowing 3D objects to be manipulated by operations performed in 2D, and 4D objects by interactions performed in 3D.|$|E
25|$|Border Memorial: Frontera de los Muertos is an {{augmented}} reality public art project and memorial, {{dedicated to the}} thousands of migrant workers who have died along the U.S./Mexico border in recent years trying to cross the desert southwest in search of work and a better life. Built for smartphone mobile devices, this project allows people to visualize the scope of the loss of life by marking each location where human remains have been recovered with a <b>virtual</b> <b>object</b> or augmentation. The public can simply download and launch a mobile application and aim their devices’ cameras at the landscape along the border and the surrounding desert. The application uses geolocation software to superimpose individual augments at the precise GPS coordinates of each recorded death, enabling the public to see the objects integrated into the physical location as if they existed in the real world.|$|E
2500|$|Using a {{positive}} lens of focal length f, a virtual image results when , the lens thus being used {{a magnifying glass}} (rather than if [...] as for a camera). Using a negative lens (...) with a real object (...) can only produce a virtual image (...) , {{according to the above}} formula. It is also possible for the object distance S1 to be negative, in which case the lens sees a so-called <b>virtual</b> <b>object.</b> This happens when the lens is inserted into a converging beam (being focused by a previous lens) before the location of its real image. In that case even a negative lens can project a real image, as is done by a Barlow lens.|$|E
40|$|We {{propose a}} system that enables dynamic 3 D {{interaction}} with real and <b>virtual</b> <b>objects</b> using an optical see-through head-mounted dis-play and an RGB-D camera. The <b>virtual</b> <b>objects</b> move according to physical laws. The system uses a physics engine for calculation of the motion of <b>virtual</b> <b>objects</b> and collision detection. In addition, the system performs collision detection between <b>virtual</b> <b>objects</b> and real objects in the three-dimensional scene obtained from the cam-era which is dynamically updated. A user wears the device and interacts with <b>virtual</b> <b>objects</b> in a seated position. The system gives users {{a great sense of}} reality through an interaction with virtual ob-jects...|$|R
40|$|To {{create an}} {{effective}} illusion of <b>virtual</b> <b>objects</b> coexisting {{with the real}} world, see-through HMD-based Augmented Reality techniques supplement the user's view with images of <b>virtual</b> <b>objects.</b> We introduce here a new paradigm, Spatially Augmented Reality (SAR), where <b>virtual</b> <b>objects</b> are rendered directly within or on the user's physical space...|$|R
40|$|This paper {{describes}} the prototype {{implementation of a}} pervasive, wearable augmented reality (AR) system based on a full body-motion-capture system using low-power wireless sensors. The system uses body motion to visualize and interact with <b>virtual</b> <b>objects</b> populating AR settings. Body motion is used to implement a whole body gesture-driven interface to manipulate the <b>virtual</b> <b>objects.</b> Gestures are mapped to correspondent behaviors for <b>virtual</b> <b>objects,</b> such as controlling the playback and volume of virtual audio players or displaying a <b>virtual</b> <b>object’s</b> metadata...|$|R
2500|$|Interaction {{with the}} virtual world is handled {{in a similar fashion}} to {{contemporary}} video games. Software tools are visually represented as tactile tools (e.g. a fishing rod) which occupy 3D virtual space and must be manipulated by hand. [...] Sophisticated tools include metatags, visually represented as o-fuda (paper talismans), and efficacious chalk circles called angō, meaning [...] "code", which have a variety of effects. Virtual objects such as pets cannot be recalled or reset; when a pet runs away, it must be chased and caught in 3D space. Virtual objects and pets are also susceptible to a form of [...] "death" [...] by data corruption or deletion. For a <b>virtual</b> <b>object</b> or pet to be deleted from cyberspace, it can be attacked by a virtual weapon, such as Searchmaton's beam weapons. [...] While users of dennō eyeglasses cannot be physically harmed by virtual weapons, their costly personal data and virtual possessions can be damaged.|$|E
50|$|A {{reconstruction}} {{program can}} create three-dimensional objects that mimic the real {{objects from the}} photographed scene. Using data from the point cloud and the user's estimation, the program can create a <b>virtual</b> <b>object</b> and then extract a texture from the footage that can be projected onto the <b>virtual</b> <b>object</b> as a surface texture.|$|E
50|$|For the <b>virtual</b> <b>object</b> in {{a virtual}} world case, an object is called smart when it {{has the ability to}} {{describe}} its possible interactions. This focuses on constructing a virtual world using only virtual objects that contain their own interaction information. There are four basic elements to constructing such a smart <b>virtual</b> <b>object</b> framework.|$|E
40|$|We {{investigated}} whether computer-generated stereo projected images (<b>virtual</b> <b>objects)</b> {{are suitable}} as targets in grasping experiments. For real and <b>virtual</b> <b>objects</b> we measured grasp quality, movement time, preshape aperture, and terminal aperture angle under two different conditions: (a) block presentation of <b>virtual</b> and real <b>objects</b> and (b) mixed presentation (real and <b>virtual</b> <b>objects</b> interspersed). Maximal number of consecutive presentations of <b>virtual</b> <b>objects</b> {{was limited to}} seven under the mixed condition. Six subjects took part in condition (a) and ten subjects in condition (b). Objects had a smooth spline-based contour with {{a limited number of}} stable grasp points. Six objects were presented in six different orientations and each grasp was repeated five times. We found that in block presentation mode grasp quality was generally lower for <b>virtual</b> <b>objects</b> (43) than for real objects (78). Analysis of the time course revealed that grasp quality was initially the same for both types of objects, but decreased for <b>virtual</b> <b>objects</b> {{during the course of the}} experiment. In mixed presentation mode, no such difference was found: grasp quality was equally high for both object types (75). No significant differences in movement time and other dynamic parameters were found. One of the major differences between the two conditions is the lack of haptic feedback for <b>virtual</b> <b>objects</b> in block presentation mode. We argue that missing haptic feedback for <b>virtual</b> <b>objects</b> leads to pantomiming and decreased grasp quality. Haptic feedback is necessary for maintaining grasp quality at a stable level...|$|R
40|$|Figure 1 : The see-through mobile AR system {{enables a}} user to {{interact}} with <b>virtual</b> <b>objects</b> overlaid on the see-through images displayed on a mobile display. In this paper, we propose an interaction {{system in which the}} appearance of the image displayed on a mobile display is consistent with that of the real space and that enables a user {{to interact with}} <b>virtual</b> <b>objects</b> overlaid on the image using the user’s hand. The three-dimensional scene obtained by a depth camera is projected according to the user’s viewpoint position obtained by face tracking, and the see-through image whose appearance is consistent with that outside the mobile display is generated. Interaction with <b>virtual</b> <b>objects</b> is realized by using the depth information obtained by the depth camera. To move <b>virtual</b> <b>objects</b> as if they were in real space, <b>virtual</b> <b>objects</b> are rendered in the world coordinate system that is fixed to a real scene even if the mobile display moves, and the direction of gravitational force added to <b>virtual</b> <b>objects</b> is made consistent with that of the world coordinate system. The former is realized by using the ICP (Iterative Closest Point) algorithm and the latter is realized by using the information obtained by an accelerometer. Thus, natural interaction with <b>virtual</b> <b>objects</b> using the user’s hand is realized...|$|R
40|$|Virtual environments engage {{millions}} of people {{and billions of dollars}} each year. What is the ontological status of the <b>virtual</b> <b>objects</b> that populate those environments? An adequate answer to that question requires a developed semantics for virtual environments. The truth-conditions must be identified for “tree”-sentences when uttered by speakers immersed in a virtual environment (VE). It will be argued that statements about <b>virtual</b> <b>objects</b> have truth-conditions roughly comparable to the verificationist conditions popular amongst some contemporary antirealists. This {{does not mean that the}} <b>virtual</b> <b>objects</b> lack ontological standing. There is an important sense in which <b>virtual</b> <b>objects</b> are no less real for being mind-dependent...|$|R
50|$|Nirvana is <b>virtual</b> <b>object</b> storage {{software}} {{developed and}} maintained by General Atomics.|$|E
5000|$|Anchors {{describe}} the spatial {{relation between the}} physical and the <b>virtual</b> <b>object.</b>|$|E
5000|$|VisualAssets {{describe}} {{the appearance of}} the <b>virtual</b> <b>object</b> in the augmented scene.|$|E
30|$|In {{contrast}} with Video-See-Through, {{the real world}} can only be suppressed by increasing the illumination level of the <b>virtual</b> <b>objects,</b> which is of course limited. Creating dark <b>virtual</b> <b>objects</b> in a bright real world is hence cumbersome.|$|R
40|$|Properly {{lighting}} <b>virtual</b> <b>objects</b> {{and having}} them cast shadows {{on each other}} based on dynamic lighting conditions is a computationally hard challenging Even more complex are the issues raised when this must {{be done in a}} Mixed Reality environment where <b>virtual</b> <b>objects</b> and <b>virtual</b> light interact with real objects and real light. This paper presents several interactive-time algorithms to enable interaction between real and <b>virtual</b> <b>objects,</b> including color matching, lighting and shadowing techniques. 1...|$|R
40|$|This paper {{describes}} a novel use of augmented reality for the visualisation of <b>virtual</b> <b>objects</b> {{as part of}} the move towards pervasive computing. It uses fiducial markers as switches to "toggle" the displayed properties of the <b>virtual</b> <b>objects.</b> Using collision detection, fiducial markers are also used to track and select nodes within <b>virtual</b> <b>objects.</b> This research uses the ARToolkit Version 2. 33 and acts as a component within the DSTO's InVision framework...|$|R
5000|$|In {{computer}} graphical user interfaces, {{drag and}} drop is a pointing device gesture in which the user selects a <b>virtual</b> <b>object</b> by [...] "grabbing" [...] it and dragging it to a different location or onto another <b>virtual</b> <b>object.</b> In general, {{it can be used}} to invoke many kinds of actions, or create various types of associations between two abstract objects.|$|E
5000|$|... 1994: Julie Martin creates first 'Augmented Reality Theater production', Dancing In Cyberspace, {{funded by}} the Australia Council for the Arts, {{features}} dancers and acrobats manipulating body-sized <b>virtual</b> <b>object</b> in real time, projected into the same physical space and performance plane. The acrobats appeared immersed within the <b>virtual</b> <b>object</b> and environments. The installation used Silicon Graphics computers and Polhemus sensing system.|$|E
5000|$|... #Caption: A Barlow lens (B) reimages a <b>virtual</b> <b>object</b> (focus of red ray path) into a {{magnified}} {{real image}} (green rays at focus) ...|$|E
30|$|For {{interaction}} with <b>virtual</b> <b>objects</b> a 5 DT data glove [20] is used. A data-glove with RFID reader (not shown here) {{was made to}} make it possible to change/manipulate <b>virtual</b> <b>objects</b> when a tagged real object is touched.|$|R
40|$|Abstract- This paper {{presents}} an interactive virtual reality game using real and <b>virtual</b> <b>objects</b> with general beam projector and computer vision. We {{show how the}} computer recognizes the objects and makes the <b>virtual</b> <b>objects</b> move after detecting the real objects in the game space created by a beam projector. To achieve the goals, we developed two methods: Find Objects Distance (FOD) method and Find Reflection Angle (FRA) method. After detecting the real and <b>virtual</b> <b>objects,</b> the methods predict the next moving direction of <b>virtual</b> <b>objects.</b> This interactive <b>virtual</b> reality (VR) game is named as “Ting Ting ” which contains various fire-shaped <b>virtual</b> <b>objects</b> and bamboo-shaped real objects. “Ting Ting ” game’s core technology {{can be used in}} various application area as an interactive communication engine. These include but not limited to, for example, an education game in schools, or an entertainment application in museums and/or science halls...|$|R
40|$|Pattern-based {{augmented}} reality systems {{are considered the}} most promising approach for accurately registering <b>virtual</b> <b>objects</b> with real-time video feeds. The problem with existing solutions {{is the lack of}} robustness to partial occlusions of the pattern, which is important when attempting natural interactions with the <b>virtual</b> <b>objects.</b> This paper describes a fast and accurate vision-based pattern tracking system that allows for autocalibrated 3 D augmentations of <b>virtual</b> <b>objects</b> onto known planar patterns. The tracking system is shown to be robust to changes in pattern scale, orientation, and most importantly partial occlusions. A method to detect a hand over top of the pattern is then described, along with a method to render the hand on top of the <b>virtual</b> <b>objects.</b> 1...|$|R
5000|$|... virtual smart objects by {{modelling}} {{both physical}} world objects and modelling humans as objects and their subsequent interactions can form a predominantly smart <b>virtual</b> <b>object</b> environment.|$|E
50|$|An ARTag is a {{fiduciary}} {{marker system}} to support augmented reality. They {{can be used}} to facilitate the appearance of virtual objects, games, and animations within the real world. Like the earlier ARToolKit system, they allow for video tracking capabilities that calculate a camera's position and orientation relative to physical markers in real time. Once the camera's position is known, a virtual camera can be positioned at the same point, revealing the <b>virtual</b> <b>object</b> at the location of the ARTag. It thus addresses two of the key problems in Augmented Reality: viewpoint tracking and <b>virtual</b> <b>object</b> interaction.|$|E
50|$|The MotionParallax3D {{displays}} are a {{class of}} virtual reality devices that {{create the illusion of}} volumetric objects by displaying a projection of a <b>virtual</b> <b>object</b> generated to match the viewer’s position relative to the screen.|$|E
50|$|HPE Unified Functional Testing may not {{recognize}} customized user interface objects and other complex objects. Users can define {{these types of}} <b>objects</b> as <b>virtual</b> <b>objects.</b> HPE Unified Functional Testing does not support <b>virtual</b> <b>objects</b> for analog recording or recording in low-level mode.|$|R
40|$|In {{this paper}} we {{investigate}} {{the use of}} <b>virtual</b> <b>objects</b> for knowledge exchange in communities. Information systems provide {{a wide range of}} new (<b>virtual)</b> <b>objects</b> for community members which support non-canonical collaboration required for knowledge creation [1, 2]. From a sociological perspective these objects are means to cross knowledge boundaries in communities [3]. In our study we extend this aspect by a technical perspective of how <b>virtual</b> <b>objects</b> effectively facilitate activities of knowledge creation. Media Synchronicity Theory [4] proposes how to best accomplish communication performance. It predicts that to achieve effective communication, the two primary communication strategies of conveyance of information and convergence on meaning need to be supported. Building upon this discussion, we examine the use of <b>virtual</b> <b>objects</b> in a dynamic process of knowledge creation. We will draw conclusions on how to appropriately use <b>virtual</b> <b>objects</b> for communication. Our empirical study is based on multiple cases [5] of knowledge communities. Qualitative data has been gathered from the participants of six focused group discussions conducted on a virtual whiteboard which comprises a media choice to interact in real time. The results detail information on the actual use (and not use) of <b>virtual</b> <b>objects</b> (media) for knowledge creation. Based on our findings we empirically confirm the core propositions of Media Synchronicity Theory. We conclude with managerial recommendations on how to employ <b>virtual</b> <b>objects</b> for increasing the effectiveness of dynamic processes of knowledge creation...|$|R
40|$|Abstract: The <b>virtual</b> <b>object’s</b> {{interactions}} {{depend on}} {{a huge number of}} human-computer definitions in virtual environment. A natural observation is that associated geometric structures between <b>virtual</b> <b>objects</b> can use to enhance autonomous interaction abilities, which will reduce the time-consuming manual operations. Based on this observation, the paper firstly studied the nature of relationship between interaction structures and interaction behaviors, and proposed a concept “Interaction Feature Pair ” (IFP) to represent the interaction structures. And then an interaction process model for <b>virtual</b> <b>objects</b> based on IFP was proposed, which gives a feasible method for <b>virtual</b> <b>objects</b> to interact autonomously. Finally, an assembly simulation system is developed using this model and an example is given to prove the validity of the model...|$|R
50|$|Vuforia is an Augmented Reality Software Development Kit (SDK) for mobile {{devices that}} enables the {{creation}} of Augmented Reality applications. It uses Computer Vision technology to recognize and track planar images (Image Targets) and simple 3D objects, such as boxes, in real-time. This image registration capability enables developers to position and orient virtual objects, such as 3D models and other media, in relation to real world images when these are viewed through the camera of a mobile device. The <b>virtual</b> <b>object</b> then tracks the position and orientation of the image in real-time so that the viewer’s perspective on the object corresponds with their perspective on the Image Target, so that {{it appears that the}} <b>virtual</b> <b>object</b> {{is a part of the}} real world scene.|$|E
5000|$|Brewster also {{discovered}} the [...] "wallpaper effect". He noticed that staring at repeated patterns in wallpapers could trick the brain into matching pairs {{of them as}} coming from the same <b>virtual</b> <b>object</b> on a virtual plane behind the walls. This {{is the basis of}} wallpaper-style [...] "autostereograms" [...] (also known as single-image stereograms).|$|E
50|$|The {{peculiarity}} {{of visual}} {{perception is that}} the brain interprets the latency of <b>virtual</b> <b>object</b> images not as latency but as a distortion of the geometry of virtual objects. In this case, the dissonance between the information the user receives from the visual perception and the vestibular apparatus can cause symptoms of virtual reality sickness which include nausea, headache, and ophthalmalgia (eye pain). Higher quality MotionParallax3D displays reduce the chances of these symptoms occurring. However, even {{in case of a}} perfect MotionParallax3D display, the symptoms of cybersickness can occur in susceptible individuals due to the dissonance of the focusing and convergence visual mechanisms; if a <b>virtual</b> <b>object</b> is located at a considerable distance from the surface on which the projected image is displayed, an attempt to fix the eyes and focus on the nearby virtual objects can lead to the opposite result.|$|E
40|$|Haptics used {{in natural}} {{contexts}} {{is much more}} efficient in identifying real <b>objects</b> than <b>virtual</b> <b>objects</b> via haptic displays. The {{aim of this study}} was to get some insight about the effects of object complexity on the identification of <b>virtual</b> <b>objects</b> varying in complexity on the basis of form properties. The <b>virtual</b> <b>objects</b> were synthetic human heads with varying number of face features. Fourteen sighted and ten visually impaired people took part in an experiment. The results were significant effects of complexity on both proportion of correct responses and exploration time, This indicates the relevance of object complexity for judgements about the usefulness of haptic displays. Training of users, simplification of <b>virtual</b> <b>objects</b> and development of more efficient haptic displays are discussed as potential solutions...|$|R
5000|$|A virtual {{touch screen}} (VTS) is a user {{interface}} system that augments <b>virtual</b> <b>objects</b> into reality either through a projector or optical display using sensors to track a person's {{interaction with the}} object. For instance, using a display and a rear projector system a person could create images that look three-dimensional and appear to float in midair. [...] Some systems utilize an optical head-mounted display to augment the <b>virtual</b> <b>objects</b> onto the transparent display utilizing sensors to determine visual and physical interactions with the <b>virtual</b> <b>objects</b> projected.|$|R
50|$|Spatially {{augmented}} reality (SAR) renders <b>virtual</b> <b>objects</b> directly within {{or on the}} user's physical space. A key benefit of SAR is that the user {{does not need to}} wear a head-mounted display. Instead, with the use of spatial displays, wide field of view and possibly high-resolution images of <b>virtual</b> <b>objects</b> can be integrated directly into the environment. For example, the <b>virtual</b> <b>objects</b> can be realized by using digital light projectors to paint 2D/3D imagery onto real surfaces, or by using built-in flat panel displays.|$|R
