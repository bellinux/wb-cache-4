3388|64|Public
25|$|Class {{prediction}} analysis: This approach, called supervised classification, {{establishes the}} basis for developing a predictive model into which future unknown test objects can be input in order to predict the most likely class membership of the test objects. Supervised analysis for class prediction involves use of techniques such as linear regression, k-nearest neighbor, learning <b>vector</b> <b>quantization,</b> decision tree analysis, random forests, naive Bayes, logistic regression, kernel regression, artificial neural networks, support vector machines, mixture of experts, and supervised neural gas. In addition, various metaheuristic methods are employed, such as genetic algorithms, covariance matrix self-adaptation, particle swarm optimization, and ant colony optimization. Input data for class prediction are usually based on filtered lists of genes which are predictive of class, determined using classical hypothesis tests (next section), Gini diversity index, or information gain (entropy).|$|E
2500|$|... 199 [...] - <b>Vector</b> <b>Quantization</b> Decompression for the National Imagery Transmission Format Standard ...|$|E
2500|$|Kurzweil {{believes}} {{these large}} scale simulations are valuable, but says a more explicit [...] "functional algorithmic model" [...] {{will be required}} to achieve human levels of intelligence. Kurzweil is unimpressed with neural networks and their potential while he's very bullish on <b>vector</b> <b>quantization,</b> hidden Markov models and genetic algorithms since he used all three successfully in his speech recognition work. Kurzweil equates pattern recognizers in the neocortex with statements in the LISP programming language, which is also hierarchical. He also says his approach is similar to Jeff Hawkins' hierarchical temporal memory, although he feels the hierarchical hidden Markov models have an advantage in pattern detection.|$|E
40|$|In {{this paper}} we {{describe}} a new method of classifier construction {{based on a}} class of sphere digraphs. We define class cover catch digraphs, and show how the minimum dominating sets define a collection of <b>vector</b> <b>quantizations</b> for the classification problem. These dominating sets, and the spheres associated with them, provide a method for classification. We illustrate this approach, and suggest a method for selecting among multiple minimum dominating sets via statistical depth...|$|R
40|$|A {{method for}} {{simultaneously}} estimating the high-resolution frames {{and the corresponding}} motion field from a compressed low-resolution video sequence is presented. The algorithm incorporates knowledge of the spatio-temporal correlation between low and high-resolution images to estimate the original high-resolution sequence from the degraded low-resolution observation. Information from the encoder is also exploited, including the transmitted motion <b>vectors,</b> <b>quantization</b> tables, coding modes and quantizer scale factors. Simulations illustrate an improvement in the peak signal-to-noise ratio when compared with traditional interpolation techniques and are corroborated with visual results...|$|R
40|$|Abstract. Self-Organizing Map (SOM) is an {{unsupervised}} learning neural network {{and it is}} used for preserving the structural relationships in the data without prior knowledge. SOM has been applied {{in the study of}} complex problems such as <b>vector</b> <b>quantizations,</b> combinatorial optimization, and pattern recognition. This paper proposes a new usage of SOM as a tool for schema transformation hoping to achieve more efficient genetic process. Every offspring is transformed into an isomorphic neural network with more desirable shape for genetic search. This helps genes with strong epistasis to stay close together in the chromosome. Experimental results showed considerable improvement over previous results. ...|$|R
50|$|Twin <b>vector</b> <b>quantization</b> (VQF) {{is part of}} the MPEG-4 {{standard}} {{dealing with}} time domain weighted interleaved <b>vector</b> <b>quantization.</b>|$|E
50|$|In {{computer}} science, learning <b>vector</b> <b>quantization</b> (LVQ), is a prototype-based {{supervised classification}} algorithm. LVQ is the supervised counterpart of <b>vector</b> <b>quantization</b> systems.|$|E
50|$|In data compression, twin <b>vector</b> <b>{{quantization}}</b> {{is related}} to <b>vector</b> <b>quantization,</b> but {{the speed of the}} quantization is doubled by the secondary vector analyzer.|$|E
5000|$|For each <b>quantization</b> <b>vector</b> {{centroid}} , let [...] {{denote the}} distance of [...] and ...|$|R
5000|$|Move {{the nearest}} <b>quantization</b> <b>vector</b> {{centroid}} towards this sample point, {{by a small}} fraction of the distance ...|$|R
3000|$|LSF vectors (compared to {{quantizing}} {{the whole}} K <b>vectors</b> in frame-by-frame <b>quantization)</b> {{compensate for the}} loss due to the modeling.|$|R
50|$|The {{usage of}} video codecs based on <b>vector</b> <b>quantization</b> has {{declined}} significantly {{in favor of}} those based on motion compensated prediction combined with transform coding, e.g. those defined in MPEG standards, as the low decoding complexity of <b>vector</b> <b>quantization</b> has become less relevant.|$|E
5000|$|... cluster: {{hierarchical}} clustering, <b>vector</b> <b>quantization,</b> K-means ...|$|E
5000|$|Daala is transform-based but uses <b>vector</b> <b>quantization</b> on {{transformed}} coefficients ...|$|E
40|$|Image {{and video}} {{analysis}} requires rich features that can characterize {{various aspects of}} visual information. These rich features are typically extracted from the pixel values of the images and videos, which require huge amount of computation and seldom useful for real-time analysis. On the contrary, the compressed domain analysis offers relevant information pertaining to the visual content {{in the form of}} transform coefficients, motion <b>vectors,</b> <b>quantization</b> steps, coded block patterns with minimal computational burden. The quantum of work done in compressed domain is relatively much less compared to pixel domain. This paper aims to survey various video analysis efforts published during the last decade across the spectrum of video compression standards. In this survey, we have included only the analysis part, excluding the processing aspect of compressed domain. This analysis spans through various computer vision applications such as moving object segmentation, human action recognition, indexing, retrieval, face detection, video classification and object tracking in compressed videos...|$|R
40|$|We {{present a}} new Trellis Coded Vector Residual Quantizer (TCVRQ) that {{combines}} trellis coding and <b>vector</b> residual <b>quantization.</b> We propose new methods for computing quantization levels and experimentally analyze {{the performances of}} our TCVRQ {{in the case of}} still image coding. Experimental comparisons show that our quantizer performs better than the standard Tree and Exhaustive Search Quantizers based on the Generalized Lloyd Algorithm (GLA) ...|$|R
40|$|Abstract. One of {{the most}} {{important}} feature of the Neural Gas is its ability to preserve the topology in the projection of highly dimensional input spaces to lower dimensions <b>vector</b> <b>quantizations.</b> For this reason, the Neural Gas {{has proven to be a}} valuable tool in data mining applica-tions. In this paper an incremental ensemble method for the combination of various Neural Gas models is proposed. Several models are trained with bootstrap samples of the data, the “codebooks ” with similar Voronoi polygons are merged in one fused node and neighborhood relations are established by linking similar fused nodes. The aim of combining the Neural Gas is to improve the quality and robustness of the topological representation of the single model. We have called this model Fusion-NG. Computational experiments show that the Fusion-NG model effectively preserves the topology of the input space and improves the representation of the single Neural Gas model. Furthermore, the Fusion-NG explicitly shows the neighborhood relations of it prototypes. We report the perfor-mance results using synthetic and real datasets, the latter obtained from a benchmark site...|$|R
5000|$|Growing Neural Gas, a neural network-like {{system for}} <b>vector</b> <b>quantization</b> ...|$|E
5000|$|Opus is transform-based but uses <b>vector</b> <b>quantization</b> on {{transformed}} coefficients ...|$|E
50|$|Fractal image {{compression}} has many similarities to <b>vector</b> <b>quantization</b> {{image compression}}.|$|E
5000|$|Parametric/Bitstream Methods (NR-B): These metrics {{make use}} of {{features}} extracted from the transmission container and/or video bitstream, e.g. MPEG-TS packet headers, motion <b>vectors</b> and <b>quantization</b> parameters. They {{do not have access}} to the original signal and require no decoding of the video, which makes them more efficient. In contrast to NR-P metrics, they have no access to the final decoded signal. However, the picture quality predictions they deliver are not very accurate.|$|R
40|$|We {{present a}} new Trellis Coded Vector Residual Quantizer (TCVRQ) that {{combines}} trellis coding and <b>vector</b> residual <b>quantization.</b> We propose new methods for computing quantization levels and experimentally analyze {{the performances of}} our TCVRQ {{in the case of}} still image coding. Experimental comparisons show that our quantizer performs better than the standard Tree and Exhaustive Search Quantizers based on the Generalized Lloyd Algorithm (GLA). 1. A NEW TRELLIS <b>VECTOR</b> RESIDUAL QUANTIZER <b>Quantization</b> is the process of approximating a continuous -amplitude signal by a digital (discrete-amplitude) signal, minimizing a distortion measure (or error). Unfortunately, fixed the coding rate R, an optimal VQ requires computational and storage resources that grow exponentially with the vector dimension. Moreover, Lin in [1] showed that the design of an optimal Vector Quantizer is an NP-complete problem. The design of sub-optimal vector quantizers is an interesting alternative to scalar quantizers f [...] ...|$|R
40|$|Abstract—Successive {{approximation}} (SA) quantization {{is part of}} many ofthe state-of-the-art {{image and}} video compression methods. In this article we first make a review of it, starting from the classical optimality considerations of Equitz and Cover and then proceeding to Mallat and Falzon results concern-ing low bit-rate transform coding. We then develop a general theory of SA quan-tization which we refer to as α-expansions. This theory explains the published results obtained by both scalar and <b>vector</b> SA <b>quantization</b> methods, and indi-cates how further performance improvements can be obtained...|$|R
5000|$|The Linde-Buzo-Gray algorithm, a {{generalization}} of this algorithm for <b>vector</b> <b>quantization</b> ...|$|E
5000|$|... 199 - <b>Vector</b> <b>Quantization</b> Decompression for the National Imagery Transmission Format Standard ...|$|E
5000|$|BTFbase - BTF {{compression}} {{based on}} a multi-level <b>vector</b> <b>quantization</b> (free BTF shader) ...|$|E
40|$|International audienceFor the {{distribution}} characteristics in {{a slice of}} pathological cell image, the system transforms them into the characteristic <b>vector</b> by <b>quantization</b> and clustering in HSV color model, it promotes the concerned isolated pixel color description into the color feature of its neighborhood region color histogram. By choosing appropriate neighborhood window size, it uses color information entropy method to gain efficient primary classification of gastric slice image and the similar retrieval results from the sample library images. Experiments show that this method has a better image classification and retrieval result...|$|R
40|$|We {{propose a}} new vector {{encoding}} scheme (tree quan-tization) that obtains lossy compact codes for high-dimensional vectors via tree-based dynamic programming. Similarly to several previous schemes such as product quantization, these codes correspond to codeword num-bers within multiple codebooks. We propose an integer programming-based optimization that jointly recovers the coding tree {{structure and the}} codebooks by minimizing the compression error on a training dataset. In the experiments with diverse visual descriptors (SIFT, neural codes, Fisher <b>vectors),</b> tree <b>quantization</b> is shown to combine fast encod-ing and state-of-the-art accuracy {{in terms of the}} compres-sion error, the retrieval performance, and the image classi-fication error. 1...|$|R
40|$|This paper {{presents}} a new Trellis Coded Vector Residual Quantizer (or TCVRQ) that combines trellis coding and <b>vector</b> residual <b>quantization.</b> The TCVRQ is a general-purpose sub-optimal Vector Quantizer with low computational costs and small memory requirement that permits high memory savings {{when compared to}} traditional quantizers. Experiments confirm that TCVRQ is a good compromise between memory/speed requirements and quality {{and that it is}} not sensitive to codebook design errors. A method is proposed for computing quantization levels and the performance of the TCVRQ when applied to speech coding at very low bit rates and to direct image coding is experimentally analyzed...|$|R
5000|$|... #Caption: <b>Vector</b> <b>quantization</b> {{of colors}} {{present in the}} image above into Voronoi cells using k-means.|$|E
50|$|<b>Vector</b> <b>quantization</b> is {{used for}} lossy data compression, lossy data correction, pattern recognition, density {{estimation}} and clustering.|$|E
50|$|Models and {{algorithms}} {{based on}} the principle of competitive learning include <b>vector</b> <b>quantization</b> and self-organizing maps (Kohonen maps).|$|E
40|$|International audienceThis paper {{presents}} bitstream-based features forperceptual quality {{estimation of}} HEVC coded videos. Variousfactors including {{the impact of}} different sizes of block-partitions,use of reference-frames, the relative amount of various predictionmodes, statistics of motion <b>vectors</b> and <b>quantization</b> parametersare taken into consideration for producing 52 features relevantfor perceptual quality prediction. The used test stimuli constitutes 560 bitstreams that have been carefully extracted for this analysisfrom the 59, 520 bistreams of the large-scale database generatedby the Joint Effort Group (JEG) of the Video Quality ExpertsGroup (VQEG). The obtained results show the signiﬁcance of theconsidered features through reasonably accurate and monotonicprediction {{of a number of}} objective quality metrics...|$|R
40|$|For visual word based {{location}} {{recognition in}} 3 D models we propose a novel distance-weighted scoring scheme. Matching visual words are not treated as perfect matches anymore but are weighted with {{the distance of}} the original SIFT feature <b>vectors</b> before <b>quantization.</b> To maintain the scalability and efficiency of vocabulary tree based approaches PCA compressed SIFT feature vectors are used instead of the original SIFT features. A different eigenspace is computed for each vocabulary tree cell to benefit from the variance reduction as result of the partitioned SIFT feature space. Experiments show a significant improvement in retrieval quality by incorporating the distance with small costs in computational time and memory. ...|$|R
40|$|Abstract—This paper {{presents}} bitstream-based {{features for}} perceptual quality estimation of HEVC coded videos. Various factors including {{the impact of}} different sizes of block-partitions, use of reference-frames, the relative amount of various prediction modes, statistics of motion <b>vectors</b> and <b>quantization</b> parameters are taken into consideration for producing 52 features relevant for perceptual quality prediction. The used test stimuli constitutes 560 bitstreams that have been carefully extracted for this analysis from the 59, 520 bistreams of the large-scale database generated by the Joint Effort Group (JEG) of the Video Quality Experts Group (VQEG). The obtained results show {{the significance of the}} considered features through reasonably accurate and monotonic prediction of a number of objective quality metrics...|$|R
