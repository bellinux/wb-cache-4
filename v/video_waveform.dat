8|22|Public
50|$|A card an {{associated}} hardware that allowed for complex <b>video</b> <b>waveform</b> and vectorscope analysis and test pattern generation.|$|E
50|$|Australian company, IRT, came up {{with the}} VIMCAS {{solution}} which integrated stereo audio into the <b>video</b> <b>waveform</b> of a television signal during the vertical blanking interval. The audio quality was rated up to 15 kHz on each channel, using 3 lines of vertical interval.|$|E
5000|$|Y carries luma (brightness or luminance) and {{synchronization}} (sync) information. Y = 0.2126 R + 0.7152 G + 0.0722 B. Before {{the advent}} of color television, the Y axis on an oscilloscope display of a <b>video</b> <b>waveform</b> represented {{the intensity of the}} scan line. With color, Y still represents intensity but it is a composite of the component colors.|$|E
40|$|International Telemetering Conference Proceedings / October 29 -November 02, 1990 / Riviera Hotel and Convention Center, Las Vegas, NevadaThe {{concepts}} of modulation index and FM improvement are simple and straightforward when the modulating signals are sinesoidal. For a complex baseband waveform such as analog TV, the FM improvement may be seriously underestimated. A method for computer simulation of <b>video</b> <b>waveforms</b> {{and the resulting}} spectra are presented...|$|R
40|$|Equal but {{opposite}} <b>video</b> <b>waveforms</b> generated. Unbalanced {{input line}} terminated in 75 -ohm resistor, R 1. Capacitor C 1 blocks dc component of input so only time-varying component fed to Q 1 and inverted in polarity. Circuit {{intended for use}} with input television signal having polarity, amplitude, and dc bias such that tips of synchronizing pulses lie at zero volts while reference white level is specified positive dc level (typically about 1 volt) ...|$|R
50|$|The suite {{room will}} also have video {{equipment}} in the Production control room for monitoring the video signal like: <b>Video</b> monitor, <b>Waveform</b> monitor and vectorscope.|$|R
50|$|For {{this reason}} a white clipper circuit {{is used to}} {{strictly}} limit the maximum amplitude of the <b>video</b> <b>waveform</b> at the transmitter. For recording and general use the white clipper is used to limit the maximum excursion of the signal. For recording this prevents overmodulation on analogue systems which tends to corrupt adjacent tracks in helical scanning; while on digital systems the peak signal amplitude is an absolute numerical value and cannot be exceeded.|$|E
50|$|In the {{original}} system, {{as applied to}} 625 line analogue TV, the audio signal was sampled twice during each television line and each sample converted to 10-bit PCM. Two such samples were inserted into the next line synchronising pulse. At the destination, the audio samples were converted back to analogue form and the <b>video</b> <b>waveform</b> restored to normal. Compandors operating on the signal before encoding and after decoding enabled the required signal-to-noise ratio to be achieved. As the PCM noise was predominantly high-pitched, the compandor only needed to operate on the high frequencies. Also, the compandor only operated at high audio levels, so that modulation of the noise by the companding would be masked by the relatively loud high-frequency audio components. A pilot tone at half the sampling frequency was transmitted to enable the expander to track the gain adjustment applied by the compressor, even when the latter was limiting.|$|E
40|$|A novel {{scheme for}} error-resilient digital video broadcasting, using Wyner-Ziv coding, is {{presented}} in this paper. We apply the general framework of systematic lossy source-channel coding to generate a supplementary bitstream that can correct transmission errors in the decoded <b>video</b> <b>waveform</b> {{up to a certain}} residual distortion. The systematic portion consists of a conventional MPEG-coded bitstream, which is transmitted over the error-prone channel without any forward error correction. The supplementary bitstream is a low rate representation of the transmitted video sequence generated using Wyner-Ziv encoding. We use the conventionally decoded error-concealed MPEG video sequence as side information to decode the Wyner-Ziv bits. The decoder combines the errorprone side information and the Wyner-Ziv description to yield an improved decoded video signal. We show how this scheme can be used to build a system which achieves graceful degradation without the need of a layered representation of the <b>video</b> <b>waveform.</b> 1...|$|E
40|$|Abstract — This paper {{discusses}} the modeling {{and analysis of}} a systematic lossy source channel coding system for errorresilient video transmission. The systematic portion of the transmission consists of a video bitstream transmitted without channel coding over an error-prone channel. Error-resilience is achieved by transmitting a supplementary bitstream generated by Wyner-Ziv encoding of the video signal. The scheme is attractive because it provides gracefully decreasing video quality over a range of symbol error rates, when compared with conventional FEC. We propose {{a model for the}} endto-end video distortion delivered by this system. The model considers the encoder’s rate-distortion trade-off, quantization mismatch from Wyner-Ziv coding, previous frame error concealment and error propagation. The model predictions agree closely with experimental results and thereby suggest an optimization algorithm to find the best rate-distortion tradeoff for systematic lossy forward error protection of <b>video</b> <b>waveforms.</b> I...|$|R
40|$|This paper proposes {{systematic}} lossy error {{protection of}} <b>video</b> <b>waveforms</b> using multiple embedded Wyner-Ziv video descriptions. A video signal transmitted over an error-prone channel, without channel coding, constitutes the systematic {{portion of the}} transmission. Error protection is achieved by additionally transmitting two or more bitstreams generated by Wyner-Ziv coding of the video signal. The Wyner-Ziv bitstreams contain multiple embedded coarsely quantized descriptions of the original video signal. In the event of channel errors, the Wyner-Ziv descriptions are decoded, thereby limiting the maximum distortion that can occur. The available Wyner-Ziv bitrate is allocated among the multiple embedded descriptions, using the observation that the visibility of slice losses and quantization artifacts depends on the employed coding structure. Experimental {{results show that the}} trade-off between error-resilience and residual quantization distortion can be better exploited using multiple embedded Wyner-Ziv descriptions, compared to earlier results with a single Wyner-Ziv bitstream. The system delivers gracefully deteriorating video quality without requiring a layered video compression scheme in the systematic portion of the transmission. 1...|$|R
40|$|In {{this paper}} {{automatic}} focusing of a visual inspection system is described. A vidicon camera was mounted {{on top of}} a high powered microscope to capture an image, process it and send an appropriate signal to a stepper motor which controls the movements of the microscope. The method used is based on the acutance of the video signal wave form. Acutance {{is a measure of the}} steepness of the <b>video</b> signal <b>waveform.</b> It implies that a sharp image will have a sharp gradient of the gray level across the image. This is measured vertically and horizontally and used to control the movements of a stepper motor t...|$|R
40|$|We {{present a}} {{practical}} scheme for error-resilient digital video broadcasting, using the Wyner-Ziv coding paradigm. We apply the general framework of systematic lossy source-channel coding {{to generate a}} supplementary bitstream that can correct transmission errors in the decoded <b>video</b> <b>waveform</b> {{up to a certain}} residual distortion. The systematic portion consists of a conventional MPEG- 2 bitstream, which is transmitted over the error-prone channel without forward error correction. The supplementary bitstream is a low rate representation of the transmitted video sequence generated using Wyner-Ziv encoding. We use the received error-prone MPEG- 2 prediction error signal as side information to decode the Wyner-Ziv bits. The decoder combines the error-prone side information and the Wyner-Ziv description to yield an improved decoded video signal. We describe a system that uses an embedded Wyner-Ziv codec to achieve graceful quality degradation without the need for a layered representation of the <b>video</b> <b>waveform...</b>|$|E
40|$|We {{present a}} novel scheme for error-resilient digital video broadcasting, using the Wyner-Ziv coding paradigm. We apply the general {{framework}} of systematic lossy source-channel coding {{to generate a}} supplementary bitstream that can correct transmission errors in the decoded <b>video</b> <b>waveform</b> {{up to a certain}} residual distortion. The systematic portion consists of a conventional MPEG-coded bitstream, which is transmitted over the errorprone channel without forward error correction. The supplementary bitstream is a low rate representation of the transmitted video sequence generated using Wyner-Ziv encoding. We use the conventionally decoded errorconcealed MPEG video sequence as side information to decode the Wyner-Ziv bits. The decoder combines the error-prone side information and the Wyner-Ziv description to yield an improved decoded video signal. Our results indicate that, over a large range of channel error probabilities, this scheme yields superior video quality when compared with traditional forward error correction techniques employed in digital video broadcasting. Keywords: Wyner-Ziv coding, systematic lossy source-channel coding, forward error protection, side information decoding. 1...|$|E
5000|$|The Helical scanner used a tape wrap of 188.57 degrees {{around a}} drum of 3.170 inches in diameter, with two play/record heads. In the NTSC {{version of the}} format, it had 5 helical tracks (segments) per field and 6 in the PAL version, each with 57 lines per segment. The VTR was {{equipped}} with a color <b>video</b> monitor, a <b>waveform</b> monitor scope, and vectorscope.|$|R
5|$|HDMI {{implements}} the EIA/CEA-861 standards, which define <b>video</b> formats and <b>waveforms,</b> {{transport of}} compressed, uncompressed, and LPCM audio, auxiliary data, and implementations of the VESA EDID. CEA-861 signals carried by HDMI are electrically {{compatible with the}} CEA-861 signals used by the digital visual interface (DVI). No signal conversion is necessary, nor is there a loss of video quality when a DVI-to-HDMI adapter is used. The CEC (Consumer Electronics Control) capability allows HDMI devices to control each other when necessary and allows the user to operate multiple devices with one handheld remote control device.|$|R
40|$|This paper {{gives an}} {{overview}} of the upcoming IEEE Gigabit Wireless LAN amendments, i. e. IEEE 802. 11 ac and 802. 11 ad. Both standard amendments advance wireless networking throughput beyond gigabit rates. 802. 11 ac adds multi-user access techniques in the form of downlink multi-user (DL MU) multiple input multiple output (MIMO) and 80 and 160 MHz channels in the 5 GHz band for applications such as multiple simultaneous video streams throughout the home. 802. 11 ad takes advantage of the large swath of available spectrum in the 60 GHz band and defines protocols to enable throughput intensive applications such as wireless I/O or uncompressed <b>video.</b> New <b>waveforms</b> for 60 GHz include single carrier and orthogonal frequency division multiplex (OFDM). Enhancements beyond the new 60 GHz PHY include Personal Basic Service Set (PBSS) operation, directional medium access, and beamforming. We describe 802. 11 ac channelization, PHY design, MAC modifications, and DL MU MIMO. For 802. 11 ad, the new PHY layer, MAC enhancements, and beamforming are presented...|$|R
5000|$|Microsoft {{released}} Windows Essentials 2012 on August 7, 2012 for Windows 7 and Windows 8 users. Windows Essentials 2012 included SkyDrive for Windows (later renamed OneDrive), {{and dropped}} Windows Live Mesh, Messenger Companion and Bing Bar. Microsoft Family Safety is also installed for Windows 7 users only, as Windows 8 has built-in family safety functionalities. Further, Windows Essentials 2012 also dropped the [...] "Windows Live" [...] branding from the installer itself, {{as well as}} from programs such as Photo Gallery and Movie Maker, which have been branded Windows Photo Gallery and Windows Movie Maker respectively. These two programs have also received several updates and enhancements since their 2011 release, including <b>video</b> stabilization, <b>waveform</b> visualizations, new narration tracks, audio emphasizing, default save as H.264 format, and enhanced text effects for Movie Maker; as well as AutoCollage integration and addition of Vimeo as a publishing partner for Photo Gallery. No significant changes or re-branding were made in this release for other programs such as Windows Live Messenger, Windows Live Mail, and Windows Live Writer.|$|R
50|$|Originally, {{waveform}} monitors {{were entirely}} analog devices; the incoming (analog) video signal was filtered and amplified, {{and the resulting}} voltage was used to drive the vertical axis of a cathode ray tube. A sync stripper circuit was used to isolate the sync pulses and colorburst from the video signal; the recovered sync information was fed to a sweep circuit which drove the horizontal axis. Early waveform monitors differed little from oscilloscopes, except for the specialized <b>video</b> trigger circuitry. <b>Waveform</b> monitors also {{permit the use of}} external reference; in this mode the sync and burst signals are taken from a separate input (thus allowing all devices in a facility to be genlocked, or synchronized to the same timing source).|$|R
40|$|Within thrips feeding behaviour, {{sequences}} of four waveforms have been distinguished {{earlier in the}} DC-EPG, i. e. P, Q, R and S, representing mandibular stylet insertion, maxillary stylet insertion, ingestion, and repetitive mandibular insertion, respectively. During signal analysis it appeared that transitions from one waveform to the next were difficult to establish, making results ambiguous. In order to improve the quantitative reliability of the thrips' EPG data, the DC-EPGs were recorded concurrently with AC-EPG signals, thus providing two signals from the same activities containing different information. The additional AC information did not solve most quantification problems, however. We now propose to merge waveforms P, Q, and S, into 'puncture phase' (indicated by PQ) and waveforms R, T, and U, into 'feeding phase' (indicated by R), {{rather than trying to}} analyse all separate waveforms. This will provide a more reliable and much less laborious analysis of thrips probing behaviour. Waveforms T and U are two novel waveforms identified here by combining DC- and AC-EPG recordings with concurrent <b>video</b> recordings. <b>Waveform</b> T represents a single mandibular thrust embedded in waveform R and waveform U represents the end of a probe, presumably the retraction of the maxillary stylets...|$|R
40|$|Digital Data Acquisition System of the Japanese Antarctic Research {{aircraft}} (JARDAS) {{was developed}} and installed on Pilatus Porter PC- 6. JARDAS consists of 8 interfaces with the external navigational and observational devices, data processor and data cartridge. As for the 4 channels of analog devices, DC voltage outputs from the radar altimeter, atmospheric pressure transducer, thermometer and vertical-gyro system are analog-to-digital converted in the data processor. As for the 4 channels of digital devices, data from the ULF/omega receiver, GPS receiver, proton magnetometer and ice radar video pulse digitizer are selected, code-transformed and re-arranged to make one data block sequence. The one data block corresponds to 1 s sampling data, and consists of 1 byte of start mark, 150 bytes of 7 -bit with even parity ASCII data for navigational and geomagnetic total intensity data, 658 bytes of binary data for ice radar <b>video</b> pulse <b>waveform</b> 1 and 2 bytes of end mark, which amounts {{to a total of}} 811 bytes. JARDAS can record 12 hrs 2 ̆ 7 output data blocks continuously on one 450 ft (7700 bpi) 3 -M type cartridge tape...|$|R
50|$|A {{video signal}} {{generator}} is a device which outputs predetermined <b>video</b> and/or television <b>waveforms,</b> and other signals used to stimulate faults in, or aid in parametric measurements of, television and video systems. There are several {{different types of}} video signal generators in widespread use. Regardless of the specific type, the output of a video generator will generally contain synchronization signals appropriate for television, including horizontal and vertical sync pulses (in analog) or sync words (in digital). Generators of composite video signals (such as NTSC and PAL) will also include a colorburst signal {{as part of the}} output. Video signal generators are available {{for a wide variety of}} applications, and for a wide variety of digital formats; many of these also include audio generation capability (as the audio track is an important part of any video or television program or motion picture).|$|R
40|$|Determination of pitch marks (PMs) is {{necessary}} in clinical voice assessment for the measurement of fundamental frequency (F 0) and perturbation. In voice with ambiguous F 0, PM determination is crucial, and its validity needs special attention. The study at hand proposes a new approach for PM determination from Laryngeal High-Speed Videos (LHSVs), {{rather than from the}} audio signal. In this novel approach, double PMs are extracted from a diplophonic voice sample, in order to account for ambiguous F 0 s. The LHSVs are spectrally analyzed in order to extract dominant oscillation frequencies of the vocal folds. Unit pulse trains with these frequencies are created as PM trains and compensated for the phase shift. The PMs are compared to Praat's single audio PMs. It is shown that double PMs are needed in order to analyze diplophonic voice, because traditional single PMs do not explain its double-source characteristic. Index Terms — Pitch marks, laryngeal high-speed <b>videos,</b> glottal area <b>waveforms,</b> diplophonia, audio and video signal processing 1...|$|R
40|$|Running is {{a popular}} {{exercise}} for all age groups. It helps heart and lung functions, enhances muscle strength and control weight. Nev-ertheless, excessive fatigue and severe injury resulting from inap-propriate running poses might reduce the benefits brought by this exercise and stop people from keeping running regularly. In this paper, we design a system that can monitor the running biome-chanics, infer running poses, analyze running patterns, and provide both real-time and off-line feedbacks to reduce unnecessary fatigue and unwanted injuries. Common inappropriate running patterns, over-striding, over-pronating and out-sync, are identified and ana-lyzed with the continuous wearable sensor data streams. Two types of correctional feedbacks are designed to provide users appropriate adjustment guidance: vibrating the areas of user body where im-proper poses are detected and visualizing running <b>video</b> with sen-sor <b>waveforms</b> to indicate which inappropriate motions trigger the vibration. With reasonable adjustment, running can be a safe and effective activity for a healthy lifestyle...|$|R
40|$|The {{dynamics}} and acoustics of conical bubble collapse occurring in a U-Tube device are studied. Two conical shapes are used, {{one with a}} smooth interior and another with a stepped interior {{in order to understand}} their associated phenomena such as: luminescence, bubble cloud formation, and emission of strong rebound pressures. High-speed <b>video</b> frames, <b>waveforms</b> from piezoelectric transducers and photomultipliers were acquired during the experimental runs. All data are synchronized on the same timeline. To explore the details of the acoustic energy dissipation, the piezoelectric waveforms were analyzed by means of Fourier transform and wavelets. Results show (I) with to respect to the frustum cone that: a) two mean flow structures are formed, {{one of them is a}} gas pocket and the other is a bubble cloud. The bubble cloud is produced when the concave meniscus is folded inwards due to fluid flow surrounding that divides the initial gas pocket in two, one remains trapped inside of meniscus area and the other is pushed towards to the cone end. These structures are the main location where the light emission occurs; (II) for the stepped cone that: a) asymmetrical counter-current flows are produced by the stepped boundaries, which generate several stagnation points. From the detailed analysis of the signals acquired, two issues were found: the frequencies and scales associated with the shock waves emission onset and its propagation, as well as frequency bands in which there is an absence of energy. The detailed flow structures as jets and splashing effects also are considered...|$|R
40|$|Paper {{presented}} to the 10 th International Conference on Heat Transfer, Fluid Mechanics and Thermodynamics, Florida, 14 - 16 July 2014. The dynamics and acoustics of conical bubble collapse occurring in a U-Tube device are studied. Two conical shapes are used, one with a smooth interior and another with a stepped interior {{in order to understand}} their associated phenomena such as: luminescence, bubble cloud formation, and emission of strong rebound pressures. High-speed <b>video</b> frames, <b>waveforms</b> from piezoelectric transducers and photomultipliers were acquired during the experimental runs. All data are synchronized on the same timeline. To explore the details of the acoustic energy dissipation, the piezoelectric waveforms were analyzed by means of Fourier transform and wavelets. Results show (I) with to respect to the frustum cone that: a) two mean flow structures are formed, {{one of them is a}} gas pocket and the other is a bubble cloud. The bubble cloud is produced when the concave meniscus is folded inwards due to fluid flow surrounding that divides the initial gas pocket in two, one remains trapped inside of meniscus area and the other is pushed towards to the cone end. These structures are the main location where the light emission occurs; (II) for the stepped cone that: a) asymmetrical counter-current flows are produced by the stepped boundaries, which generate several stagnation points. From the detailed analysis of the signals acquired, two issues were found: the frequencies and scales associated with the shock waves emission onset and its propagation, as well as frequency bands in which there is an absence of energy. The detailed flow structures as jets and splashing effects also are considered. cf 201...|$|R
40|$|International Telemetering Conference Proceedings / November 04 - 07, 1991 / Riviera Hotel and Convention Center, Las Vegas, NevadaA {{synopsis}} {{of the very}} recent evolution of telemetry chart recorders from “closed” chart paper output devices to powerful “open” Data Management Systems. A Data Management System (DMS) is defined as one which incorporates a <b>video</b> screen for <b>waveform</b> preview and monitoring, direct connection of hard or optical disk via SCSI for real-time data archiving, and DR 11 digital interfacing. The DMS concept of providing real-time waveform monitoring independent of hard copy recording is discussed, {{as well as the}} capabilities of the hard copy recorder. The realities of budget shortfalls makes wholesale system upgrades to eliminate DAC’s entirely difficult at best. These concerns—and a potential remedy: a DMS which accepts any mix of analog and digital waveforms—are reviewed. Objectives: How DMS’s can be integrated with existing telemetry systems, encompass the functionality of conventional recorders and add new capabilities, with an emphasis on how data can be digitally pre-formatted in real-time, simplifying—or even eliminating—post-mission reduction and analysis. A demonstration of how a video display allows real-time trace viewing—a major weakness of conventional thermal array recorders...|$|R
40|$|The {{objective}} {{of this study was}} to measure the response of a gas-gas injector to velocity perturbations through CH* emission intensity at a range of instability levels and to provide quality data for comparison with high-fidelity models. To achieve this, an optically accessible, 2 -D combustion chamber which self-excites a transverse instability was modified to incorporate a gas-gas shear coaxial injector at the location which coincides with the 1 W velocity anti-node in the chamber. By changing the driving injector orientation, different levels of instability were produced, ranging from 8 % to 65 % of the mean chamber pressure. Filtered high speed video was used to measure CH* chemiluminescence, which was used to provide insight into the coupling processes which drive instability. Several diagnostics were used in the analysis of the CH* video, including the root mean square (RMS) of each pixel over the data interval and the Rayleigh Index. CH* videos synced with pressure and velocity <b>waveform</b> <b>videos</b> provided further insight into the conditions within the chamber and the corresponding injector response, and provided parameters to compare with high-fidelity models. Finally, the relationship between CH* intensity and pressure/velocity perturbations was examined using several methods with the hope of identifying relationships that could be used to develop transfer functions. However, a more thorough investigation of the relationships is needed before this is accomplished. ...|$|R
40|$|The NASA Kennedy Space Center (KSC) and Air Force Eastern Range (ER) operate an {{extensive}} suite of lightning sensors because Florida experiences the highest area density of ground strikes in the United States, with area densities approaching 16 fl/sq km/yr when accumulated in 10 x 10 km (100 sq km) grids. The KSC-ER use data derived from two cloud-to-ground (CG) lightning detection networks, the "Cloud-to-Ground Lightning Surveillance System" (CGLSS) and the U. S. National Lightning Detection Network (TradeMark) (NLDN) plus a 3 -dimensional lightning mapping system, the Lightning Detection and Ranging (LDAR) system, to provide warnings for ground operations and to insure mission safety during space launches. For operational applications at the KSC-ER {{it is important}} to understand the performance of each lightning detection system in considerable detail. In this work we examine a specific subset of the CGLSS stroke reports that have low values of the negative inferred peak current, Ip, i. e. values between 0 and - 7 kA, and were thought to produce a new ground contact (NGC). When possible, the NLDN and LDAR systems were used to validate the CGLSS classification and to determine how many of these reported strokes were first strokes, subsequent strokes in a pre-existing channel (PEC), or cloud pulses that the CGLSS misclassified as CG strokes. It is scientifically important to determine the smallest current that can reach the ground either in the form of a first stroke or by way of a subsequent stroke that creates a new ground contact. In Biagi et al (2007), 52 low amplitude, negative return strokes ([Ip] < or = 10 kA) were evaluated in southern Arizona, northern Texas, and southern Oklahoma. The authors found that 50 - 87 % of the small NLDN reports could be classified as CG (either first or subsequent strokes) on the basis of <b>video</b> and <b>waveform</b> recordings. Low amplitude return strokes are interesting because they are usually difficult to detect, and they are thought to bypass conventional lightning protection that relies on a sufficient attractive radius to prevent "shielding failure" (Golde, 1977). They also have larger location errors compared to the larger current events. In this study, we use the estimated peak current provided by the CGLSS and the results of our classification to determine the minimum Ip for each category of CG stroke and its probability of occurrence. Where possible, these results are compared to the findings in the literature...|$|R

