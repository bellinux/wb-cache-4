30|10000|Public
50|$|The market {{environment}} {{at the time}} was not inconsistent with an increase in counterparty risk and a higher degree of information asymmetry. In the second half of 2007, market participants and regulators started to become aware of the risks in securitized products and derivatives. Many banks {{were in the process of}} writing down the values of their mortgage-related portfolios. House prices were falling all over the country and the ratings agencies had just started to downgrade subprime mortgages. Concerns about structured investment vehicles (SIVs) and mortgage and bond insurers were growing. Moreover, there was <b>very</b> <b>high</b> <b>uncertainty</b> about how to value complex securitized instruments and where in the financial system these securities were concentrated.|$|E
50|$|Another {{example of}} {{correlation}} was {{drawn by the}} Sigma Two Group in 2003. They have studied the correlation between countries' cultural dimensions and their predominant religion based on the World Factbook 2002. On average predominantly Catholic countries show <b>very</b> <b>high</b> <b>uncertainty</b> avoidance, relatively high power distance, moderate masculinity and relatively low individualism, whereas predominantly atheist countries have low uncertainty avoidance, very high power distance, moderate masculinity, and very low individualism. Coelho (2011) found inverse correlations between rates of specific kinds of innovation in manufacturing companies {{and the percentage of}} large companies per country as well as the employment of a specific kind of manufacturing strategy. The national culture measure of power distance is positively correlated with the ratio of companies with process innovation only over the companies with any of the three types of innovation considered in the country (determinant of correlation: 28%). Hence in countries with higher power distance, innovative manufacturing companies are somewhat more bound to resort to process innovations.|$|E
40|$|This paper uses a two-step {{approach}} {{to characterize the}} evolution of US macroeconomic and financial variables during episodes of <b>very</b> <b>high</b> <b>uncertainty.</b> First, we identify episodes of <b>very</b> <b>high</b> <b>uncertainty</b> using a regime-switching model. Second, we assess the behaviour of macroeconomic and financial variables during these episodes of <b>very</b> <b>high</b> <b>uncertainty.</b> This methodology {{is analogous to the}} approach followed by Baele et al. (2013), who study episodes of flights to safety in financial markets. We find that <b>very</b> <b>high</b> <b>uncertainty</b> episodes are associated with a weaker growth performance and sharp declines in stock prices. However, we find that this relation is non-linear in that uncertainty does not seem to matter during periods characterized by medium or low uncertainty...|$|E
50|$|If interdependencies are <b>very</b> <b>high</b> in both <b>uncertainty</b> and importance, {{then they}} are likely to produce chaotic situations.|$|R
30|$|The {{present results}} show that the finer the mesh (i.e., the smaller ℓ), the <b>higher</b> the {{measurement}} <b>uncertainties.</b> From a purely metrological point of view, finer meshes are to be discarded since they will lead to <b>very</b> <b>high</b> measurement <b>uncertainties</b> or even to singular DVC matrices that will not lead to trustworthy results, if any is obtained. However, on the mechanical side of the problem, coarse meshes may degrade the quality of the simulations.|$|R
40|$|This thesis {{presents}} {{methods for}} optical frequency-domain sensing of {{the strain on}} a fibre {{for use in a}} range of geophysical purposes. The Long Period Fibre Grating is modelled in detail to ascertain its viability as a hydrophone instrument, requiring <b>very</b> <b>high</b> sensitivity. <b>Uncertainty</b> analysis of such gratings illustrates that this will be difficult with available fibres. Other configurations such as down-hole pressure sensors are possible for these gratings, and several other sensing mechanisms...|$|R
40|$|A {{new method}} for delayed gamma-ray {{spectrometry}} {{to quantify the}} relative content of fissile material is developed and demonstrated to support international efforts in bolstering non-destructive assay capabilities. Previous traditional delayed gamma-ray spectrometry techniques rely upon nuclear data that often carry <b>very</b> <b>high</b> <b>uncertainty.</b> The new method removes the requirement for nuclear data by using the time-dependent decay characteristics of key fission fragment peaks. Having developed the theory and algorithm, the method is demonstrated using empirical fission data to estimate the contents of fissile materials of different compositions. The resultant estimates are accurate within 2 % of the actual contents and precise within 2 %...|$|E
30|$|There is a <b>very</b> <b>high</b> <b>uncertainty</b> in {{the future}} climate change in the Himalayas and few studies {{has been carried out}} towards {{predicting}} future climate scenario in the Nepal Himalayas. In this study, climate change projection has been carried out for the Marsyangdi River Basin in the Nepal Himalaya which is focused on quantifying impacts of climate change with meteorological parameters (temperature and precipitation) for the future period, based on the outputs from fifth assessment report of Intergovernmental Panel on Climate Change. The study makes use of CanESM 2 dataset which are statistically downscaled using statistical downscaling model (SDSM). Climate projections are available for three representative concentration pathways (RCPs) namely RCP 2.6, RCP 4.5 and RCP 8.5 for up to 2100.|$|E
40|$|In this paper, {{we present}} a {{distributed}} matrix exponential learning (MXL) algorithm {{for a wide range}} of distributed optimization problems and games that arise in signal pro- cessing and data networks. To analyze it, we introduce a novel stability concept that guarantees the existence of a unique equilibrium solution; under this condition, we show that the algorithm converges even in the presence of highly defective feedback that is subject to measurement noise, er- rors, etc. For illustration purposes, we apply the proposed method to the problem of energy efficiency (EE) maximiza- tion in multi-user, multiple-antenna wireless networks with imperfect channel state information (CSI), showing that users quickly achieve a per capita EE gain between 100 % and 400 %, even under <b>very</b> <b>high</b> <b>uncertainty...</b>|$|E
40|$|Quantifying {{nutrient}} {{and sediment}} loads in catchments is difficult owing to diffuse controls related to storm hydrology. Coarse sampling and interpolation methods {{are prone to}} <b>very</b> <b>high</b> <b>uncertainties</b> due to under-representation of high discharge, short duration events. Additionally, important low-flow processes such as diurnal signals linked to point source impacts are missed. Here we demonstrate a solution based on a time-integrated approach to sampling with a standard 24 bottle autosampler configured to take a sample every 7 h over a week according to a Plynlimon design. This is evaluated {{with a number of}} other sampling strategies using a two-year dataset of sub-hourly discharge and phosphorus concentration data. The 24 / 7 solution is shown to be among the least uncertain in estimating load (inter-quartile range: 96 % to 110 % of actual load in year 1 and 97 % to 104 % in year 2) due to the increased frequency raising the probability of sampling storm events and point source signals. The 24 / 7 solution would appear to be most parsimonious in terms of data coverage and certainty, process signal representation, potential laboratory commitment, technology requirements and the ability to be widely deployed in complex catchments...|$|R
40|$|The {{nature of}} {{technical}} expertise {{has become increasingly}} important and problematic in the post-modern era, as structured hierarchies and production methods are revised. Financial services, {{one of our most}} important economic sectors, has also been confronting <b>very</b> <b>high</b> degrees of <b>uncertainty</b> that reflect great institutional and market changes. In this fluid and competitive environment technological change - in particular the widening scope of information technology (IT) - has become vitally important. ...|$|R
50|$|In {{addition}} to measuring UI, the laboratory must also measure v and g using experimental methods {{that do not}} depend on the definition of mass. The overall precision of m depends on the precisions of the measurements of U, I, v and g. Since there are already methods of measuring v and g to <b>very</b> <b>high</b> precision, the <b>uncertainty</b> of the mass measurement is dominated by the measurement of UI, which is the value measured by the watt balance.|$|R
40|$|The paper {{deals with}} low-frequency {{magnetic}} field measurements, {{with regard to}} exposure of human beings, carried out by using a broadband and isotropic instrument. These measurements are characterized by a <b>very</b> <b>high</b> <b>uncertainty</b> values if compared with the ones usually related to other electrical measurements. These large uncertainty values imply {{a high risk of}} wrong decision when there is the need to establish if a site complies or does not comply with a specified emission limits. A reduction of the uncertainty values implies a reduction of the risk. With this aim, in the paper we propose an approach which, in case of fields generated by electric power systems (50 or 60 Hz), allows an effective reduction of the uncertainty values in the magnetic field measurements...|$|E
40|$|A {{process for}} using {{ground-based}} photographic imagery {{to detect and}} locate power distribution assets is presented. The primary feature of the system presented here is it's very low cost compared to more traditional inspection methods because the process takes place entirely in virtual space. Specifically, the system can locate assets with a precision comparable to typical GPS units used for similar purposes, and can readily identify utility assets, for example, transformers, if appropriate training data are provided. Further human intervention would only be necessary in {{a small fraction of}} cases, where <b>very</b> <b>high</b> <b>uncertainty</b> is flagged by the system. The feasibility of the process is demonstrated here, and a path to full integration is presented. Electric Power Research Institute (EPRI) Mechanical EngineeringMastersUniversity of New Mexico. Dept. of Mechanical EngineeringMammoli, AndreaCaudell, ThomasVorobief, Pete...|$|E
40|$|Humans {{have doubled}} levels of {{reactive}} nitrogen in circulation, {{largely as a}} result of fertilizer application and fossil fuel burning. This massive alteration of the nitrogen cycle affects climate, food security, energy security, human health and ecosystem services. Our estimates show that nitrogen currently leads to a net-cooling effect on climate with <b>very</b> <b>high</b> <b>uncertainty.</b> The many complex warming and cooling interactions between nitrogen and climate need to be better assessed, taking also into account the other effects of nitrogen on human health, environment and ecosystem services. Through improved nitrogen management substantial reductions in atmospheric greenhouse gas concentrations could be generated, also allowing for other co-benefits, including improving human health and improved provision of ecosystem services, for example clean air and water, and biodiversity. © 2011 Elsevier B. V...|$|E
40|$|Fatigue is {{the most}} common {{mechanical}} cause of engineering failures, but it is not well understood. Current methods of predicting fatigue failure rely on empirically-derived equations instead of having a truly scientific foundation. These have <b>very</b> <b>high</b> <b>uncertainties,</b> and they often do not consider the cycling frequency even though it has been proven to affect fatigue life. The power density theory is a new way of describing fatigue. It is based on the concept of power density, which is physically equivalent to the amount of power deposited into a unit volume. Power density results from changes in stress magnitude over time. Stress alternations that occur across a broad bandwidth of frequencies must be factored in. Higher frequencies coupled with faster changes in stress contribute more power density. Power density accumulates at every frequency and time, damaging the part. After this accumulation reaches the material?s power density threshold, a fundamental property of the material, it is expected to fail by fatigue. A previously-published multiaxial vibration fatigue test was referenced and replicated using computer simulations, and the concept of power density was applied to predict its results. This served as a feasibility study for the theory, as well {{as an example of how}} to apply it. The power density response of the system was analyzed, and the failure locations were predicted for each of the ten load cases considered. The predicted failure locations followed the same trend as the experimental results. These results were promising, so more research is recommended to further test and develop the power density theory. Further examination of the theory could result in a better understanding of fatigue failure, improving engineering work across many industries...|$|R
40|$|We {{present a}} {{maintenance}} scheduling problem arising from semi-conductor manufacturing which {{is characterized by}} low resource contention and multiple complex objectives and preferences. Since semi-conductor manufacturing involves a <b>very</b> <b>high</b> degree of <b>uncertainty</b> {{at the level of}} detailed op-erations, such as machine availability, yield, and processing times, generating a maintenance schedule which takes into account production operations is a challenging problem. We have developed an integrated approach, with respect to pro-duction operations, which uses simulation to estimate ex-pected levels of work in process in the fab. We present details of our solution approach which is based on goal program-ming, constraint programming and mixed-integer program-ming...|$|R
40|$|This paper {{proposes a}} new control {{strategy}} for Propofol injection during anesthesia in patients undergoing surgery {{and where the}} bispectral index (BIS) {{is considered to be}} the regulated variable. The proposed control shows the nice feature of being completely independent of the knowledge of the pharmacokinetics that govern the diffusion of the drug. The paper also proposes a certification framework that gives a probabilistic guarantee regarding the containment of the BIS inside the desired interval {{as well as for the}} time needed for the BIS to be steered to this interval. Moreover, this certification is given for realistic (and hence <b>very</b> <b>high)</b> level of <b>uncertainties</b> on the parameters that define the unknown-to-the-controller dynamics. This last feature is checked using a widely used model...|$|R
40|$|The paper {{deals with}} {{magnetic}} field measurement, {{carried out by}} using a broadband and isotropic instrument. These measurements are characterized by a <b>very</b> <b>high</b> <b>uncertainty</b> values if compared with the ones usually related to other electrical measurements. For these reasons, if the measurements are performed to assess the exposure of human beings, these large uncertainty values imply {{a high risk of}} wrong decision when there is the need to establish if a site complies or does not comply with a specified emission limits. A reduction of the uncertainty values implies a reduction of the risk. With this aim, we propose an approach which, in particular but very typical cases, allows an effective reduction of the uncertainty values in the magnetic field measurements, avoiding the usage of very expensive instrumentation...|$|E
40|$|The {{momentum}} {{of the global economy}} has continued to weaken recently. The USA and the euro area registered only moderate quarterly growth of 0. 2 percent each in the second quarter. The more unfavourable economic outlook, the controversial debate about the government debt ceiling in the USA, and the debt crisis in the euro area triggered <b>very</b> <b>high</b> <b>uncertainty</b> in financial markets in August. Following a severe decline, stock prices stabilised for the time being {{towards the end of the}} month. The Austrian economy expanded by 0. 7 percent in real terms in the second quarter, but, as evidenced by the WIFO Business Cycle Survey, will very likely not be able to elude the cooling of global economic activity in the third quarter. HICP core inflation was 3. 1 percent in Austria, exceeding the euro area average by 1. 6 percentage points. Business Cycle Report...|$|E
40|$|The paper {{deals with}} low-frequency {{magnetic}} field measurements {{carried out by}} using a broadband and isotropic instrument. These measurements are characterized by <b>very</b> <b>high</b> <b>uncertainty</b> values, which imply {{a high risk of}} wrong decisions when there is the need to establish if a site complies or does not comply with specified emission limits. To reduce this risk, we decided to perform the so called “uncertainty management” that is the discipline of optimizing the cost of a measurement versus the uncertainty target. The task is achieved by using the PUMA method that is an iterative technique originally conceived for geometrical and mechanical measurements. The approach is completely based on the “Guide to the Expression of Uncertainty in Measurement” rules, but provides a more engineering methodology. By using this approach, it is possible to avoid the usage of too expensive instrumentations or, on the contrary, too expensive resources for the uncertainty estimation...|$|E
40|$|The paper {{helicopter}} {{project has}} been given to graduate students of the French engineering school Ecole des Mines de Saint Etienne, as a practical work of two lectures: design of experiments and global optimization. The project consists of designing a very simplified paper helicopter for maximum flying time. Its main difficulty lies in the <b>very</b> <b>high</b> level of <b>uncertainty,</b> as flying times for the same design can vary very substantially due to experimental conditions or manufacturing imprecisions. The students were asked to combine design of experiments and surrogate modeling techniques (in order to deal with noise) with global optimization strategies. Despite its apparent simplicity, the project was found quite challenging and the strategies experimented by students were eventually close to what would require a real-case problem...|$|R
40|$|In {{areas where}} heavy metals are {{introduced}} into or onto land {{where they would}} not normally be present at elevated concentrations, then that land could {{be considered to be}} contaminated. A simple way of determining the magnitude of contamination by heavy metals is to measure the total metal concentration in the soil. However, this simple measure is a poor way of assessing the potential risks to the environment and human health. A more effective risk assessment can be achieved by analysing the proportion of the total metal that exists in a mobile or bioavailable form, in other words, the metal solubility. Unfortunately metal solubility is more difficult and costly to measure than total metal concentration in the soil. This thesis examines the application of a metal solubility model to geochemical survey data consisting of pH and metal concentrations. The solubility predictions were interpolated in order to produce maps; however, the interpolated data had <b>very</b> <b>high</b> <b>uncertainties.</b> Further analysis showed that pH was the greatest source of uncertainty in the algorithm, contributing the most for lead, with 76 % of the uncertainty being due to pH. pH was least influential for copper, contributing 49 % of the uncertainty, but pH was the highest contributor in each metal. In order to examine the accuracy of the algorithm without geostatistical influences, a field work study was undertaken to measure metal solubility directly at the original survey sites. This showed that the algorithm was very good at predicting metal solubility at point sources. In order to assess the shortscale spatial variability of pH, and the errors in pH measurements, a second field work project was conducted, measuring the pH on 200 samples from a single field. This work showed that pH does vary across a field, but more importantly allowed a quantification of the uncertainty involved in sampling and measuring pH. Results show that despite the short-scale variability in pH, point predictions are accurate (the average difference between measured and predicted pZn 2 + is 6 %), xvi and might be of use to land managers. However, interpolating solubility predictions for mapping produces unacceptably <b>high</b> <b>uncertainties</b> (mean values were 188 % for Pb, 417 % for Cu and 153 % for Zn) for land management or the development of policy measures related to soil. Further work could include calculating the measured Pb and Cu solubility and comparing these to the predictions. A study to investigate how pH and Zn 2 + vary together across a field would also be of interest. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|M. Tech. The Inductively Coupled Plasma Mass Spectrometry (ICP-MS) {{technique}} showed <b>very</b> <b>high</b> <b>uncertainties</b> {{associated with}} {{the determination of the}} concentrations of minor elements in the Platinum Group Metal (PGM) smelting plant samples. This project reports on the work done for the identification of, and subsequent minimisation of the sources of uncertainties {{associated with the}} measurements of minor elements in the PGM Furnace Matte material by ICP-MS. To perform these studies, Elan 6000, Shimadzu ICPM- 8500 and Finnigan Element 2 ICP-M spectrometers were employed. Synthetic Furnace Matte samples (SFMSs) were prepared and used to ascertain the uncertainties associated with the alleged sources at Mintek and Leo-Tech laboratories. The Element 2 spectrometer dominated the other two ICP-M spectrometers in terms of the accuracy for the determination of the concentrations of minor elements in SFMSs. The evidence of spectral interferences from the significant deviations in the measurement results between the isotopes of the same element was observed in the quantification of Zn, Se, Te and Sn in SFMS by the quadrupole Elan 6000 and the Shimadzu spectrometers. It also transpired that the accuracy of the quantitative determination of minor elements in the Furnace Matte (FM) matrix by ICP-MS was hampered by the matrix elements with the severity depending on the specific analyte and the make and model of the ICP-M spectrometer. The Anglo platinum FM material that was analysed in the second round robin was used as a Certified Reference Material (CRM) in the analysis of the Lonmin FM sample. It was revealed that the laboratory standard operating procedures for the preparation, dilution and subsequent analysis of the sample are potential sources of uncertainty in measurement results. The two-fold dilution of the sample for the lessening of the matrix effects was not effective. The use of multi-walled nanotubes for the alleviation of the matrix effects by removal was also not successful. Nevertheless standard addition method (SAM), combined with internal standardisation can be used as an effective calibration method in ICP-MS to achieve less matrix interfered results over the combination of the common external standardisation and internal standardisation methods...|$|R
40|$|International audienceIn this paper, we {{investigate}} a distributed learning scheme {{for a broad}} class of stochastic optimization problems and games that arise in signal processing and wireless communications. The proposed algorithm relies on the method of matrix exponential learning (MXL) and only requires locally computable gradient observations that are possibly imperfect and/or obsolete. To analyze it, we introduce {{the notion of a}} stable Nash equilibrium and we show that the algorithm is globally convergent to such equilibria – or locally convergent when an equilibrium is only locally stable. We also derive an explicit linear bound for the al- gorithm’s convergence speed, which remains valid under measurement errors and uncertainty of arbitrarily high variance. To validate our theoretical analysis, we test the algorithm in realistic multi-carrier/multiple- antenna wireless scenarios where several users seek to maximize their energy efficiency. Our results show that learning allows users to attain a net increase between 100 % and 500 % in energy efficiency, even under <b>very</b> <b>high</b> <b>uncertainty...</b>|$|E
40|$|Climatic {{change may}} have very diverse impacts on lakes and their water quality. This paper groups them to hydrologic, thermal, hydraulic, chemical, biochemical, and {{ecological}} ones. Their interrelations and potential changes, and {{their contributions to}} lake water quality problems, are reviewed and discussed, and checklist tables for planning, management, and impact assessment purposes are provided. Water quality problems are clustered to eutrophication, oxygen depletion, hygienic problems, salinization, acidification, toxic and cumulative substances, turbidity and suspended matter, and thermal pollution. Many of these problems may worsen due to the climatic change, yet an extreme uncertainty exists at any case specific level. Therefore, the possibility of water quality and quantity shifts due to climatic change are suggested to be integrated into contemporary planning and management in an adaptive manner, and {{the research and development}} of the impact assessment methodology are suggested to be focused on approaches that can handle extreme uncertainties. The <b>very</b> <b>high</b> <b>uncertainty</b> level in water quality management has obtained an additional highly uncertain component...|$|E
40|$|In this paper, we {{investigate}} a distributed learning scheme {{for a broad}} class of stochastic optimization problems and games that arise in signal processing and wireless communications. The proposed algorithm relies on the method of matrix exponential learning (MXL) and only requires locally computable gradient observations that are possibly imperfect and/or obsolete. To analyze it, we introduce {{the notion of a}} stable Nash equilibrium and we show that the algorithm is globally convergent to such equilibria - or locally convergent when an equilibrium is only locally stable. We also derive an explicit linear bound for the algorithm's convergence speed, which remains valid under measurement errors and uncertainty of arbitrarily high variance. To validate our theoretical analysis, we test the algorithm in realistic multi-carrier/multiple-antenna wireless scenarios where several users seek to maximize their energy efficiency. Our results show that learning allows users to attain a net increase between 100 % and 500 % in energy efficiency, even under <b>very</b> <b>high</b> <b>uncertainty.</b> Comment: 31 pages, 3 figure...|$|E
30|$|There is {{substantial}} {{interest in developing}} a coherent and effective North American renewable energy policy {{as a way to}} secure energy but also to mitigate global climate change. Based on surveys of the public in Canada, Mexico, and the United States, the article shows the levels of concern over climate change threats, perceived risk, knowledge of climate change policies, levels of uncertainty, and other perception factors to help understand the relationships between public perceptions and policy preferences for renewable energy. Results show national differences between the three countries in nearly all climate change perceptions, with Mexico reflecting the highest levels of concern and the United States the lowest. Mexico also shows the greatest support for renewable energy sources. However, the results show <b>very</b> <b>high</b> levels of <b>uncertainty</b> about climate change dimensions concerning risk, science, and knowledge and the effectiveness of policy approaches. The data demonstrate strong statistical correlations between risk perception factors and preferences for mitigation policies in the form of renewable energy policies.|$|R
40|$|Copyright © 2013 Johnny Moretto et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. The estimation of underwater features of chan-nel bed surfaces {{without the use of}} bathymetric sensors results in <b>very</b> <b>high</b> levels of <b>uncertainty.</b> A revised approach enabling an automatic ex-traction of the wet areas to create more accurate and detailed Digital Terrain Models (DTMs) is here presented. LiDAR-derived elevations of dry surfaces, water depths of wetted areas derived from aerial photos and a predictive depth-col-our relationship were adopted. This methodol-ogy was applied at two different reaches of a northeastern Italian gravel-bed river (Tagliamen-to) before and after two flood events occurred in November and December 2010. In-channel dGPS survey points were performed taking different depth levels and different colour scales of the river bed. More than 10, 473 control points wer...|$|R
40|$|In {{recent years}} {{there has been}} a growth in size and complex- ity of the {{projects}} managed by public or private organiza- tions. This leads to increased probability of project failures, frequently due to the difficulty and the ability to achieve the objectives such as on-time delivery, cost containment, ex- pected quality achievement. In particular, {{one of the most common}} causes of project failure is the <b>very</b> <b>high</b> degree of <b>uncertainty</b> that affects the expected performance of the project, especially when different stakeholders with diver- gent aims and goals are involved in the project. In this paper we present a new visualization technique, called 3 DRC, that addresses the prevention and proactive handling of the potential controversies among project stake- holders. The approach is based on the 3 D radar charts, to allow easier and more immediate analysis and management of the project views giving a contribution in reducing the project uncertainty and, consequently, the risk of project failure...|$|R
40|$|Vision {{metrology}} {{and computer}} vision can be successfully used for archaeological heritage 3 D reconstruction in <b>very</b> <b>high</b> <b>uncertainty</b> 3 D measurement projects. Of those archaeological objects requiring very accurate measurements (< 1 mm), ancient mosaics comprise {{some of the}} most important. The aim {{of this paper is to}} assess the photogrammetric/computer vision approach in a vision metrology context as part of a 3 D mosaics survey. In order to evaluate the optimal photogrammetric/computer vision workflow in this work, three different surveys were performed on three mosaics of different sizes and locations. Two of these are stored at the Antonino Salinas Regional Archaeological Museum in Palermo (Italy) and the other is located at the Baglio Anselmi Regional Archaeological Museum in Marsala (Italy). The mosaics survey was undertaken in order to obtain a very detailed 3 D model and a full-scale ortho-image (scale 1 : 1), which would be useful for documentation and restoration processes. The research involved an evaluation of the potential and the related issues of the photogrammetric/computer vision approach for 3 D mosaic documentation, particularly regarding the issue of camera calibration...|$|E
40|$|This paper {{deals with}} the {{availability}} analysis for the wind turbines (WT) located in hill and main pass having uncertainty in wind. Availability is a key performance index for WTs. In this paper, the concept of Markov analysis (MA) is used to model the failure characteristics of the WTs for calculating probability and reliability of reaching various system states of WT having the capacities of 225 kW, 250 kW, and 400 kW. Due to uncertainty in the wind, the probability of component failure is independent of the past history. Hence MA is considered as the best mathematical tool for modelling WT with complex system. The field data obtained from the Muppandal site in India such as Mean time between failure (MTBF), Mean time to repair (MTTR), failure rate and repair rate are used to compute the WT availability, using ITEM Toolkit version 8. 0. 2 {{as a measure of}} performance. No attempt is made to analyze the individual state of the WT system. In this paper, for seven critical components, a resultant of 128 states has been analyzed. This analysis yields some surprising results that some WTs are the most unreliable due to the rapid failure of its sub assemblies in view of the <b>very</b> <b>high</b> <b>uncertainty</b> in the wind and frequent grid failures...|$|E
30|$|There is a <b>very</b> <b>high</b> <b>uncertainty</b> in {{predicting}} future ground disasters with rainfall and elucidation {{of the relationship}} between the slope collapse and piping phenomenon has been required (Araki et al. 2005; Iwami et al. 2013; Okumura et al. 2013; Araki et al. 2014; Araki et al. 2015; Yasufuku et al. 2015). In order to increase the interest of residents in ground engineering, there is a need for a visually-intuitive evaluation method of the ground. Furthermore, not only the numerical evaluation of the state of the ground by visualization is possible, but visual confirmation can also be performed at the same time. In this way new knowledge will be obtained about complicated seepage behavior of unsaturated ground. Visualization of the internal structure of the soil was carried out mainly using X-ray micro CT (Watanabe et al. 2015). The measurement method of the water contents in the ground using gamma rays has been studied by Kono et al. (1981). An MRI (magnetic resonance imaging) is used for the discovery the lesions in the medical field, and images of the inside of the human body can be directly obtained in a non-destructive manner. Moreover, the device can continue photographing for a long time. In this study, non-destructive unsaturated seepage behavior was continuously visualized by using MRI with test equipment constructed of non-metal parts. Additionally, the water content of the soil from the MRI image was evaluated, and the results of previous experiments were considered and compared.|$|E
40|$|Estimating {{underwater}} {{features of}} channel bed surfaces {{without the use}} of bathymetric sensors results in <b>very</b> <b>high</b> levels of <b>uncertainty.</b> A novel approach to create more accurate and detailed Digital Terrain Models (DTMs) integrates LiDAR-derived elevations of dry surfaces, water depth of wetted areas derived from aerial photos and a predictive depth-colour relationship. This method was applied in three different sub-reaches of a northeastern Italian gravel-bed river (Brenta) before and after flood events occurred in November and December 2010 (recurrence interval: 8 and 10 years). From the data collected through channel field survey, a regression model which calculates channel depths using the correct intensity of three colour bands was implemented. LiDAR and depth points were merged and interpolated into a DTM which features an average error of ± 18 cm. The morphological evolution and the sediment volume change calculated through a difference of DTMs shows deposition and erosion areas, indicating a deficit which reduces as it goes downstream...|$|R
40|$|A fuzzy ruled-based {{system was}} {{developed}} {{in this study and}} resulted in an index indicating the level of uncertainty related to commercial transactions between cassava growers and their dealers. The fuzzy system was developed based on Transaction Cost Economics approach. The fuzzy system was developed from input variables regarding information sharing between grower and dealer on &# 8220;Demand/purchase Forecasting&# 8221;, &# 8220;Production Forecasting&# 8221; and &# 8220;Production Innovation&# 8221;. The output variable is the level of uncertainty regarding the transaction between seller and buyer agent, which may serve as a system for detecting inefficiencies. Evidences from 27 cassava growers registered in the Regional Development Offices of Tupa and Assis, São Paulo, Brazil, and 48 of their dealers supported the development of the system. The mathematical model indicated that 55 % of the growers present a <b>Very</b> <b>High</b> level of <b>uncertainty,</b> 33 % present Medium or High. The others present Low or Very Low level of uncertainty. From the model, simulations of external interferences can be implemented in order to improve the degree of uncertainty and, thus, lower transaction costs...|$|R
40|$|ABSTRACT: A fuzzy ruled-based {{system was}} {{developed}} {{in this study and}} resulted in an index indicating the level of uncertainty related to commercial transactions between cassava growers and their dealers. The fuzzy system was developed based on Transaction Cost Economics approach. The fuzzy system was developed from input variables regarding information sharing between grower and dealer on “Demand/purchase Forecasting”, “Production Forecasting ” and “Production Innovation”. The output variable is the level of uncertainty regarding the transaction between seller and buyer agent, which may serve as a system for detecting inefficiencies. Evidences from 27 cassava growers registered in the Regional Development Offices of Tupa and Assis, São Paulo, Brazil, and 48 of their dealers supported the development of the system. The mathematical model indicated that 55 % of the growers present a <b>Very</b> <b>High</b> level of <b>uncertainty,</b> 33 % present Medium or High. The others present Low or Very Low level of uncertainty. From the model, simulations of external interferences can be implemented in order to improve the degree of uncertainty and, thus...|$|R
