467|1044|Public
5|$|Reflector sights {{were invented}} as an {{improved}} gun-sight and since their invention {{they have been}} adapted to many types of weapons. When used with different types of guns, reflector sights are considered an improvement over simple iron sights (sights composed of two spaced metal aiming points {{that have to be}} aligned). Iron sights take considerable experience and skill in the user who has to hold a proper eye position and focus exclusively on the front sight, keeping it centered on the (unfocused) rear sight, while keeping the whole centered on a target at different distances, requiring alignment of all three planes of focus to achieve a hit. The reflector sight's single, parallax-free <b>virtual</b> <b>image,</b> in focus with the target, removes this aiming problem, helping poor, average, and expert shooters alike.|$|E
5|$|Reflector sights work {{by using}} a lens or an image-forming curved mirror with a {{luminous}} or reflective overlay image or reticle at its focus, creating an optical collimator that produces a <b>virtual</b> <b>image</b> of that reticle. The image is reflected off some form of angled beam splitter or the partially silvered collimating curved mirror itself so that the observer (looking through the beam splitter or mirror) will see the image at {{the focus of the}} collimating optics superimposed in the sight's field of view in focus at ranges up to infinity. Since the optical collimator produces a reticle image made up of collimated light, light that is nearly parallel, the light making up that image is theoretically perfectly parallel with the axis of the device or gun barrel it is aligned with, i.e. with no parallax at infinity. The collimated reticle image can also be seen at any eye position in the cylindrical volume of collimated light created by the sight behind the optical window. But this also means, for targets closer than infinity, sighting towards the edge of the optical window can make the reticle move in relation to the target since the observer is sighting down a parallel light bundle at the edge. Eye movement perpendicular to the device's optical axis will make the reticle image move in exact relationship to eye position in the cylindrical column of light created by the collimating optics.|$|E
25|$|In {{some cases}} S2 is negative, {{indicating}} that the image is formed {{on the opposite side}} of the lens from where those rays are being considered. Since the diverging light rays emanating from the lens never come into focus, and those rays are not physically present at the point where they appear to form an image, this is called a <b>virtual</b> <b>image.</b> Unlike real images, a <b>virtual</b> <b>image</b> cannot be projected on a screen, but appears to an observer looking through the lens as if it were a real object at the location of that <b>virtual</b> <b>image.</b> Likewise, it appears to a subsequent lens as if it were an object at that location, so that second lens could again focus that light into a real image, S1 then being measured from the <b>virtual</b> <b>image</b> location behind the first lens to the second lens. This is exactly what the eye does when looking through a magnifying glass. The magnifying glass creates a (magnified) <b>virtual</b> <b>image</b> behind the magnifying glass, but those rays are then re-imaged by the lens of the eye to create a real image on the retina.|$|E
40|$|The cloud {{virtualization}} technology {{improves the}} economy of scale for data centers through server consolidation, application consolidation and resources consolidation. Virtualization allows the provider to move <b>Virtual</b> <b>Images</b> from more congested host to less-congested hosts, as required. Enterprises also get improved server reliability, which in turn increases application performance. Despite these benefits, it includes major security challenges with the portability of <b>Virtual</b> <b>Images</b> between different cloud providers. The security and integrity of <b>Virtual</b> <b>images</b> is {{the foundation for the}} overall security of the cloud. Many of the <b>Virtual</b> <b>images</b> are intended to be shared by diverse and unrelated users. Unfortunately, existing approaches to cloud security built by cloud practitioners fall short when dealing with <b>Virtual</b> <b>images.</b> Secure transmission of <b>virtual</b> <b>Images</b> can bepossible by providing authentication using Blind Authentication protocol (BAP). The proposed approach authenticates the allocation of <b>virtual</b> <b>images</b> using Blind authentication protocol. It provides provable protection against replay and client side attacks even if the keys of the user are compromised. The encryption also provides template protection, revocability and alleviates the concerns on privacy in widespread use of biometrics. Carrying out the authentication in the encrypted domain is a secure process, while the encryption key acts as an additional layer of security...|$|R
5000|$|... ‘Natural Magic,’ a {{pamphlet}} on optics dealing with <b>virtual</b> <b>images,</b> London, 1832.|$|R
40|$|Previous augmented-reality (AR) {{applications}} {{have required}} users {{to observe the}} integration of real and <b>virtual</b> <b>images</b> on a display. This study proposes a novel concept regarding AR applications. By integrating AR techniques with marker identification, virtual-image output, imaging, and image-interaction processes, this study rendered <b>virtual</b> <b>images</b> that can interact with predefined markers in a real three-dimensional (3 D) environment...|$|R
25|$|In optics, a <b>virtual</b> <b>image</b> is {{an image}} formed when the {{outgoing}} rays from a point on an object always diverge. The image appears to be located {{at the point of}} apparent divergence. Because the rays never really converge, a <b>virtual</b> <b>image</b> cannot be projected onto a screen. In diagrams of optical systems, virtual rays are conventionally represented by dotted lines. Virtual images are located by tracing the real rays that emerge from an optical device (lens, mirror, or some combination) backward to a perceived point of origin.|$|E
25|$|With {{diverging}} lenses, incoming parallel rays diverge {{after going}} through the lens, {{in such a way}} that they seem to have originated at a spot one focal length in front of the lens. This is the lens's front focal point. Rays from an object at finite distance are associated with a <b>virtual</b> <b>image</b> that is closer to the lens than the focal point, and on the same side of the lens as the object. The closer the object is to the lens, the closer the <b>virtual</b> <b>image</b> is to the lens. As with mirrors, upright images produced by a single lens are virtual, while inverted images are real.|$|E
25|$|A {{diverging}} lens (one that is thicker {{at the edges}} than the middle) or a convex mirror forms a <b>virtual</b> <b>image.</b> Such an image is so reduced in size {{when compared to the}} original object. A converging lens (one that is thicker in the middle than at the edges) or a concave mirror is also capable of producing a <b>virtual</b> <b>image</b> if the object is within the focal length. Such an image will be magnified. In contrast, an object placed in front of a converging lens or concave mirror at a position beyond the focal length produces a real image. Such an image may be magnified or reduced depending on the position of the object.|$|E
5000|$|IBM Workload Deployer - a {{hardware}} appliance that {{provides access to}} IBM middleware <b>virtual</b> <b>images</b> and patterns ...|$|R
40|$|We {{present in}} our paper a secure, {{flexible}} and transparent security architecture for <b>virtual</b> disk <b>images.</b> <b>Virtual</b> disk <b>images</b> are often overlooked in security concepts, {{especially in a}} grid environment where disk images {{are considered to be}} secure as long as they reside within the secured borders of the data center. However, for some applications this level of assurance is not satisfactory. In our security architecture, virtualized guests transparently benefit from integrity as well as confidentiality assurance. Traditional <b>virtual</b> disk <b>images</b> lack the ability of an efficient integrity protection mechanism. We base our concepts on trusted computing utilizing the Trusted Platform Module (TPM) to efficiently deliver integrity assurance to <b>virtual</b> disk <b>images.</b> Further, we allow a restrictive rule-set to be imposed by the <b>virtual</b> disk <b>image</b> owner, and we enable the owner to retain control over the <b>virtual</b> disk <b>image</b> throughout its life-cycle. ...|$|R
50|$|These {{devices are}} sold as {{hardware}} network appliances {{and in some}} instances as <b>virtual</b> <b>images</b> that run on basic server hardware.|$|R
25|$|A {{plane mirror}} forms a <b>virtual</b> <b>image</b> {{positioned}} behind the mirror. Although {{the rays of}} light seem to come from behind the mirror, light from the source only exists {{in front of the}} mirror. The image in a plane mirror is not magnified (that is, the image is {{the same size as the}} object) and appears to be as far behind the mirror as the object is in front of the mirror.|$|E
25|$|Other uses are in imaging {{systems such}} as monoculars, binoculars, telescopes, microscopes, cameras and projectors. Some of these {{instruments}} produce a <b>virtual</b> <b>image</b> when applied to the human eye; others produce a real image that can be captured on photographic film or an optical sensor, or can be viewed on a screen. In these devices lenses are sometimes paired up with curved mirrors to make a catadioptric system where the lens's spherical aberration corrects the opposite aberration in the mirror (such as Schmidt and meniscus correctors).|$|E
25|$|Linear {{magnification}} M is {{not always}} the most useful measure of magnifying power. For instance, when characterizing a visual telescope or binoculars that produce only a <b>virtual</b> <b>image,</b> one would be more concerned with the angular magnification—which expresses how much larger a distant object appears through the telescope compared to the naked eye. In the case of a camera one would quote the plate scale, which compares the apparent (angular) size of a distant object {{to the size of the}} real image produced at the focus. The plate scale is the reciprocal of the focal length of the camera lens; lenses are categorized as long-focus lenses or wide-angle lenses according to their focal lengths.|$|E
5000|$|Cloud {{storage can}} be used for copying <b>virtual</b> machine <b>images</b> from the cloud to {{on-premises}} locations or to import a <b>virtual</b> machine <b>image</b> from an on-premises location to the cloud image library. In addition, cloud storage {{can be used}} to move <b>virtual</b> machine <b>images</b> between user accounts or between data centers.|$|R
30|$|In all amplitude, intensity, and <b>virtual</b> <b>images</b> {{shown in}} the evaluation, the pixel values are {{normalized}} to values between 0 and 1 for better comparison.|$|R
40|$|Achieving a high {{fill factor}} is a {{bottleneck}} problem for capturing high-quality images. There are {{hardware and software}} solutions to overcome this problem. In the solutions, the fill factor is known. However, this is an industrial secrecy by most image sensor manufacturers due to its direct effect on {{the assessment of the}} sensor quality. In this paper, we propose a method to estimate the fill factor of a camera sensor from an arbitrary single <b>image.</b> The <b>virtual</b> response function of the imaging process and sensor irradiance are estimated from the generation of <b>virtual</b> <b>images.</b> Then the global intensity values of the <b>virtual</b> <b>images</b> are obtained, which are the result of fusing the <b>virtual</b> <b>images</b> into a single, high dynamic range radiance map. A non-linear function is inferred from the original and global intensity values of the <b>virtual</b> <b>images.</b> The fill factor is estimated by the conditional minimum of the inferred function. The method is verified using images of two datasets. The results show that our method estimates the fill factor correctly with significant stability and accuracy from one single arbitrary image according to the low standard deviation of the estimated fill factors from each of images and for each camera. open access</p...|$|R
2500|$|Using a {{positive}} lens of focal length f, a <b>virtual</b> <b>image</b> results when , the lens thus being used {{a magnifying glass}} (rather than if [...] as for a camera). Using a negative lens (...) with a real object (...) can only produce a <b>virtual</b> <b>image</b> (...) , {{according to the above}} formula. It is also possible for the object distance S1 to be negative, in which case the lens sees a so-called virtual object. This happens when the lens is inserted into a converging beam (being focused by a previous lens) before the location of its real image. In that case even a negative lens can project a real image, as is done by a Barlow lens.|$|E
2500|$|In 1868 James Clerk Maxwell had an {{improved}} zoetrope constructed. Instead of slits it used concave lenses with a focal length equaling {{the diameter of}} the cylinder. The <b>virtual</b> <b>image</b> was thus seen in the centre and appeared much more sharp and steady than in the original zoetrope. Maxwell drew several strips that mostly demonstrated subjects relating to physics, like the vibrations of a harp string or Helmholtz's vortex rings threading through each other. An article about the [...] "Zootrope perfectionné" [...] was published in French scientific magazine Le Cosmos in 1869, but the device was never marketed. Maxwell's original zoetrope and some strips are kept {{in the collection of the}} Cavendish Museum in Cambridge.|$|E
2500|$|Using an {{inappropriate}} measurement of magnification can be formally correct but yield a meaningless number. For instance, using {{a magnifying glass}} of 5nbsp&cm focal length, held 20nbsp&cm from the eye and 5nbsp&cm from the object, produces a <b>virtual</b> <b>image</b> at infinity of infinite linear size: [...] But the angular magnification is 5, meaning that the object appears 5 times larger to the eye than without the lens. When taking {{a picture of the}} moon using a camera with a 50nbsp&mm lens, one is not concerned with the linear magnification [...] Rather, the plate scale of the camera is about 1°/mm, from which one can conclude that the 0.5nbsp&mm image on the film corresponds to an angular size of the moon seen from earth of about 0.5°.|$|E
40|$|The aim of {{this work}} was to develop an image {{algorithm}} to detect changes in colour related with changes in leaf pigments content in leafy spinach during storage. The experiment was carried out on packed ready-to-use spinach stored at 4. 5 °C. Seventy-five leaves of spinach were analyzed at zero time and after storage for 7, 14 and 21 days. Multispectral images were acquired in the red (R), infrared (IR) and blue (B) regions. <b>Virtual</b> <b>images</b> were calculated {{on the basis of}} spectral indexes usually employed for estimation of leaf pigment content. By considering the sensitive bands to chlorophyll and carotenoids, new <b>virtual</b> <b>images</b> were proposed. A no supervised classification was applied to the obtained <b>virtual</b> <b>images</b> and the results were evaluated according to colorimetric measurements (CIE L*a*b* coordinates). The new proposed indexes were able to correctly classify a high percentage of the samples according to the colour evolution...|$|R
40|$|The aim of {{this work}} was to compare an image {{algorithm}} to detect changes in quality related with changes in leaf pigments content in leafy spinach during storage with a visual evaluation using a 1 - 4 scale, where 1 corresponds to fresh samples without any spoilage and 4 to samples with severe deterioration, {{in order to obtain}} a sensory evaluation index (ISE) for each sample. The experiment was carried out on packed ready-to-use spinach stored at 4. 5 °C or 10 ºC. Seventy-five leaves of spinach were analyzed at zero time and after 7, 14 and 21 days of storage at 4. 5 º C. Twenty-four e samples were measured at zero time and after 3, 6 and 9 days of storage at 10 º C. Multispectral images were acquired in the red (R, 680 ± 20 nm), infrared (IR, 800 ± 20 nm) and blue (B, 450 ± 20 nm) regions. <b>Virtual</b> <b>images</b> were calculated on the basis of spectral indexes usually employed for estimation of leaf pigment content. By considering the sensitive bands to chlorophyll, new <b>virtual</b> <b>images</b> were proposed. A non-supervised classification was applied to the obtained <b>virtual</b> <b>images</b> and the results were evaluated according to colorimetric measurements (CIE L*a*b* coordinates) and visual evaluation. <b>Virtual</b> <b>images</b> computed from R and B ranges gave the better results detecting changes in quality along period storage at 4. 5 º C. These <b>virtual</b> <b>images</b> were able to classify samples into two reference classes, including respectively the major part of the samples analyzed on zero time and on the 7 th day and samples analyzed on the 14 th and the 21 th days...|$|R
40|$|Abstract. In {{traditional}} algorithms for <b>virtual</b> 3 D <b>image</b> synthesis, {{the high}} complexity of virtual objects will cause serious image drift {{and reduce the}} fidelity of <b>virtual</b> 3 D <b>image</b> synthesis. To solve the problem, this paper proposes a <b>virtual</b> 3 D <b>image</b> synthesis method based on characteristics' dynamic optimization algorithm. According to edge data of virtual objects, the method calculates the dynamic change center to provide accurate data basis for <b>virtual</b> 3 D <b>image</b> synthesis; extracts <b>image</b> characters of <b>virtual</b> <b>images,</b> establishes mesh model for image features, obtains the spatial location of the characteristic points to complete 3 D image synthesis and improve the fidelity. Experimental {{results show that the}} algorithm is simple simulation method with strong practicality...|$|R
5000|$|... #Caption: Top: The {{formation}} of a <b>virtual</b> <b>image</b> using a diverging lens. Bottom: The {{formation of}} a <b>virtual</b> <b>image</b> using a convex mirror. In both diagrams, f is the focal point, O is the object and I is the image, shown in grey. Solid blue lines indicate light rays. It {{can be seen that}} the light rays appear to emanate from the <b>virtual</b> <b>image</b> but do not actually exist at the position of the <b>virtual</b> <b>image.</b> Thus an image cannot be seen by placing a screen at the position of the <b>virtual</b> <b>image.</b>|$|E
5000|$|<b>Virtual</b> <b>Image</b> Capture and <b>Virtual</b> <b>Image</b> Stitcher: Two {{software}} {{products to}} capture mult-field images and stitch them into one single and very large {{image in the}} fields of optical and electron microscopy (image stitching).|$|E
5000|$|... #Caption: A {{negative}} lens produces a demagnified <b>virtual</b> <b>image.</b>|$|E
50|$|Tourism: {{management}} of the Office of Tourism, {{management of}} the convention centre, {{the construction of new}} facilities and their management, creation of a centre on technology information from <b>virtual</b> <b>images.</b>|$|R
30|$|We {{illustrate}} {{the performance of}} the proposed method by comparing <b>virtual</b> reflectance <b>images</b> with observed images. The calibrations between the individual cameras and the point cloud are calculated on the first scene. Then, calibration data is used to generate <b>virtual</b> <b>images</b> of the evaluation scene for each camera. For comparison, each camera has been calibrated with the proposed method and the baseline method to the point cloud.|$|R
40|$|While Virtual Reality (VR) {{replaces the}} entire real world with <b>virtual</b> <b>images,</b> Augmented Reality (AR) superimposes <b>virtual</b> <b>images</b> {{on the real}} world. Augmented Reality (a most useful form of Mixed Reality (MR)) is a popular concept for using {{computers}} to overlay virtual information onto {{a view of the}} real world. In 2000, Phillip Dunston then at the University of Washington in Seattle and his research team presented the initial concept of AR CAD developed for supporting design and construction. The AR CAD concept is the addition of an AR assistant viewer to standard CAD to provide a more intuitive interaction with design models...|$|R
50|$|In {{some cases}} S2 is negative, {{indicating}} that the image is formed {{on the opposite side}} of the lens from where those rays are being considered. Since the diverging light rays emanating from the lens never come into focus, and those rays are not physically present at the point where they appear to form an image, this is called a <b>virtual</b> <b>image.</b> Unlike real images, a <b>virtual</b> <b>image</b> cannot be projected on a screen, but appears to an observer looking through the lens as if it were a real object at the location of that <b>virtual</b> <b>image.</b> Likewise, it appears to a subsequent lens as if it were an object at that location, so that second lens could again focus that light into a real image, S1 then being measured from the <b>virtual</b> <b>image</b> location behind the first lens to the second lens. This is exactly what the eye does when looking through a magnifying glass. The magnifying glass creates a (magnified) <b>virtual</b> <b>image</b> behind the magnifying glass, but those rays are then re-imaged by the lens of the eye to create a real image on the retina.|$|E
5000|$|... #Caption: <b>Virtual</b> <b>image</b> {{formation}} using {{a positive}} lens as a magnifying glass.|$|E
5000|$|... #Caption: A {{plane mirror}} showing the <b>virtual</b> <b>image</b> of an urn nearby.|$|E
40|$|Depth {{image-based}} rendering (DIBR) is {{a method}} for generating new <b>virtual</b> <b>images</b> from known viewpoints. However, holes often appear in the rendered <b>virtual</b> <b>images</b> due to occlusion and inaccurate depth information. In this paper, we present a novel hole-filling algorithm to improve the image quality of DIBR. In the proposed method, depth information {{is added to the}} priority calculation function when determining the order of hole-filling. Then, the gradient information is used as auxiliary information when searching for the optimal matching block. Experimental results show that the proposed algorithm achieves better objective quality and also improves the subjective quality of the rendered images...|$|R
40|$|This paper {{describes}} {{the processes of}} creating a hybrid experience of live performance and virtual imagery in 1 + x: Mid-range Projections, a dance work that uses real-time processing of live feed video to create a duet between dancers and their <b>virtual</b> <b>images.</b> 1 + x is about creating experiences of interiority and intimacy, ironically, through the juxtaposition of <b>virtual</b> <b>images</b> of performers with their real selves. This paper examines how the ideal of reinserting intimacy, humanity and 2 ̆ 7 presence 2 ̆ 7 into a technologically generated image space, {{in the case of}} 1 + x, depends on the precise and delicate manipulation of live-feed video in collaboration with the audience 2 ̆ 7 s experience of interactivity. The result is a performance work that creates a poesis of 2 ̆ 7 presence 2 ̆ 7, through a diegesis that melds real and <b>virtual</b> <b>images</b> on the same screen and the same conceptual and spatial plane - a 2 ̆ 7 becoming virtual 2 ̆ 7. <br /...|$|R
5000|$|... 2003 Kun Shan University, Tainan, Taiwan POC; Dieter Jung - Anders als man denkt, Museum im Kulturspeicher, Würzburg, Germany; Kunst-Museum in Ahlen, Germany; Kunstmuseum Heidenheim, Heidenheim, Germany; Dieter Jung, <b>Virtual</b> <b>Images,</b> Kun Shan University, Tainan, Taiwan ...|$|R
