457|219|Public
500|$|The {{music video}} for [...] "Freedom" [...] was {{directed}} by Sean De Sparengo, and filmed in London in July 2011. It took one day to film, and is primarily set in an underground club with red lighting, reminiscent of that featured in the group's 2002 single [...] "Freak like Me". De Sparengo commented on the concept and <b>visual</b> <b>attributes</b> of the video, saying: [...] "One {{of the interesting things}} for the concept for this video is that we knew we had to make everything look incredibly beautiful. It was about making a video where the Sugababes felt like they were {{at the top of their}} game." [...] The video made its premiere in August 2011. A [...] clip was uploaded to the Sugababes official VEVO channel on YouTube. Daily Mirror praised the band's appearance in the video.|$|E
5000|$|Discovery of {{the many}} visual {{areas of the brain}} and their {{functional}} specialisation for different <b>visual</b> <b>attributes</b> such as colour, motion and form.|$|E
5000|$|Assessment {{of whether}} the patient’s deficit in body part {{representation}} extends to individual body parts, and whether <b>visual</b> <b>attributes</b> {{of the body and}} its parts, such as viewing angle, affect their recognition.|$|E
40|$|These two {{approaches}} are evaluated both qualitatively and quantitatively. The qualitative evaluation shows that both approaches can discover human-understandable attributes. The quantitative evaluation demonstrates the comparable {{performance of the}} mentioned approaches in terms of discovering discriminative attributes. Furthermore, the generative model can localize all parts of a <b>visual</b> <b>attribute</b> by multiple patches, whereas the discriminative model shows only a predominant part of each <b>visual</b> <b>attribute</b> by a single patch...|$|R
50|$|Likewise, {{brightness}} {{is sometimes}} applied to various measures, including light levels, though it more properly applies to a subjective <b>visual</b> <b>attribute.</b>|$|R
40|$|Color is an {{important}} <b>visual</b> <b>attribute</b> for both human vision and computer processing. This chapter {{provides an overview of}} MPEG- 7 color descriptors. As is the case with the other descriptors, extraction of these descriptors and their use in similarity matching are outside the scope of the normative components of the standard. Nevertheless, efficient extraction an...|$|R
50|$|The {{search engine}} does {{not rely on}} tags that may, {{or may not have}} been, added to images; instead, it {{analyses}} images down to the pixel level, to identify <b>visual</b> <b>attributes</b> like shape, texture, colour and the kind of objects they contain. PIXSTA’s technology compares images directly at this visual level rather than at the textual level, making tags unnecessary.|$|E
50|$|Patrons {{used these}} <b>visual</b> <b>attributes</b> {{to express their}} {{individuality}} by decorating each dome and minaret with distinct patterns. Patterns carved on domes ranged from ribs and zigzags to floral and geometric star designs. The funerary dome of Aytimish al-Bajasi and the mausoleum dome of Qaitbay's sons reflect the diversity and detail of Mamluk architecture. The creativity of Mamluk builders was effectively emphasized with these leitmotifs.|$|E
50|$|The Organ Historical Society hosts annual conventions. Over {{the course}} of a week, {{attendees}} enjoy numerous concerts in various venues in the convention's host city and its surrounding area featuring a wide variety of historic pipe organs. The purpose in visiting the instruments is to apprecitate, hear, and see them in their surroundings, compare them with similar instruments, and experience their aural, mechanical and <b>visual</b> <b>attributes.</b> Demonstrations are intended to showcase the instruments.|$|E
40|$|We {{propose a}} new {{technique}} for <b>visual</b> <b>attribute</b> transfer across images that may have very different appearance but have perceptually similar semantic structure. By <b>visual</b> <b>attribute</b> transfer, we mean transfer of visual information (such as color, tone, texture, and style) from one image to another. For example, one image could be that of a painting or a sketch while {{the other is a}} photo of a real scene, and both depict the same type of scene. Our technique finds semantically-meaningful dense correspondences between two input images. To accomplish this, it adapts the notion of "image analogy" with features extracted from a Deep Convolutional Neutral Network for matching; we call our technique Deep Image Analogy. A coarse-to-fine strategy is used to compute the nearest-neighbor field for generating the results. We validate the effectiveness of our proposed method in a variety of cases, including style/texture transfer, color/style swap, sketch/painting to photo, and time lapse. Comment: Accepted by SIGGRAPH 201...|$|R
40|$|We {{present a}} {{comprehensive}} evaluation of performance of shot-based visual feature representations for MediaEval 2012 - Violent Scenes Detection Affect Task. In spite of using keyframe-based as last year, {{we try to}} apply shot-based features using the global features (color moments, color histogram, edge orientation histogram, and local binary patterns) for violent scenes detection. Besides that, we also evaluate the performance of late fusion with <b>visual</b> <b>attribute...</b>|$|R
40|$|The {{purpose of}} this study is to {{determine}} <b>visual</b> aesthetic <b>attributes</b> for user experience. As interactive digital media and their associated content have diversified, there are difficulties in finding universal visual aesthetic guidelines. While previous studies look into each unique user experience, there is little focusing on meta analysis of visual aesthetics in providing user experience. Thus, by means of content analysis, this study attempts to determine <b>visual</b> aesthetics <b>attributes</b> for sense-based user experience. As a result, a consolidated model which comprises of <b>visual</b> aesthetics <b>attributes</b> and its inter connections with regard to human senses is developed. This model offers guidance for creative industry practitioners in designing and developing aesthetic interactive digital media and creative content...|$|R
50|$|Spending {{several weeks}} each winter in Tucson, Arizona, with her husband, Maurice Block Jr., Amanda grew {{to love the}} desert landscapes and strong <b>visual</b> <b>attributes</b> of the area. The desert and its impact upon her became a common theme. She often did rough watercolors there, trying to capture the feeling and color palette of the unique place. Unfortunately, these studies do not survive, but their impact clearly inform her large {{abstract}} landscapes.|$|E
50|$|Conceptually {{similar to}} Google AdWords, PIXSTA AdImages generates {{revenue from the}} {{advertisers}} {{in the form of}} click-throughs, which take the visitor directly to the appropriate product page on the retailer site. When on a media brand's website, Click-through revenue is shared between PIXSTA and the media brand. Equivalent to Google AdSense, Pixsta's 'ImageSense' allows publishers to price contextual pages via image matching technology. Ads displayed are heavily targeted by the understanding of <b>visual</b> <b>attributes.</b> Due to the high contextual attributes profitability rates are rumoured to be higher than Google AdSense.|$|E
5000|$|Internally, VirtualGL's {{interposer}} engine also {{maintains a}} map of windows to Pbuffers, matches <b>visual</b> <b>attributes</b> between the destination X display (the [...] "2D X Server") and the 3D X Server, and performs {{a variety of other}} hashing functions to assure that the GLX redirection is seamless. But essentially, once the OpenGL context is established on the application server's X display, VirtualGL gets {{out of the way and}} allows all subsequent OpenGL commands to pass through unimpeded to the application server's 3D hardware. Thus, the application can automatically use whatever OpenGL features and extensions are provided by the application server's hardware and drivers.|$|E
25|$|With {{apparent}} Biblical authority, and {{the added}} convenience of giving Moses {{a unique and}} easily identifiable <b>visual</b> <b>attribute</b> (something the other Old Testament prophets notably lacked), it remained standard in Western art to depict Moses with small horns until well after the mistranslation was realized by the Renaissance. In this depiction of Moses, the error has been identified but the artist has chosen to place horns of light on Moses head to aid in identification.|$|R
40|$|Abstract—Transparency {{perception}} {{is recognized as}} one of the important phenomena for the theory of early vision. This is because transparency perception suggests that a simple theory reconstructing a single-valued field of a <b>visual</b> <b>attribute,</b> such as an optical-flow field, cannot model the neural mechanism in the brain and raises the fundamental issue of how the <b>visual</b> <b>attribute</b> is represented and computed in the visual cortex. Here we present a systematic study that examined the perceptual cost for motion transparency psychophysically. It has been known that the perceptual performance in motion transparency is worse than that predicted by assuming overlapping motions are detected individually. This perceptual “cost ” would reflect the neural encoding mechanism for transparent motions. The present results showed that the properties of the perceptual costs varied with the measures of psychophysical performance; the perceptual cost evaluated by motion detection thresholds became smaller as the directional difference between overlapping motions decreased, whereas the cost examined with precisions of directional judgments became worse. A computational analysis with a simple population coding model suggests that the percep-tual costs evaluated by two measures would arise from different mechanisms. I...|$|R
40|$|Freedom is a model-driven, web-based, {{end-user}} development {{platform that}} is optimized for business users. Freedom simplifies development by quickly {{focusing on the}} business goal at hand, using the <b>visual</b> <b>attribute</b> of the task as the development abstraction using WYSIWYG editing. End-user development quality concerns are addressed in Freedom through division of labor and task-related focus. To reduce barriers to adoption by business users, improve the end-user experience, and simplify integration, Freedom leverages Web 2. 0 technologies such as AJAX and RES...|$|R
50|$|Devlin et al. (2006) {{states that}} the left {{posterior}} fusiform gyrus is not a 'word form area' as such, but instead hypothesizes that the area is dedicated to determining word meaning. That is to say, that {{this area of the}} brain is where bottom-up information (visual shapes of words (form), and other <b>visual</b> <b>attributes</b> if necessary) comes into contact with top-down information (semantics and phonology of words). Therefore, the left fusiform gyrus is thought to be the interface in the processing of the words not a dictionary that computes a word based on its form alone, as the lexical word form hypothesis states. This paper also presents evidence that refutes the lexical hypothesis.|$|E
50|$|Particle {{systems are}} an {{extremely}} popular technique for creating visual effects {{in movies and}} games because of their ease of implementation, efficiency, extensibility, and artist control. The update cycle of particle systems usually consists of the three phases: generation, simulation, and extinction. These phases respectively consist of {{the introduction of new}} particles, simulating them through the next timestep, and removing particles that have exceeded their life-span. The physical and <b>visual</b> <b>attributes</b> of particles are usually randomized on generation with the range and distribution of attributes controlled by the artist. Particle systems can further be made to generate particle systems themselves to create more complex and dynamic effects, and their high-level behavior can be choreographed through a framework of operators as in the canonical Sims paper.|$|E
5000|$|The {{music video}} for [...] "Freedom" [...] was {{directed}} by Sean De Sparengo, and filmed in London in July 2011. It took one day to film, and is primarily set in an underground club with red lighting, reminiscent of that featured in the group's 2002 single [...] "Freak like Me". De Sparengo commented on the concept and <b>visual</b> <b>attributes</b> of the video, saying: [...] "One {{of the interesting things}} for the concept for this video is that we knew we had to make everything look incredibly beautiful. It was about making a video where the Sugababes felt like they were {{at the top of their}} game." [...] The video made its premiere in August 2011. A [...] clip was uploaded to the Sugababes official VEVO channel on YouTube. Daily Mirror praised the band's appearance in the video.|$|E
40|$|Abstract. Wear is an {{important}} source of information supporting our tasks in everyday life. This paper presents the idea of using wear as a new <b>visual</b> <b>attribute</b> for visualizing the computational wear of digital objects, which is defined as a set of attributes resulting from the history of user interactions on the digital objects. As a case study to investigate the feasibility of the proposed idea, we succeeded in visualizing the computational wear of WWW sites with the icons of physically worn appearance. 1...|$|R
40|$|Visual masking {{can result}} from the {{interference}} of perceptual signals. According {{to the principle of}} functional specialization, interference should be greatest when signal and mask belong to the same <b>visual</b> <b>attribute</b> (e. g., color or motion) and least when they belong to different ones. We provide evidence to support this view and show that the time course of masking is <b>visual</b> <b>attribute</b> specific. First, we show that a color target is masked most effectively by color (homogeneous target-mask pair) and least effectively by motion (heterogeneous pair) and vice versa for a motion target. Second, we show that the time at which the mask is most effective depends strongly on the target-mask pairing. Heterogeneous masking is strongest when the mask is presented before the target (forward masking) but this is not true of homogeneous masking. This finding supports a delayed cross-feature interaction due to segregated processing sites. Third, lengthening the stimulus onset asynchrony between target and mask leads to a faster improvement in color than in motion detectability, lending support for a faster color processing system and consistent with reports of perceptual asynchrony in vision. In summary, we present three lines of psychophysical evidence, all of which support a segregated neural coding scheme for color and motion in the human brain...|$|R
50|$|As {{shoes in}} their various {{appearances}} have been worn throughout all social classes since earliest human history, forcibly presenting {{a person to}} the public in bare feet usually conveys a shaming effect to the captive. The default of footwear is a customary symbol of subjugation and is, therefore, a common method to showcase a person's loss of status or absence of rights. As bare feet are a highly noticeable <b>visual</b> <b>attribute</b> in almost every social situation, this particular form of appearance often incidentally raises suspicion or disdain among bystanders.|$|R
50|$|Hsu et al. (2004) {{compared}} MIB to {{a similar}} phenomenon of perceptual filling-in (PFI), which likewise reveals a striking dissociation between the percept and the sensory input. They describe both as <b>visual</b> <b>attributes</b> which are perceived in a certain region of the visual field regardless {{of being in the}} background (in the same manner as colour, brightness or texture) thus inducing target disappearance. They argue that because in both MIB and PFI the disappearance; or the incorporation of the background motion stimuli; becomes more profound with an increase in eccentricity, decrease in contrast and when perceptual grouping with other stimuli is controlled for; the two illusions are very likely to be a result of intermutual processes. Since MBI and PFI show to be structurally similar, it seems plausible that MIB can be a phenomenon responsible for completing missing information across the blind spot and scotomas where motion is involved.|$|E
5000|$|These {{findings}} {{raised the}} question of how the signals processed in these separate visual areas are integrated to give a unified picture of the visual world. In psychophysical experiments undertaken with colleagues, he showed that we perceive, and become aware of, different <b>visual</b> <b>attributes</b> at different times, with colour preceding motion by about 80 ms and form (orientation) by about 40 ms, leading to the view that there is a temporal asynchrony in vision which is the result of different processing speeds for different attributes. This in turn led him to suggest that visual consciousness is not unified; rather there are many visual micro-consciousness which are distributed in time and space, and that activity in each visual area can acquire a conscious correlate without the necessity of reporting to another cortical area, though acknowledging that there must be other enabling systems, possibly located in the reticular formation. Thus, functional specialisation manifests itself in the temporal sequence with which we see different attributes such as colour ...|$|E
5000|$|Phantom Quest Corp. {{has been}} almost universally {{compared}} to Ghost Sweeper Mikami, a franchise with a nearly identical premise that was released slightly earlier. Anime journalist John Oppliger is convinced that Madhouse based Phantom Quest Corp. on the latter series, noting stark similarities between the protagonists of the two works. Christopher Macdonald of the Anime News Network felt the art of Phantom Quest Corp. was subpar for a Madhouse release, but {{was impressed by the}} animation, stating, [...] "Fight scenes and other scenes involving high-speed movement are absolutely astounding in animation quality". Macdonald also enjoyed the comedy found in the series, but disliked the lack of character development and how each episode's plot stands on its own without an actual story arc. Mania.com's Chris Beveridge and Raphael See of THEM Anime Reviews agreeably noted that there are several enjoyable moments throughout the OVA despite this seemingly generic quality. They made similar positive comments regarding its <b>visual</b> <b>attributes,</b> with Beveridge calling the final battle in the fourth episode [...] "just great both in choreography and animation". She likened the series to a mix between Ghostbusters, The X-Files, and Sledge Hammer!.|$|E
40|$|Abstract—Texture is an {{important}} <b>visual</b> <b>attribute</b> both for human perception and image analysis systems. We review re-cently proposed texture similarity metrics and applications that critically depend on such metrics, with emphasis on image and video compression and content-based retrieval. Our focus is on natural textures and structural texture similarity metrics (STSIMs). We examine the relation of STSIMs to existing models of texture perception, texture analysis/synthesis, and texture seg-mentation. We {{emphasize the importance of}} signal characteristics and models of human perception, both for algorithm development and testing/validation. Index Terms—Structural similarity metrics, structurally loss-less compression, matched-texture coding I...|$|R
30|$|The image {{processing}} pipeline (face detection, alignment, attribute detection) took roughly 5 s per image, {{with most of the}} computation spent on attribute detection. GeoFaces will evolve as we collect more images and improve the methods for detecting, aligning, and filtering. The full dataset, including face patches and <b>visual</b> <b>attribute</b> values, is freely available online [38]. The remainder of this work describe various ways of using this dataset to better understand the relationship between human appearance and geographic location. See the Appendix for additional non-geographic analysis of the dataset, such as how the expected size of a face relates to the textual tags of the enclosing image.|$|R
40|$|Hair is an {{important}} <b>visual</b> <b>attribute</b> characterizing the personal physical appearance and unique identity of individuals. The strong impact of hairstyles is noticeable in everyday life, movies and pictures – but also on digital models used e. g. in virtual reality (VR) environments, computer games or 3 D productions for the film industry. However, representing hair is a critical issue in 3 D games and production movies: the creation of complex, attractive virtual hairstyles using computer graphics tools requires a specialized know-how and is a long and tedious task even for professional 3 D modelers. Therefore, the use of complex hairstyles is often avoided in favor of time-saving design choices...|$|R
50|$|So that {{customer}} {{can locate}} their desired product inside the SRP and easily {{remove it from}} the SRP, the requirement “easy shop” needs to be fulfilled. Considering this functional requirement, physical and <b>visual</b> <b>attributes</b> {{need to be taken}} into account. The front panel of a SRP should reflect the products included in the SRP. Accordingly, the SRP often includes the brand or manufacturer’s logo to ensure rapid recognition and visibility. This makes it easier for the customer to select the desired product, possibly among several variants. SRP should also provide any relevant information from the primary packaging. SRP should improve product identification and thus make a positive contribution to the shopping experience. As a result, SRP should be designed brightly {{to attract the attention of}} customers. Individual product variant can be especially enhanced via color printing; plain SRP may reduce the shopping experience. Moreover a customer needs to be able to take the desired product out of the SRP easily and place it back if unwanted. Therefore, products should not be arranged too close in a SRP and should not stick on the tray. After removal of products, the SRP must nevertheless remain stable and still visually appealing for the customer when in a half-empty state.|$|E
5000|$|The {{interface}} {{elements of}} a WPF application are maintained as a class of [...] objects. Visual objects provide a managed interface to a composition tree which is maintained by Media Integration Layer (MIL). Each element of WPF creates and adds one or more composition nodes to the tree. The composition nodes contain rendering instructions, such as clipping and transformation instructions, along with other <b>visual</b> <b>attributes.</b> Thus the entire application is represented {{as a collection of}} composition nodes, which are stored in a buffer in the system memory. Periodically, MIL walks the tree and executes the rendering instructions in each node, thus compositing each element on to a DirectX surface, which is then rendered on screen. MIL uses the painter's algorithm, where all the components are rendered from back of the screen to the front, which allows complex effects like transparencies to be easily achieved. This rendering process is hardware accelerated using the GPU. The composition tree is cached by MIL, creating a retained mode graphics, so that any changes to the composition tree needs only to be incrementally communicated to MIL. This also frees the applications of managing repainting the screen; MIL can do that itself as it has all the information necessary. Animations can be implemented as time-triggered changes to the composition tree. On the user visible side, animations are specified declaratively, by setting some animation effect to some element via a property and specifying the duration. The code-behind updates the specific nodes of the tree, via [...] objects, to represent both the intermediate states at specified time intervals as well as the final state of the element. MIL will render the changes to the element automatically.|$|E
5000|$|Stephen Holden {{writing in}} The New York Times applauded {{some of the}} realism {{displayed}} in the film, commenting, [...] "Nothing {{in the rest of}} the film comes close to matching the impact of Gideon’s carving the bullet from his arm with his hunting knife, then cauterizing the wound while emitting agonizing howls. This scene is enough to give you vicarious hypothermia." [...] He also expressed his satisfaction with the <b>visual</b> <b>attributes</b> of the picture by saying [...] "Its strongest element is the austere majesty of the cinematography by John Toll ("Braveheart," [...] "Legends of the Fall," [...] "The Thin Red Line"), in which the severe beauty of the Western landscape looms over the characters as a silent rebuke." [...] Critic Josh Rosenblatt, writing for The Austin Chronicle viewed Seraphim Falls as [...] "Meditative, beautifully shot, and blessed with a healthy dose of cynicism" [...] and a [...] "morality play without the morality and a Western Purgatorio that, in the end, demands its protagonists resign themselves to their loneliness and brutality and avail themselves of the redemptive power of sheer exhaustion." [...] Kevin Crust of the Los Angeles Times gave the film a somewhat mixed rating calling it [...] "A beautifully shot chase film by writer-director David Von Ancken and co-writer Abby Everett Jaques, it moves along with minimalist efficiency" [...] but overall admitting it ran out of [...] "gas during an overlong allegorical final section." [...] Author Joshua Rothkopf of Time Out commented that the film [...] "has all the good looks of its wintry Oregon locales, not to mention the equally craggy faces of Liam Neeson and a grizzled-up Pierce Brosnan, embroiled in a Fugitive-like pursuit with the latter on the run." ...|$|E
5000|$|From the Early Christian era {{the phrase}} [...] "crown of {{immortality}}" [...] was widely {{used by the}} Church Fathers in writing about martyrs; the immortality was now both of reputation on earth, and of eternal life in heaven. The usual <b>visual</b> <b>attribute</b> of a martyr in art, was a palm frond, not a wreath. The phrase may have originated in scriptural references, or from incidents such as this reported by Eusebius (Bk V of History) describing the persecution in Lyon in 177, in which he refers to literal crowns, and also brings in an athletic metaphor of the [...] "victor's crown" [...] at the end: ...|$|R
40|$|We {{propose a}} novel {{attention}} model that can accurately attend to target objects of various scales and shapes in images. The model is trained to gradually suppress irrelevant regions in an input image via a progressive attentive process over {{multiple layers of}} a convolutional neural network. The attentive process in each layer determines whether to pass or suppress features at certain spatial locations {{for use in the}} next layer. We further employ local contexts to estimate attention probability at each location since it is difficult to infer accurate attention by observing a feature vector from a single location only. The experiments on synthetic and real datasets show that the proposed attention network outperforms traditional attention methods in <b>visual</b> <b>attribute</b> prediction tasks...|$|R
40|$|Pressure widgets are user {{interface}} elements that exploit {{the capabilities of}} pressure-sensing technology present in digitizer tablets and interactive pen-based display devices. They provide users with a visual indication {{of the amount of}} pressure being applied, as well as meaningful feedback as to the consequences of varying pressure in these widgets. We present empirical work that investigates how changes in key visual design dimensions of pressure widgets affect their usability. Our results indicate that variations in visual design, including changes in pressure to <b>visual</b> <b>attribute</b> mappings, can have significant impact on a pressure widget's usage speed, accuracy, and interference between the pressure and the spatial x-y movement components of the stylus. Our results can be used to refine the design of pressure widgets...|$|R
