14|51|Public
50|$|<b>Video</b> <b>calibration</b> {{software}} is software {{used to improve}} the quality of commercial video reproduction.|$|E
50|$|Though the ISF popularized {{the value}} of {{professional}} <b>video</b> <b>calibration,</b> {{in recent years the}} equipment necessary to carry out the necessary tasks has become inexpensive enough that enthusiasts and prosumers have been empowered to calibrate their own displays.|$|E
5000|$|All <b>video</b> <b>calibration</b> {{software}} interfaces with a {{color analyzer}} that reads the luminance {{and the color}} from a commercial display. The software then interprets that data, usually in xyY format, and then displays it on a laptop PC. That data allows a calibrator to correct any errors in ...|$|E
40|$|Analysis {{of amateur}} {{video of the}} early reentry phases of the Columbia {{accident}} is discussed. With poor video quality and little theoretical guidance, the analysis team estimated mass and acceleration ranges for the debris shedding events observed in the <b>video.</b> Camera <b>calibration</b> and optical performance issues are also described...|$|R
40|$|Several {{different}} recent {{applications of}} velocimetry at Langley Research Center {{are described in}} order to show the need for <b>video</b> camera <b>calibration</b> for quantitative measurements. Problems peculiar to video sensing are discussed, including synchronization and timing, targeting, and lighting. The extension of the measurements to include radiometric estimates is addressed...|$|R
40|$|EN] In {{this paper}} we study a variational problem {{derived from a}} {{computer}} vision application: <b>video</b> camera <b>calibration</b> with smoothing constraint. By <b>video</b> camera <b>calibration</b> we meanto estimate the location, orientation and lens zoom-setting of the camera for each video frame taking into account image visible features. To simplify the problem {{we assume that the}} camera is mounted on a tripod, in such case, for each frame captured at time t, the calibration is provided by 3 parameters : (1) P(t) (PAN) which represents the tripod vertical axis rotation, (2) T(t) (TILT) which represents the tripod horizontal axis rotation and (3) Z(t) (CAMERA ZOOM) the camera lens zoom setting. The calibration function t -> u(t) = (P(t),T(t),Z(t)) is obtained as the minima of an energy function I[u]. In thIs paper we study the existence of minima of such energy function as well as the solutions of the associated Euler-Lagrange equations...|$|R
5000|$|<b>Video</b> <b>calibration</b> {{software}}: {{software that}} receives the {{signals from the}} color analyzer and displays the data in numerical format which is interpreted in a human interface {{in the form of}} real-time charts and graphs. Calibrators use this information to guide decisions about how to properly adjust the displays.|$|E
5000|$|According to an {{analysis}} by Raymond Soneira, president of DisplayMate Technologies, a <b>video</b> <b>calibration</b> equipment producer, the industry-standard color spaces used by content providers mean there is no existing source material that contains the fourth color channel. He therefore concludes that any [...] "extra" [...] colors displayed must be created in the television itself through video processing, resulting in exaggerated, less accurate color.|$|E
50|$|Color HCFR is a {{freeware}} program {{developed by}} enthusiasts in France. It {{was released in}} 2006. ChromaPure is a commercial product that was released in 2009. It was developed by a partnership between an American ISF calibrator and a programmer. CalMAN is a commercial product that was originally released as a Microsoft Excel-based program in 2002. In 2007 it was redesigned as a standalone Windows program based around a new Calibration Optimization and Reporting Environment (CORE) engine. Written exclusively using 64-bit double-precision floating point math, the CORE engine outperforms even popular spreadsheet applications for calculation accuracy because it never has to drop to single-precision or integer operations for convenience or speed purposes. It was developed by a partnership between an American video engineer and a programmer with a goal to make <b>video</b> <b>calibration</b> increasingly accessible and more powerful.|$|E
40|$|Abstract. This paper {{presents}} an improved initial guessing method for the famous Zhang's camera calibrationmethod for amoving camera. Given {{three or more}} known points in a scene, we can compute initial guessing values for maximum likelihood estimation for varying focal camera calibration. This method is useful in structure from motion, which needs consecutive <b>video</b> camera <b>calibration</b> using a known camera distortion...|$|R
40|$|International audienceIn {{this paper}} we {{address the problem of}} {{geometric}} <b>video</b> projector <b>calibration</b> using a markerless planar surface (wall) and a partially calibrated camera. Instead of using control points to infer the camera-wall orientation, we find such relation by efficiently sampling the hemisphere of possible orientations. This process is so fast that even the focal of the camera can be estimated during the sampling process. Hence, physical grids and full knowledge of camera parameters are no longer necessary to calibrate a video projector...|$|R
40|$|Jet Propulsion Laboratory (JPL) {{developed}} <b>video</b> overlay <b>calibration</b> {{and demonstration}} techniques for ground-based telerobotics. Through a technology sharing agreement with JPL, Deneb Robotics added {{this as an}} option to its robotics software, TELEGRIP. The software is used for remotely operating robots in nuclear and hazardous environments in industries including automotive and medical. The option allows the operator to utilize video to calibrate 3 -D computer models with the actual environment, and thus plan and optimize robot trajectories before the program is automatically generated...|$|R
40|$|The videomusic Walk That Way. Tuesday, Turn. was {{realized}} in 2006 at the composer’s studio and premiered on June 3, 2006 during a concert produced by BEAST at Vivid in Birmingham (England, UK). Walk That Way. Tuesday, Turn. {{was awarded a}} Mention at the 35 th Bourges International Electroacoustic Music and Sonic Art Competition (France, 2008). The 5. 1 Surround version was {{realized in}} June 2011 by Dominique Bassal in Montréal (Québec). The <b>video</b> <b>calibration</b> was realized in June 2011 by Geoff Cox in Huddersfield (England, UK) ...|$|E
40|$|In this paper, we {{introduce}} a fully autonomous vehicle classification system that continuously learns from {{large amounts of}} unlabeled data. For that purpose, we propose a novel on-line co-training method based on visual and acoustic information. Our system does not need complicated microphone arrays or <b>video</b> <b>calibration</b> and automatically adapts to specific traffic scenes. These specialized detectors are more accurate and more compact than general classifiers, which allows for light-weight usage in low-cost and portable embedded systems. Hence, we implemented our system on an off-the-shelf embedded platform. In the experimental part, we show that the proposed method is able to cover the desired task and outperforms single-cue systems. Furthermore, our co-training framework minimizes the labeling effort without degrading the overall system performance...|$|E
40|$|Non-invasive {{methods for}} {{monitoring}} foraging choice in free ranging grazing animals are largely limited to accelerometers and <b>video</b> <b>calibration.</b> Acoustic {{data from a}} wireless microphone attached to the skull {{has been used to}} distinguish between resting and feeding bouts in free ranging cattle, sheep and goats. Similar data has been reported in restrained sheep presented with forage of differing dry matter content. We take these approaches further by using a small video camera attached to a halter in free range sheep, and software developed specifically for the analysis of animal sounds. Combined biting and mastication sounds allowed us to distinguish between foraged grasses and browsing activity, and non-foraging chewing activity in four sheep of differing body size and breed in the height of a UK summer for up to 8 hours...|$|E
30|$|First, video {{localization}} {{based on}} face and head detection {{is used to}} obtain the visual location of each speaker which is approximated after processing the 2 D image information and obtained from at least two synchronized color <b>video</b> cameras through <b>calibration</b> parameters[29] and an optimization method[30].|$|R
40|$|The MSS sensor image {{processing}} and resolution capabilities {{as well as}} a general system description are presented in chart form. Emphasis is placed on absolute radiometric <b>calibration,</b> <b>video</b> and wedge level timing sequence, focal plane dimensions, sampling sequence, and a description of the mirror coordinate systems...|$|R
40|$|An {{efficient}} {{technique for}} <b>video</b> camera <b>calibration</b> is presented. Unlike the methods reported up to now, {{it does not}} require parameter estimation (lense distortion coefficients, focal distance, rotation and translation matrices, etc.). The method consists essentially of a mapping from the camera space onto that of real positions by means of a neural network (two-layer perceptron) trained with an original algorithm for neural network construction. This is an approximate technique, i. e. precision increases with the sample size and can be as large as needed by growing that size and the computing time. We show {{that it is possible to}} obtain good approximations with low computational costs (processor and memory) ...|$|R
40|$|In the {{introductory}} {{chapter of the}} master work theme actuality is presented, the aim (development of 3 D scanner with low hardware requirements and easy usability), objectives, and novelty of a 3 D scanner, methods and tools used. The first chapter the market of 3 D printers and 3 D scanner has been overviewed and analyzed as well as data collection, digital modeling and a reverse engineering. Performed brief overview of a triangulation method laser scanning. Also the David Laseridscanner has been overviewed. In the second chapter presented: physical setup and working principle of the device: structure and a working principle: setting the device, taking a <b>video,</b> <b>calibration,</b> programs. Also the overview of a photo camera specifications, motor specifications, overview of a laser mechanics, and a detailed presentation of a scanner code. In the third chapter test of the scanning quality, results and measurements are provided, also potential modifications and improvements were represented. Moreover the prototype of a scanner are submitted. The conclusions summarize results of the tests and received information about the 3 D scanner...|$|E
40|$|Our {{objective}} {{is the development}} and evaluation of a low-cost, vehicle-mounted sensor suite capable of generating map data with lane and road boundary information accurate to the 10 cm (4 in) level. Such a map {{could be used for}} a number of different applications including GNSS/GPS based lane departure avoidance systems, smart phone based dynamic curve speed warning systems, basemap improvements, among others. The sensor suite used consists of a high accuracy GNSS receiver, a side-facing video camera, and a computer. Including cabling and mounting hardware, the equipment costs were roughly $ 30, 000. Here, the side-facing camera is used to record video of the ground adjacent to the passenger side of the vehicle. The video is processed using a computer vision algorithm that locates the fog line within the video frame. Using vehicle position data (provided by GNSS) and previously collected <b>video</b> <b>calibration</b> data, the fog line is located in real-world coordinates. The system was tested on two roads (primarily two-lane, undivided highway) for which high accuracy (< 10 cm) maps were available. This offset between the reference data and the computed fog line position was generally better than 7. 5 cm (3 in). The results of this work demonstrate that it is feasible to use a camera to detect the position of a road’s fog lines, or more broadly any other lane markings, which when integrated into a larger mobile data collection system, can provide accurate lane and road boundary information about road geometry...|$|E
40|$|BACKGROUND Stereotactic {{navigation}} {{technology can}} enhance guidance during surgery and enable the precise reproduction of planned surgical strategies. Currently, specific systems (such as the CAS-One system) {{are available for}} instrument guidance in open liver surgery. This study aims to evaluate the implementation of such a system for the targeting of hepatic tumors during robotic liver surgery. MATERIAL AND METHODS Optical tracking references were attached {{to one of the}} robotic instruments and to the robotic endoscopic camera. After instrument and <b>video</b> <b>calibration</b> and patient-to-image registration, a virtual model of the tracked instrument and the available three-dimensional images of the liver were displayed directly within the robotic console, superimposed onto the endoscopic video image. An additional superimposed targeting viewer allowed for the visualization of the target tumor, relative to the tip of the instrument, for an assessment of the distance between the tumor and the tool for the realization of safe resection margins. RESULTS Two cirrhotic patients underwent robotic navigated atypical hepatic resections for hepatocellular carcinoma. The augmented endoscopic view allowed for the definition of an accurate resection margin around the tumor. The overlay of reconstructed three-dimensional models was also used during parenchymal transection for the identification of vascular and biliary structures. Operative times were 240 min in the first case and 300 min in the second. There were no intraoperative complications. CONCLUSIONS The da Vinci Surgical System provided an excellent platform for image-guided liver surgery with a stable optic and instrumentation. Robotic image guidance might improve the surgeon's orientation during the operation and increase accuracy in tumor resection. Further developments of this technological combination are needed to deal with organ deformation during surgery...|$|E
40|$|D shape {{reconstruction}} is {{a common}} task {{in many areas of}} interests. Yet, very often it involves sophisticated hardware and software. This paper represents cheap and simple system, both in hardware and software sense, for 3 D subjects shape reconstruction. It consists of video camera(s) and video projector and is based on well known principle of structured light. A major contribution of the paper is method, which avoids explicit traditional <b>video</b> projector <b>calibration.</b> A system accuracy results are shown along with experimental results of reconstruction for human face and toy mushroom. The system can be easily implemented and adjusted for all kinds of other applications...|$|R
40|$|A Microscope Quantitative Luminescence Imaging System (M-QLIS} {{has been}} {{designed}} and constructed. The M-QLIS is designed for use in studies of chemiluminescent phenomena associated with absorption of radio-frequency radiation. The system consists of a radio-frequency waveguide/sample holder, microscope, intensified <b>video</b> camera, radiometric <b>calibration</b> source and optics, and computer-based image processor with radiometric analysis software. The system operation, hardware, software, and radiometric procedures are described...|$|R
50|$|Whereas {{laying down}} bars and tone prior to program start {{establishes}} <b>video</b> and audio <b>calibration</b> levels on the tape, the 2-pop is primarily used for picture and sound synchronization. Therefore, while the loudness of the 2-pop {{may be the}} same as the bars and tone audio level in use, this is not a requirement. The loudness level should be sufficient to be heard clearly.|$|R
40|$|In the 1970 s NASA and the Department of Agriculture {{attempted}} {{to use the}} new Landsat MSS system for agricultural purposes. The program had relatively little success. With the advent of differential GPS, yield monitors on harvest equipment and higher spatial resolution remote sensing systems it seemed likely the situation should be reexamined. Therefore, a campaign of data acquisition involving remote sensing and other modalities with dependent research was assembled and funded by the Space Grant Consortia in Alabama and Georgia. The design of the remote sensing data acquisition was driven by the biology and physics of the crop system and limited by the available sensor platforms. Major parameters included crop stage, spatial resolution, seasonal and daily weather conditions, and which portion of the EM spectrum would actually capture the most discriminating information. Joint visible and Near IR with Thermal IR would permit use of existing indices, such as greenness, as well as phenomena driven by the plant' s evapotranspiration. Spatial resolution in the 2 - 5 meter range was chosen, avoiding many complexities caused by aliasing crop row spacing at, higher resolutions yet finer than the harvester's tightest recording rate. This dictates use of an airborne system. Use of an airborne system also makes scheduling around weather much simpler than use of satellite data. Active <b>video</b> <b>calibration</b> was recognized as essential if quantitative measures were ever to be obtained or reproduced. The system would also have to have onboard geoOF 1 Based on these elements 3 data acquisitions have been flown. Seven flight lines were flown twice in 1998 and 16 lines flown in 1999. Total raw data is several GBytes. All of the data has now been geometrically corrected and some preliminary analysis accomplished. The thermal bands have an extremely high correlation with yield. For one@test case with corn, correlation in excess of 0. 86 was obtained from a data acquisition two months prior to harvest! Soil images show significant within field variation in clay, soil brightness and emissivity. Light wind has been found to effect the reflectance and temperature of broad leaf crops, including soybeans, cotton and peanuts. Clearly, this work has already demonstrated some very important results. With continued development of the remote sensing technology {{there is good reason to}} believe this research will soon be able to help the individual farmer...|$|E
40|$|In this dissertation, we {{introduce}} a hybrid radio frequency and video framework that enables identity aware-augmented perception. Identity-aware augmented perception enhances users 2 ̆ 7 {{perception of the}} surrounding by collecting and analyzing information pertaining to each identifiable or tractable target nearby aggregated from various sensors, and presents it visually or audibly augmenting users 2 ̆ 7 own sensory perceptions. We target two application areas of disaster management and assistive technologies. Incident commanders and first responders can use the technology to perceive information specific to a victim, e. g. triage level, critical conditions, visually superimposed on third person or first person video. The {{blind and visually impaired}} can use the technology to perceive the direction and distance of static landmarks and moving people nearby, and target specific information, e. g. a store 2 ̆ 7 s name and opening hours, a friend 2 ̆ 7 s status on social networks. Identifying who is who in video is an important yet challenging problem that can greatly benefit existing video analytics and augmented reality applications. Identity information can be used to improve the presentation of target information on graphical user interface, enable role-based target analytics over long term, and achieve more efficient and accurate surveillance video indexing and querying. Instead of relying on target appearance, we propose a hybrid approach that combines complimentary radio frequency (RF) signal with video to identify targets. Recovering target identities in video using RF is not only useful in its own right, but also provides an alternative formulation that helps to solve difficult problems in individual video and RF domains, e. g., persistent video tracking, accurate target localization using RF signal, anchorless target localization, multi-camera target association, automatic RF and <b>video</b> <b>calibration.</b> We provide a comprehensive RF and video fusion framework to enable identity-aware augmented perception in a variety of scenarios. We propose a two stage data fusion scheme based on tracklets, and formulate the tracklet identification problem under different RF and camera measurement models using network flow or graphical model. We first start from a basic calibrated single fixed camera, fixed RF readers configuration. Then we consider anchorless target identification using pair-wise measurements between mobile RF devices to reduce deployment complexity. Then we incorporate multiple cameras, to improve coverage, camera deployment flexibility, identification accuracy and enable multi-view augmented perception. We propose a self-calibrating identification algorithm, that simplifies manual calibration and improve identification accuracy in environments with obstruction. Finally, we solve the problem of annotating video taken by mobile cameras to provide first-person perception, taking advantage of target appearance, location and identity given by the fixed video hybrid system. ...|$|E
40|$|The Keck Laboratory for the Analysis of Vision Motion is a state-of-the art multi-perspective imaging {{laboratory}} recently {{established at}} the University of Maryland. In this paper, we describe the design and architecture of the lab, that is currently being used to support many computer vision studies. In particular, we discuss: camera synchronization, image resolution analysis, image noise analysis, stereo error analysis, <b>video</b> capture, lighting, <b>calibration</b> hardware. (Also UMIACS-TR- 2002 - 11...|$|R
5000|$|The {{founder of}} Monster Cable, Noel Lee, agreed to use Drey's dance hit [...] "Why Should I Believe You" [...] to film the first music video ever in WMV HD (720p) with 7.1 digital audio. It was filmed at Microsoft Studios in Washington, {{and the content}} was used in a nationwide, big chain store <b>calibration</b> <b>video</b> called “Monster / ISF HDTV Calibration Wizard DVD”, which was also narrated by Drey.|$|R
40|$|This paper {{deals with}} monocular <b>video</b> {{sequences}} without <b>calibration</b> to recover {{a maximum of}} information on displacement and projection parameters. In this paper, we propose {{a new way to}} deal with the huge number of particular cases of homographic relations and validate this approach with some experiments showing that if several models are correct, the model with less parameters gives the best estimation. The experiments presented in this paper show also that even if the motion is approximate, the method is still robust. ...|$|R
40|$|There {{has been}} recent {{interest}} in using video cameras along with computer vision and photogrammetric techniques {{to aid in}} the daily positioning of patients for radiation therapy. We describe here a method to calibrate video cameras to the beam coordinate system of a radiation therapy treatment machine. Standard camera calibration relates the threedimensional coordinate system of a calibration phantom to the two-dimensional image coordinates. Using a calibration phantom designed for simultaneous X-ray and video imaging, both types of images can be calibrated to a single coordinate system. A series of X-ray images of the calibration phantom are taken using the motion of the treatment machine. Camera calibration parameters derived from these images are used to find a transformation from the coordinate system of the calibration phantom to the beam coordinate system of the treatment machine. This transformation was applied to the <b>video</b> camera <b>calibration</b> to provide a camera calibration dire [...] ...|$|R
40|$|A {{versatile}} visual {{pattern generator}} is described {{that can be}} programmed by a microcomputer and is developed {{as a part of}} a portable visual evoked potential analysis system. The hardware is contained on one printed circuit board (3 " X 10 ", 7. 5 X 25. 5 cm) residing in an interface connector of a microcomputer (Apple II). The generator produces signals for commercial 50 Hz <b>video</b> monitors; a <b>calibration</b> procedure based on a photocell measurement corrects for the non-linear voltage intensity characteristic of the phosphor of the video monito...|$|R
40|$|An {{interferometer}} {{is constructed}} {{for use with}} 3 -cm microwaves using the apparatus of Disc 21 Demonstration 14. A mesh screen at 45 ? {{with respect to the}} incoming beam, shown in Figure 1, functions as a half-silvered mirror. If the path lengths of the two arms of the interferometer are equal or differ by exactly one wavelength, the two waves will be in phase when they recombine and are picked up by the receiver at the left of the Figure. If the two path lengths differ by an odd number of half wavelengths, the two waves will be out of phase and interfere destructively. Thus as the reflector at the right is moved to the right the receiver will indicate a series of maxima and minima, as shown clearly in the <b>video.</b> Distance <b>calibration</b> is included by graphics overlay for the movable mirror position, so measurement may be made to determine the wavelength of the microwaves...|$|R
40|$|The paper {{discusses}} {{the evaluation of}} a video image-splitting device for human motion research. NuView, a commonly available device and a digital video camera (DV) were used to capture stereo-video footage of athletes in training. The device permits two distinct views (left and right perspective views) to enter a single lens DV. A single convergence control in the device allows users to obtain stereo view of near and far objects. The research involves the calibration of a customized system for optimum motion tracking accuracy. A stereo-digitizing photogrammetric technique {{was used to determine}} the accuracy of the system. The results show that system can achieve optimum spatial data accuracy of 15 mm at an object distance of 8 m. The different in the horizontal and vertical accuracies are similar to those obtained by conventional stereo-aerial photogrammetric technique, i. e. the horizontal component is approximately two times better than z component. Key words and phrases: digital video footage, <b>video</b> camera <b>calibration,</b> motion research, Motio...|$|R
40|$|Traditionally, {{building}} 3 D reconstructions {{of large}} scenarios {{such as a}} museum or historical site has been costly, time consuming and required the contribution of expert personnel. Usually the results showed an artificial look and had little interactivity. However, newly developed technologies {{in the areas of}} <b>video</b> analysis, camera <b>calibration</b> and texture fusion allow us to think in a much more satisfying scenario where the user with the only aid of a domestic video camera is able to acquire all the information it is required to construct the 3 D model of the desired environment in an easy and comfortable manner...|$|R
40|$|The sensors {{used in the}} muon barrel {{position}} monitor are video cameras. A {{critical point}} {{in the design of}} this monitor is the geometrical calibration of the camera. The goal of a camera calibration is to locate the camera elements (sensor + lens) in space in order to use it later for geometrical measurement. This report describes the measurement of the experimental performance of the video camera system and a study of the errors of the main elements : survey network, sensor, source and calibration plates. The main result is that the source repeatability has to be improved and that the survey is critical. This note finishes with a simulation of the calibration after correcting the source problem and after improvement of the survey precision. <b>Video</b> camera <b>calibration.</b> Laurent Brunel April 1996 2 Table of contents INTRODUCTION 4 1. SETUP 5 1. 1 The laboratory 5 1. 2 The mechanics 6 1. 3 The optics 6 1. 4 The sensor 6 1. 5 The grabber 7 2. SURVEY 7 3. LINEARITY TESTS 7 3. 1 Principle 7 3. 2 [...] ...|$|R
50|$|Hayes {{has given}} lectures, {{seminars}} and workshops at Yale, Harvard and Columbia Universities, the Rhode Island School of Design and Carnegie Institute, {{as well as the}} Metropolitan Museum of Art and Lamar University. A member of the Microsoft development team, he was an early proponent and innovator of digital imaging and an expert in <b>video,</b> scanning, color <b>calibration</b> and large format printing. He is on the Microsoft Online Research Panel, evaluating software in Beta, and new computers and hardware. He has written software programs for many years and before Microsoft Word and Word Perfect, he created the word processing programs for the Metropolitan Museum of Art.|$|R
