2|49|Public
50|$|Like all the modules of {{the open}} TELEMAC-MASCARET system, TOMAWAC was {{developed}} {{in accordance with the}} quality assurance procedures followed in Electricité de France's Studies and Research Division. The software is supplied with a complete set of documents: theoretical description, user's manual and first steps, <b>validation</b> <b>file,</b> etc.|$|E
40|$|Abstract. Input {{validation}} is {{the problem}} that need to solve for all Web applications, to calibrate data for its constraints and rules. The Struts 2 framework provides a variety of check method, and validation. xml file {{is the most common}} method in this research. Firstly, study Struts 2 framework architecture composed of three parts such as model, view and controller; Then, show the input checking process of Struts 2 through a graphical mode;Finally, the design of validation. xml <b>validation</b> <b>file</b> example, realizes the checkers such as required string, string length, integer, date and double precision floating point number. Using Validation. xml authentication of Struts 2 has the advantages of convenient and suitable, and can also be combined with other methods in practical application...|$|E
5000|$|Recipient <b>validation</b> in flat <b>file,</b> LDAP, Redis, or qmail-deliverabled.|$|R
40|$|Objectives: To {{validate}} and accredit a set {{of three}} multiplex endpoint PCR assays, targeting the most important carbapenemase and minor extended-spectrum β-lactamase (ESBL) resistance genes, {{according to the international}} ISO 15189 particular requirements for the quality and competence of medical laboratories. Methods: Specific primers targeting ESBLs and carbapenemases were collected from the literature or designed internally. The multiplex PCRs were validated for sensitivity, specificity, intra- and inter-run reproducibility and accuracy by means of external quality control (EQC) using a collection of 137 characterized and referenced isolates. For each multiplex PCR assay, the presence of an extraction control ruled out false-negative results due to PCR inhibition or extraction faults. Amplicons were separated by capillary electrophoresis (QIAxcel system, Qiagen). The protocols and <b>validation</b> <b>files</b> were reviewed in the setting of an external audit conducted by the Belgian organization for accreditation (BELAC). Results: Sensitivity, specificity and reproducibility for each targeted gene were 100 %. All isolates from the three EQC panels were correctly identified by each PCR assay (accuracy 100 %). The <b>validation</b> <b>files</b> were controlled by BELAC, and the PCR protocols were accepted as accredited according to ISO 15189. Conclusions: Three home-made multiplex PCRs targeting the major carbapenemases and four minor class A ESBL genes encountered in Gram-negative bacteria were accredited according to the ISO 15189 standards. This validation scheme could provide a useful model for laboratories aiming to accredit their own protocols. © The Author 2013. Published by Oxford University Press on behalf of the British Society for Antimicrobial Chemotherapy. All rights reserved...|$|R
5000|$|The example {{found in}} the Official ADF Specifications (PDF) does not pass XML <b>validation</b> and <b>files</b> based on this example may be {{rejected}} by ADF lead software. Below is an example that passes XML validation and has been accepted and parsed by ADF lead software: ...|$|R
50|$|On December 14, 2010, Microsoft {{announced}} its intent to backport the Office <b>File</b> <b>Validation</b> feature to Office 2003 and Office 2007. On April 12, 2011, Office <b>File</b> <b>Validation</b> was backported as an add-in for Office 2003 Service Pack 3 and Office 2007 Service Pack 2, {{and was later}} made available through Microsoft Update on June 28, 2011. Office <b>File</b> <b>Validation</b> in Office 2003 and Office 2007 differs from the version in Office 2010 as these two releases {{do not include the}} Protected View feature. When users attempt to open a document that fails validation, they must first agree to a warning prompt before it can be opened. Additionally, the configuration options in these two releases are only made available through the Windows Registry, whereas Office 2010 also provides Group Policy options.|$|R
50|$|To save {{server-side}} resources upload components {{can apply}} client-side <b>validation</b> to <b>files</b> selected for upload in several groups: file types (which can be selectively defined or barred by the developer), file size (which {{can be set}} to minimum/maximum allowed sizes), and image size (which can have minimum/maximum sizes configured among other parameters).|$|R
5000|$|Cryptography improvements, {{including}} {{support for}} Cryptography API: Next Generation in Access, Excel, InfoPath, OneNote, PowerPoint, and Word; Suite B support; and integrity <b>validation</b> of encrypted <b>files</b> ...|$|R
50|$|CoreFiling Limited (trading as DecisionSoft Limited until 2009) is {{a private}} limited {{software}} house based in Oxford, UK. Its product range enables creation, <b>validation,</b> search, <b>filing</b> and audit of XBRL/iXBRL documents. Its products are used by financial regulators and financial institutions. Its products are sold directly and through resellers including BDO International, Unit4, and IRIS Software. It {{is a member of}} the XBRL Consortium.|$|R
40|$|This study {{aimed to}} {{translate}} the Talent Development Environment Questionnaire (TDEQ) into Spanish and provide an initial <b>validation.</b> The <b>files</b> content information about the methodology and a data set. Also it has been added a legend in order to clarify symbols and numbers included in the data set. It is a new step for TDEQ once it has been validated (also a shortened version). It {{will be available in}} Spanish for academic and professional use...|$|R
5000|$|Oxygen XML Editor 9.3+ {{allows users}} to extract, validate, edit, {{transform}} (using XSLT or XQuery) to other file formats, compare and process the XML data stored in OpenDocument <b>files.</b> <b>Validation</b> uses the latest ODF Documents version 1.1 Relax NG Schemas.|$|R
40|$|An {{automatic}} {{scheme to}} detect clouds from locally received AVHRR satellite imagery {{is being developed}} at SMHI for the EUMETSAT Satellite Application Facility to support Nowcasting and Very Short Range Forecasting (SAFNWC). The scheme is utilising a set of pixel-based multi-spectral image features (including local texture). Data are segmented into seven categories, including cloudfree, cloud contaminated, and cloud-filled, according to dynamically changeable thresholds. These are not only dependent on the sun-satellite viewing geometry, {{but also on the}} atmospheric environment and the characteristics of the surface, through the use of NWP model output and other ancillary data. The determination of the thresholds, which is a critical part of the algorithm, is made by the aid of off-line calculations with radiative transfer models (RTTOV and 6 S). In this way the thresholds are tuned to the precise spectral characteristics of the AVHRR sensors, and thus it is possible to adapt the scheme to future changes in the sensor response functions. In this paper a basic description of the scheme is given, and some preliminary results are shown. The accuracy and limitations of the method are illustrated using a database of training and <b>validation</b> <b>files.</b> 1...|$|R
50|$|Garfinkel {{introduced}} {{the use of}} fast object <b>validation</b> for reassembling <b>files</b> that have been split into two pieces. This technique {{is referred to as}} Bifragment Gap Carving (BGC). A set of starting fragments and a set of finishing fragments are identified. The fragments are reassembled if together they form a valid object.|$|R
40|$|ISO 10303 - 11 (EXPRESS) and ISO 10303 - 21 (STEP Files) are the {{upcoming}} international {{standards for the}} machine independent exchange of product data defined within the STEP #Standard for the Exchange of Product Model Data# effort. The <b>validation</b> of STEP <b>files</b> means to check whether the product data de#ned by #ISO#TC 184 #SC 4 1993 # meets the corresponding data specification which {{is defined by the}} EXPRESS language #ISO#TC 184 #SC 4 1992 #. Due to the complexity of EXPRESS models and STEP <b>Files</b> a reliable <b>validation</b> of STEP <b>files</b> can only be performed by tools. When applied in the context of very large databases, i. e., validating huge STEP <b>Files,</b> the <b>validation</b> tools can easily exceed the limits given by the underlying operating system. The resulting problems mainly relate to the required main memory and to the run-time when performing a full validation. In thi...|$|R
30|$|Traffic {{capturing}} is a {{key part}} of the whole process, since TCP/IP metrics, required for the construction of S-KPI models, are obtained from.pcap files. In this work, tcpdump is used for this purpose. This tool works by capturing and displaying a description of packets on a network interface that match certain criteria. Criteria comprise boolean search operators, host names, IP addresses, network names, and protocols. In the testbed, traffic is captured both at the terminal and the connection to the Internet only for <b>validation</b> purposes..pcap <b>files</b> captured at the terminal are uploaded to the computer where NetEm is installed and automatically deleted from the terminal to avoid draining its storage capacity.|$|R
40|$|Though {{there has}} been {{significant}} research into file carvers, {{there has been}} little comparison or <b>validation</b> of different <b>file</b> carvers. Such comparison and validation is vital if {{the state of the art}} is to progress. We present a methodology for comparing file carvers based on realistic data and present the results of applying the carver to the Foremost, Scalpel, PhotoRec, and Adroit, carvers...|$|R
40|$|This TIR {{describes}} the INS/CDBS Team’s responsibilities. It defines the standard {{procedures for the}} delivery of calibration pipeline and SYNPHOT reference files to the Data Management Systems (DMS). It provides guidelines for test and <b>validation</b> of reference <b>files</b> by the INS/CDBS Team. This is an update to TIR CDBS 2005 - 02 A. It clarifies procedures to deliver all type of SYNPHOT files, including Atlases and bandpasses...|$|R
50|$|The {{repository}} stores {{information about}} fields (or data elements) {{in the application}} including descriptions, column headings, edit codes, visualizations, default values, help text, and prompt programs. It holds information about files and application database including physical files, logical files (or views), relationships, file definition attributes, <b>file</b> <b>validation</b> rules, trigger programs, multilingual definitions, virtual fields, and predetermined join fields. Objects and components used for event-driven Windows applications also reside in the repository.|$|R
40|$|ISO 10303 - 11 (EXPRESS) and ISO 10303 - 21 (STEP Files) are the {{upcoming}} international {{standards for the}} machine independent exchange of product data defined within the STEP (Standard for the Exchange of Product Model Data) effort. The <b>validation</b> of STEP <b>files</b> means to check whether the product data defined by (ISO/TC 184 /SC 4 1993) meets the corresponding data specification which {{is defined by the}} EXPRESS language (ISO/TC 184 /SC 4 1992). Due to the complexity of EXPRESS models and STEP <b>Files</b> a reliable <b>validation</b> of STEP <b>files</b> can only be performed by tools. When applied in the context of very large databases, i. e., validating huge STEP <b>Files,</b> the <b>validation</b> tools can easily exceed the limits given by the underlying operating system. The resulting problems mainly relate to the required main memory and to the run-time when performing a full validation. In this paper we first present a general investigation of the conformance checking by giving a detailed classification of the test. Based on t [...] ...|$|R
30|$|A set of 100 InDel markers {{that were}} evenly {{spread across the}} rice genome were selected, and primers were {{designed}} for experimental <b>validation</b> (Additional <b>file</b> 6 : Table S 6). The 100 InDel markers were developed by e-PCR based on the InDel polymorphisms between the Nipponbare and 9311 reference sequences; thus, these markers should theoretically be polymorphic between Nipponbare and 9311. To test their accuracy, we amplified the genomic DNA of Nipponbare and 9311 by PCR. The PCR results showed that only seven primer pairs could not simultaneously generate PCR products from the genomic DNA of both Nipponbare and 9311, and 89 primer pairs were polymorphic between Nipponbare and 9311; thus, the PCR success rate was 93  %, while the accuracy of InDel markers was 95.70  %.|$|R
40|$|Main {{changes from}} last release (3. 2. 8 -> 3. 3. 0) This release is {{primarily}} aimed at performance and resource usage improvements. We have optimized the memory usage in CMOR and PrePARE. In addition, {{we changed the}} file IO library for PrePARE from CDMS 2 to CDUNIF to achieve {{an order of magnitude}} speedup in <b>file</b> <b>validation</b> operations. These changes should achieve a considerable speedup in ESGF data publication, and a considerable reduction in memory usage enabling better parallelisation and scaling of multiple concurrent write operations Accelerate <b>file</b> <b>validation</b> by PrePARE from 2 s to 0. 1 s (IPSL request) Fix PrePARE/CMOR memory issue where table references were not dynamically assigned (MOHC request) remove fixed double array to dynamic double due to number of objects increasing Optimize PrePARE <b>file</b> <b>validation</b> speed to reduce validation operations Call Cdunif. so library directly instead of cdms 2 "long_name" variable attribute is no longer being validated by PrePARE (CERFACS request) Update "license" REGEX to allow "/" at the end of URL (GFDL request) Update "Conventions" REGEX to allow "CMIP- 6. 0 " up to the current "CMIP- 6. 2 " release Update grid tables remove standard_name to vertices_latitude and vertices_longitude for CF- 1 and CMIP- 6. 2 compliance CMOR 3. 3. 0 documentation [URL] or in pdf form at [URL] DOI (source code) [URL] Github (source code) [URL] Conda installation To install cmor into your root anaconda environment conda install -c conda-forge -c pcmdi cmor Or to generate a dedicated anaconda environment conda create -n cmor 3. 3. 0 -c conda-forge -c pcmdi cmor Using this cmor meta yaml file conda env create -n cmor 3. 3. 0 -f cmor- 3. 3. 0. ym...|$|R
50|$|Office <b>File</b> <b>Validation,</b> {{previously}} {{included only}} in Publisher 2007 for PUB files, has {{been incorporated into}} Excel, PowerPoint, and Word in Office 2010 to validate the integrity of proprietary binary file formats (e.g., DOC, PPT, and XLS) that were introduced in previous versions of Microsoft Office. When users open a document, the structure of its file format is scanned {{to ensure that it}} conforms with specifications defined by XML schema; if a <b>file</b> fails the <b>validation</b> process it will, by default, be opened in Protected View, a new read-only, isolated sandbox environment to protect users from potentially malicious content. this design allows users to visually assess potentially unsafe documents that fail validation. Microsoft has stated that it is possible for documents to fail validation as a false positive. To improve Office <b>File</b> <b>Validation,</b> Office 2010 collects various information about files that have failed validation and also creates copies of these files for optional submission to Microsoft through Windows Error Reporting. Users are prompted approximately every two weeks from the date of a failed validation attempt to submit copies of files or other information for analysis; prompts include a list of files that will be submitted to Microsoft and require explicit user consent prior to data submission. Administrators can disable data submission.|$|R
40|$|Errors in files {{detected}} and corrected during operation. Permanent <b>File</b> <b>Validation</b> (PFVAL) utility {{computer program}} provides CDC CYBER NOS sites with mechanism to verify integrity of permanent file base. Locates and identifies permanent file errors in Mass Storage Table (MST) and Track Reservation Table (TRT), in permanent file catalog entries (PFC's) in permit sectors, and in disk sector linkage. All detected errors written to listing file and system and job day files. Program operates by reading system tables, catalog track, permit sectors, and disk linkage bytes to vaidate expected and actual file linkages. Used extensively {{to identify and}} locate errors in permanent files and enable online correction, reducing computer-system downtime...|$|R
40|$|TraitDB is a web {{application}} built to facilitate storage, searching, subsetting, and sharing trait data. Data are loaded from CSV files and organized into projects. Each project has {{a template for}} datasets, and TraitDB performs extensive <b>validation</b> when CSV <b>files</b> are uploaded. This is the TraitDB version currently deployed as an application at NESCent on a development server, and planned {{to be the one}} deployed for the Tree of Sex dataset by NESCent working group member Heath Blackmon (see The Tree of Sex Consortium (2014) Tree of Sex: A database of sexual systems. Sci. Data 1 : 140015 doi: 10. 1038 /sdata. 2014. 15) ...|$|R
40|$|Managing the {{validation}} {{and migration}} from SAS ® version 9. 1. 3 to 9. 2 {{on a new}} server presents many challenges. In May, 2010, such a project was completed. The manager of such a project should take the following into account: (1) purchasing hardware and software, (2) following company validation standards, (3) regular meetings with IT, Quality and other personnel, (4) {{the time required to}} write and execute <b>validation</b> documents, (5) <b>file</b> migration from the old to the new server, (6) start-up of the new SAS system, and (7) hiring personnel with validation experience, if needed. The successful manager should not underestimate the amount of time and resources to accomplish this type of project...|$|R
40|$|The aggregate-disaggregate-aggregate freight {{models that}} have been {{developed}} in Norway and Sweden take into account the logistic decisions at firm level. The Swedish model is documented in i. e. the Swedish Base Matrices Report and the Method Report Logistics model. This report gives an overview about how the Swedish transport and logistics system is represented in the Logistics Model Version 2, which is a deterministic cost minimization model. Further validation and development is necessary in several aspects. The report describes the setup data (version 2009 - 01 - 19) that is needed to run the Version 2 model. In order not to overload the report all input, output, control <b>files,</b> <b>validation</b> material etc. is included on a separate CD...|$|R
30|$|A {{cross between}} the {{resistant}} variety CR 2711 – 76 and Jaya {{was made and}} F 1 plants were produced. The true F 1 plants were selfed and they produced F 2 seeds. In addition, the F 1 plants were further backcrossed to the recurrent parent Jaya to generate BC 1 F 1 seeds. A total of 151  F 2 and BC 1 F 1 plants were genotyped and were selfed to generate F 3 and BC 1 F 2 progenies, respectively. The F 3 plants were phenotyped for BPH resistance or susceptibility. Foreground selection was practiced to generate BC 2 F 1 progenies after backcrossing with Jaya as the recurrent parent, based on the identified gene flanking marker genotype data. Phenotypic selection and background analysis were performed to generate BC 3 F 1 plants {introgressed lines (ILs)} that possessed the new BPH resistance gene. To confirm their resistance and to validate the co-segregation of the newly developed markers, BC 2 F 1 plants were selfed and the BC 2 F 2 plants were used for marker <b>validation</b> (Additional <b>file</b> 7 : Figure S 7).|$|R
40|$|Structured {{clinical}} encounter notes offer {{many advantages}} compared to free text notes. However, cost {{and other issues}} limit their widespread use. At our institution, the electronic medical record (EMR) used by an Internal Medicine practice was designed years ago to utilize structured data. This paper describes a technique to improve {{the process by which}} structured notes are dictated and transcribed. We outsourced transcription services and then passed the files through parsing software to structure the data to the granularity required for our database. Software was built to handle all aspects of the process including <b>file</b> <b>validation,</b> parsing, and review. During our pilot project, more than 10, 000 notes were parsed, reviewed and uploaded into the EMR. Keywords: structured data, electronic medical record, dictation, transcriptio...|$|R
40|$|Metadata plays {{a crucial}} role in {{supporting}} the discovery, understanding and management of the large product data collections generated throughout all phases of the product lifecycle. Product data models are annotated with metadata which represent meaning in conformance with evolving metadata schemas while, for business, contractual and legal reasons, these semantically enriched models are ingested into OAIS (Open Archival Information System) based archives for later reuse. Notably, {{it is not uncommon for}} a product service provider to operate products for several decades; even after the engineers whose embodied knowledge supports their operation retire or leave the company. This product longevity and volatile knowledge, alongside rapid technological innovations and evolving metadata schemas, require that special preservation processes be used to keep the archived product data and metadata interpretable. While preservation of the data is concerned with product data model normalization, <b>validation</b> and <b>file</b> format migration, the preservation processes for metadata are of a different nature given that referenced schemas evolve independently from the products they describe. Although widely referenced, the OAIS reference model unfortunately does not observe metadata schema versioning or metadata harmonization in any depth. This paper therefore aims to introduce dedicated metadata preservation functionality into OAIS archives, based on operational schema update processing...|$|R
40|$|This {{work was}} {{performed}} under NASA's Verification and Validation Program {{as an independent}} check of data supplied by Positive Systems, Inc. through the Earth Science Enterprise's Scientific Data Purchase (SDP) Program. This document serves {{as the basis for}} reporting results associated with validation of multispectral imagery according to the specifications of contract NAS 13 - 98049. The validation was performed under the Positive Systems Imaging System Validation Work Instruction CRSP-WI- 28 : Spectral registration, spatial resolution, endlaps, sidelaps, and image quality were evaluated. The validation was proceded by Shipment Verification, as described in the Work Instruction CRSP-WI- 22 : Every image was passed through an automatic ingest verification and thumbnail review process to identify omissions, problems with media integrity, and gross errors in data quality. <b>Validation</b> of metadata <b>files</b> is not within the scope of this report, but it was performed separately...|$|R
40|$|Cloud {{computing}} {{and computer}} forensics for business applications The paper reviews {{issues related to}} teaching computer forensics with Cloud Computing. It examines the key issues that Computer Forensics is facing today and the challenges and opportunities that Cloud Computing brings for computer forensics. Computing {{can be seen in}} all basic tasks of Computer Forensic Investigation: Data Acquisition and <b>Validation,</b> Bookmaking data, <b>File</b> Signature Analysis and Hash Analysis, Analysis of Data, securing evidence files and reporting. Colleges and university are beginning to identify and manage applications and services available through Cloud Computing. For example, Ohio State University and many other universities (Indiana University 2009 Teaching, administrative support, and research. IBM and Google had started programs on college campuses to promote computer known as “clouds” Teaching Computer Forensics in Cloud Computing environment offers opportunities far beyond traditional computer forensics at the advance edge of technological innovations...|$|R
40|$|This TIR {{describes}} the INS/CDBS Team’s responsibilities. It defines the standard {{procedures for the}} delivery of calibration pipeline and SYNPHOT/pysynphot reference files to the Data Management Systems (DMS). It provides guidelines for test and <b>validation</b> of reference <b>files</b> by the INS/CDBS Team. This is an update to TIR CDBS 2008 - 02. In this revision we clarify several of the steps and add new procedures for {{the format of the}} DESCRIP keyword. We also provide more examples in the different sections of the document and detail on how to set your environment to run the fitsverify script. Here we also state the policies and procedures for the changes and delivery of the TMT table. We also update information to account for changes in the testing procedures for SYNPHOT/pysynphot files and clarify the procedures to deliver all types of SYNPHOT/pysynphot files, including Atlases and bandpasses...|$|R
40|$|A {{lightning}} talk {{delivered at}} the Library Research and Innovative Practice Forum, McKeldin Library, June 4, 2015. The tool described is available at [URL] lightning talk describes a Python script for the <b>validation</b> of CSV <b>files</b> against arbitrary sets of rules specified in a schema file. The motivation for creating the tool was that CSV (comma-separated values) files have become a de facto standard for moving data between systems, and for any sort of batch ingest process. But CSV data can be messy, and often there are problems that appear only when the data is being loaded, after {{it is out of}} the hands of the librarians who have created the data and into the hands of systems staff. The tool is intended to empower data creators to validate CSV files against the requirements of the systems for which the data are being prepared, so that they can correct any problems themselves before sending the data along the pipeline...|$|R
40|$|The {{document}} {{presents a}} design methodology, based on Software Defined Radios (SDR), and focused to model CDMA Reverse Traffic Channel functional blocks. Also, a validation methodology is presented, {{in order to}} check results obtained after functional implementation of blocks in VHDL. A procedure for automatic VHDL code generation of analyzed blocks is proposed, {{by means of a}} generalized structure, based on modified Linear Feedback Shift Registers (mLFSR), which allowthe generation of additional functional outputs from linear combination of register positions. Also, a matrix model is proposed, in order to setup feedback loops and functional outputs. Blocks are modeled as a Hidden Marvok Model (HMM). For <b>validation</b> methodology, graphic <b>file</b> generation using *. vec files is proposed, using results from Simulink, which may be used in development environments like MAX PLUS II or QUARTUS II. Generated files allow direct simulation and corresponding results checking. For the generation of VHDL code, an approach entity-based approach is made, as a focus to hierarchicaldecomposition based on functional units where a VHDL entity is taken as a hardware object abstraction, in this case a Functional Block...|$|R
40|$|Many file formats have {{features}} that are not compatible with the aims of long-term preservation. For example, the use of encryption may result in content becoming inaccessible over time, and files that have dependencies on external resources (e. g. fonts that are not embedded) may not be rendered correctly. Such features are potential preservation risks, and for digital archives {{it is important to}} be aware of the fact what risks are associated with a specific format, and how files can be assessed for the presence of a risk. The overall aim of this deliverable is to address this situation by presenting an open knowledge base of format-specific preservation risks. This was realised by creating a platform that is called the OPF File Format Risk Registry (FFRR), which is part of the Open Planets Knowledge Base Wiki. For a given format, FFRR lists the risks that are associated with it, it provides information on how to test if a file is affected a risk, and it gives recommendations on how to deal with file objects that are affected by a risk. It also provides evidence in the form of sample files. FFRR content was created for the formats JP 2 (JPEG 2000 Part 1) and PDF. These formats were chosen because of their particular importance to other SCAPE partners, and they are used here as a proof of concept of the FFRR approach. Research was done on tools that are able to detect preservation risks for these formats. The results of this research were included in FFRR. As no suitable tool existed for JP 2, a new tool was developed for this format. As file format risks may be explicitly addressed by institutional policies, a general methodology was developed for an automated assessment (or <b>validation)</b> of <b>files</b> against a policy...|$|R
40|$|A {{full year}} of the Advanced-Along Track Scanning Radiometer (AATSR) Meteo product is now {{available}} from 19 August 2002. These data have been downloaded from an ESA FTP server in near-real time as BUFR formatted <b>files.</b> <b>Validation</b> has been performed on these data at the Met Office on a daily basis, with a 2 -day lag from data receipt. Meteo product skin Sea Surface Temperatures (SST) have been compared to point measurements of buoy SST; a 1 degree climate SST analysis field compiled from in situ measurements and AVHRR SSTs; plus a 5 degree 5 day averaged in situ data-set. Comparisons of the AATSR Meteo product against TMI SSTs are presented. These validation activities have confirmed the Meteo product skin SST to be of a good quality. From August 2002, only a maximum of 10 orbits of AATSR Meteo product BUFR formatted files were available, which increased to the full 14 orbits per day from November 2002. During the first 6 months of data receipt, {{it was not unusual}} for around 2 orbits per day to be unavailable. Data availability has become more reliable from February 2003 onwards...|$|R
