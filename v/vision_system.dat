5653|3899|Public
5|$|The {{crew members}} also {{checked out the}} Advanced Space <b>Vision</b> <b>System,</b> a precise {{alignment}} system for the robot arm that was tested on STS-74. The OSVS, which was used during the mating operation, consisted {{of a series of}} large dots placed on the exterior of the docking module and the docking system.|$|E
5|$|In Japan, Waseda University {{initiated}} the WABOT project in 1967, and in 1972 completed the WABOT-1, the world's first full-scale intelligent humanoid robot, or android. Its limb control system {{allowed it to}} walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors. Its <b>vision</b> <b>system</b> allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.|$|E
25|$|In 2009, the third-generation Cirrus SR22 GTS came {{equipped}} with a new enhanced <b>vision</b> <b>system</b> (EVS), a sophisticated dual-wavelength instrument that offers both infrared and synthetic vision.|$|E
40|$|This {{report is}} part of the {{research}} work which is conducted in order to develop a benchmark or an abstract model for the <b>vision</b> <b>systems.</b> This work leads to <b>system</b> taxonomy for <b>vision</b> <b>systems</b> which {{can be used as a}} reference model for classification and comparison of <b>vision</b> <b>systems.</b> Moreover, it will facilitate in development of generic solutions in <b>vision</b> <b>systems...</b>|$|R
40|$|Robots {{rely on the}} {{computer}} <b>vision</b> <b>systems</b> to obtain the environmental information. As a result, the accuracy of {{the computer}} <b>vision</b> <b>systems</b> is essential for the control of the robots. Many computer <b>vision</b> <b>systems</b> make use of markers of the well-designed patterns to calculate the system parameters. Undesirably, the noise exists universally, which decreases the calibration accuracy and consequently decreases {{the accuracy of the}} computer <b>vision</b> <b>systems.</b> In this paper, we propose a pattern modeling method to remove the noise by decreasing the degree of freedom of the total calibration markers to one. The theorem is proposed and proved. The proposed method can be readily adopted by different computer <b>vision</b> <b>systems,</b> e. g. structured light based computer <b>vision</b> <b>systems</b> and stereo <b>vision</b> based <b>systems.</b> IEE...|$|R
5000|$|ASTM F1592 {{deals with}} fixed {{detention}} hollow metal <b>vision</b> <b>systems</b> of various materials {{and types of}} construction. The detention hollow metal <b>vision</b> <b>systems</b> are used by several organizations.|$|R
25|$|The {{organization}} {{of a computer}} <b>vision</b> <b>system</b> is highly application dependent. Some systems are stand-alone applications which solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer <b>vision</b> <b>system</b> also depends on if its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions which are found in many computer vision systems.|$|E
25|$|In October 2016, NASA {{reported}} using the Xombie rocket {{to test the}} Lander <b>Vision</b> <b>System</b> (LVS), {{as part of the}} Autonomous Descent and Ascent Powered-flight Testbed (ADAPT) experimental technologies, for the Mars 2020 mission landing.|$|E
25|$|The AIBO {{has seen}} much {{use as an}} {{inexpensive}} platform for artificial intelligence education and research, because it integrates a computer, <b>vision</b> <b>system,</b> and articulators in a package vastly cheaper than conventional research robots. One focal point for that development has been the Robocup Leagues.|$|E
40|$|<b>Vision</b> <b>systems</b> are {{increasingly}} {{used in many}} applications including optical character recognition, mechanical inspection, automotive safety, surveillance and traffic monitoring. The current trend in <b>vision</b> <b>systems</b> is to propose solutions for specific problems as each application has different requirements and constraints. There is no generalized model or benchmark, {{to the best of}} our knowledge, which can be used for providing generic solutions for different class of <b>vision</b> <b>systems.</b> Providing a generic model in <b>vision</b> <b>systems</b> is a challenging task due to number of influencing factors. However, common characteristic can be identified in order to propose an abstract model. The majority of vision applications focus on the detection, analysis and recognition of objects. These tasks are reduced to vision functions which can be used to characterize the <b>vision</b> <b>systems.</b> In this report, we have analysed different types of <b>vision</b> <b>systems,</b> both wire and wireless, individual <b>vision</b> <b>systems</b> as well as a vision node in a Wireless Vision Sensor Network (WVSN). This analysis leads to the development of a system taxonomy, in which vision functions are considered as characteristics of the systems. The taxonomy is evaluated by using a quantitative parameter which shows that it covers 95 percent of the investigated <b>vision</b> <b>systems</b> and its flow is ordered for 50 percent of the systems. The proposed taxonomy will assist designers to classify their systems and enable researchers to compare their results with a similar class of systems. Moreover, it will help designers/researchers to propose generic architectures for different class of <b>vision</b> <b>systems...</b>|$|R
50|$|Modern glass cockpits {{might include}} Synthetic Vision (SVS) or Enhanced <b>Vision</b> <b>systems</b> (EVS). Synthetic <b>Vision</b> <b>systems</b> display a {{realistic}} 3D {{depiction of the}} outside world (similar to a flight simulator), based on a database of terrain and geophysical features in conjunction with the attitude and position information gathered from the aircraft navigational <b>systems.</b> Enhanced <b>Vision</b> <b>systems</b> add real-time information from external sensors, such as an infrared camera.|$|R
40|$|Humans can {{effortlessly}} see {{and interpret}} the world around them. Yet, the development of computer <b>vision</b> <b>systems</b> capable of doing the same is an extremely challenging task. In developing <b>vision</b> <b>systems,</b> computer <b>vision</b> experts should explicitly express the knowledge required to guide the systems. The complex nature of <b>vision</b> <b>systems</b> however means that vision experts are only quasiexperts. They lack a strong understanding of the specific vision task {{or the ability to}} articulate their knowledge. Although some approaches automatically derive this knowledge using pattern recognition and machine learning methods, lack of sufficient labelled data makes these infeasible in some domains. The evolving nature of expertise, incrementally available data and shifting nature of the underlying vision tasks make it difficult for experts to express all knowledge a priori. This is especially true of domains such as medical imaging, where data tends to trickle in over time and expert's discover the precise knowledge only by applying different vision algorithms over time. Therefore, <b>vision</b> experts develop <b>systems</b> incrementally, by trial-and-error, and each ad-hoc revision to the system, although well intended, may in fact degrade the system performance. This thesis proposes ways to mitigate the risks that ad-hoc incremental revisions pose to <b>vision</b> <b>systems.</b> It presents ProcessRDR and ProcessNet frameworks, which are an adaptation of the incremental validated change strategy employed by Ripple Down Rules for the vision domain. These frameworks assist the expert in systematically revising parts of a system, while maintaining the integrity of the whole. The thesis also establishes the role of quasi-expertise in vision domains, and studies its influences {{on the quality of the}} resulting <b>vision</b> <b>systems.</b> The studies suggest ways for experts to mitigate the influences of quasi-expertise and support the need for incremental development of <b>vision</b> <b>systems.</b> Incremental development of computer <b>vision</b> <b>systems</b> give experts the capacity to adapt <b>vision</b> <b>systems,</b> as better expertise and more data becomes available. Although the frameworks do not eliminate quasiexpertise or lack of labelled data, they do support the expert in incrementally developing <b>vision</b> <b>systems</b> despite it. The systematic incremental revisions mean that the <b>vision</b> <b>systems</b> can continue to improve over time...|$|R
25|$|For the 2010 {{model year}} refresh, the Range Rover {{received}} an updated exterior grille, a multicamera <b>vision</b> <b>system,</b> bumpers, LED head/tail lights, two new engines (5.0 normally aspirated and 5.0 supercharged v8), and new features. It was unveiled at the 2009 New York Auto Show.|$|E
25|$|Areas of {{artificial}} intelligence deal with autonomous planning or deliberation for robotical systems to navigate through an environment. A detailed {{understanding of these}} environments is required to navigate through them. Information about the environment {{could be provided by}} a computer <b>vision</b> <b>system,</b> acting as a vision sensor and providing high-level information about the environment and the robot.|$|E
25|$|Accuracy â€“ {{how closely}} a robot can reach a {{commanded}} position. When the absolute {{position of the}} robot is measured and compared to the commanded position the error {{is a measure of}} accuracy. Accuracy can be improved with external sensing for example a <b>vision</b> <b>system</b> or Infra-Red. See robot calibration. Accuracy can vary with speed and position within the working envelope and with payload (see compliance).|$|E
50|$|Hollow metal <b>vision</b> <b>systems</b> {{are used}} in wall openings, such as hollow metal doors of the type {{commonly}} used in correctional and detention facilities. These <b>vision</b> <b>systems</b> allow the doors to function effectively as barriers while enabling correctional officers, etc. to observe activity {{on the other side}} of the door. <b>Vision</b> <b>systems</b> in hollow metal doors can be permanently sealed with security glass, or can be designed to allow limited passing of items through the door (such as meal trays, reading materials, etc.).|$|R
40|$|This thesis was {{submitted}} for {{the degree of}} Doctor of Philosophy and awarded by Brunel University. This thesis discusses development of a future <b>visioning</b> <b>system</b> model that can be adopted to create new product concepts for consumer electronics companies operating in a highly competitive business environment. The research work investigates consumer electronic product companies and their market environment to identify problematic issues and indicates that a proactive new product strategy which opens new markets through developing concept-led products is a strategic priority, thus the concept development stage in new product development process {{is in need of}} improvement. An evaluation of existing concept development tools for the purpose of proactive product strategy is presented and concludes that future visioning procedure is the most appropriate tool. To develop a future <b>visioning</b> <b>system</b> model as a concept development tool, the theoretical future <b>visioning</b> <b>system</b> models are analysed and mapped to extract essential structure and contents of future visioning procedure. The consequent future <b>visioning</b> <b>system</b> model is then revised according to the findings and suggestions from the field research work which investigated four major consumer electronics product companies in practice. The findings also validates the necessity of adopting a proactive product strategy and evaluates acceptability of the future <b>visioning</b> <b>system</b> model for practical use. The final future <b>visioning</b> <b>system</b> model is defined after the opinions of the design managers are considered and applied. The major suggestions from the research findings are: (1) Executing proactive product strategy can be a valuable strategic tool (2) A new process is necessary for the companies to create one-step-ahead product (3) Future <b>visioning</b> <b>system</b> is recommended as an advanced approach that creates new product concept. (4) Future <b>visioning</b> <b>system</b> model should consist of eight stages: project initiation, environmental scanning, future visioning, generating product concepts, scenario planning, concept testing, concept visualisation, and finalized concepts. (5) Product concepts can be generated from future vision by applying backcasting. (6) Scenario planning should be used in the future <b>visioning</b> <b>system</b> model as a concept testing tool providing objective validating criteria. (7) Executing a future <b>visioning</b> <b>system</b> model creates new roles for the designer such as information integrator, process moderator, and futurist...|$|R
40|$|In this paper, {{we propose}} a Confidence-driven {{architecture}} to control trade-off between precision and latency of computer <b>vision</b> <b>systems</b> dynamically. It {{is important for}} <b>vision</b> <b>systems</b> to achieve data stream interpretation accurately without latency. However, there is a trade-off: a long computation time is require...|$|R
25|$|The {{interest}} points {{obtained from}} the scale-adapted Laplacian blob detector or the multi-scale Harris corner detector with automatic scale selection are invariant to translations, rotations and uniform rescalings in the spatial domain. The images that constitute the input to a computer <b>vision</b> <b>system</b> are, however, also subject to perspective distortions. To obtain interest points that are more robust to perspective transformations, a natural approach is to devise a feature detector that is invariant to affine transformations.|$|E
25|$|Possessing {{detailed}} hyperspectral colour vision, the Mantis shrimp {{has been}} reported to have the world's most complex colour <b>vision</b> <b>system.</b> Trilobites, which are now extinct, had unique compound eyes. They used clear calcite crystals to form the lenses of their eyes. In this, they differ from most other arthropods, which have soft eyes. The number of lenses in such an eye varied, however: some trilobites had only one, and some had thousands of lenses in one eye.|$|E
25|$|Embraer {{offers an}} {{enhanced}} flight <b>vision</b> <b>system</b> constituted by the Rockwell Collins HGS-3500 Head-up display {{combined with the}} EVS-3000 Infrared camera, permitting a decision altitude necessitating visual references of 100ft above touchdown at a projected price of $515,000. Federal Aviation Administration's draft AC 20-167A further proposes a descent below 100ft if the required visual references can be observed using the EFVS, similar to Cat II and III approaches with limited instrument landing systems in many small airports.|$|E
40|$|C. R. Bull, R. Zwiggelaar and J. V. Stafford, 'Imaging as a {{technique}} for assessment and {{control in the}} field', Aspects of Applied Biology 43, 197 - 204 (1995) This paper discusses the potential of using <b>vision</b> <b>systems</b> to monitor and control arable farming operations. Several examples, predominately concerned with the detection of weeds in crops, that have been or are being developed are discussed. These examples are used to illustrate the type of areas in which <b>vision</b> <b>systems</b> could be applied. Particular emphasis {{is given to the}} benefits of <b>vision</b> <b>systems</b> and the difficulties associated with reproducing even a small proportion of the visual and interpretive skills of a human. The future prospects of <b>vision</b> <b>systems</b> in the agricultural industry are discussed. Peer reviewe...|$|R
5000|$|Elbit <b>Vision</b> <b>Systems</b> - {{manufacture}} of computerised <b>vision</b> and imaging <b>systems</b> for in-line {{inspection and monitoring}} of production processes ...|$|R
5000|$|The Schefenacker <b>Vision</b> <b>Systems</b> Enterprise Bargaining Agreement ...|$|R
25|$|A {{contemporary}} {{description of}} the color <b>vision</b> <b>system</b> provides an understanding of primary colors {{that is consistent with}} modern color science. The human eye normally contains only three types of color photoreceptors, known as long-wavelength (L), medium-wavelength (M), and short-wavelength (S) cone cells. These photoreceptor types respond to different (though overlapping) ranges of the visible electromagnetic spectrum, and middle wavelengths stimulate more than one photoreceptor type. Humans and other species with three such types of color photoreceptor are known as trichromats.|$|E
25|$|The {{cockpit of}} the PC-24 is {{designed}} for efficiency and to reduce pilot workload. Marketed as the Advanced Cockpit Environment (ACE), information is displayed on four 12-inch liquid-crystal displays (LCDs). Pilatus selected Honeywell to develop and supply the ACE avionics suite. It uses {{some of the same}} avionics used in the Pilatus PC-12, in addition to those developed specifically for the PC-24. Standard avionics equipment includes a synthetic <b>vision</b> <b>system,</b> autothrottle, Traffic Collision Avoidance System (TCAS II), localizer performance with vertical guidance and graphic flight planning system.|$|E
25|$|T-55M2A1 Leon 2 - Also {{designed}} by the Peruvian engineer Sergio Casanave, this proposed upgrade include a new thermal fire control system and optics, ability to fire M-43A1/M-43A3 APFSDS ammo (up to 2,600m) and the launcher for the 9M117 (3UBK23-1) Bastion laser beam-guided anti-tank missiles with a range up to 6,000m and 750mm RHAe penetration after ERA, new engine developing 630hp and new night <b>vision</b> <b>system.</b> At least three physical demonstrators were made, but none become an operational prototype. Rejected by the Peruvian Army.|$|E
5000|$|Intelligent <b>Vision</b> <b>Systems</b> for Industry, Springer-Verlag, 1997.|$|R
5000|$|Near {{all-weather}} capability enabled by synthetic <b>vision</b> <b>systems.</b>|$|R
40|$|Abstractâ€”This paper {{presents}} the TerraMax <b>vision</b> <b>systems</b> {{used during the}} 2007 DARPA Urban Challenge. First, a descrip-tion of the different <b>vision</b> <b>systems</b> is provided, focusing on their hardware configuration, calibration method, and tasks. Then, each component is described in detail, focusing on the algorithms and sensor fusion opportunities: obstacle detection, road marking detection, and vehicle detection. The conclusions summarize the lesson learned from the developing of the passive sensing suite and its successful fielding in the Urban Challenge. Index Termsâ€”Autonomous vehicles, data fusion, lane detection, obstacle detection, Urban Challenge, <b>vision</b> <b>systems.</b> I...|$|R
25|$|In {{addition}} to its astronauts and satellites, {{some of the most}} notable Canadian technological contributions to space exploration include the Canadarm on the Space Shuttle, as well as the Canadarm2 {{and the rest of the}} Mobile Servicing System on the International Space Station. The Canadarm and Canadarm2 employ the Advanced Space <b>Vision</b> <b>System</b> which allows more efficient use of the robotic arms. Another Canadian technology of note is the Orbiter Boom Sensor System, which is an extension of the Canadarm used to inspect the Space Shuttle's thermal protection system for damage while in orbit.|$|E
25|$|Japanese {{robotics}} {{have been}} leading the field since the 1970s. Waseda University initiated the WABOT project in 1967, and in 1972 completed the WABOT-1, the first android, a full-scale humanoid intelligent robot. Its limb control system {{allowed it to}} walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors. Its <b>vision</b> <b>system</b> allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.|$|E
25|$|In the mid-20th century, Ichiro Kato {{professor}} of Waseda University studied humanoid robots. He initiated the WABOT project in 1967, and in 1972 completed the WABOT-1, the world's first full-scale humanoid intelligent robot. WABOT-1 had two arms, walked on two legs, and sees with two camera eyes. It was thus the first android. Its limb control system {{allowed it to}} walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors. Its <b>vision</b> <b>system</b> allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth. Japan has since been leading the field of robotics.|$|E
40|$|Abstract: The {{hardware}} {{for video}} capture purposes in PC-based <b>vision</b> <b>systems</b> are considered in this paper. Discussed devices working with television video signal (framegrabber) and the digital devices for serial bus USB and IEEE- 1394. The software components {{are necessary for}} the interface with devices from the <b>vision</b> <b>systems</b> software. The various software development kit (SDK) types are considered In this paper. Now there are accessible software, {{which can be used}} during <b>vision</b> <b>systems</b> software development. There are libraries of image processing algorithms, interactive image processing applications. In some libraries of image processing algorithms there are groups of the subroutines for the framegrabber control. In the given paper a number of the given category software is described and the opportunities of their application for real time <b>vision</b> <b>systems</b> software development are discussed. Note: Publication language:russia...|$|R
5000|$|KhperaSot - Cylindrical {{autonomous}} robots with onboard <b>vision</b> <b>systems.</b>|$|R
5000|$|Mitsubishi Electric Power Products, Inc. - Diamond <b>Vision</b> <b>Systems</b> ...|$|R
