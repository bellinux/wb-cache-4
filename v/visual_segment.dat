10|109|Public
50|$|On 22 June 2010, a late-running Pacific Blue {{flight to}} Sydney {{took off from}} Queenstown. At the time, the airport had no runway lights, and the airline {{mandated}} a departure curfew of 30 minutes before evening civil twilight, allowing enough time for the aircraft {{to return to the}} airport in case of an emergency. The Boeing 737-800 took off on a departure requiring a <b>visual</b> <b>segment,</b> after curfew, and in poor weather. Passengers described a distressing takeoff procedure, with the aircraft flying very low above Lake Wakatipu and the surrounding mountain terrain. The take-off was deemed an endangerment to the safety of the 70 passengers and crew aboard by the Civil Aviation Authority. Both pilots were suspended over the incident, and in April 2011, the flight's captain was charged under the Civil Aviation Act with unnecessary endangerment. This charge was later reduced to one of a careless use of an aircraft, with a maximum fine of NZ$7,000. In March 2013, the pilot, Roderick Gunn, was found guilty.|$|E
40|$|The paper {{deals with}} the {{creation}} of <b>visual</b> <b>segment</b> trees involved in MPEG- 7 description schemes. After a brief overview of MPEG- 7 description schemes in general and of the Segment Description Scheme in particular, tools allowing the creation of segment trees are discussed. It is proposed to create a binary partition tree in a first step and to restructure the tree in a second step. Several examples involving spatial and temporal partition trees are presented. Peer ReviewedPostprint (published version...|$|E
40|$|In rats {{which were}} {{rendered}} monocular amblyopic by lid suturing one eye during a critical period, {{the intensity of}} neuronal activation {{in parts of the}} monocular segments of the striate cortex (layers 4 and 6) and lateral geniculate nucleus, and in the <b>visual</b> <b>segment</b> of the thalamic reticular nucleus, was determined after exploration of a novel-complex environment. Quantitative analysis of the number of Foslabelled neurons per unit area showed that, in comparison to the structures contralateral to the normal eye, in the side contralateral to the deprived amblyopic eye there is a gradient of diminished activation. The strongest activation asymmetry was observed in the visual reticular segment, while in layers 6 and 4 of the visual cortex the activation asymmetry was less strong and weakest, respectively. In the lateral geniculate there was no Fos-detectable activation asymmetry. Furthermore, there was a positive correlation between the time rats spent in exploration and the degree of activation asymmetry in the visual reticular segment...|$|E
50|$|<b>Visual</b> <b>segments</b> showing {{things being}} crushed by a {{hydraulic}} press, thrown through fluorescent lights or dropped off {{an office building}} to smash on the ground, were also common.|$|R
5000|$|In {{interviews}} on CNN and ABC World News, Lavie {{explained how}} the video was made, given that its 57 contributors {{would not be}} performing in the same studio and generally {{did not even know}} each other. [...] After deciding on the assignments of singers to song segments, she sent all the singers the same instrumental backing. A Radio Canada feature included a video segment of Montréal singer Heidi Jutras' vocal performance, in which she suppressed the instrumental accompaniment by using earphones. The singers then returned their respective vocal video segments to Lavie electronically, for the <b>visual</b> <b>segments</b> to be edited, and the audio to be mixed and edited, over the course of [...] "three days (and) one sleepless night." [...] Lavie specifically pointed out the challenge of mixing the vocal segments, made [...] "tedious" [...] by the segments' different sound levels, recorded in different acoustic environments (bathrooms, kitchens, bedrooms), and made by microphones of different type and quality. The resulting composite vocal and <b>visual</b> <b>segments,</b> combined with the single instrumental backing, constituted the resulting video.|$|R
5000|$|The Tongue's {{third album}} {{entitled}} Surrender to Victory, would officially {{be released on}} 29 March 2013. The release of a single, entitled [...] "Drums", preceded the album release date and a promotional video was published on the label's YouTube profile. Filmed by Benny Edwards and edited by Frank Meets Wolf, the video contains <b>visual</b> <b>segments</b> that assert that Tongue is one of Australia's most respected MCs and that Surrender to Victory is the [...] "album of his career". The video also reveals that the album was produced by Cam Bluff and features Suffa from the Hilltop Hoods.|$|R
40|$|This paper {{deals with}} the {{creation}} of <b>visual</b> <b>segment</b> trees involved in MPEG- 7 Description Schemes. After a brief overview of MPEG 7 description schemes in general and of the Segment Description Scheme in particular, tools allowing the creation of segment trees are discussed. It is proposed to create a Binary Partition Tree in a first step and to restructure the tree in a second step. Several examples involving spatial and temporal partition trees are presented. 1. INTRODUCTION The goal of the MPEG- 7 standard [6] is to allow interoperability among devices that deal with audio-visual content description. The standard will involve four types of normative elements: descriptors (Ds), description schemes (DSs), description definition language (DDL), and coded descriptions. A descriptor is a representation of a feature that characterizes the audio-visual content. A description scheme specifies the structure and semantics of the relationships between its components, which may be both descrip [...] ...|$|E
40|$|AbstractFlight deck-based vision systems, such as Synthetic Vision Systems (SVS) and Enhanced Flight Vision Systems (EFVS), {{have the}} {{potential}} to provide additional margins of safety for aircrew performance and enable the implementation of operational improvements for low visibility surface, arrival, and departure operations in the terminal environment with equivalent efficiency to visual operations. Twelve air transport-rated crews participated in a motion-base simulation experiment to evaluate the use of SVS/EFVS in Next Generation Air Transportation System low visibility approach and landing operations at Chicago O’Hare airport. Three monochromatic, collimated head-up display (HUD) concepts (conventional HUD, SVS HUD, and EFVS HUD) and three instrument approach types (straight-in, 3 -degree offset, 15 -degree offset) were experimentally varied to test the efficacy of the SVS/EFVS HUD concepts for offset approach operations. The findings suggest making offset approaches in low visibility conditions with an EFVS HUD or SVS HUD appear feasible. Regardless of offset approach angle or HUD concept being flown, all approaches had comparable ILS tracking during the instrument segment and were within the lateral confines of the runway with acceptable sink rates during the <b>visual</b> <b>segment</b> of the approach...|$|E
40|$|Flight deck-based vision systems, such as Synthetic Vision Systems (SVS) and Enhanced Flight Vision Systems (EFVS), {{have the}} {{potential}} to provide additional margins of safety for aircrew performance and enable the implementation of operational improvements for low visibility surface, arrival, and departure operations in the terminal environment with equivalent efficiency to visual operations. Twelve air transport-rated crews participated in a motion-base simulation experiment to evaluate the use of SVS/EFVS in Next Generation Air Transportation System low visibility approach and landing operations at Chicago O'Hare airport. Three monochromatic, collimated head-up display (HUD) concepts (conventional HUD, SVS HUD, and EFVS HUD) and three instrument approach types (straight-in, 3 -degree offset, 15 -degree offset) were experimentally varied to test the efficacy of the SVS/EFVS HUD concepts for offset approach operations. The findings suggest making offset approaches in low visibility conditions with an EFVS HUD or SVS HUD appear feasible. Regardless of offset approach angle or HUD concept being flown, all approaches had comparable ILS tracking during the instrument segment and were within the lateral confines of the runway with acceptable sink rates during the <b>visual</b> <b>segment</b> of the approach. Keywords: Enhanced Flight Vision Systems; Synthetic Vision Systems; Head-up Display; NextGe...|$|E
5000|$|... 1983 Twilight Zone: The Movie (<b>visual</b> effects - <b>segment</b> 4 / as David Allan) ...|$|R
50|$|In UK {{version of}} the Teletubbies one of the live-action <b>visual</b> 5-minute <b>segments</b> (seen from a Teletubby belly) {{featured}} number counting involving vehicles in lines. A row of JCBs are seen in line, their hydraulics operated {{as if they are}} 'dancing'.|$|R
40|$|The task of {{multimedia}} event detection (MED) aims at training {{a set of}} models that can automatically detect the most event-relevant videos from large datasets. In this paper, we attempt to build a robust spatial-temporal deep neural network for large-scale video event detection. In our setting, each video follows a multiple instance assumption, where its <b>visual</b> <b>segments</b> contain both spatial and temporal properties of events. Regarding these properties, we try to implement the MED system by a two-step training phase: unsupervised recurrent video reconstruction and supervised fine-tuning. We conduct extensive experiments on the challenging TRECVID MED 14 dataset, which indicate that with the consideration of both spatial and temporal information, the detection performance can be further boosted compared with the state-of-the-art MED models...|$|R
40|$|Synthetic Vision Systems and Enhanced Flight Vision System (SVS/EFVS) {{technologies}} {{have the potential}} to provide additional margins of safety for aircrew performance and enable operational improvements for low visibility operations in the terminal area environment with equivalent efficiency as visual operations. To meet this potential, research is needed for effective technology development and implementation of regulatory and design guidance to support introduction and use of SVS/EFVS advanced cockpit vision technologies in Next Generation Air Transportation System (NextGen) operations. A fixed-base pilot-in-the-loop simulation test was conducted at NASA Langley Research Center that evaluated the use of SVS/EFVS in NextGen low visibility ground (taxi) operations and approach/landing operations. Twelve crews flew approach and landing operations in a simulated NextGen Chicago O Hare environment. Various scenarios tested the potential for EFVS for operations in visibility as low as 1000 ft runway visibility range (RVR) and SVS to enable lower decision heights (DH) than can currently be flown today. Expanding the EFVS <b>visual</b> <b>segment</b> from DH to the runway in visibilities as low as 1000 RVR appears to be viable as touchdown performance was excellent without any workload penalties noted for the EFVS concept tested. A lower DH to 150 ft and/or possibly reduced visibility minima by virtue of SVS equipage appears to be viable when implemented on a Head-Up Display, but the landing data suggests further study for head-down implementations...|$|E
40|$|In vertebrates, the retinal pigment {{epithelium}} (RPE) and photoreceptors of the neural retina (NR) comprise a functional unit required for vision. During vertebrate eye development, a conversion of the RPE into NR can be induced by growth factors in vivo at optic cup stages, but the reverse process, the conversion of NR tissue into RPE, has not been reported. Here, we show that bone morphogenetic protein (BMP) signalling can reprogram the NR into RPE at optic cup stages in chick. Shortly after BMP application, expression of Microphthalmia-associated transcription factor (Mitf) is induced in the NR and selective cell death on the basal side of the NR induces an RPE-like morphology. The newly induced RPE differentiates and expresses Melanosomalmatrix protein 115 (Mmp 115) and RPE 65. BMP-induced Wnt 2 b expression is observed in regions of the NR that become pigmented. Loss of function studies show that conversion of the NR into RPE requires both BMP and Wnt signalling. Simultaneous to the appearance of ectopic RPE tissue, BMP application reprogrammed the proximal RPE into multi-layered retinal tissue. The newly induced NR expresses <b>visual</b> <b>segment</b> homeobox-containing gene (Vsx 2), and the ganglion and photoreceptor cell markers Brn 3 α and Visinin are detected. Our results show that high BMP concentrations are required to induce the conversion of NR into RPE, while low BMP concentrations can still induce transdifferentiation of the RPE into NR. This knowledge {{may contribute to the}} development of efficient standardized protocols for RPE and NR generation for cell replacement therapies...|$|E
40|$|Approach {{and landing}} {{operations}} {{during periods of}} reduced visibility have plagued aircraft pilots {{since the beginning of}} aviation. Although techniques are currently available to mitigate some of the visibility conditions, these operations are still ultimately limited by the pilot's ability to "see" required visual landing references (e. g., markings and/or lights of threshold and touchdown zone) and require significant and costly ground infrastructure. Certified Enhanced Flight Vision Systems (EFVS) have shown promise to lift the obscuration veil. They allow the pilot to operate with enhanced vision, in lieu of natural vision, in the <b>visual</b> <b>segment</b> to enable equivalent visual operations (EVO). An aviation standards document was developed with industry and government consensus for using an EFVS for approach, landing, and rollout to a safe taxi speed in visibilities as low as 300 feet runway visual range (RVR). These new standards establish performance, integrity, availability, and safety requirements to operate in this regime without reliance on a pilot's or flight crew's natural vision by use of a fail-operational EFVS. A pilot-in-the-loop high-fidelity motion simulation study was conducted at NASA Langley Research Center to evaluate the operational feasibility, pilot workload, and pilot acceptability of conducting straight-in instrument approaches with published vertical guidance to landing, touchdown, and rollout to a safe taxi speed in visibility as low as 300 feet RVR by use of vision system technologies on a head-up display (HUD) without need or reliance on natural vision. Twelve crews flew various landing and departure scenarios in 1800, 1000, 700, and 300 RVR. This paper details the non-normal results of the study including objective and subjective measures of performance and acceptability. The study validated the operational feasibility of approach and departure operations and success was independent of visibility conditions. Failures were handled within the lateral confines of the runway for all conditions tested. The fail-operational concept with pilot in the loop needs further study...|$|E
40|$|International audienceWe {{address the}} problem of {{detecting}} multiple audiovisual events related to the edit structure of a video by incorporating an unsupervised cluster analysis technique into a cluster selection method designed to measure coherence between audio and <b>visual</b> <b>segments.</b> First, mutual information measure is used to select audio-visually consistent clusters from two dendrograms representing hierarchical clustering results respectively for the audio and visual modalities. A cluster analysis technique is then applied to define events from the audio-visual (AV) clusters with segments co-occurring frequently. Candidate events are then characterized by groups of AV clusters from which models are built by automatically selecting positive and negative examples. Experiments on the standard Canal 9 data set demonstrates that our method is capable of discovering multiple audiovisual events in a totally unsupervised manner...|$|R
50|$|The game plays as a Japanese {{role-playing}} game, {{but also}} includes <b>visual</b> novel <b>segments</b> where the player must interview characters {{to identify a}} potential traitor among the player's party of characters. Similar to gameplay mechanics found in Ace Attorney and Danganronpa games, the player must review statements from characters and try to identify contradictions in their accounts of events.|$|R
3000|$|While our {{experiments}} clearly {{showed that}} the choice for coherent auditory and <b>visual</b> <b>segments</b> will improve the perceived naturalness, {{at this point the}} exact impact of selecting the audio and the video fragments separately but from a same audiovisual database is still unclear. Future experiments using a larger database with more candidate units will hopefully answer this question. We should also investigate the impact of the different synthesis strategies on the overall quality of the synthetic audiovisual speech. For the second experiment the participants assessed the quality of the visual speech only. If, in contrast, we would ask to rate the combined auditory and visual speech quality, {{it is likely that the}} audio-driven synthesized samples would get a better rating since their audio track consists of natural speech. Note, however, that the results described in this paper illustrate that the outcome of such experiments is hard to predict since many intermodal effects can have an influence on the perception of an audiovisual speech signal. Sample syntheses created by the multimodal unit selection technique can be found on our website: [URL] [...]...|$|R
40|$|A fixed-based {{simulation}} {{experiment was}} conducted in NASA Langley Research Center s Integration Flight Deck simulator to investigate enabling technologies for equivalent visual operations (EVO) in the emerging Next Generation Air Transportation System operating environment. EVO implies the capability to achieve or even improve on the safety of current-day Visual Flight Rules (VFR) operations, maintain the operational tempos of VFR, and perhaps even retain VFR procedures - all independent of the actual weather and visibility conditions. Twenty-four air transport-rated pilots evaluated the use of Synthetic/Enhanced Vision Systems (S/EVS) and eXternal Vision Systems (XVS) technologies as enabling technologies for future all-weather operations. The experimental objectives were to determine the feasibility of XVS/SVS/EVS to provide for all weather (visibility) landing capability without the need (or ability) for a visual approach segment and to determine the interaction of XVS/EVS and peripheral vision cues for terminal area and surface operations. Another key element of the testing investigated the pilot's awareness and reaction to non-normal events (i. e., failure conditions) that were unexpectedly introduced into the experiment. These non-normal runs served as critical determinants in the underlying safety of all-weather operations. Experimental data from this test are cast into performance-based approach and landing standards which might establish a basis for future all-weather landing operations. Glideslope tracking performance appears to have improved with {{the elimination of the}} approach <b>visual</b> <b>segment.</b> This improvement can most likely be attributed {{to the fact that the}} pilots didn't have to simultaneously perform glideslope corrections and find required visual landing references in order to continue a landing. Lateral tracking performance was excellent regardless of the display concept being evaluated or whether or not there were peripheral cues in the side window. Although workload ratings were significantly less when peripheral cues were present compared to when there were none, these differences appear to be operationally inconsequential. Larger display concepts tested in this experiment showed significant situation awareness (SA) improvements and workload reductions compared to smaller display concepts. With a fixed display size, a color display was more influential in SA and workload ratings than a collimated display...|$|E
5|$|The {{gameplay}} alternates {{between two}} types of sections: Escape sections, where the player completes puzzles in escape-the-room scenarios; and Novel sections, where the player reads the game's narrative through <b>visual</b> novel <b>segments,</b> and makes decisions that influence the story toward one of twenty-four different endings. The player is given access to a flowchart, which allows them to revisit any previously completed section, and choose a different option to cause the story to proceed in another direction.|$|R
3000|$|... standard: the Multimedia Content Description Interface. It {{focuses on}} visual {{information}} description including low-level <b>visual</b> Descriptors and <b>Segment</b> Description Schemes. The paper also discusses some challenges in visual information analysis {{that will have}} to be faced in the future to allow efficient MPEG- [...]...|$|R
40|$|The present {{research}} aimed to investigate whether, as previously observed with pictures, background auditory rhythm would also influence visual word recognition. In a lexical decision task, participants {{were presented with}} bisyllabic <b>visual</b> words, <b>segmented</b> into two successive groups of letters, while an irrelevant strongly metric auditory sequence was played in a loop. The first group of letters could either be congruent with the syllabic division of the word (e. g. val in val/se) or not (e. g. va in va/lse). In agreement with the Dynamic Attending Theory (DAT), our results confirmed that {{the presentation of the}} correct first syllable on-beat (i. e. in synchrony with a peak of covert attention) facilitated visual word recognition compared to when it was presented off-beat. However, when an incongruent first syllable was displayed on-beat, this led to an aggravation of impaired recognition. Thus, our results suggest that oscillatory attention tapped into cognitive processes rather than perceptual or decisional and motor stages. We like to think of our paradigm, which combines background auditory rhythm with <b>segmented</b> <b>visual</b> stimuli, as a sort of temporal magnifying glass which allows for the enlargement of the reaction time differences between beneficial and detrimental processing conditions in human cognition...|$|R
500|$|The {{gameplay}} of Virtue's Last Reward {{is divided}} into two types of sections: Novel and Escape. In Novel sections, the player advances through the storyline and converses with non-playable characters through <b>visual</b> novel <b>segments.</b> These sections require little interaction from the player other than reading the dialogue and text that appear on the screen. During Novel sections, the player may be presented with decision options that affect the course of the game. One recurring decision option is a prisoner's dilemma-type choice where the player must choose to [...] "ally" [...] or [...] "betray" [...] the characters they are pitted against, with different results depending on what choices the two parties picked.|$|R
40|$|International audienceWe {{describe}} automatic visual speech segmentation using facial data {{captured by}} a stereo-vision technique. The segmentation {{is performed using}} an HMM-based forced alignment mechanism widely used in automatic speech recognition. The idea {{is based on the}} assumption that using visual speech data alone for the training might capture the uniqueness in the facial compo- nent of speech articulation, asynchrony (time lags) in visual and acoustic speech segments and significant coarticulation effects. This should provide valuable information that helps to show the extent to which a phoneme may affect surrounding phonemes visually. This should provide information valuable in labeling the <b>visual</b> speech <b>segments</b> based on dominant coarticulatory contexts...|$|R
40|$|Abstract—In {{this paper}} we propose a novel salient object de-tection {{algorithm}} based on segments, named SODS (Salient Object Detection based on Segments). We first segment an input image, and then extract a set of features including multi-scale contrast, center-surround histogram, and color spatial distribution based on segments to describe a salient object locally, regionally, and globally. These three features are then combined linearly to get a saliency map to rep-resent the salient object. We validate our approach on two public datasets. Experimental results prove that our method is much faster, more robust and accurate than ex-isting salient object detection methods. Keywords—Salient object detection, <b>Visual</b> attention, <b>Segment</b> I...|$|R
40|$|This paper {{describes}} a Bayesian algorithm for rigid/non-rigid 2 D visual object tracking based on sparse image features. The algorithm {{is inspired by}} the way human <b>visual</b> cortex <b>segments</b> and tracks different moving objects within its FOV by constructing dynamical nonretinotopic layers. The method is explained as a recursive algorithm between time slices (intra-slice) and as a forward-backward message passing within every time slice (inter-slice) under the Probabilistic Graphical Model (PGM) framework. Finally, an observation model function that resembles the Generalized Hough Transform and allows exploiting internal structure {{of the problem is}} employed in order to increase the robustness and accuracy of the algorithm against clutter and missed detections...|$|R
5000|$|In Charleston, Dr. Coles {{coordinated the}} <b>visual</b> arts <b>segment</b> of the Spoleto Festival USA from 1977-1980; was Chairman of the Board for the Carolina Art Association from 1978-1980; served as President and Chairman of the Board for the Gibbes Art Museum for those same years; and earned the Mayor's Honor Award for {{historic}} preservation in 1980. From 1987-1995, he reviewed poetry for the American Medical Association. In Buffalo, he wrote scripts and presented on-air for the WBFO radio program on jazz history. In 1995, Coles was the featured poet for the Atlanta Arts Festival, and won the Banff (Canada) Professional and Amateur Award for best musical performance. H ...|$|R
5000|$|The {{gameplay}} of Virtue's Last Reward {{is divided}} into two types of sections: Novel and Escape. In Novel sections, the player advances through the storyline and converses with non-playable characters through <b>visual</b> novel <b>segments.</b> These sections require little interaction from the player other than reading the dialogue and text that appear on the screen. During Novel sections, the player may be presented with decision options that affect the course of the game. One recurring decision option is a prisoner's dilemma-type choice where the player must choose to [...] "ally" [...] or [...] "betray" [...] the characters they are pitted against, with different results depending on what choices the two parties picked.|$|R
40|$|From the introduction: The {{division}} of text into <b>visual</b> <b>segments</b> such as sentences, paragraphs and sections achieves many functions, such as easing navigation, achieving pragmatic effect, improving readability and reflecting the organisation of information (Wright, 1983; Schriver 1997). In this paper, we report a small experiment that investigates {{the effect of}} different layout configurations on {{the interpretation of the}} antecedent of anaphoric referring expressions. Layout has so far played little role in Natural Language Generation (NLG) systems. The layout of output texts is generally very simple. At worst, it consists of only a single paragraph consisting of a few sentences; at best it is predetermined by schemas (Coch, 1996; Porter and Lester, 1997) or discourse plans (Milosavljevic, 1999). However, recent work by Power (2000) and Bouayad et al. (2000) has integrated graphically signalled segments (e. g., by whitespace, punctuation, font and face alternation) such as paragraphs, lists, text-sentences and text-clauses in a hierarchical tree-like representation called the document structure. 2 This work was carried out within the ICONOCLAST project (Integrating CONstraints On Layout and Style), which aims at automatically generating formatted texts in which the formatting decisions affect the wording and vice-versa. 3 If document structure affects the comprehensibility of referring expressions, this {{must be taken into account}} in any attempt to generate felicitous formatted texts. This will go a step further from current research in the automatic generation of referring expressions, where only the effect of discourse structure and grammatical function has been investigated (Dale and Reiter, 1995; Cristea et al., 1998;Walker et al., 1998; Kibble and Power, 1999) ...|$|R
40|$|This thesis {{presents}} Duo, {{the first}} wearable system to autonomously learn a kinematic {{model of the}} wearer via body-mounted absolute orientation sensors and a head-mounted camera. With Duo, we demonstrate the significant benefits of endowing a wearable system {{with the ability to}} sense the kinematic configuration of the wearer's body. We also show that a kinematic model can be autonomously estimated offline from less than an hour of recorded video and orientation data from a wearer performing unconstrained, unscripted, household activities within a real, unaltered, home environment. We demonstrate that our system for autonomously estimating this kinematic model places very few constraints on the wearer's body, the placement of the sensors, and the appearance of the hand, which, for example, allows it to automatically discover a left-handed kinematic model for a left-handed wearer, and to automatically compensate for distinct camera mounts, and sensor configurations. Furthermore, we show that this learned kinematic model efficiently and robustly predicts the location of the dominant hand within video from the head-mounted camera even in situations where vision-based hand detectors would be likely to fail. (cont.) Additionally, we show ways in which the learned kinematic model can facilitate highly efficient processing of large databases of first person experience. Finally, we show that the kinematic model can efficiently direct visual processing so as to acquire a large number of high quality segments of the wearer's hand and the manipulated objects. Within the course of justifying these claims, we present methods for estimating global image motion, segmenting foreground motion, segmenting manipulation events, finding and representing significant hand postures, <b>segmenting</b> <b>visual</b> regions, and detecting visual points of interest with associated shape descriptors. We also describe our architecture and user-level application for machine augmented annotation and browsing of first person video and absolute orientations. Additionally, we present a real-time application in which the human and wearable cooperate through tightly integrated behaviors coordinated by the wearable's kinematic perception, and together acquire high-quality <b>visual</b> <b>segments</b> of manipulable objects that interest the wearable. by Charles Clark Kemp. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2005. Includes bibliographical references (p. 215 - 220) ...|$|R
40|$|We {{describe}} automatic visual speech segmentation using facial data {{captured by}} a stereo-vision technique. The segmentation {{is performed using}} an HMM-based forced alignment mechanism widely used in automatic speech recognition. The idea {{is based on the}} assumption that using visual speech data alone for the training might capture the uniqueness in the facial component of speech articulation, asynchrony (time lags) in visual and acoustic speech segments and significant coarticulation effects. This should provide valuable information that helps to show the extent to which a phoneme may affect surrounding phonemes visually. This should provide information valuable in labeling the <b>visual</b> speech <b>segments</b> based on dominant coarticulatory contexts. Index Terms: facial speech, speech segmentation, forced alignment, coarticulation...|$|R
30|$|For {{description}} convenience, {{the manual}} segmentation results {{are treated as}} the optimal ones and displayed in the figures from Figs.  5 b, 6 b, 7 b, 8 b, 9 b, 10 b, 11 b and 12 b. The segmented results by different methods are also given in Figs.  5, 6, 7, 8, 9, 10, 11 and 12, respectively. From them, it can be observed that, due to the complex background, the segmented results by the standard Otsu method are the worst in visual effect and those by both the 2 -D Otsu method and the 2 -D maximum entropy method are slightly better. That is to say, there are still more images not well segmented by the two methods. For example, for the 2 -D Otsu method, {{there is only one}} image to be well segmented as shown in Fig.  10 d, and for the 2 -D maximum entropy method, there are two images to be well segmented as shown in Figs.  8 e and 9 e, respectively. In comparison with these three methods, the visual thresholding results by the proposed method indicate that, in addition to a completely segmenting for the targets from complex background, the new method exhibits less background noises. This could be attributed to the utilization of the background constraint for the original image. The background constraint dramatically simplifies the original infrared image by weakening the gray level changes of background. In addition, one can observe that the segmented results of the new method are comparative with those by manual segmentation in visual effect. As a whole, from the <b>visual</b> <b>segmented</b> results it can be concluded that the proposed method enjoys better visual effect, indicating the new method with better segmentation performance.|$|R
40|$|Indexing {{and searching}} {{collections}} of handwritten archival documents and manuscripts {{has always been}} a challenge because handwriting recognizers do not perform well on such noisy documents. Given a collection of documents written by a single author (or a few authors), one can apply a technique called word spotting. The approach is to cluster word images based on their <b>visual</b> appearance, after <b>segmenting</b> them from the documents. Annotation can then be performed for clusters rather than documents...|$|R
40|$|AbstractFor the {{deficiencies}} of narrow or widen algorithms on woolen sweater CAD, this paper puts forward a new algorithm named r_n remainder array algorithm {{base on the}} traditional algorithms by {{the analysis of the}} rules of it. It is divided into non-remainder segment and the remainder <b>segment.</b> <b>Visual</b> C++ will be used to control and simulation the curve of woolen sweater garment piece. r_n remainder array algorithm has the character of the simple expression, convenient computing and good simulation results...|$|R
40|$|We present {{neural network}} {{simulations}} {{of how the}} <b>visual</b> cortex may <b>segment</b> objects and bind attributes based on depth-from-occlusion. We briefly discuss one particular subprocess in our occlusion-based model most relevant to segmentation and binding: determination of the direction of figure. We propose that our model allows us to address a central issue in object recognition: how the visual system defines an object. In addition, we test our model on "illusory " stimuli, with the network's response indicating the existence of robust psychophysical properties in the system...|$|R
5|$|The {{international}} {{success of}} Xenosaga Episode I prompted Namco to offer then more support, with Namco's then-Vice President Yoichi Haraguchi {{to name the}} company as a valuable development partner alongside Namco Tales Studio. A manga adaptation was written by Atsushi Baba and published through Monthly Comic Zero Sum. The manga was released by the comic's publisher Ichijinsha across three volumes between 2004 and 2006. Following the release of Episode I, a supplementary disc was created titled Xenosaga Freaks. Released on April 28, 2004, Freaks is split into four segments; a <b>visual</b> novel <b>segment</b> featuring multiple characters from the game, a minigame dubbed XenoPitten, a dictionary that explains the game's terminology, and a demo for the game's official sequel. Freaks {{was part of a}} movement with the Xenosaga series {{to turn it into a}} multimedia franchise, with the project growing substantially larger than previously planned.|$|R
