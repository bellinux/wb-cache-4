551|82|Public
5000|$|The <b>Viterbi</b> <b>decoding</b> {{algorithm}} {{is widely used}} in the following areas: ...|$|E
50|$|With the {{algorithm}} called iterative <b>Viterbi</b> <b>decoding</b> {{one can find}} the subsequence of an observation that matches best (on average) to a given hidden Markov model. This algorithm is proposed by Qi Wang et al. to deal with turbo code. Iterative <b>Viterbi</b> <b>decoding</b> works by iteratively invoking a modified Viterbi algorithm, reestimating the score for a filler until convergence.|$|E
5000|$|Convolutional {{codes are}} {{implemented}} as either systematic or non-systematic codes. Non-systematic convolutional codes can provide better performance under maximum-likelihood (<b>Viterbi)</b> <b>decoding.</b>|$|E
40|$|Viterbi Decoder is the {{dominant}} module to determining the power consumption of the system. High speed and low power design of Viterbi Decoder with data rate 1 / 2 and convolution encoding with a constraint length K = 9 is presented in this paper. The Proposed Viterbi decoder can be reduce the power consumption without reducing the decoding speed and also increases {{the length of the}} bits. The operating frequency of convolution encoder and <b>Viterbi</b> <b>decoded</b> is 306. 65 MHz and power consumption is 45. 01 Mw using Xpower tools in Xilinx and Spartan 3 E FPGA kit...|$|R
40|$|I {{will show}} that there is a deep {{relation}} between error-correction codes and certain mathematical models of spin glasses. In particular minimum error probability decoding is equivalent to finding the ground state of the corresponding spin system. The most probable value of a symbol is related to the magnetization at a different temperature. Convolutional codes correspond to one-dimensional spin systems and <b>Viterbi's</b> <b>decoding</b> algorithm to the transfer matrix algorithm of Statistical Mechanics. A particular spin-glass model, which is exactly soluble, corresponds to an ideal code, i. e. a code which allows error-free communication if the rate is below channel capacity. Comment: Proceedings of the Marseille Satellite Colloquium "Mathematical Results in Stat. Mechanics...|$|R
40|$|In {{this paper}} we {{investigate}} {{the problem of}} identifying and localizing speakers with distant microphone arrays, thus extending the classical speaker diarization task {{to answer the question}} “who spoke when and where”. We consider a streaming audio scenario, where the diarization output is to be generated in realtime with as low latency as possible. Rather than carrying out the individual segmentation and classification tasks (speech detection, change detection, gender/speaker classification) sequentially, we propose a simultaneous segmentation and classification by applying a Viterbi decoder. It uses a transition matrix estimated online from position information and speaker change hypotheses, instead of fixed transition probabilites. This avoids early hard decisions and is shown to outperform the sequential approach. Index Terms: speaker diarization, acoustic scene analysis, <b>Viterbi</b> <b>decode...</b>|$|R
5000|$|<b>Viterbi</b> <b>decoding</b> was {{developed}} by Andrew J. Viterbi and published in the paper [...] "Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm", IEEE Transactions on Information Theory, Volume IT-13, pages 260-269, in April, 1967.|$|E
5000|$|Iterative <b>Viterbi</b> <b>decoding</b> is an {{algorithm}} that {{spots the}} subsequence S of an observation O = {o1, ..., on} having the highest average probability (i.e., probability scaled by {{the length of}} S) of being generated by a given hidden Markov model M with m states. The algorithm uses a modified Viterbi algorithm as an internal step.|$|E
50|$|Most {{communication}} systems employ <b>Viterbi</b> <b>decoding</b> involving data packets of fixed sizes, with a fixed bit/byte pattern {{either at the}} beginning or/and {{at the end of}} the data packet. By using the known bit/byte pattern as reference, the start node may be set to a fixed value, thereby obtaining a perfect Maximum Likelihood Path during traceback.|$|E
50|$|It is {{observed}} by Forney that <b>Viterbi's</b> maximum likelihood <b>decoding</b> of convolutional codes also used algorithms of GDL-like generality.|$|R
50|$|A Viterbi decoder {{uses the}} <b>Viterbi</b> {{algorithm}} for <b>decoding</b> a bitstream that has beenencoded using convolutional code or trellis code.|$|R
40|$|Analytical and {{experimental}} {{results are presented}} {{of the effects of}} receiver tracking phase error, caused by weak signal conditions on either the uplink or the downlink or both, on the performance of the concatenated Reed-Solomon (RS) Viterbi channel coding system. The test results were obtained under an emulated S band uplink and X band downlink, two way space communication channel in the telecommunication development laboratory of JPL with data rates ranging from 4 kHz to 20 kHz. It is shown that, with ideal interleaving, the concatenated RS/Viterbi coding system is capable of yielding large coding gains at very low bit error probabilities over the <b>Viterbi</b> <b>decoded</b> convolutional only coding system. Results on the effects of receiver tracking phase errors on the performance of the concatenated coding system with antenna array combining are included...|$|R
5000|$|The {{simplest}} algorithm {{to describe}} is the [...] "stack algorithm" [...] {{in which the}} best [...] paths found so far are stored. Sequential decoding may introduce an additional error above <b>Viterbi</b> <b>decoding</b> when the correct path has [...] or more highly scoring paths above it; {{at this point the}} best path will drop off the stack and be no longer considered.|$|E
50|$|Longer {{constraint}} length {{codes are}} more practically decoded {{with any of}} several sequential decoding algorithms, of which the Fano algorithm is the best known. Unlike <b>Viterbi</b> <b>decoding,</b> sequential decoding is not maximum likelihood but its complexity increases only slightly with constraint length, allowing the use of strong, long-constraint-length codes. Such codes {{were used in the}} Pioneer program of the early 1970s to Jupiter and Saturn, but gave way to shorter, Viterbi-decoded codes, usually concatenated with large Reed-Solomon error correction codes that steepen the overall bit-error-rate curve and produce extremely low residual undetected error rates.|$|E
5000|$|Convolutional codes work on bit or symbol {{streams of}} {{arbitrary}} length. They {{are most often}} soft decoded with the Viterbi algorithm, though other algorithms are sometimes used. <b>Viterbi</b> <b>decoding</b> allows asymptotically optimal decoding efficiency with increasing constraint length of the convolutional code, but {{at the expense of}} exponentially increasing complexity. A convolutional code that is terminated is also a 'block code' in that it encodes a block of input data, but the block size of a convolutional code is generally arbitrary, while block codes have a fixed size dictated by their algebraic characteristics. Types of termination for convolutional codes include [...] "tail-biting" [...] and [...] "bit-flushing".|$|E
40|$|Abstract We {{present a}} method {{to speed up the}} dynamic program {{algorithms}} used for solving the HMM decoding and training problems for discrete time-independent HMMs. We discuss the application of our method to <b>Viterbi’s</b> <b>decoding</b> and training algorithms (IEEE Trans. Inform. Theory IT- 13 : 260 – 269, 1967), {{as well as to the}} forward-backward and Baum-Welch (Inequalities 3 : 1 – 8, 1972) algorithms. Our approach is based on identifying repeated substrings in the observed input sequence. Initially, we show how to exploit repetitions of all sufficiently small substrings (this is similar to the Four Russians method). Then, we describe four algorithms based alternatively on run length encoding (RLE), Lempel-Ziv (LZ 78) parsing, grammarbased compression (SLP), and byte pair encoding (BPE). Compared to Viterbi’s algorithm, we achieve speedups of �(log n) using the Four Russians method, � (r log r...|$|R
40|$|An Advanced Imaging Communication System (AICS) was {{proposed}} in the mid- 1970 s {{as an alternative to}} the Voyager data/communication system architecture. The AICS achieved virtually error free communication with little loss in the downlink data rate by concatenating a powerful Reed-Solomon block code with the Voyager convolutionally coded, <b>Viterbi</b> <b>decoded</b> downlink channel. The clean channel allowed AICS sophisticated adaptive data compression techniques. Both Voyager and the Galileo mission have implemented AICS components, and the concatenated channel itself is heading for international standardization. An analysis that assigns a dollar value/cost savings to AICS mission performance gains is presented. A conservative value or savings of $ 3 million for Voyager, $ 4. 5 million for Galileo, and as much as $ 7 to 9. 5 million per mission for future projects such as the proposed Mariner Mar 2 series is shown...|$|R
40|$|I {{will show}} that there is a deep {{relation}} between error-correction codes and certain mathematical models of spin glasses. In particular minimum error probability decoding is equivalent to finding the ground state of the corresponding spin system. The most probable value of a symbol is related to the magnetization at a different temperature. Convolutional codes correspond to one-dimensional spin systems and <b>Viterbi's</b> <b>decoding</b> algorithm to the transfer matrix algorithm of Statistical Mechanics. I will also show how the recently discovered (or rediscovered) capacity approaching codes (turbo codes and low density parity check codes) can be analysed using statistical mechanics. It is possible to show, using statistical mechanics, that these codes allow error-free communication for signal to noise ratio above a certain threshold. This threshold depends on the particular code, and can be computed analytically in many cases...|$|R
50|$|The error {{floor is}} a {{phenomenon}} encountered in modern iterated sparse graph-based error correcting codes like LDPC codes and turbo codes. When the bit error ratio (BER) is plotted for conventional codes like Reed-Solomon codes under algebraic decoding or for convolutional codes under <b>Viterbi</b> <b>decoding,</b> the BER steadily decreases {{in the form of}} a curve as the SNR condition becomes better. For LDPC codes and turbo codes there is a point after which the curve does not fall as quickly as before, in other words, there is a region in which performance flattens. This region is called the error floor region. The region just before the sudden drop in performance is called the waterfall region.|$|E
40|$|Abstract: In this study, Convolutional coder {{software}} implementation using <b>Viterbi</b> <b>decoding</b> algorithm for bitstream {{that had been}} encoded using forward error correction was presented. It discussed the detailed description and steps involved in simulating a communication channel using convolutional encoding with <b>Viterbi</b> <b>decoding.</b> The steps involved generating random binary data, convolutionally encoding the data, passing the encoded data through a noisy channel, quantizing the received channel symbols and finally performing <b>Viterbi</b> <b>decoding</b> on quantized channel symbols to recover original binary data. In this study, researchers aim was to convince and to explain the reader the advantages of convolutional coding with <b>Viterbi</b> <b>decoding</b> over conventional decoding techniques in terms of BER. Key words: Component, forward error correction, convolution coding, <b>viterbi</b> <b>decoding</b> algorith...|$|E
40|$|Convolution code {{is a kind}} of {{widely used}} error-correcting codes in the error control field, in order to solve the <b>Viterbi</b> <b>decoding</b> of higher complex degree and lower speed etc. problem, a kind of {{efficient}} and reliable Viterbi decode method has been put forward specially. Firstly, the principle of Viterbi decode has been introduced by detail; Secondly, in order to improve the parallel decoding speed, <b>Viterbi</b> <b>decoding</b> algorithm is improved; And then, according to the improved algorithm to achieve high speed and parallel <b>Viterbi</b> <b>decoding</b> method, which is realized easily by FPGA; Finally, the function simulation and test for (2, 1, 7) convolution code has been carried out. The experimental results show that: when the system clock is 64 MHz, eventually the decoding rate of not less than 16 Mbps, improved <b>Viterbi</b> <b>decoding</b> algorithm has lower complexity, improved <b>Viterbi</b> <b>decoding</b> efficiency. </p...|$|E
3000|$|The DFHMM {{also applies}} the {{factorial}} <b>Viterbi</b> algorithm to <b>decode</b> the states. However, it is executed in a lower-dimensional space D≪d. It is applied twice, once for FHMM [...]...|$|R
40|$|Abstract. We {{present a}} method {{to speed up the}} dynamic program {{algorithms}} used for solving the HMM decoding and training problems for discrete time-independent HMMs. We discuss the application of our method to <b>Viterbi’s</b> <b>decoding</b> and training algorithms [21], {{as well as to the}} forward-backward and Baum-Welch [4] algorithms. Our approach is based on identifying repeated substrings in the observed input sequence. We describe three algorithms based alternatively on byte pair encoding (BPE) [19], run length encoding (RLE) and Lempel-Ziv (LZ 78) pars-ing [22]. Compared to Viterbi’s algorithm, we achieve a speedup of Ω(r) using BPE, a speedup of Ω (r log r) using RLE, and a speedup of Ω (log n k) using LZ 78, where k is the number of hidden states, n is the length of the observed sequence and r is its compression ratio (under each compression scheme). Our experimental results demonstrate that our new algorithms are indeed faster in practice. Furthermore, unlike Viterbi’s algorithm, our algorithms are highly parallelizable. Key words: HMM, Viterbi, dynamic programming, compression...|$|R
40|$|Abstract: I {{will show}} that there is a deep {{relation}} between error-correction codes and certain mathematical models of spin glasses. In particular minimum error probability decoding is equivalent to finding the ground state of the corresponding spin system. The most probable value of a symbol is related to the magnetization at a different temperature. Convolutional codes correspond to one-dimensional spin systems and <b>Viterbi’s</b> <b>decoding</b> algorithm to the transfer matrix algorithm of Statistical Mechanics. A particular spin-glass model, which is exactly soluble, corresponds to an ideal code, i. e. a code which allows error-free communication if the rate is below channel capacity. The mathematical theory of communication[1, 2] is probabilistic in nature. Both the production of information and its transmission are considered as probabilistic events. A source is producing information messages according to a certain probability distribution. Each message consists of a sequence of N bits σ = {σ 1, · · ·,σN}, σi = ± 1 and it is assumed that the probability Ps(σ) ≡ exp −Hs(σ) of any particular sequence σ is known. According to Shannon the information content of the message is − lnPs(σ) and the average information of the source is given by Ps(σ) lnPs(σ...|$|R
40|$|In this study, Convolutional coder {{software}} implementation using <b>Viterbi</b> <b>decoding</b> algorithm for bitstream {{that had been}} encoded using forward error correction was presented. It discussed the detailed description and steps involved in simulating a communication channel using convolutional encoding with <b>Viterbi</b> <b>decoding.</b> The steps involved generating random binary data, convolutionally encoding the data, passing the encoded data through a noisy channel, quantizing the received channel symbols and finally performing <b>Viterbi</b> <b>decoding</b> on quantized channel symbols to recover original binary data. In this study, researchers aim was to convince and to explain the reader the advantages of convolutional coding with <b>Viterbi</b> <b>decoding</b> over conventional decoding techniques in terms of BER...|$|E
40|$|Different {{methods are}} {{considered}} for frame synchronization of a concatenated block code/Viterbi link. Synchronization after <b>Viterbi</b> <b>decoding,</b> synchronization before <b>Viterbi</b> <b>decoding</b> based on hard-quantized channel symbols are all compared. For each scheme, the probability {{under certain conditions}} of true detection of sync within four 10, 000 bit frames is tabulated...|$|E
40|$|Abstract—Algorithms {{developed}} with a Gaussian noise assumption perform poorly in impulsive noise, {{such as that}} described by the symmetric α-stable (SαS) distribution. We investigate the performance of antipodal signaling and <b>Viterbi</b> <b>decoding</b> of convolutional codes in SαS noise. We demonstrate that the p-norm branch metric is robust in SαS noise. Index Terms—Convolutional codes, impulse noise, <b>Viterbi</b> <b>decoding...</b>|$|E
40|$|We {{present a}} method {{to speed up the}} dynamic program {{algorithms}} used for solving the HMM decoding and training problems for discrete time-independent HMMs. We discuss the application of our method to <b>Viterbi’s</b> <b>decoding</b> and training algorithms (IEEE Trans. Inform. Theory IT- 13 : 260 – 269, 1967), {{as well as to the}} forward-backward and Baum-Welch (Inequalities 3 : 1 – 8, 1972) algorithms. Our approach is based on identifying repeated substrings in the observed input sequence. Initially, we show how to exploit repetitions of all sufficiently small substrings (this is similar to the Four Russians method). Then, we describe four algorithms based alternatively on run length encoding (RLE), Lempel-Ziv (LZ 78) parsing, grammar-based compression (SLP), and byte pair encoding (BPE). Compared to Viterbi’s algorithm, we achieve speedups of Θ(log n) using the Four Russians method, Ω(r/log r) using RLE, Ω(log n/k) using LZ 78, Ω(r/k) using SLP, and Ω(r) using BPE, where k is the number of hidden states, n is the length of the observed sequence and r is its compression ratio (under each compression scheme). Our experimental results demonstrate that our new algorithms are indeed faster in practice. We also discuss a parallel implementation of our algorithms...|$|R
50|$|A Viterbi decoder {{uses the}} <b>Viterbi</b> {{algorithm}} for <b>decoding</b> a bitstream {{that has been}} encoded using forward error correction based on a convolutional code.The Hamming distance {{is used as a}} metric for hard decision Viterbi decoders.The squared Euclidean distance is used as a metric for soft decision decoders.|$|R
30|$|Following the {{development}} of the <b>Viterbi</b> MLSE <b>decoding</b> algorithm [6], the BCJR algorithm [1], named after its developers Bahl–Cocke–Jelinek–Raviv, also known as the MAP algorithm, was developed in 1974, also for the decoding of convolutional codes. In the artificial intelligence community, this algorithm was developed independently by Pearl [12] and is called BP. The MAP algorithm is able to produce the posterior probability of each symbol in the estimated transmitted sequence, as opposed to maximizing the probability of the whole transmitted sequence, as done by the Viterbi MLSE equalizer [2, 6, 7, 11].|$|R
30|$|Generate new state-level {{alignments}} for {{the training}} data set using <b>Viterbi</b> <b>decoding</b> with the most recent DTAMs.|$|E
40|$|The {{performance}} of IEEE 802. 11 g wireless {{local area network}} (WLAN) standard receivers when the signal is transmitted over a frequency-selective, slowly fading Nakagami channel in a pulse-noise interference environment when errors-and-erasures <b>Viterbi</b> <b>decoding</b> is used is examined. The different combinations of modulation (both binary and non-binary) and convolutional code rate specified by the WLAN standard are examined. The performance obtained with errors-and-erasures decoding (EED) is compared with the performance obtained with errors-only hard decision <b>Viterbi</b> <b>decoding</b> (HHD) as well as that obtained with soft decision <b>Viterbi</b> <b>decoding</b> (SDD) for binary modulation, while for non-binary modulation, EED performance is compared with HDD performance. It was found that EED can significantly improve performance under some conditions when pulse-noise interference is present...|$|E
40|$|Abstract Soft output <b>Viterbi</b> <b>decoding</b> {{has evolved}} {{as a key}} tech nology for {{advanced}} decoding systems during the recent years The article presents a modication of the soft output Viterbi algorithm and the resulting hardware architecture The new approach leads to storage savings while maintaining the low computational complexity of former approaches and is thus advantageous for hardware {{as well as for}} software implementations The complexity of soft output <b>Viterbi</b> <b>decoding</b> with the new approach is clearly less than twice that of hard decision Viterbi decodin...|$|E
40|$|The <b>Viterbi</b> Algorithm for <b>decoding</b> {{convolutional}} {{codes and}} Trellis Coded Modulation is suited to VLSI implementation but contains a feedback loop which limits {{the speed of}} pipelined architecture. The feedback loop is circumvented by decoding independent sequences simultaneously, resulting in a 5 - 9 fold speed-up with a two-fold hardware expansion...|$|R
40|$|Abstract: I {{will show}} that there is a deep {{relation}} between error-correction codes and certain mathematical models of spin glasses. In particular minimum error probability decoding is equivalent to finding the ground state of the corresponding spin system. The most probable value of a symbol is related to the magnetization at a different temperature. Convolutional codes correspond to one-dimensional spin systems and <b>Viterbi’s</b> <b>decoding</b> algorithm to the transfer matrix algorithm of Statistical Mechanics. I will also show how the recently discovered (or rediscovered) capacity approaching codes (turbo codes and low density parity check codes) can be analysed using statistical mechanics. It is possible to show, using statistical mechanics, that these codes allow error-free communication for signal to noise ratio above a certain threshold. This threshold depends on the particular code, and can be computed analytically in many cases. LPTENS 01 / 31 It has been known[1 − 4] that error-correcting codes are mathematically equivalent to some theoretical spin-glass models. As it is explained in Forney’s paper in this volume, there have been recently very interesting new developments in error-correcting codes. It is now possible to approach practically very close to Shannon’s channel capacity. First came the discovery of turbo codes by Berrou and Glavieux[5] and later the rediscovery of low density parity check codes[6], first discovere...|$|R
50|$|Both <b>Viterbi</b> and {{sequential}} <b>decoding</b> algorithms return hard decisions: {{the bits}} {{that form the}} most likely codeword. An approximate confidence measure {{can be added to}} each bit by use of the Soft output Viterbi algorithm. Maximum a posteriori (MAP) soft decisions for each bit can be obtained by use of the BCJR algorithm.|$|R
