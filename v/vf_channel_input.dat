0|2005|Public
40|$|The {{investigation}} {{object is}} {{the phase of}} frequency distortions in <b>VF</b> <b>channels.</b> The aim is to develop and investigate the discrete measurement methods of phase distortions by the mathematical modelling of objects and also to develop and investigate the principles for hardware realization of the PFD meter in the <b>VF</b> <b>channel</b> has been substantiated. The new algorithms for PFD measurement have been proposed. The measurement error evaluation {{at the expense of}} the channel noises, non-linear distortions and harmonic interferences has been performed. The algorithmic procedure for discovery and measurement of the phase-frequency distortions in <b>VF</b> <b>channels</b> permitting to use more widely the digital engineering possibilities for measurement automation of the transmission system parameters and methods of its hardware realization has been proposedAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|Abstract—Optimal power {{allocation}} for orthogonal {{frequency division}} multiplexing (OFDM) wiretap channels with Gaussian <b>channel</b> <b>inputs</b> {{has already been}} studied in some previous works from an information theoretical viewpoint. However, these results are not sufficient for practical system designs. One reason is that discrete <b>channel</b> <b>inputs,</b> such as quadrature amplitude modulation (QAM) signals, instead of Gaussian <b>channel</b> <b>inputs,</b> are deployed in current practical wireless systems to maintain moderate peak transmission power and receiver complexity. In this paper, we investigate the power allocation and artificial noise design for OFDM wiretap channels with discrete <b>channel</b> <b>inputs.</b> We first prove that the secrecy rate function for discrete <b>channel</b> <b>inputs</b> is nonconcave {{with respect to the}} transmission power. To resolve the corresponding nonconvex secrecy rate maximization problem, we develop a low-complexity powe...|$|R
50|$|Note that Envy24HT-S {{supports}} 24-bit / 192 kHz mode only in 4 channel output (4 <b>channel</b> <b>input)</b> mode, {{while in}} 8 channel output (2 <b>channel</b> <b>input)</b> mode it {{is limited to}} 20-bit / 48 kHz.|$|R
40|$|OR {{multi-access}} {{channel is}} a simple model where the channel output is the Boolean OR among the Boolean <b>channel</b> <b>inputs.</b> We revisit this model, showing that employing Bloom filter, a randomized data structure, as <b>channel</b> <b>inputs</b> achieves its capacity region with joint decoding and the symmetric sum rate of 2 bits per channel use without joint decoding. We then proceed to the "many-access" regime where the number of potential users grows without bound, treating both activity recognition and message transmission problems, establishing scaling laws which are optimal within a constant factor, based on Bloom filter <b>channel</b> <b>inputs.</b> Comment: 5 page...|$|R
40|$|Abstract—For most {{discrete}} memoryless channels, {{there does}} not exist a linear code which uses all of the <b>channel’s</b> <b>input</b> symbols. Therefore, linearity of the code for such channels is a very restrictive condition {{and there should be}} a loosening of the algebraic structure of the code to a degree that the code can admit any <b>channel</b> <b>input</b> alphabet. For any <b>channel</b> <b>input</b> alphabet size, there always exists an Abelian group structure defined on the alphabet. We investigate the capacity of Abelian group codes over discrete memoryless channels and provide lower and upper bounds on the capacity. I...|$|R
40|$|International audienceCorrelation between channel {{state and}} source symbol is under {{investigation}} for a joint source-channel coding problem. We investigate simultaneously the lossless transmission of information and the empirical coordination of <b>channel</b> <b>inputs</b> with the symbols of source and states. Empirical coordination is achievable if the sequences of source symbols, <b>channel</b> states, <b>channel</b> <b>inputs</b> and <b>channel</b> outputs are jointly typical for a target joint probability distribution. We characterize the joint distributions that are achievable under lossless decoding constraint. The performance of the coordination is evaluated by an objective function. For example, we determine the minimal distortion between symbols of source and <b>channel</b> <b>inputs</b> for lossless decoding. We show that the correlation source/channel state improves the feasibility of the transmission...|$|R
3000|$|... {{forward channel}} matrix. The <b>channel</b> <b>input</b> from the BS must satisfy an (average) {{transmit}} power constraint of [...]...|$|R
40|$|It {{is shown}} that polar codes achieve the {{symmetric}} capacity of discrete memoryless <b>channels</b> with arbitrary <b>input</b> alphabet sizes. It is shown that in general, channel polarization happens in several, rather than only two levels {{so that the}} synthesized channels are either useless, perfect or "partially perfect". Any subset of the <b>channel</b> <b>input</b> alphabet which is closed under addition, induces a coset partition of the alphabet through its shifts. For any such partition of the input alphabet, there exists a corresponding partially perfect channel whose outputs uniquely determine the coset to which the <b>channel</b> <b>input</b> belongs. By a slight modification of the encoding and decoding rules, it is shown that perfect transmission of certain information symbols over partially perfect channels is possible. Our result is general regarding both the cardinality and the algebraic structure of the <b>channel</b> <b>input</b> alphabet; i. e we show that for any <b>channel</b> <b>input</b> alphabet size and any Abelian group structure on the alphabet, polar codes are optimal. It is also shown through an example that polar codes when considered as group/coset codes, do not achieve the capacity achievable using coset codes over arbitrary channels...|$|R
40|$|We derive a single-letter {{formula for}} the zero-rate {{reliability}} (error exponent) of a finite-state channel whose state variable depends deterministically (and recursively) on past <b>channel</b> <b>inputs,</b> where the code complies with a given <b>channel</b> <b>input</b> constraint. Special attention is then {{devoted to the}} important special case of the Gaussian channel with inter-symbol interference (ISI), where more explicit results are obtained. Comment: 22 pages; submitted to IEEE Transactions on Information Theor...|$|R
40|$|Correlation between channel {{state and}} source symbol is under {{investigation}} for a joint source-channel coding problem. We investigate simultaneously the lossless transmission of information and the empirical coordination of <b>channel</b> <b>inputs</b> with the symbols of source and states. Empirical coordination is achievable if the sequences of source symbols, <b>channel</b> states, <b>channel</b> <b>inputs</b> and <b>channel</b> outputs are jointly typical for a target joint probability distribution. We characterize the joint distributions that are achievable under lossless decoding constraint. The performance of the coordination is evaluated by an objective function. For example, we determine the minimal distortion between symbols of source and <b>channel</b> <b>inputs</b> for lossless decoding. We show that the correlation source/channel state improves the feasibility of the transmission. Comment: Conference IEEE ITW 201...|$|R
3000|$|... < K) are {{selected}} for transmission during each coherence interval, the <b>channel</b> <b>input</b> × {{can be written}} as [...]...|$|R
3000|$|... [i] {{denote the}} {{complex-valued}} <b>channel</b> <b>input</b> and the fading coefficient of the j th user, respectively. We assume that {h [...]...|$|R
40|$|Optimal power {{allocation}} for orthogonal {{frequency division}} multiplexing (OFDM) wiretap channels with Gaussian <b>channel</b> <b>inputs</b> {{has already been}} studied in some previous works from an information theoretical viewpoint. However, these results are not sufficient for practical system design. One reason is that discrete <b>channel</b> <b>inputs,</b> such as quadrature amplitude modulation (QAM) signals, instead of Gaussian <b>channel</b> <b>inputs,</b> are deployed in current practical wireless systems to maintain moderate peak transmission power and receiver complexity. In this paper, we investigate the power allocation and artificial noise design for OFDM wiretap channels with discrete <b>channel</b> <b>inputs.</b> We first prove that the secrecy rate function for discrete <b>channel</b> <b>inputs</b> is nonconcave {{with respect to the}} transmission power. To resolve the corresponding nonconvex secrecy rate maximization problem, we develop a low-complexity power allocation algorithm, which yields a duality gap diminishing in the order of O(1 /√(N)), where N is the number of subcarriers of OFDM. We then show that independent frequency-domain artificial noise cannot improve the secrecy rate of single-antenna wiretap channels. Towards this end, we propose a novel time-domain artificial noise design which exploits temporal degrees of freedom provided by the cyclic prefix of OFDM systems to jam the eavesdropper and boost the secrecy rate even with a single antenna at the transmitter. Numerical results are provided to illustrate the performance of the proposed design schemes. Comment: 12 pages, 7 figures, accepted by IEEE Transactions on Wireless Communications, Jan. 201...|$|R
40|$|Abstract—We {{consider}} a new fundamental question re-garding the point-to-point memoryless channel. The source-channel separation theorem indicates that random codebook construction for lossy source compression and channel coding can be independently constructed and paired to achieve optimal performance for coordinating a source sequence with a reconstruction sequence. But {{what if we}} want the <b>channel</b> <b>input</b> to also be coordinated with the source and reconstruction? Such situations arise in network communication problems, where the correlation inherent in the information sources {{can be used to}} correlate <b>channel</b> <b>inputs.</b> Hybrid codes {{have been shown to be}} useful in a number of network communication problems. In this work we highlight their advantages over purely digital codebook construction by applying them to the point-to-point setting, coordinating both the <b>channel</b> <b>input</b> and the reconstruction with the source. ...|$|R
30|$|The bounds in (20) can be {{generalized}} by randomizing the <b>channel</b> <b>inputs.</b> We now prove that PDF with randomization achieves the secrecy capacity.|$|R
3000|$|... in Theorem 6 by {{evaluating}} (38)-(41) {{with zero}} mean jointly Gaussian <b>channel</b> <b>inputs</b> X 1, X 2 and X 3. That is, [...]...|$|R
40|$|Introduction Determining the {{achievable}} {{rates at}} which {{information can be}} reliably transmitted across noisy channels {{has been one of}} the central pursuits in information theory since Shannon invented the subject in 1948. In this chapter, we consider these rates for the class of channels known as finite state channels (FSC). A FSC is a discrete-time channel where the distribution of the channel output depends on both the <b>channel</b> <b>input</b> and the underlying channel state. This allows the channel output to depend implicitly on previous inputs and outputs via the channel state. In practice, there are three types of channel variation which FSCs are typically used to model. A flat fading channel is a time-varying channel whose state is independent of the <b>channel</b> <b>inputs.</b> An intersymbol-interference (ISI) channel is a time-varying channel whose state is a deterministic function of the previous <b>channel</b> <b>inputs.</b> <b>Channels</b> which exhibit both fading and ISI can also modeled, and their state is a stoc...|$|R
50|$|Communication {{channels}} are also studied in a discrete-alphabet setting. This corresponds to abstracting a real world communication {{system in which}} the analog → digital and digital → analog blocks are out of the control of the designer. The mathematical model consists of a transition probability that specifies an output distribution for each possible sequence of <b>channel</b> <b>inputs.</b> In information theory, it is common to start with memoryless channels in which the output probability distribution only depends on the current <b>channel</b> <b>input.</b>|$|R
40|$|We {{consider}} a unit memory channel, called Binary State Symmetric Channel (BSSC), {{in which the}} channel state is the modulo 2 addition of the current <b>channel</b> <b>input</b> and the previous channel output. We derive closed form expressions for the capacity and corresponding <b>channel</b> <b>input</b> distribution, of this BSSC with and without feedback and transmission cost. We also show that {{the capacity of the}} BSSC is not increased by feedback, and it is achieved by a first order symmetric Markov process...|$|R
40|$|We {{consider}} a new fundamental question regarding the point-to-point memoryless channel. The source-channel separation theorem indicates that random codebook construction for lossy source compression and channel coding can be independently constructed and paired to achieve optimal performance for coordinating a source sequence with a reconstruction sequence. But {{what if we}} want the <b>channel</b> <b>input</b> to also be coordinated with the source and reconstruction? Such situations arise in network communication problems, where the correlation inherent in the information sources {{can be used to}} correlate <b>channel</b> <b>inputs.</b> Hybrid codes {{have been shown to be}} useful in a number of network communication problems. In this work we highlight their advantages over purely digital codebook construction by applying them to the point-to-point setting, coordinating both the <b>channel</b> <b>input</b> and the reconstruction with the source. Comment: Allerton 2011, 5 pages, 1 figure, uses IEEEtran. cl...|$|R
40|$|Worldwide NTSC/PAL/SECAM color {{demodulation}} support One 10 -bit {{analog-to-digital converter}} (ADC), 4 × oversampling per channel for CVBS, Y/C, and YPrPb modes Analog video <b>input</b> <b>channels</b> with on-chip antialiasing filter ADV 7280 : up to 4 <b>input</b> <b>channels</b> ADV 7280 -M: up to 8 <b>input</b> <b>channels</b> Video <b>input</b> support for CVBS (composite), Y/C (S-Video) ...|$|R
30|$|Data augmentation, {{especially}} random rotation, {{is adopted}} {{in one of}} the CNN <b>channel</b> <b>input,</b> which allows the identification model to learn rotation-invariant features to enhance the classification performance.|$|R
40|$|AbstractA {{computational}} {{scheme for}} calculating {{the capacity of}} continuous-input discrete-output memoryless channels is presented. By adopting relative entropy as a performance measure between two channel transition probabilities the method suggests an algorithm to discretize continuous <b>channel</b> <b>inputs</b> into a set of finite desired <b>channel</b> <b>inputs</b> so that the discrete version of the well-known Arimoto-Blahut algorithm is readily applied. Compared to recent algorithms developed by Chang and Davisson, the algorithm has a simple structure for numerical implementations. To support this justification a numerical example is studied and the relative performance is compared based on computing time...|$|R
5000|$|... #Caption: The channel {{model for}} the binary erasure channel showing a mapping from <b>channel</b> <b>input</b> X to <b>channel</b> output Y (with known erasure symbol ?). The {{probability}} of erasure is ...|$|R
40|$|M-ary signal {{transmission}} over AWGN channel with additive Q-ary interference where {{the sequence of}} i. i. d. interference symbols is known causally at the transmitter is considered. Shannon’s theorem for channels with side information at the transmitter is used to formulate {{the capacity of the}} channel. It is shown that by using at most MQ − Q + 1 out of M Q input symbols of the associated channel, the capacity is achievable. For the special case where the Gaussian noise power is zero, a sufficient condition, which is independent of interference, is given for the capacity to be log 2 M bits per channel use. The problem of maximization of the transmission rate under the constraint that the <b>channel</b> <b>input</b> given any current interference symbol is uniformly distributed over the <b>channel</b> <b>input</b> alphabet is investigated. For this setting, the general structure of a communication system with optimal precoding is proposed. The extension of the proposed precoding scheme to continuous <b>channel</b> <b>input</b> alphabet is also investigated...|$|R
40|$|An encryption/decryption {{approach}} is proposed dedicated to one-way communication between a transmitter {{which is a}} computationally powerful party and a receiver with limited computational capabilities. The proposed encryption technique combines traditional stream ciphering and simulation of a binary channel which degrades <b>channel</b> <b>input</b> by inserting random bits. A statistical model of the proposed encryption is analyzed from the information-theoretic point of view. In the addressed model an attacker faces the problem implied by observing the messages through a channel with random bits insertion. The paper points {{out a number of}} security related implications of the considered channel. These implications have been addressed by estimation of the mutual information between the <b>channel</b> <b>input</b> and output and estimation of the number of candidate <b>channel</b> <b>inputs</b> for a given channel output. It is shown that deliberate and secret key controlled insertion of random bits into the basic ciphertext provides security enhancement of the resulting encryption scheme...|$|R
40|$|The paper {{describes}} {{the determination of}} the additive white Gaussian noise channel information transmission capacity when the <b>channel</b> <b>input</b> and output are limited by certain constraints. The <b>channel</b> <b>input</b> constraints are those of signal amplitude, or signal amplitude and average power. The input signal amplitude and average power constraints are defined by restricting the <b>channel</b> <b>input</b> to values within the finite interval [—A, + A] and also to have average power equal to some specified value. The channel output constraint is that of signal clipping due to quantisation applied at the receiver. The input signal amplitude/output signal clipping constrained capacity, and the input signal amplitude and average power/output signal clipping constrained capacity are determined separately. It is found that there are unique, optimum and discrete input signal amplitude distributions, taking a finite number of values, and optimum output signal clippings that achieve these capacity values. The optimum input distribution values are also used to determine the optimum amplitude probability density function at the channel output...|$|R
50|$|The {{number of}} faders is often {{fewer than the}} number of <b>input</b> <b>channels.</b> The extra <b>input</b> <b>channels</b> are not {{accessible}} until a bank of faders is switched to control them.|$|R
3000|$|... are zero-mean, unit-variance Gaussian noises {{that are}} {{independent}} {{of each other and}} of the <b>channel</b> <b>inputs.</b> The difference with the model in [17] is the presence of the intra-cluster signal in (1).|$|R
40|$|A {{computational}} {{scheme for}} calculating {{the capacity of}} continuous-input discrete-output memoryless channels is presented. By adopting relative entropy as a performance measure between two channel transition probabilities the method suggests an algorithm to discretize continuous <b>channel</b> <b>inputs</b> into a set of finite desired <b>channel</b> <b>inputs</b> so that the discrete version of the well-known Arimoto-Blahut algorithm is readily applied. Compared to recent algorithms developed by Chang and Davisson, the algorithm has a simple structure for numerical implemen-tations. To support this justification a numerical example is studied and the relative performance is compared based on computing time. ? I 990 Academx Press. Inc I...|$|R
40|$|Abstract—This paper {{studies the}} {{decoupling}} principle of a linear vector channel, {{which is an}} extension of CDMA and MIMO channels. We show that the scalar-channel characterization obtained via the decoupling principle is valid not only for collections {{of a large number of}} elements of input vector, as discussed in previous studies, but also for individual elements of input vector, i. e. the linear vector channel for individual elements of <b>channel</b> <b>input</b> vector is decomposed into a bank of independent scalar Gaussian channels in the large-system limit, where dimensions of <b>channel</b> <b>input</b> and output are both sent to infinity while their ratio fixed. I...|$|R
40|$|Abstract- We {{obtain the}} {{convergence}} of the Godard family (including SATO and CM algorithms) in a unified way. Our assumptions are quite realistic: the <b>channel</b> <b>input</b> can be asymptotically stationary and ergodic, the channel impulse response is finite and can be stationary, ergodic (this models fading channels) and the equalizer length is finite. The noise is i. i. d. The <b>channel</b> <b>input</b> can be discrete or continuous. Our approach allows us to approximate the whole trajectory of the equalizer coefficients. This provides estimates of the rate of convergence and the system performance (symbol error rate) can be evaluated under transience and steady state...|$|R
40|$|This paper {{studies the}} {{decoupling}} principle of a linear vector channel, {{which is an}} extension of CDMA and MIMO channels. We show that the scalar-channel characterization obtained via the decoupling principle is valid not only for collections {{of a large number of}} elements of input vector, as discussed in previous studies, but also for individual elements of input vector, i. e. the linear vector channel for individual elements of <b>channel</b> <b>input</b> vector is decomposed into a bank of independent scalar Gaussian channels in the large-system limit, where dimensions of <b>channel</b> <b>input</b> and output are both sent to infinity while their ratio fixed...|$|R
3000|$|The {{following}} corollary {{gives an}} inner {{bound for the}} capacity region of the binary noiseless MAC by applying a slightly generalized binary DPC at the informed encoder in which the <b>channel</b> <b>input</b> [...]...|$|R
50|$|The Analog Digital Converter module (ADC) {{uses the}} {{successive}} approximation method to convert analog input values (voltages) to discrete digital values with 10-bit resolution. One ADC kernel (ADC0) operates on a user-selectable number of <b>input</b> <b>channels.</b> The <b>input</b> <b>channels</b> can be selected and arbitrated flexibly.|$|R
40|$|We study finite {{alphabet}} channels with Unit Memory {{on the previous}} Channel Outputs called UMCO channels. We identify necessary and sufficient conditions, to test whether the capacity achieving <b>channel</b> <b>input</b> distributions with feedback are time-invariant, and whether feedback capacity is characterized by single letter, expressions, {{similar to that of}} memoryless channels. The method is based on showing that a certain dynamic programming equation, which in general, is a nested optimization problem over the sequence of <b>channel</b> <b>input</b> distributions, reduces to a non-nested optimization problem. Moreover, for UMCO channels, we give a simple expression for the ML error exponent, and we identify sufficient conditions to test whether feedback does not increase capacity. We derive similar results, when transmission cost constraints are imposed. We apply the results to a special class of the UMCO channels, the Binary State Symmetric Channel (BSSC) with and without transmission cost constraints, to show that the optimization problem of feedback capacity is non-nested, the capacity achieving <b>channel</b> <b>input</b> distribution and the corresponding channel output transition probability distribution are time-invariant, and feedback capacity is characterized by a single letter formulae, precisely as Shannon's single letter characterization of capacity of memoryless channels. Then we derive closed form expressions for the capacity achieving <b>channel</b> <b>input</b> distribution and feedback capacity. We use the closed form expressions to evaluate an error exponent for ML decoding. Comment: submitted to IEEE Transactions on Information Theory, Paper no. IT- 16 - 090...|$|R
