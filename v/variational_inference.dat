686|202|Public
5000|$|Maximum Entropy Discrimination: This is a <b>variational</b> <b>inference</b> {{framework}} {{that allows for}} introducing and accounting for additional large-margin constraints ...|$|E
5000|$|In <b>variational</b> <b>inference,</b> the {{posterior}} distribution over {{a set of}} unobserved variables [...] given some data [...] is approximatedby a variational distribution, : ...|$|E
50|$|Stan {{implements}} reverse-mode automatic differentiation {{to calculate}} gradients of the model, which {{is required by}} HMC, NUTS, L-BFGS, BFGS, and <b>variational</b> <b>inference.</b> The automatic differentiation within Stan can be used outside of the probabilistic programming language.|$|E
40|$|This paper {{presents}} a collapsed <b>variational</b> Bayesian <b>inference</b> algorithm for PCFGs {{that has the}} advantages of two dominant Bayesian training algorithms for PCFGs, namely <b>variational</b> Bayesian <b>inference</b> and Markov chain Monte Carlo. In three kinds of experiments, we illustrate that our algorithm achieves close performance to the Hastings sampling algorithm while using {{an order of magnitude}} less training time; and outperforms the standard <b>variational</b> Bayesian <b>inference</b> and the EM algorithms with similar training time. ...|$|R
40|$|Approximate {{inference}} for Bayesian {{models is}} dominated by two approaches, <b>variational</b> Bayesian <b>inference</b> and Markov Chain Monte Carlo. Both approaches have their own ad-vantages and disadvantages, and they can complement each other. Recently researchers have proposed collapsed <b>variational</b> Bayesian <b>inference</b> to combine the advantages of both. Such inference methods have been success-ful in several models whose hidden variables are conditionally independent given the pa-rameters. In this paper we propose two col-lapsed <b>variational</b> Bayesian <b>inference</b> algo-rithms for hidden Markov models, a pop-ular framework for representing time series data. We validate our algorithms on the nat-ural language processing task of unsupervised part-of-speech induction, showing that they are both more computationally efficient than sampling, and more accurate than standard <b>variational</b> Bayesian <b>inference</b> for HMMs. ...|$|R
5000|$|Variational message passing: a modular {{algorithm}} for <b>variational</b> Bayesian <b>inference.</b>|$|R
5000|$|Zoubin Ghahramani is a {{world leader}} {{in the field of}} machine learning, {{significantly}} advancing the state-of-the-art in algorithms that can learn from data. He is known in particular for fundamental contributions to probabilistic modeling and Bayesian nonparametric approaches to machine learning systems, and to the development of approximate <b>variational</b> <b>inference</b> algorithms for scalable learning. He is one of the pioneers of semi-supervised learning methods, active learning algorithms, and sparse Gaussian processes. His development of novel infinite dimensional nonparametric models, such as the infinite latent feature model, has been highly influential.|$|E
50|$|The {{parameter}} learning task in HMMs is to find, given an output sequence or {{a set of}} such sequences, the best set of state transition and emission probabilities. The task is usually to derive the maximum likelihood estimate of {{the parameters of the}} HMM given the set of output sequences. No tractable algorithm is known for solving this problem exactly, but a local maximum likelihood can be derived efficiently using the Baum-Welch algorithm or the Baldi-Chauvin algorithm. The Baum-Welch algorithm is a special case of the expectation-maximization algorithm. If the HMMs are used for time series prediction, more sophisticated Bayesian inference methods, like Markov chain Monte Carlo (MCMC) sampling are proven to be favorable over finding a single maximum likelihood model both in terms of accuracy and stability. Since MCMC imposes significant computational burden, in cases where computational scalability is also of interest, one may alternatively resort to variational approximations to Bayesian inference, e.g. Indeed, approximate <b>variational</b> <b>inference</b> offers computational efficiency comparable to expectation-maximization, while yielding an accuracy profile only slightly inferior to exact MCMC-type Bayesian inference.|$|E
40|$|We {{introduce}} incremental <b>variational</b> <b>inference</b> {{and apply}} it to latent Dirichlet allocation (LDA). Incremental <b>variational</b> <b>inference</b> is inspired by incremental EM and provides an alternative to stochastic <b>variational</b> <b>inference.</b> Incremental LDA can process massive document collections, does not require to set a learning rate, converges faster to a local optimum of the variational bound and enjoys the attractive property of monotonically increasing it. We study the performance of incremental LDA on large benchmark data sets. We further introduce a stochastic approximation of incremental <b>variational</b> <b>inference</b> which extends to the asynchronous distributed setting. The resulting distributed algorithm achieves comparable performance as single host incremental <b>variational</b> <b>inference,</b> but with a significant speed-up...|$|E
5000|$|Expectation-maximization algorithm: {{a related}} {{approach}} which {{corresponds to a}} special case of <b>variational</b> Bayesian <b>inference.</b>|$|R
40|$|Abstract. The article {{describe}} the model, derivation, {{and implementation of}} <b>variational</b> Bayesian <b>inference</b> for linear and logistic regression, both with and without automatic relevance determination. It has the dual function of acting as a tutorial for the derivation of <b>variational</b> Bayesian <b>inference</b> for simple models, as well as documenting, and providing brief examples for the MATLAB functions that implement this inference. These functions are freely available online. 1...|$|R
40|$|Abstract Latent Dirichlet {{allocation}} (LDA) is a Bayesian {{network that}} has recently gainedmuch popularity in applications ranging from document modeling to computer vision. Due {{to the large}} scale nature of these applications, current <b>inference</b> pro-cedures like <b>variational</b> Bayes and Gibb sampling have been found lacking. In this paper we propose the collapsed <b>variational</b> Bayesian <b>inference</b> algorithm for LDA,and show that it is computationally efficient, easy to implement and significantly more accurate than standard <b>variational</b> Bayesian <b>inference</b> for LDA...|$|R
40|$|Stochastic <b>variational</b> <b>inference</b> is a {{promising}} method for fitting large-scale probabilistic models with hidden structures. Different from traditional stochastic learning, stochastic <b>variational</b> <b>inference</b> uses the natural gradient, which is particularly efficient for computing probabilistic distributions. One {{of the issues}} in stochastic <b>variational</b> <b>inference</b> is to set an appropriate learning rate. Inspired by a recent approach for setting the learning rate for stochastic learning (Schaul et al., 2012), we present a strategy for setting the learning rate for stochastic <b>variational</b> <b>inference</b> and demonstrate it is effective in learning large-scale complex models. ...|$|E
40|$|We {{present a}} truncation-free {{stochastic}} <b>variational</b> <b>inference</b> algorithm for Bayesian nonparametric models. While traditional <b>variational</b> <b>inference</b> algorithms require truncations {{for the model}} or the variational distribution, our method adapts model complexity on the fly. We studied our method with Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large data sets. Our method performs better than previous stochastic <b>variational</b> <b>inference</b> algorithms. ...|$|E
40|$|We develop {{stochastic}} <b>variational</b> <b>inference,</b> a scalable algorithm for approximating posterior distributions. We {{develop this}} technique {{for a large}} class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic <b>variational</b> <b>inference,</b> we analyze several large collections of documents: 300 K articles from Nature, 1. 8 M articles from The New York Times, and 3. 8 M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional <b>variational</b> <b>inference,</b> which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic <b>variational</b> <b>inference</b> lets us apply complex Bayesian models to massive data sets...|$|E
40|$|Abstract. Forming {{consensus}} clusters {{from multiple}} input clusterings can improve accuracy and robustness. Current clustering ensemble methods require specifying {{the number of}} consensus clusters. A poor choice can lead to under or over fitting. This paper proposes a nonparametric Bayesian clustering ensemble (NBCE) method, which can discover the number of clusters in the consensus clustering. Three inference methods are considered: collapsed Gibbs sampling, <b>variational</b> Bayesian <b>inference,</b> and collapsed <b>variational</b> Bayesian <b>inference.</b> Comparison of NBCE with several other algorithms demonstrates its versatility and superior stability. ...|$|R
40|$|Genome-wide {{regression}} using {{a number}} of genome-wide markers as predictors is now widely used for genome-wide association mapping and genomic prediction. We developed novel software for genome-wide regression which we named VIGoR (<b>variational</b> Bayesian <b>inference</b> for genome-wide regression). <b>Variational</b> Bayesian <b>inference</b> is computationally much faster than widely used Markov chain Monte Carlo algorithms. VIGoR implements seven regression methods, and is provided as a command line program package for Linux/Mac, and as a cross-platform R package. In addition to model fitting, cross-validation and hyperparameter tuning using cross-validation can be automatically performed by modifying a single argument. VIGoR is available at [URL] The R package is also available at [URL]...|$|R
40|$|Gaussian {{processes}} (GPs) {{are powerful}} non-parametric function estimators. However, their applications are largely {{limited by the}} expensive computational cost of the inference procedures. Existing stochastic or distributed synchronous <b>variational</b> <b>inferences,</b> although have alleviated this issue by scaling up GPs to millions of samples, are still far from satisfactory for real-world large applications, where the data sizes are often orders of magnitudes larger, say, billions. To solve this problem, we propose ADVGP, the first Asynchronous Distributed <b>Variational</b> Gaussian Process <b>inference</b> for regression, on the recent large-scale machine learning platform, PARAMETERSERVER. ADVGP uses a novel, flexible variational framework based on a weight space augmentation, and implements the highly efficient, asynchronous proximal gradient optimization. While maintaining comparable or better predictive performance, ADVGP greatly improves upon {{the efficiency of the}} existing variational methods. With ADVGP, we effortlessly scale up GP regression to a real-world application with billions of samples and demonstrate an excellent, superior prediction accuracy to the popular linear models. Comment: International Conference on Machine Learning 201...|$|R
40|$|This {{appendix}} {{has been}} provided to give readers additional information about the HDSP. In this appendix, we provide the detailed derivation of <b>variational</b> <b>inference,</b> the posterior word count analysis, and more examples from the Wikipedia and OHSUMED corpora. A. <b>Variational</b> <b>inference</b> for HDSP In this section, we provide the detailed derivation for mean-field <b>variational</b> <b>inference</b> for HDSP. First, the evidence of lower bound for HDSP is obtained by taking a Jensen’s inequality on the marginal log likelihood of the observed data, ln p(D,Θ) dΘ...|$|E
40|$|<b>Variational</b> <b>Inference</b> is {{a popular}} {{technique}} to approximate a possibly intractable Bayesian posterior with a more tractable one. Recently, Boosting <b>Variational</b> <b>Inference</b> has been proposed as a new paradigm to approximate the posterior by a mixture of densities by greedily adding components to the mixture. In the present work, we study the convergence properties of this approach from a modern optimization viewpoint by establishing connections to the classic Frank-Wolfe algorithm. Our analyses yields novel theoretical insights on the Boosting of <b>Variational</b> <b>Inference</b> regarding the sufficient conditions for convergence, explicit sublinear/linear rates, and algorithmic simplifications...|$|E
40|$|<b>Variational</b> <b>inference</b> is a {{scalable}} {{technique for}} approximate Bayesian inference. Deriving <b>variational</b> <b>inference</b> algorithms requires tedious model-specific calcula-tions; {{this makes it}} difficult for non-experts to use. We propose an automatic varia-tional inference algorithm, automatic differentiation <b>variational</b> <b>inference</b> (advi); we implement it in Stan (code available), a probabilistic programming system. In advi the user provides a Bayesian model and a dataset, nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the vari-ational objective. We compare advi to mcmc sampling across hierarchical gen-eralized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With advi we can use <b>variational</b> <b>inference</b> on any model we write in Stan. ...|$|E
40|$|This paper {{presents}} {{studies on}} a deterministic annealing algorithm based on quantum annealing for <b>variational</b> Bayes (QAVB) <b>inference,</b> {{which can be}} seen {{as an extension of the}} simulated annealing for <b>variational</b> Bayes (SAVB) <b>inference.</b> QAVB is as easy as SAVB to implement. Experiments revealed QAVB finds a better local optimum than SAVB in terms of the variational free energy in latent Dirichlet allocation (LDA). ...|$|R
40|$|We {{develop a}} novel algorithm, called VIP*, for {{structured}} <b>variational</b> approximate <b>inference.</b> This algorithm extends known algorithms to allow efficient multiple potential updates for overlapping clusters, and overcomes the difficulties imposed by deterministic constraints. The algorithm’s convergence is proven and its applicability demonstrated for genetic linkage analysis. 1...|$|R
30|$|The rest of {{the paper}} is {{organized}} as follows. The real-valued array signal model is given in Section 2. The <b>variational</b> Bayesian <b>inference</b> is briefly reviewed, and the proposed tracking method is introduced in Sections 3 and 4. Numerical examples and simulation results are given in Section 5. Section 6 concludes the paper.|$|R
40|$|<b>Variational</b> <b>inference</b> is an {{umbrella}} term for algorithms which cast Bayesian inference as optimization. Classically, <b>variational</b> <b>inference</b> uses the Kullback-Leibler divergence {{to define the}} optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine <b>variational</b> <b>inference</b> from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator <b>variational</b> <b>inference</b> (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for <b>variational</b> <b>inference.</b> We can characterize different properties of variational objectives, such as objectives that admit data subsampling [...] -allowing inference to scale to massive data [...] -as well as objectives that admit variational programs [...] -a rich class of posterior approximations that {{does not require a}} tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images. Comment: Appears in Neural Information Processing Systems, 201...|$|E
40|$|We {{present a}} general method for {{deriving}} collapsed <b>variational</b> <b>inference</b> algo- rithms for probabilistic {{models in the}} conjugate exponential family. Our method unifies many existing approaches to collapsed <b>variational</b> <b>inference.</b> Our collapsed <b>variational</b> <b>inference</b> leads to a new lower bound on the marginal likelihood. We exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equations have been derived. Empirically we show significant speed-ups for probabilistic models optimized using our bound. Comment: Accepted at NIPS 201...|$|E
40|$|Many computationally-efficient {{methods for}} Bayesian deep {{learning}} rely on continuous optimization algorithms, but {{the implementation of}} these methods requires significant changes to existing code-bases. In this paper, we propose Vprop, a method for Gaussian <b>variational</b> <b>inference</b> that can be implemented with two {{minor changes to the}} off-the-shelf RMSprop optimizer. Vprop also reduces the memory requirements of Black-Box <b>Variational</b> <b>Inference</b> by half. We derive Vprop using the conjugate-computation <b>variational</b> <b>inference</b> method, and establish its connections to Newton's method, natural-gradient methods, and extended Kalman filters. Overall, this paper presents Vprop as a principled, computationally-efficient, and easy-to-implement method for Bayesian deep learning...|$|E
40|$|Abstract. This article {{presents}} a <b>variational</b> Bayes <b>inference</b> for normalized Gaussian network, {{which is a}} kind of mixture models of local experts. In order to search for the optimal model structure, we develop a hierarchical model selection method. The performance of our method is evaluated by using function approximation and nonlinear dynamical system identification problems. Our method achieved better performance than existing methods. ...|$|R
40|$|The {{following}} {{technical report}} presents a formal approach to probabilistic minimalist grammar induction. We describe a formalization of a minimalist grammar. Based on this grammar, we define a generative model for minimalist derivations. We then present a generalized algorithm {{for the application}} of <b>variational</b> Bayesian <b>inference</b> to lexicalized mildly context sensitive language grammars which in this paper is applied to the previously defined minimalist grammar...|$|R
40|$|<b>Variational</b> Bayes (VB) <b>inference</b> {{is one of}} {{the most}} {{important}} algorithms in machine learning and widely used in engineering and industry. However, VB is known to suffer from the problem of local optima. In this Letter, we generalize VB by using quantum mechanics, and propose a new algorithm, which we call quantum annealing <b>variational</b> Bayes (QAVB) <b>inference.</b> We then show that QAVB drastically improve the performance of VB by applying them to a clustering problem described by a Gaussian mixture model. Finally, we discuss an intuitive understanding on how QAVB works well. Comment: 7 pages, 4 figure...|$|R
40|$|Recent {{progress}} in <b>variational</b> <b>inference</b> has {{paid much attention}} to the flexibility of variational posteriors. Work has been done to use implicit distributions, i. e., distributions without tractable likelihoods as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and can hardly scale to high-dimensional latent variable models. In this paper, we present an implicit <b>variational</b> <b>inference</b> approach with kernel density ratio fitting that addresses these challenges. As far as we know, for the first time implicit <b>variational</b> <b>inference</b> is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks...|$|E
40|$|Stochastic <b>variational</b> <b>inference</b> for {{collapsed}} models {{has recently}} been successfully applied to large scale topic modelling. In this paper, we propose a stochastic collapsed <b>variational</b> <b>inference</b> algorithm in the sequential data setting. Our algorithm is applicable to both finite hidden Markov models and hierarchical Dirichlet process hidden Markov models, and to any datasets generated by emission distributions in the exponential family. Our experiment results on two discrete datasets show that our inference is both more efficient and more accurate than its uncollapsed version, stochastic <b>variational</b> <b>inference.</b> Comment: NIPS Workshop on Advances in Approximate Bayesian Inference, 201...|$|E
40|$|We propose extreme {{stochastic}} <b>variational</b> <b>inference</b> (ESVI), an asynchronous and lock-free algorithm {{to perform}} <b>variational</b> <b>inference</b> on massive real world datasets. Stochastic <b>variational</b> <b>inference</b> (SVI), the state-of-the-art algorithm for scaling <b>variational</b> <b>inference</b> to large-datasets, is inherently serial. Moreover, {{it requires the}} parameters to fit {{in the memory of}} a single processor; this is problematic when the number of parameters is in billions. ESVI overcomes these limitations by requiring that each processor only access a subset of the data and a subset of the parameters, thus providing data and model parallelism simultaneously. We demonstrate the effectiveness of ESVI by running Latent Dirichlet Allocation (LDA) on UMBC- 3 B, a dataset that has a vocabulary of 3 million and a token size of 3 billion. To best of our knowledge, this is an order of magnitude larger than the largest dataset on which results using <b>variational</b> <b>inference</b> have been reported in literature. In our experiments, we found that ESVI outperforms VI and SVI, and also achieves a better quality solution. In addition, we propose a strategy to speed up computation and save memory when fitting large number of topics...|$|E
40|$|The ill-posed {{nature of}} missing {{variable}} models offers a challenging testing ground for new computational techniques. This {{is the case}} for the mean-field <b>variational</b> Bayesian <b>inference.</b> The behavior of this approach in the setting of the Bayesian probit model is illustrated. It is shown that the mean-field variational method always underestimates the posterior variance and, that, for small sample sizes, the meanfield variational approximation to the posterior location could be poor...|$|R
40|$|This paper {{presents}} infinite latent process decomposition (iLPD), a new microarray analysis method, as {{an extension}} or latent process decomposition [1]. Our method assumes {{an infinite number of}} latent processes. Further, our new collapsed <b>variational</b> Bayesian <b>inference</b> improves the inference proposed in [2] in the treatment of Dirichlet hyperparameters. We also give the results of the comparison experiment. 2010 IEEE International Conference on Bioinformatics and Biomedicine Workshops (BIBMW) : HongKong, China, 2010. 12. 18 - 2010. 12. 1...|$|R
40|$|We {{propose a}} novel {{interpretation}} of the collapsed <b>variational</b> Bayes <b>inference</b> with a zero-order Taylor expansion approximation, called CVB 0 inference, for latent Dirichlet allocation (LDA). We clarify {{the properties of the}} CVB 0 inference by using the alpha-divergence. We show that the CVB 0 inference is composed of two different divergence projections: alpha= 1 and - 1. This interpretation will help shed light on CVB 0 works. Comment: Appears in Proceedings of the 29 th International Conference on Machine Learning (ICML 2012...|$|R
