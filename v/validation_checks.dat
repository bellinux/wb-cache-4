105|288|Public
50|$|Scales and indexes {{have to be}} validated. Internal <b>validation</b> <b>checks</b> the {{relation}} between the individual measures included in the scale, and the composite scale itself. External <b>validation</b> <b>checks</b> {{the relation}} between the composite scale and other indicators of the variable, indicators not included in the scale. Content validation (also called face validity) checks how well the scale measures what is supposed to measured. Criterion <b>validation</b> <b>checks</b> how meaningful the scale criteria are relative to other possible criteria. Construct <b>validation</b> <b>checks</b> what underlying construct is being measured. There are three variants of construct validity. They are convergent validity, discriminant validity, and nomological validity (Campbell and Fiske, 1959; Krus and Ney, 1978). The coefficient of reproducibility indicates how well the data from the individual measures included in the scale can be reconstructed from the composite scale.|$|E
5000|$|External <b>validation</b> <b>checks</b> {{whether the}} {{experimental}} {{results can be}} generalized.|$|E
50|$|So {{what is a}} ‘query’? A query is {{an error}} {{generated}} when a validation check detects {{a problem with the}} data. <b>Validation</b> <b>checks</b> are run automatically whenever a page is saved “submitted” and can identify problems with a single variable, between two or more variables on the same eCRF page, or between variables on different pages. A variable can have multiple <b>validation</b> <b>checks</b> associated with it.|$|E
5000|$|Online <b>validation</b> <b>checking</b> {{and details}} of Unique Master Citizen Number ...|$|R
50|$|An {{example of}} a <b>validation</b> <b>check</b> is the {{procedure}} used to verify an ISBN.|$|R
5000|$|For {{received}} PDUs, iSCSI Engine core {{should do}} <b>validation</b> <b>check</b> to detect malformed PDU.|$|R
5000|$|Criterion <b>validation</b> <b>checks</b> how {{meaningful}} {{the research}} criteria are {{relative to other}} possible criteria. When the criterion is collected later {{the goal is to}} establish predictive validity.|$|E
50|$|Failures or {{omissions}} in {{data validation}} {{can lead to}} data corruption or a security vulnerability. Data <b>validation</b> <b>checks</b> that data are valid, sensible, reasonable, and secure before they are processed.|$|E
5000|$|... data dictionary: Terms, definitions, naming {{conventions}} {{and one or}} more representations of the data elements in a computer system. Data dictionaries often define data types, <b>validation</b> <b>checks</b> such as enumerated values and the formal definitions {{of each of the}} enumerated values.|$|E
30|$|Although these methodologies for calculating pore {{pressure}} were developed for passive margins where the {{maximum principal stress}} is mostly the vertical, a <b>validation</b> <b>check</b> was conducted in areas with directly measured {{pore pressure}} and negligible error was found.|$|R
50|$|A {{validation}} process involves two distinct steps: (a) <b>Validation</b> <b>Check</b> and (b) Post-Check action. The check step uses {{one or more}} computational rules (see section below) {{to determine if the}} data is valid. The Post-validation action sends feedback to help enforce validation.|$|R
5000|$|Ability {{to create}} both simple and complex <b>validation</b> edit <b>checks</b> and data derivations ...|$|R
5000|$|Construct <b>validation</b> <b>checks</b> what {{underlying}} construct {{is being}} measured. There are three variants of construct validity: convergent validity (how well the research relates to {{other measures of}} the same construct), discriminant validity (how poorly the research relates to measures of opposing constructs), and nomological validity (how well the research relates to other variables as required by theory).|$|E
5000|$|<b>Validation</b> <b>checks</b> the {{accuracy}} of the model's representation of the real system. Model validation is defined to mean [...] "substantiation that a computerized model within its domain of applicability possesses a satisfactory range of accuracy consistent with the intended application of the model". A model should be built for a specific purpose or set of objectives and its validity determined for that purpose.|$|E
50|$|INP validates each {{data field}} {{as it is}} entered. Validation types vary from simple alphabetic/numeric through ranges of numbers to tables of all {{allowable}} values. If a field is incorrect INP displays the validation criteria, beeps, and returns you {{to the start of}} the offending field. A mechanism exists to override <b>validation</b> <b>checks.</b> An optional audit trail of changes is maintained for reference and recovery.|$|E
30|$|A {{step towards}} a {{solution}} would be if organizations dealing with GI cancers create a framework for biological studies and for clinical trials dealing with tumor entities. This, together with integrating other variables such as providing raw data with a <b>validation</b> <b>check</b> would be helpful. The replacement of cell lines by patient-derived xenografts (PDX) for in vitro studies may significantly enhance the value and rigor of basic and translational research.|$|R
50|$|In 2013 Style Intelligence 11.4 was released. New {{features}} include: Viewsheet Annotation, Shared Bookmarks, Discrete Measure Representation, Independent Visual Binding for Chart Measures, Dynamic Sorting for Viewsheet Chart Axes, Screen Layout Guide for Viewsheets, Data Model <b>Validation</b> <b>Checking,</b> Metadata Refresh for Individual Data Sources, Restructured Tabs in Enterprise Manager, Worksheet Organization in Enterprise Manager, Script Access to Viewsheet Component Position, Search Access to Non-Archived Reports, and Depreciation of Portlet Dashboards.|$|R
40|$|We {{introduce}} {{the notion of}} persistent authenticated dictionaries, that is, dictionaries where the user can make queries of the type "was element e in set S at time t?" and get authenticated answers. Applications include credential and certificate <b>validation</b> <b>checking</b> in the past (as in digital signatures for electronic contracts), digital receipts, and electronic tickets. We present two data structures that can efficiently support an infrastructure for persistent authenticated dictionaries, and we compare their performance...|$|R
5000|$|Good quality {{source data}} {{has to do}} with “Data Quality Culture” and must be {{initiated}} {{at the top of the}} organization. It is not just a matter of implementing strong <b>validation</b> <b>checks</b> on input screens, because almost no matter how strong these checks are, they can often still be circumvented by the users. There is a nine-step guide for organizations that wish to improve data quality: ...|$|E
5000|$|The data {{must remain}} {{meaningful}} {{for the application}} logic. For example, if elements of addresses are to be obfuscated and city and suburbs are replaced with substitute cities or suburbs, then, if within the application there is a feature that validates postcode or post code lookup, that function must still be allowed to operate without error and operate as expected. The same is also true for credit-card algorithm <b>validation</b> <b>checks</b> and Social Security Number validations.|$|E
50|$|The GStat {{architecture}} makes a {{clear separation}} between data, infrastructure monitoring, content validation and visualization. At the core is the data model used {{to maintain a}} snapshot of the information system and a cache of the main entities. Probes are used to monitor the information system components and <b>validation</b> <b>checks</b> are used ensure that the information content is correct. A visualization framework is used for displaying the resulting data. The modular approach enables the software to be reused in other application scenarios.|$|E
40|$|Introduction: The {{performance}} for CPU-time consumption and the required storage are checked with {{the accuracy of}} calculation {{for the application of}} the higher order Born approximation to Indirect Boundary Element Method, that is solved by the conjugate gradient approach with elimination of small matrix elements. The sample problem for <b>validation</b> <b>check</b> is the seismic response of 3 D valley with the oblique incidence of SH-wave. It is shown that the third orde...|$|R
40|$|Abstract. We {{introduce}} {{the notion of}} persistent authenticated dictionaries, that is, dictionaries where the user can make queries of the type “was element e in set S at time t? ” and get authenticated answers. Applications include credential and certificate <b>validation</b> <b>checking</b> in the past (as in digital signatures for electronic contracts), digital receipts, and electronic tickets. We present two data structures that can efficiently support an infrastructure for persistent authenticated dictionaries, and we compare their performance. ...|$|R
40|$|Project (M. S., Computer Science) [...] California State University, Sacramento, 2011 Most modern web {{applications}} rely on retrieving updated {{data from}} a database. In response to a request from a web page, the application will generate a SQL query, and often incorporate portions of the user input into the query. SQL injection refers to injecting crafted malicious SQL query segments to change the intended effect of a SQL query. The hacker could access unauthorized data, or even gain complete control over the web server or back-end database system. SQL injection attack {{has become one of}} the top web application vulnerabilities. In this project, I surveyed different types of SQL injection attacks and the corresponding countermeasure strategies proposed by other researchers. A new technique to detect and prevent SQL injection attacks is presented; the basic idea is to insert a validation process between the generation of SQL query and the query execution. The technique consists of both static analysis of web application code and runtime <b>validation</b> <b>check</b> of dynamically generated SQL query. Following four steps are involved: Identify hotspot; analyze SQL query; initialization; and runtime <b>validation</b> <b>check.</b> The project was implemented using JAVA. Performance evaluation was also conducted. Computer Scienc...|$|R
50|$|When the LF files a {{document}} to the Court, the submission is actually {{received by the}} GW which then performs certain validations, computes the fees to be charged and identifies to which user department the submission is to be routed to. Replies from the Court are received by the GW and routed to the correct LF for retrieval. The GW performs the following crucial functions:(a) Automated <b>validation</b> <b>checks</b> when documents are filed;(b) Implementation of certain special rules;(c) Automated routing of submissions into Courts’ in-trays;(d) Computation of stamp and other filing fees;(e) Exchange of information between the Back End and the Front End.|$|E
50|$|The {{concept of}} a code cave is often used by hackers to execute {{arbitrary}} code in a compiled program. It can be an extremely helpful tool to make additions and removals to a compiled program including the addition of dialog boxes, variable modification, or removal of software key <b>validation</b> <b>checks.</b> Often using a call instruction commonly found on many CPUs, the code jumps to the new subroutine and pushes the next address onto the stack. After execution of the subroutine, a return instruction {{can be used to}} pop the previous location off of the stack into the program counter. This allows the existing program to jump to the newly added code without making significant changes to the program itself.|$|E
5000|$|It is programmer-intensive. XML schemas are {{notoriously}} tricky to write by hand, a recommended {{approach is to}} create them by defining relational tables, generating XML-schema code, and then dropping these tables. This is problematic in many production operations involving dynamic schemas, where new attributes are required to be defined by power-users who understand a specific application domain (e.g. inventory management or biomedicine) but are not necessarily programmers. By contrast, in production systems that use EAV, such users define new attributes (and the data-type and <b>validation</b> <b>checks</b> associated with each) through a GUI application. Because the validation-associated metadata is required to be stored in multiple relational tables in a normalized design, a GUI application that ties these tables together and enforces the appropriate metadata-consistency checks is the only practical way to allow entry of attribute information, even for advanced developers - even if the end-result uses XML or JSON instead of separate relational tables.|$|E
40|$|This paper {{deals with}} the quality {{evaluation}} (validation) and improvement of Spoken Language Resources (SLR). We discuss a number of aspects of SLR validation. We review the work done so far in this field. The most important <b>validation</b> <b>check</b> points and our view on their rank order are listed. We propose a strategy for validation and improvement of SLR that is presently considered at the European Language Resources Association, ELRA. And finally, we show some of our future plans in these directions. 1...|$|R
40|$|In {{this paper}} we present an {{approach}} towards stringmatching patterns through an application that aims at detectingvarious patterns comprising of varied string combinations. Theapplication highlights the various facets of pattern detectionrequirements and thus aims at finding the occurrences of thosepatterns {{in the actual}} string along with its frequency ofoccurrence. The <b>validation</b> <b>check</b> is also performed by using thetool to identify whether the entered pattern string complies withthe accepted standard set. The tool as a whole provides acommon place for all the features to be used out so thatnavigation to different sources is not required...|$|R
50|$|However, {{since the}} {{executable}} is signed, simply changing {{the value of}} the flag is not possible as this alters the signature of the executable causing it to fail <b>validation</b> when <b>checked.</b>|$|R
50|$|Software <b>validation</b> <b>checks</b> {{that the}} {{software}} product satisfies or fits {{the intended use}} (high-level checking), i.e., the software meets the user requirements, not as specification artifacts or as needs of those who will operate the software only; but, as {{the needs of all}} the stakeholders (such as users, operators, administrators, managers, investors, etc.). There are two ways to perform software validation: internal and external. During internal software validation it is assumed that the goals of the stakeholders were correctly understood and that they were expressed in the requirement artifacts precise and comprehensively. If the software meets the requirement specification, it has been internally validated. External validation happens when it is performed by asking the stakeholders if the software meets their needs. Different software development methodologies call for different levels of user and stakeholder involvement and feedback; so, external validation can be a discrete or a continuous event. Successful final external validation occurs when all the stakeholders accept the software product and express that it satisfies their needs. Such final external validation requires the use of an acceptance test which is a dynamic test.|$|E
50|$|The {{method is}} to check that data falls the {{appropriate}} parameters defined by the systems analyst. A judgement as to whether data is valid is {{made possible by the}} validation program, but it cannot ensure complete accuracy. This can only be achieved through the use of all the clerical and computer controls built into the system at the design stage. The difference between data validity and accuracy can be illustrated with a trivial example. A company has established a Personnel file and each record contains a field for the Job Grade. The permitted values are A, B, C, or D. An entry in a record may be valid and accepted by the system if it is one of these characters, {{but it may not be}} the correct grade for the individual worker concerned. Whether a grade is correct can only be established by clerical checks or by reference to other files. During systems design, therefore, data definitions are established which place limits on what constitutes valid data. Using these data definitions, a range of software <b>validation</b> <b>checks</b> can be carried out.|$|E
30|$|Consequently, {{they decided}} to add <b>validation</b> <b>checks</b> for configurations, improve detection, and {{diagnose}} service failure.|$|E
50|$|Requirements <b>validation</b> - <b>Checking</b> {{that the}} {{documented}} requirements and models are consistent {{and meet the}} needs of the stakeholder. Only if the final draft passes the validation process, the RS becomes official.|$|R
40|$|Given {{a regular}} {{language}} L, the Brzozowski derivative of L {{with respect to}} some string s is a regular expression which defines what strings can follow s in strings appearing in L. The calculation of Brzozowski derivatives is a convenient and beautiful tool for the algebraic manipulation of content models. It can provide convenient solutions to several problems arising in developing, manipulating, and validating content models in W 3 C XML Schema: <b>validation,</b> <b>checking</b> the unique-particle-attribution constraint, and testing whether the language accepted by one content model is a subset of that accepted by another. Applications of Brzozowski derivatives to XML Schema processin...|$|R
30|$|<b>Validation</b> process <b>checks</b> if {{the model}} {{accurately}} represents the real device {{from the perspective}} of the intended use of the model [14]. It consists of comparing the model simulation results with the device measurements.|$|R
