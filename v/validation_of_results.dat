288|10000|Public
5000|$|Artifact Evaluation [...] - {{collaborative}} <b>validation</b> <b>of</b> <b>results</b> from published {{papers at}} the computer systems conferences.|$|E
50|$|For {{train and}} road traffic noise, the {{description}} of the sources is usually made in terms of known parameters, such as speed, number of vehicles etc. Measurements are used for the <b>validation</b> <b>of</b> <b>results.</b>|$|E
5000|$|SYBR Green: [...] When the SYBR Green binds to the {{double-stranded}} DNA of the PCR products, it will emit light upon excitation. The {{intensity of}} the fluorescence increases as the PCR products accumulate. This technique is easy to use since designing of probes is not necessary given lack of specificity of its binding. However, since the dye does not discriminate the double-stranded DNA from the PCR products and those from the primer-dimers, overestimation of the target concentration is a common problem. Where accurate quantification is an absolute necessity, further assay for the <b>validation</b> <b>of</b> <b>results</b> must be performed. Nevertheless, amongst the real-time RT-PCR product detection methods, SYBR Green is the most economical and easiest to use. TaqMan Probes: [...] TaqMan probes are oligonucleotides that have a fluorescent probe attached to the 5' end and a quencher to the 3' end. During PCR amplification, these probes will hybridize to the target sequences located in the amplicon and as polymerase replicates the template with TaqMan bound, it also cleaves the fluorescent probe due to polymerase 5'- nuclease activity. Because the close proximity between the quench molecule and the fluorescent probe normally prevents fluorescence from being detected through FRET, the decoupling results in the increase of intensity of fluorescence proportional {{to the number of}} the probe cleavage cycles. Although well-designed TaqMan probes produce accurate real-time RT-PCR results, it is expensive and time-consuming to synthesize when separate probes must be made for each mRNA target analyzed.|$|E
30|$|Statistical <b>validation</b> <b>of</b> {{research}} <b>results.</b>|$|R
30|$|The <b>validation</b> <b>of</b> tool <b>results</b> {{was carried}} out by {{comparing}} simulation results with test data [11, 12].|$|R
50|$|The {{technique}} is sometimes complemented or combined with viscometry and polystyrene standards {{are available for}} <b>validation</b> <b>of</b> the <b>results.</b>|$|R
50|$|For any system, one of {{the first}} tasks of {{reliability}} engineering is to adequately specify the reliability and maintainability requirements allocated from the overall availability needs and, more importantly, derived from proper design failure analysis or preliminary prototype test results. Clear requirements (able to designed to) should constrain the designers from designing particular unreliable items / constructions / interfaces / systems. Setting only availability, reliability, testability, or maintainability targets (e.g., max. failure rates) is not appropriate. This is a broad misunderstanding about Reliability Requirements Engineering. Reliability requirements address the system itself, including test and assessment requirements, and associated tasks and documentation. Reliability requirements are included in the appropriate system or subsystem requirements specifications, test plans, and contract statements. Creation of proper lower-level requirements is critical.Provision of only quantitative minimum targets (e.g., MTBF values or failure rates) is not sufficient for different reasons. One reason is that a full validation (related to correctness and verifiability in time) of a quantitative reliability allocation (requirement spec) on lower levels for complex systems can (often) not be made as a consequence of (1) the fact that the requirements are probabilistic, (2) the extremely high level of uncertainties involved for showing compliance with all these probabilistic requirements, and because (3) reliability is a function of time, and accurate estimates of a (probabilistic) reliability number per item are available only very late in the project, sometimes even after many years of in-service use. Compare this problem with the continuous (re-)balancing of, for example, lower-level-system mass requirements in the development of an aircraft, which is already often a big undertaking. Notice that in this case masses do only differ in terms of only some %, are not a function of time, the data is non-probabilistic and available already in CAD models. In case of reliability, the levels of unreliability (failure rates) may change with factors of decades (multiples of 10) as result of very minor deviations in design, process, or anything else. The information is often not available without huge uncertainties within the development phase. This makes this allocation problem almost impossible to do in a useful, practical, valid manner that does not result in massive over- or under-specification. A pragmatic approach is therefore needed—for example: the use of general levels / classes of quantitative requirements depending only on severity of failure effects. Also, the <b>validation</b> <b>of</b> <b>results</b> is a far more subjective task than for any other type of requirement. (Quantitative) reliability parameters—in terms of MTBF—are by far the most uncertain design parameters in any design.|$|E
30|$|Soda {{wind farm}} {{used in this}} case study {{is located in the}} Jaisalmer {{district}} of western Rajasthan in India. Modeling of wind speed probability and power curve of wind turbines installed at Soda site were done for analytically estimating the capacity factor of wind turbine. Estimated capacity factors were then compared with the measured values of wind farm for <b>validation</b> <b>of</b> <b>results.</b>|$|E
40|$|Monte-Carlo {{arithmetic}} {{is a form}} of self-validating arithmetic {{that accounts}} for the effect of rounding errors. We have implemented a floating point unit that can perform either IEEE 754 or Monte-Carlo floating point computation, allowing hardware accelerated <b>validation</b> <b>of</b> <b>results</b> during execution. Experiments show that our approach has a modest hardware overhead and allows the propagation of rounding error to be accurately estimated...|$|E
50|$|Genetic {{correlations}} have {{applications in}} <b>validation</b> <b>of</b> GWAS <b>results,</b> breeding, prediction <b>of</b> traits, and discovering {{the etiology of}} traits & diseases.|$|R
5000|$|Artifact Evaluation [...] - {{collaborative}} <b>validation</b> <b>of</b> experimental <b>results</b> from published {{papers at}} the leading ACM and IEEE computer systems conferences ...|$|R
30|$|<b>Validation</b> <b>of</b> AST <b>results</b> on {{nutrient}} agar (NA) medium {{used as a}} substitute for MHA by some microbiology laboratories in Alexandria, Egypt.|$|R
40|$|The Bachelor´s thesis {{focuses on}} a static {{solution}} of reinforced concrete cylindrical tank. The thesis comprises dimensioning of {{the bottom of the}} tank and its walls to selected load cases and processing a drawing documentation. The calculation of internal forces is done using the software Scia Engineer. The <b>validation</b> <b>of</b> <b>results</b> is performed by hand calculation using differential equations on a simplified model for the results of internal forces in the wall of the tank...|$|E
40|$|The paper {{presents}} an overview and validation of a Navier-Stokes solver included in Flow- 3 D (from Flow Science Inc.) for the computation {{of the total}} resustance and flow characteristics of and advancing floating hull in otherwise calm real fluid. The choice of a Wigley hull in fixed conditions {{for a range of}} Froude Number allos for direct <b>validation</b> <b>of</b> <b>results</b> with experimental data obtained in the literature. Peer reviewed: NoNRC publication: Ye...|$|E
40|$|The {{techniques}} {{involved in}} the development of the brain computer interfaces require the <b>validation</b> <b>of</b> <b>results</b> on data recorded from a number of channels for a number of repetitions. But the sheer volume of data in this case calls for a simpler approach and the necessary skill to handle the data. The paper discusses the various techniques such as common spatial patterns (CSP) and evaluates the performance of these techniques in the light of BCI goals...|$|E
40|$|This {{deliverable}} {{reports the}} <b>results</b> <b>of</b> empirical evaluation activities undertaken by S-Cube partners for the <b>validation</b> <b>of</b> research <b>results.</b> The systematic guidelines and templates {{used for the}} documentation <b>of</b> <b>validation</b> <b>results</b> and their related aspects are detailed. The initial set <b>of</b> collected <b>validation</b> <b>results,</b> documented according to these guidelines, is then presented. Finally, planned, upcoming evaluation activities are introduced. S-Cub...|$|R
50|$|Due to {{the absence}} of another antibody-based method for R-loop immunoprecipitation, <b>validation</b> <b>of</b> DRIP-seq <b>results</b> is difficult. However, <b>results</b> <b>of</b> other R-loop {{profiling}} methods, such as DRIVE-seq, may be used to measure consensus.|$|R
5000|$|... {{the need}} for <b>validation</b> and {{verification}} <b>of</b> <b>results</b> against performance targets.|$|R
40|$|This paper {{presents}} {{the effectiveness of}} perceptual features and iterative clustering approach for performing both speech and speaker recognition. Procedure used for formation of training speech is different for developing training models for speaker independent speech and text independent speaker recognition. So, this work mainly emphasizes the utilization of clustering models developed for the training data to obtain better accuracy as 91 %, 91 % and 99. 5 % for mel frequency perceptual linear predictive cepstrum with respect to three categories such as speaker identification, isolated digit recognition and continuous speech recognition. This feature also produces 9 % as low equal error rate which {{is used as a}} performance measure for speaker verification. The work is experimentally evaluated on the set of isolated digits and continuous speeches from TI digits_ 1 and TI digits_ 2 database for speech recognition and on speeches of 50 speakers randomly chosen from TIMIT database for speaker recognition. The noteworthy feature of speaker recognition algorithm is to evaluate the testing procedure on identical messages of all the 50 speakers, 2 theoretical <b>validation</b> <b>of</b> <b>results</b> using F-ratio and <b>validation</b> <b>of</b> <b>results</b> by statistical analysis using χ distribution...|$|E
40|$|Abstract — A two pronged strategy, one {{involving}} the Support Vector Machine (SVM) as the classifier {{and the other}} including physicochemical properties as additional features, is proposed and implemented here for improved prediction of multi-domains in protein chains. It is experimentally observed to have achieved an accuracy of 76. 46 after 25 fold cross <b>validation</b> <b>of</b> <b>results</b> on curated data, derived from CATH database. Index Terms — Protein domain boundary, physicochemical properties, conformational flexibility, amino acid linker index, linker region. I...|$|E
30|$|MP was {{the lead}} {{developer}} of the emergency smart phone app and web app. He also coordinated the integration of results into the production environment of the emergency services and cooperated with Slovenian Administration for Civil Protection and Disaster Relief on the <b>validation</b> <b>of</b> <b>results.</b> MŠ worked on the access time calculations and statistical analysis. He also performed {{the review of the}} related work and wrote most of the paper contents. Both authors jointly worked on common operational picture. Both authors also read and reviewed this manuscript.|$|E
30|$|The {{numerical}} {{solutions are}} presented through graphs for physical {{interpretation of the}} proposed study. For the <b>validation</b> <b>of</b> numerical <b>results,</b> we compared our results with previously published works with {{the absence of the}} curvature parameter and dust particles.|$|R
30|$|Algorithm 1 shows a {{pseudocode}} {{representation of}} the quasi-static schedule employed to coordinate execution of the DTSMD system. Here, the File Writer actor is added to write the output results to a text file for convenience in interpretation and <b>validation</b> <b>of</b> the <b>results.</b>|$|R
30|$|The {{non-parametric}} missForest {{technique for}} matrix completion (Stekhoven and Bühlmann 2012) {{is based on}} averaging over a random forest of regression trees and was used for additional <b>validation</b> <b>of</b> the <b>results.</b> Trees were built based on observed and bootstrapped parts of the training data set.|$|R
40|$|This review takes a sceptical view of {{the impact}} of breast cancer studies that have used microarrays to {{identify}} predictors of clinical outcome. In addition to discussing general pitfalls of microarray experiments, we also critically review the key breast cancer studies to highlight methodological problems in cohort selection, statistical analysis, <b>validation</b> <b>of</b> <b>results</b> and reporting of raw data. We conclude that the optimum use of microarrays in clinical studies requires further optimisation and standardisation of methodology and reporting, together with improvements in clinical study design...|$|E
40|$|Tensegrity-based {{structures}} hold {{promise for}} the field of lightweight, compliant robots. However, most prior efforts to model and plan the shape of these structures have focused on special cases or on static structures. This work attempts to generalize a dynamic-relaxation form-finding method into a form that provides solutions to problems analogous to Forward and Inverse Kinematics problems in serial-chain robots. Details of the implementation of these algorithms into a model tensegrity robot are also presented, as well as suggestions for experimental <b>validation</b> <b>of</b> <b>results...</b>|$|E
40|$|We present herein {{a robust}} {{algorithm}} for cell tracking {{in a sequence}} of time-lapse 2 -D fluorescent microscopy images. Tracking is performed automatically via a multiphase active contours algorithm adapted to the segmentation of clustered nuclei with obscure boundaries. An ellipse fitting method is applied to avoid problems typically associated with clustered, overlapping, or dying cells, and to obtain more accurate segmentation and tracking results. We provide quantitative <b>validation</b> <b>of</b> <b>results</b> obtained with this new algorithm by comparing them to the results obtained from the established CellProfiler, MTrack 2 (plugin for Fiji), and LSetCellTracker software...|$|E
40|$|Abstract − The {{software}} <b>validation</b> <b>of</b> {{measuring instruments}} {{is a complex}} procedure, which {{can be divided into}} the <b>validation</b> <b>of</b> separate software functions. The paper deals with the software <b>validation</b> <b>of</b> functions for serial line communication. It describes methods for preparation and validation. Methods are based on demands and facts, which are important for measuring instruments under legal control. They were developed during type approval process of an automated liquid level measuring instrument, but they can be easily adopted and used with other measuring instruments or communication interfaces. At the end <b>of</b> <b>validation</b> procedure, <b>results</b> <b>of</b> methods are gathered together and evaluated...|$|R
40|$|A {{problem which}} appears quite {{often in the}} <b>validation</b> <b>of</b> the <b>results</b> <b>of</b> {{numerical}} algorithms using intervals is the branching based on the comparison of two intervals when these intervals have a noon-empty intersection. This paper deal with some possibility of ordering intervals with some probability and a techniqu...|$|R
30|$|In time domain, the <b>validation</b> <b>of</b> the <b>results</b> {{with the}} real {{recordings}} {{will be carried out}} using kurtosis [19]. Although the true kurtosis value of the atrial component is unknown, a large value of kurtosis is associated with remaining QRST complexes and consequently implies a poor extraction.|$|R
40|$|The {{flow field}} in the {{underfloor}} region of a high speed train is investigated by means of Particle Image Velocimentry (PIV) measurements in a full scale test. The results {{provide a basis for}} the <b>validation</b> <b>of</b> <b>results</b> from numerical simulations as well as from scaled experiments in wind tunnels and moving model experiments. This paper presents the first time PIV measurement performed in the field on a full scale running train. Several aspects and difficulties connected to such a measurement under non-laboratory conditions will be addressed. First resulting velocity fields will be presente...|$|E
40|$|Individuals with Autism Spectrum Disorders (ASD) have {{deficits in}} the {{recognition}} of facial expressions when compared with typically developing population (Begger et al., 2006 & Deruelle et al., 2004). Computer training and multi-technology {{have been shown to}} be successful in teaching emotional skills to children with autism (Golan & Baron-Cohen, 2006; Silver & Oakes, 2001; Tanaka et al., 2010). We can find numerous educational technological resources that aim to teach facial recognition to people with ASD, but few present empirical <b>validation</b> <b>of</b> <b>results.</b> The majority of these techniques are non-standardize...|$|E
40|$|Considerable {{effort is}} being spent on {{improving}} the quality and nature of teacher assessment practices. Attitudes towards, beliefs about, and intentions concerning any psychological object are strong predictors of human action and behaviour vis a vis that object (Ajzen, 1991). Therefore, understanding how teachers think about assessment {{is an important factor}} in determining how assessment reforms are implemented in school contexts. Inferences about human thinking should not be a function of data collection methods; hence, <b>validation</b> <b>of</b> <b>results</b> across methods is an important aspect to improving our understanding of the structure of conceptions...|$|E
30|$|PC {{carried out}} the {{experimental}} work and wrote the manuscript. AG, CR, ZR participated in study design and coordination, {{as well as the}} <b>validation</b> <b>of</b> experimental <b>results.</b> HM and SVG contributed to the selection of the open source SCADA system. All authors have read and approved the final manuscript.|$|R
30|$|In {{order to}} achieve the {{above-mentioned}} goals, a methodology was developed and followed. The approach consisted of the following three steps: (a) desktop research of definitions, context and policy; (b) two rounds of Delphi study, conducted online; and (c) <b>validation</b> <b>of</b> the <b>results</b> by an expert focus workshop.|$|R
30|$|It is {{essential}} to stress that the MTT assay that reflects viable cell metabolism, providing only a general indication of a cell function (proliferation) per se. Therefore, in a future research using this assay, a cell genetic testing of micronucleus or DNA damage, should be considered for <b>validation</b> <b>of</b> the <b>results.</b>|$|R
