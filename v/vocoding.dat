90|137|Public
2500|$|Not all of {{the songs}} are mid-tempo or ballads, Bangladesh also {{produced}} a song called [...] "Let Me Go" [...] which Hammp described as an [...] "up-tempo club song." [...] On the chorus, Norwood sings [...] "You know how I get when you let me go", {{and later in the}} song she makes reference to Twitter and her mother. The song interpolates [...] "Tonight" [...] by Swedish pop singer Lykke Li. Deluxe edition bonus track [...] "Can You Hear Me Now?”, producer Danja's sole contribution on Two Eleven, works up an extended musical foreplay around a single mind-numbing groove. Built around an instrumental that was originally produced for Diddy – Dirty Money's 2010 album Last Train to Paris, it was re-constructed by Love for Norwood. Danja used heavy <b>vocoding</b> during production of the song.|$|E
2500|$|The {{software}} instruments {{included in}} Logic Pro X include:Drum Kit Designer, Drum Machine Designer, ES, ES2, EFM1, ES E, ES M, ES P, EVOC 20 PolySynth, EXS24 mkII, Klopfgeist, Retro Synth, Sculpture, Ultrabeat, Vintage B3, Vintage Clav, Vintage Electric Piano. These instruments produce sound in various ways, through subtractive synthesis (ES, ES2, ES E, ES M, ES P, Retro Synth), [...] {{frequency modulation synthesis}} (EFM1), wavetable synthesis (ES2, Retro Synth), <b>vocoding</b> (EVOC 20 PolySynth), sampling (EXS24 mikII, Drum Kit Designer), and component modeling techniques (Ultrabeat, Vintage B3, Vintage Clav, and Vintage Electric Piano, Sculpture). As of version 10.2, Logic Pro X also includes Alchemy, a sample-manipulation synthesizer that was previously developed by Camel Audio. The software instruments are activated by MIDI {{information that can be}} inputed via a midi instrument or drawn into the midi editor.|$|E
2500|$|In August 2003 {{the band}} {{released}} Tour de France Soundtracks, its first album of new material since 1986's Electric Café. In January and February 2003, before {{the release of}} the album, the band started the extensive Minimum-Maximum world tour, using four customised Sony VAIO laptop computers, effectively leaving the entire Kling Klang studio at home in Germany. The group also obtained a new set of transparent video panels to replace its four large projection screens. This greatly streamlined the running of all of the group's sequencing, sound-generating, and visual-display software. From this point, the band's equipment increasingly reduced manual playing, replacing it with interactive control of sequencing equipment. Hütter retained the most manual performance, still playing musical lines by hand on a controller keyboard and singing live vocals and having a repeating ostinato. Schneider's live <b>vocoding</b> had been replaced by software-controlled speech-synthesis techniques. In November, the group made a surprising appearance at the MTV European Music Awards in Edinburgh, Scotland, performing [...] "Aerodynamik". The same year a promotional box set entitled 12345678 (subtitled The Catalogue) was issued, with plans for a proper commercial release to follow. [...] The box featured remastered editions of the group's eight core studio albums, from Autobahn to Tour de France Soundtracks. This long-awaited box-set would eventually be released in a different set of remasters in November 2009.|$|E
40|$|Exposure to audiovisually {{presented}} <b>vocoded</b> {{speech is}} more effective than exposure to auditory-only <b>vocoded</b> speech in improving the subsequent ability to understand <b>vocoded</b> speech [1]. In addition, improvements in the audiovisual training condition were more rapid and greater in magnitude than in the auditory-only condition. The current study was conducted to establish whether exposure to concurrent textual information also results in improvements in the ability to recognize <b>vocoded</b> speech. Baseline measures of identification performance with auditory-only <b>vocoded</b> speech sentences were assessed for 45 participants. Participants then performed a speech identification task, where they were exposed to <b>vocoded</b> speech in either audiovisual (Group 1), auditory-only (Group 2), or auditory-only with concurrent text conditions (Group 3). Following exposure, participants were tested again on identification performance with auditory-only <b>vocoded</b> speech. Exposure to concurrent text improved subsequent understanding of <b>vocoded</b> speech, to a level similar to that seen with audiovisual speech exposure. In a second experiment, groups of normal hearing adults were exposed to <b>vocoded</b> non-lexical nonsense words in auditory-only with text and audiovisual speech presentation conditions. Exposure to both nonsense audiovisual and concurrent text conditions improved subsequent understanding of lexical auditoryonly <b>vocoded</b> speech, and {{there was no difference between}} the levels of improvement. In summary, to computerbased audiovisual and concurrent text exposure improves the ability to recognize <b>vocoded</b> speech over exposure to auditory stimuli alone. This effect does not appear to be dependent on exposure to lexical items. 1...|$|R
5000|$|Sakamoto - <b>vocoded</b> vocals, piano, {{electric}} piano, percussion, orchestration ...|$|R
5000|$|Haruomi Hosono - bass, Synth bass, keyboards, <b>Vocoded</b> vocals, {{production}} ...|$|R
5000|$|MACHINE 2: Proto Unit V3: Hardwired polypohnic pop {{sequences}} and {{voltage control}} melody, female <b>vocoding</b> ...|$|E
50|$|FL Studio {{is bundled}} {{with a variety}} of sound {{processing}} effects, including common audio effects such as chorus, compression, delay, flanger, phaser, reverb, equalization, <b>vocoding,</b> maximization, and limiting.|$|E
50|$|The unit itself {{incorporates}} effects such as distortion, compression, {{noise and}} curve generators, ring modulation, <b>vocoding,</b> parametric equalizers, tremolo, reverb and delay. The {{effects can be}} routed in a large variety of ways, {{and it is possible}} to modify different parameters.|$|E
50|$|CubeBreath by xoxos. Audio input is <b>vocoded</b> in {{tune with}} the music.|$|R
5000|$|Röyksopp - production, programming, synthesiser, {{sampling}} vocals strings, string arrangements <b>vocoded</b> mystique bass ...|$|R
5000|$|Tom Beaufoy - keyboards, turntables, programming, samples, <b>vocoded</b> vocals on [...] "They Live" ...|$|R
50|$|The song is also memorable for the <b>vocoding</b> effects {{given to}} the voice during the final chorus, which allow it to sound {{something}} like a robot. These effects were {{the subject of a}} protest by the Russian delegation, however they were not found to be against the rules and the result stood.|$|E
50|$|In some instances, {{most notably}} The Chipmunks, {{manipulation}} of voices may be employed, either {{to achieve a}} desired vocal effect, or to make it dissimilar {{to the voice of}} the actual singer. The manipulation is done by either modifying the playback speed of the vocal track or by putting it through a synthesizer (<b>Vocoding).</b>|$|E
50|$|The Korg VC-10 is an {{analogue}} vocoder {{launched in}} 1978. <b>Vocoding</b> refers to voice encoding {{of speech and}} singing with musical synthesis. It gained popularity in the 1970s following utilisation by bands such as Kraftwerk and Electric Light Orchestra. The VC-10 allows basic functionality in operation and modulation of signal carriers. It has two microphone input options.|$|E
5000|$|Armand Van Helden- Why Can't U Free Some Time (Superchumbo's <b>Vocode</b> Mix) FFRR 2001 ...|$|R
40|$|Attending to one {{speaker in}} multi-speaker {{situations}} is challenging. One neural mechanism proposed to underlie {{the ability to}} attend to a particular speaker is phase-locking of lowfrequency activity in auditory cortex to speech's temporal envelope (“speech-tracking”), which is more precise for attended speech. However, {{it is not known}} what brings about this attentional effect, and specifically if it reflects enhanced processing of the fine structure of attended speech. To investigate this question we compared attentional effects on speechtracking of natural versus <b>vocoded</b> speech which preserves the temporal envelope but removes the fine structure of speech. Pairs of natural and <b>vocoded</b> speech stimuli were presented concurrently and participants attended to one stimulus and performed a detection task while ignoring the other stimulus. We recorded magnetoencephalography (MEG) and compared attentional effects on the speech-tracking response in auditory cortex. Speech-tracking of natural, but not <b>vocoded,</b> speech was enhanced by attention, whereas neural tracking of ignored speech was similar for natural and <b>vocoded</b> speech. These findings suggest that the more precise speech-tracking of attended natural speech is related to processing its fine structure, possibly reflecting the application of higher-order linguistic processes. In contrast, when speech is unattended its fine structure is not processed to the same degree and thus elicits less precise speech-tracking more similar to <b>vocoded</b> speech...|$|R
5000|$|The song {{features}} a heavily <b>vocoded</b> voice singing the phrase [...] "Mr. Blue Sky". A second <b>vocoded</b> segment {{at the end}} of the song was often interpreted as [...] "Mister Blue Sky"; it is actually [...] "Please turn me over" [...] as it is the end of side three, and the listener is being instructed to flip the LP over. This was confirmed by Jeff Lynne on 3 October 2012 on The One Show.|$|R
50|$|Electro (or electro-funk) is a {{genre of}} {{electronic}} music and early hip hop directly {{influenced by the}} use of the Roland TR-808 drum machines, and funk. Records in the genre typically feature drum machines and heavy electronic sounds, usually without vocals, although if vocals are present they are delivered in a deadpan manner, often through electronic distortion such as <b>vocoding</b> and talkboxing. This is the main distinction between electro and previously prominent genres such as disco, in which the electronic sound was only part of the instrumentation. It also palpably deviates from its predecessor boogie for being less vocal-oriented and more focused on electronic beats produced by drum machines.|$|E
5000|$|Directstep, {{released}} only in Japan, {{was one of}} {{the earliest}} albums ever released on CD. Webster Lewis became second keyboardist on this album in order for Hancock to handle the multiple layers of electronic texture that he hoped to achieve. Hancock re-recorded [...] "I Thought It Was You" [...] (originally on Sunlight), making it even more electronic with his <b>vocoding.</b> [...] "Butterfly" [...] was also re-recorded (originally on Thrust) making Directstep the second album after the original version (the first being Flood), to have a rendition of [...] "Butterfly". (The fourth would be Dis Is da Drum and the tune is also featured on Kimiko Kasai's LP, Butterfly, which Herbie plays on.) [...] "Shiftless Shuffle" [...] would later be re-recorded for 1980's Mr. Hands.|$|E
50|$|Sonarr and Storm The Studio were unusual {{programs}} {{in that they}} featured two live studios patched in so that they both could be on air simultaneously, with the second studio being used to remix listener phone calls. These were then sampled and added to the dance mix. This {{was known as the}} 'Transformation Line' and was introduced with a weird robotic voice that was generated using a Commodore 64. An old analogue <b>vocoding</b> keyboard was also used at times, as was an Ensoniq EPS 16+ sampler and a Casio FZ 1 sampler. Live beats and loops were often added to the Transformation line and live remixes of dance tracks were also produced. This had never been done before in Melbourne.|$|E
40|$|The {{benefits}} of combined electric and acoustic stimulation (EAS) {{in terms of}} speech recognition in noise are well established; however the underlying factors responsible for this benefit are not clear. The present study tests the hypothesis that having access to acoustic information in the low frequencies {{makes it easier for}} listeners to glimpse the target. Normal-hearing listeners were presented with <b>vocoded</b> speech alone (V), low-pass (LP) filtered speech alone, combined <b>vocoded</b> and LP speech (LP+V) and with <b>vocoded</b> stimuli constructed so that the low-frequency envelopes were easier to glimpse. Target speech was mixed with two types of maskers (steady-state noise and competing talker) at − 5 to 5 dB signal-to-noise ratios. Results indicated no advantage of LP+V in steady noise, but a significant advantage over V in the competing talker background, an outcome consistent with the notion that it is easier for listeners to glimpse the target in fluctuating maskers. A significant improvement in performance was noted with the modified glimpsed stimuli over the original <b>vocoded</b> stimuli. These findings taken together suggest that a significant factor contributing to the EAS advantage is the enhanced ability to glimpse the target...|$|R
40|$|Objectives: The {{purpose of}} this study is to {{evaluate}} the performance of a number of speech intelligibility indices in terms of predicting the intelligibility of <b>vocoded</b> speech. Design: Noise-corrupted sentences were <b>vocoded</b> in a total of 80 conditions, involving three different signal-to-noise ratio levels (� 5, 0, and 5 dB) and two types of maskers (steady state noise and two-talker). Tone-vocoder simulations and combined electric-acoustic stimulation (EAS) simulations were used. The <b>vocoded</b> sentences were presented to normal-hearing listeners for identification, and the resulting intelligibility scores were used to assess the correlation of various speech intelligibility measures. These included measures designed to assess speech intelligibility, including the speech transmission index (STI) and articulation index based measures, as well as distortions in hearing aids (e. g., coherence-based measures). These measures employed primarily either the temporal-envelope or the spectral-envelope information in th...|$|R
5000|$|A second edit of {{the track}} [...] "Monday" [...] appears on the album while(1<2). Arriving at two minutes is a <b>vocoded</b> edit of the {{original}} vocals.|$|R
50|$|The {{software}} instruments {{included in}} Logic Pro X include:Drum Kit Designer, Drum Machine Designer, ES, ES2, EFM1, ES E, ES M, ES P, EVOC 20 PolySynth, EXS24 mkII, Klopfgeist, Retro Synth, Sculpture, Ultrabeat, Vintage B3, Vintage Clav, Vintage Electric Piano. These instruments produce sound in various ways, through subtractive synthesis (ES, ES2, ES E, ES M, ES P, Retro Synth), {{frequency modulation synthesis}} (EFM1), wavetable synthesis (ES2, Retro Synth), <b>vocoding</b> (EVOC 20 PolySynth), sampling (EXS24 mikII, Drum Kit Designer), and component modeling techniques (Ultrabeat, Vintage B3, Vintage Clav, and Vintage Electric Piano, Sculpture). As of version 10.2, Logic Pro X also includes Alchemy, a sample-manipulation synthesizer that was previously developed by Camel Audio. The software instruments are activated by MIDI {{information that can be}} inputed via a midi instrument or drawn into the midi editor.|$|E
5000|$|Not all of {{the songs}} are mid-tempo or ballads, Bangladesh also {{produced}} a song called [...] "Let Me Go" [...] which Hammp described as an [...] "up-tempo club song." [...] On the chorus, Norwood sings [...] "You know how I get when you let me go", {{and later in the}} song she makes reference to Twitter and her mother. The song interpolates [...] "Tonight" [...] by Swedish pop singer Lykke Li. Deluxe edition bonus track [...] "Can You Hear Me Now?”, producer Danja's sole contribution on Two Eleven, works up an extended musical foreplay around a single mind-numbing groove. Built around an instrumental that was originally produced for Diddy - Dirty Money's 2010 album Last Train to Paris, it was re-constructed by Love for Norwood. Danja used heavy <b>vocoding</b> during production of the song.|$|E
5000|$|Robotic {{voices in}} music {{may also be}} {{produced}} by speech synthesis. This does not usually create a [...] "singing" [...] effect (although it can). Speech synthesis means that, unlike in <b>vocoding,</b> no human speech is employed as basis. One example of such use is the song Das Boot by U96. A more tongue-in-cheek musical use of speech synthesis is MC Hawking. Most notably, Kraftwerk, who had previously used the vocoder extensively in their 1970s recordings, began opting for speech synthesis software in place of vocoders starting with 1981's Computer World album; on newer recordings and in the reworked versions of older songs that appear on The Mix and the band's current live show, the previously vocoder-processed vocals have been almost completely replaced by software-synthesized [...] "singing". Vocaloid is a singing synthesizer application software developed by the Yamaha Corporation {{that is designed to}} synthesize singing by entering lyrics and melody.|$|E
40|$|Including disfluencies in {{synthetic}} {{speech is}} being explored {{as a way}} of making synthetic speech sound more natural and conversational. How to measure whether the resulting speech is actually more natu-ral, however, is not straightforward. Conventional approaches to synthetic speech evaluation fall short as a listener is either primed to prefer stimuli with filled pauses or, when they aren’t primed they prefer more fluent speech. Psycholinguistic reaction time experiments may circumvent this issue. In this pa-per, we revisit one such reaction time experiment. For natural speech, delays in word onset were found to facilitate word recognition regardless of the type of delay; be they a filled pause (um), silence or a tone. We expand these experiments by examining the effect of using <b>vocoded</b> and synthetic speech. Our results partially replicate previous findings. For natural and <b>vocoded</b> speech, if the delay is a silent pause, significant increases in the speed of word recognition are found. If the delay comprises a filled pause there is a significant increase in reaction time for <b>vocoded</b> speech but not for natural speech. For synthetic speech, no clear effects of delay on word recognition are found. We hypothesise this is be-cause it takes longer (requires more cognitive re-sources) to process synthetic speech than natural or <b>vocoded</b> speech...|$|R
40|$|Using a vocoder, median-plane sound {{localization}} {{performance was}} measured in eight normal-hearing listeners {{as a function of}} the number of spectral channels. The channels were contiguous and logarithmically spaced in the range from 0. 3 to 16 kHz. Acutely testing <b>vocoded</b> stimuli showed significantly worse localization compared to noises and 100 pulse∕s click trains, both of which were tested after feedback training. However, localization for the <b>vocoded</b> stimuli was better than chance. A second experiment was performed using two different 12 -channel spacings for the <b>vocoded</b> stimuli, now including feedback training. One spacing was from experiment 1. The second spacing (called the speech-localization spacing) assigned more channels to the frequency range associated with speech. There was no significant difference in localization between the two spacings. However, even with training, localizing 12 -channel <b>vocoded</b> stimuli remained worse than localizing virtual wideband noises by 4. 8 ° in local root-mean-square error and 5. 2 % in quadrant error rate. Speech understanding for the speech-localization spacing was not significantly different from that for a typical spacing used by cochlear-implant users. These experiments suggest that current cochlear implants have a sufficient number of spectral channels for some vertical-plane sound localization capabilities, albeit worse than normal-hearing listeners, without loss of speech understanding...|$|R
40|$|The {{choice of}} {{processing}} parameters for <b>vocoded</b> signals {{may have an}} important effect {{on the availability of}} various auditory features. Experiment 1 varied envelope cutoff frequency (30 and 300 Hz), carrier type (sine and noise), and number of bands (2 – 5) for <b>vocoded</b> speech presented to normal-hearing listeners. Performance was better with a high cutoff for sine-vocoding, with no effect of cutoff for noise-vocoding. With a low cutoff, performance was better for noise-vocoding than for sine-vocoding. With a high cutoff, performance was better for sine-vocoding. Experiment 2 measured perceptibility of cues to voice pitch variations. A noise carrier combined with a high cutoff allowed intonation to be perceived to some degree but performance was best in high-cutoff sine conditions. A low cutoff led to poorest performance, regardless of carrier. Experiment 3 tested the relative contributions of comodulation across bands and spectral density to improved performance with a sine carrier and high cutoff. Comodulation across bands had no effect so it appears that sidebands providing a denser spectrum improved performance. These results indicate that carrier type in combination with envelope cutoff can alter the available cues in <b>vocoded</b> speech, factors which must be considered in interpreting results with <b>vocoded</b> signals...|$|R
5000|$|The vocoder {{examines}} {{speech by}} measuring how its spectral characteristics change over time. This {{results in a}} series of signals representing these modified frequencies at any particular time as the user speaks. In simple terms, the signal is split into a number of frequency bands (the larger this number, the more accurate the analysis) and the level of signal present at each frequency band gives the instantaneous representation of the spectral energy content.Thus, the vocoder dramatically reduces the amount of information needed to store speech, from a complete recording to a series of numbers. To recreate speech, the vocoder simply reverses the process, processing a broadband noise source by passing it through a stage that filters the frequency content based on the originally recorded series of numbers.Information about the instantaneous frequency of the original voice signal (as distinct from its spectral characteristic) is discarded; it was not important to preserve this {{for the purposes of the}} vocoder's original use as an encryption aid. It is this [...] "dehumanizing" [...] aspect of the <b>vocoding</b> process that has made it useful in creating special voice effects in popular music and audio entertainment.|$|E
5000|$|In August 2003 {{the band}} {{released}} Tour de France Soundtracks, its first album of new material since 1986's Electric Café. In January and February 2003, before {{the release of}} the album, the band started the extensive Minimum-Maximum world tour, using four customised Sony VAIO laptop computers, effectively leaving the entire Kling Klang studio at home in Germany. The group also obtained a new set of transparent video panels to replace its four large projection screens. This greatly streamlined the running of all of the group's sequencing, sound-generating, and visual-display software. From this point, the band's equipment increasingly reduced manual playing, replacing it with interactive control of sequencing equipment. Hütter retained the most manual performance, still playing musical lines by hand on a controller keyboard and singing live vocals and having a repeating ostinato. Schneider's live <b>vocoding</b> had been replaced by software-controlled speech-synthesis techniques. In November, the group made a surprising appearance at the MTV European Music Awards in Edinburgh, Scotland, performing [...] "Aerodynamik". The same year a promotional box set entitled 12345678 (subtitled The Catalogue) was issued, with plans for a proper commercial release to follow. The box featured remastered editions of the group's eight core studio albums, from Autobahn to Tour de France Soundtracks. This long-awaited box-set would eventually be released in a different set of remasters in November 2009.|$|E
40|$|Abstract Background Degrading speech {{through an}} {{electronic}} synthesis technique called <b>vocoding</b> {{has been shown}} to affect cerebral processing of speech in several cortical areas. However, {{it is not clear whether}} the effects of speech degradation by <b>vocoding</b> are related to acoustical degradation or by the associated loss in intelligibility. Using <b>vocoding</b> and a parametric variation of the number of frequency bands used for the encoding, we investigated the effects of the degradation of auditory spectral content on cerebral processing of intelligible speech (words), unintelligible speech (words in a foreign language), and complex environmental sounds. Results <b>Vocoding</b> was found to decrease activity to a comparable degree for intelligible and unintelligible speech in most of the temporal lobe. Only the bilateral posterior temporal areas showed a significant interaction between <b>vocoding</b> and intelligibility, with a stronger vocoding-induced decrease in activity for intelligible speech. Comparisons to responses elicited by environmental sounds showed that portions of the temporal voice areas (TVA) retained their greater responses to voice even under adverse listening conditions. The recruitment of specific networks in temporal regions during exposure to degraded speech follows a radial and anterior-posterior topography compared to the networks recruited by exposure to speech that is not degraded. Conclusions Different brain networks are involved in vocoded sound processing of intelligible speech, unintelligible speech, and non-vocal sounds. The greatest differences are between speech and environmental sounds, which could be related to the distinctive temporal structure of speech sounds. </p...|$|E
5000|$|To {{convert the}} samples {{back into a}} voice waveform, they were first turned back into the dozen low-frequency <b>vocoded</b> signals. An {{inversion}} of the vocoder process was employed, which included: ...|$|R
40|$|Cochlear {{implants}} {{provide good}} speech discrimination ability despite highly {{limited amount of}} information they transmit compared with normal cochlea. Noise <b>vocoded</b> speech, simulating cochlear implants in normal hearing listeners, have demonstrated that spectrally and temporally degraded speech contains sufficient cues to provide accurate speech discrimination. We hypothesized that neural activity patterns generated in the primary auditory cortex by spectrally and temporally degraded speech sounds will account for the robust behavioral discrimination of speech. We examined the behavioral discrimination of noise <b>vocoded</b> consonants and vowels by rats and recorded neural activity patterns from rat primary auditory corte...|$|R
50|$|Cipher Method is Negative Format's fourth CD. The {{album was}} built with complex beat and rhythm structures, mixing Negative Format's trance melodies with dark {{atmospheric}} overtones, and {{a mixture of}} <b>vocoded</b> and EBM vocal passages.|$|R
