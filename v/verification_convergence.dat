0|31|Public
40|$|We {{describe}} formal <b>verification</b> of <b>convergence</b> {{and performance}} properties of an engine control algorithm being developed for Magneti-Marelli. We study the cutoff mode, where the driver releases the accelerator and the controller regulates fuel injection {{to minimize the}} oscillations while decelerating. The engine and its controller are modeled with hybrid automata and the sliding action of the hybrid controller is formally verified with the model checker HYTECH...|$|R
3000|$|The {{convergence}} of the algorithm was assessed with two criteria: current density and water gain rate. <b>Verification</b> cases produced <b>convergence</b> of the MEA current density [...] I [...] to within [...] [*] 1 [...]...|$|R
40|$|Wing Body Code (WIBCO) program simulates flow-field {{configurations}} for {{reduction of}} design cost and improvement of aircraft performance. Inputs to WIBCO consist of ambient flow conditions and geometric configuration data; grid control and relaxation parameters are internally set. Outputs include input data echo, grid system <b>verification,</b> relaxation-solution <b>convergence</b> history, and computed velocities, pressures, forces, moments, reference lengths, and areas. Program {{is written in}} FORTRAN IV for batch execution...|$|R
40|$|The Lagrange mesh {{method is}} a very {{accurate}} and simple procedure to compute eigenvalues and eigenfunctions of nonrelativistic and semirelativistic Hamiltonians. We show here {{that it can be}} used successfully to solve the equations of both the relativistic flux tube model and the rotating string model, in the symmetric case. <b>Verifications</b> of the <b>convergence</b> of the method are given. Comment: 2 figure...|$|R
3000|$|... [...]). These {{specific}} {{measurements were}} taken experimentally, but critically, {{not used to}} identify the CVS model. Hence, they represent true, independent validation tests of the identified subject-specific model's ability to capture the subject's true dynamics. These comparisons provide a first validation of the subject-specific model's ability to accurately represent the subject at a specific point in time. Second, <b>verification</b> of model <b>convergence</b> is checked by comparing the model outputs to the measured data used in the identification process.|$|R
40|$|We {{describe}} formal <b>verification</b> of <b>convergence</b> {{and performance}} properties of an engine control algorithm being developed for Magneti-Marelli. We study the cutoff mode, where the driver releases the accelerator and the controller regulates fuel injection {{to minimize the}} oscillations while decelerating. The engine and its controller are modeled with hybrid automata and the sliding action of the hybrid controller is formally verified with the model checker HyTech. 1 Introduction 1. 1 Verification of embedded systems Embedded systems are informally defined {{as a collection of}} programmable parts surrounded by ASICs and other standard components that interact continuously with an environment through sensors and actuators. Embedded systems often are used in life-critical situations, where reliability and safety are more important criteria than performance. The closed-loop system [...] -which involves a discrete controller and its analog environment [...] -has behaviors that are inherently hybrid, i. e., [...] ...|$|R
40|$|Discussed {{here is the}} {{calibration}} of {{the scale}} factors and rate biases for the Cosmic Background Explorer (COBE) spacecraft gyroscopes, {{with the emphasis on}} the adaptation for COBE of an algorithm previously developed for the Solar Maximum Mission. Detailed choice of parameters, <b>convergence,</b> <b>verification,</b> and use of the algorithm in an environment where the reference attitudes are determined form the Sun, Earth, and star observations (via the Diffuse Infrared Background Experiment (DIRBE) are considered. Results of some recent experiments are given. These include tests where the gyro rate data are corrected for the effect of the gyro baseplate temperature on the spacecraft electronics...|$|R
40|$|Convergence of acoustic/prosodic (a/p) {{features}} {{between two}} speakers {{is a well-known}} property of human dialogue. It {{has been suggested that}} this particular aspect of human interaction should be implemented in spoken dialogue systems, {{so that they can be}} perceived as more “humanlike”. This paper presents a quantitative analysis method that can provide information required for modeling the phenomenon of convergence. The analysis is a combination of TAMA, a previously introduced data extraction method, and bivariate time series analysis. Results show significant correlation of a/p features between speaker dyads in the recorded dialogues analyzed, and indicate a significant,amount of feedback, which a statistical <b>verification</b> of bidirectional <b>convergence...</b>|$|R
40|$|AbstractWe {{present a}} novel {{three-dimensional}} boundary-element formulation that fully characterizes the mechanical {{behavior of the}} external boundary of a multi-layered viscoelastic coating attached to a hard rotating spherical core. The proposed formulation incorporates both, the viscoelastic, and the inertial effects of the steady-state rolling motion of the sphere, including the Coriolis effect. The proposed formulation is based on Fourier-domain expressions of all mechanical governing equations. It relates two-dimensional Fourier series expansions of surface displacements and stresses, which results {{in the formation of}} a compliance matrix for the outer boundary of the deformable coating, discretized into nodes. The computational cost of building such a compliance matrix is optimized, based on configurational similarities and symmetry. The proposed formulation is applied, in combination with a rolling contact solving strategy, to evaluate the viscoelastic rolling friction of a coated sphere on a rigid plane. Steady-state results generated by the proposed model are verified by comparison to those obtained from running dynamic simulations on a three-dimensional finite element model, beyond the transient. A detailed application example includes a <b>verification</b> of <b>convergence</b> and illustrates the dependence of rolling resistance on the applied load, the thickness of the coating, and the rolling velocity...|$|R
40|$|Abstract − The article {{discusses}} {{the measurement of}} surface displacement of an electrically conductive solid material. Deflection is excited by electro-magneto-acoustic transducer (hereinafter referred to as EMAT). Measurement is realized by laser interferometer. In our research the precise surface displacement measurement is necessary for two reasons. First is EMAT transducer numerical model <b>convergence</b> <b>verification,</b> second is the experimental optimization of EMAT transducer parameters such as sensitivity, frequency and direction characteristics. Used laser interferometer allows the accurate assessment of surface displacement in a relatively small item, which gives an advantage over using piezoelectric acoustic emission sensors that are limited by aperture effect at small wavelengths. In addition, laser interferometer is capable of surface displacement measuring directly under the EMAT device, which is very difficult with using other methods...|$|R
40|$|International audienceThis {{contribution}} {{presents a}} numerical strategy {{to evaluate the}} effective properties of image-based microstructures {{in the case of}} random material properties. The method relies on three points: (i) a high-order fictitious domain method; (ii) an accurate spectral stochastic model and (iii) an efficient model reduction method based on the Proper Generalized Decomposition in order to decrease the computational cost introduced by the stochastic model. A feedback procedure is proposed for an automatic estimation of the random effective properties with a given confidence. Numerical <b>verifications</b> highlight the <b>convergence</b> properties of the method for both deterministic and stochastic models. The method is finally applied to a real 3 D bone microstructure where the empirical probability density function of the effective behaviour could be obtained...|$|R
40|$|A {{method for}} {{computing}} exponential matrices, which often arise naturally in {{the solution of}} systems of linear differential equations, is developed. An exponential matrix is generated as a linear combination of a finite number (equal to the matrix order) of matrices, the coefficients of which are scalar infinite sums. The method can be generalized to apply to any formal power series of matrices. Attention is focused upon the exponential function, and the matrix exponent is assumed tri-diagonal in form. In such cases, the terms in the coefficient infinite sums can be extracted, as recursion relations, from the characteristic polynomial of the matrix exponent. Two numerical examples are presented in some detail: (1) the three dimensional infinitesimal rotation rate matrix, which is skew symmetric, and (2) an N-dimensional tri-diagonal and symmetric finite difference matrix which arises in the numerical solution of the heat conduction partial differential equation. In the second example, the known eigenvalues and eigenvectors of the finite difference matrix permit an analytical solution for the exponential matrix, through the theory of diagonalization and similarity transformations, which is used for independent <b>verification.</b> The <b>convergence</b> properties of the scalar infinite summations are investigated for finite difference matrices of various orders up to ten, and {{it is found that}} the number of terms required for convergence increases slowly with the order of the matrix...|$|R
40|$|An {{appropriate}} {{combination of}} multiple biometric sensors in-creases {{the reliability of}} verification through biometrics. In this pa-per we propose an effective method of fusion of biometrics based on a dynamic selection of threshold point of fingerprint and iris biometrics towards identifier of an optimal set of rules for fu-sion. The effectiveness of the method has been established us-ing several benchmark databases using Simulated Annealing ap-proach. The selection of a proper set of parameters for SA is a multi-objective decision making optimization problem. Initially the matching scores for individual biometric classifiers are computed. Next, a SA-based procedure is followed to simultaneously optimize the parameters and the fusion rules for fingerprint and iris biomet-rics. An experimental <b>verification</b> of the <b>convergence</b> nature of the simulated annealing method with the worst case behavior for op-timum rule selection is analyzed and a comparative result of the method with the Ant colony optimization technique is also given...|$|R
40|$|The recent {{determination}} of the β [...] function of the QCD running coupling α_MS(Q^ 2) to 5 -loopsblue, provides a <b>verification</b> of the <b>convergence</b> of a novel method for determining the fundamental QCD parameter Λ_s based on the Light-Front Holographic approach to nonperturbative QCD. The new 5 -loop analysis, together with improvements in determining the holographic QCD nonperturbative scale parameter κ from hadronic spectroscopy, leads to an improved precision {{of the value of}} Λ_s in the MS scheme close to a factor of two; we find Λ^(3) _MS= 0. 339 ± 0. 019 GeV for n_f= 3, in excellent agreement with the world average, Λ_MS^(3) = 0. 332 ± 0. 017 GeV. We also discuss the constraints imposed on the scale dependence of the strong coupling in the nonperturbative domain by superconformal quantum mechanics and its holographic embedding in anti-de Sitter space. Comment: 16 pages, 4 Figures. Published versio...|$|R
40|$|Self-stabilizing {{distributed}} systems are {{a class of}} {{distributed systems}} which converge to correct system states even if they start from arbitrary system states. A self-stabilizing system can recover from finite number of transient faults (e. g., message loss, memory corruption). Therefore, they are fault-tolerant systems. When we design a self-stabilizing system, its <b>verification,</b> such as <b>convergence</b> from arbitrary initial system state to a correct system state, is a difficult task; typically, a proof is long, complex and error-prone. In this paper, we introduce a description language and its processor for self-stabilizing systems of arbitrary network topology to verify mechanically, i. e., a verification system. For an incorrect self-stabilizing system, the verification system outputs a counterexample which consists of an initial system state and an execution sequence which does not converge. Some distributed self-stabilizing systems have been verified by our verification system and the [...] ...|$|R
40|$|This paper {{describes}} {{a study of}} FE model verification. Before undertaking a model updating procedure, {{it is important to}} determine whether the initial model can be updated or not. Two methods for model <b>verification</b> - a <b>convergence</b> check and a configuration check - are proposed here to address the problems of discretisation and configuration errors in FE models, respectively. The differences between the predictions from a given FE model by using different mass matrix formulations are seen to be closely related to the convergence range of the model prediction. The residuals between the experimental mode shapes and those obtained by curve-fitting the experimental mode shapes with eigenvectors predicted from the FE model are studied here. It is concluded that these residuals can reveal the existence of configuration errors in the FE model. Case studies based on theoretical models show that these two methods are efficient at distinguishing configuration errors from parameter errors and estimating discretisation errors and the compensation for the...|$|R
40|$|This {{progress}} report describes {{the development of}} a front tracking method for the solution of the governing equations of motion for two-phase micromixing of incompressible, viscous, liquid-liquid solvent extraction processes. The ability to compute the detailed local interfacial structure of the mixture allows characterization of the statistical properties of the two-phase mixture in terms of droplets, filaments, and other structures which emerge as a dispersed phase embedded into a continuous phase. Such a statistical picture provides the information needed for building a consistent coarsened model applicable to the entire mixing device. Coarsening is an undertaking for a future mathematical development and is outside the scope of the present work. We present here a method for accurate simulation of the micromixing dynamics of an aqueous and an organic phase exposed to intense centrifugal force and shearing stress. The onset of mixing {{is the result of the}} combination of the classical Rayleigh- Taylor and Kelvin-Helmholtz instabilities. A mixing environment that emulates a sector of the annular mixing zone of a centrifugal contactor is used for the mathematical domain. The domain is small enough to allow for resolution of the individual interfacial structures and large enough to allow for an analysis of their statistical distribution of sizes and shapes. A set of accurate algorithms for this application requires an advanced front tracking approach constrained by the incompressibility condition. This research is aimed at designing and implementing these algorithms. We demonstrate <b>verification</b> and <b>convergence</b> results for one-phase and unmixed, two-phase flows. In addition we report on preliminary results for mixed, two-phase flow for realistic operating flow parameters...|$|R
40|$|A {{recently}} proposed configuration-interaction based impurity solver is used {{in combination}} with the single-site and four-site cluster dynamical mean field approximations to investigate the three-band copper oxide model believed to describe the electronic structure of high transition temperature copper-oxide superconductors. Use of the configuration interaction solver enables <b>verification</b> of the <b>convergence</b> of results with respect to the number of bath orbitals. The spatial correlations included in the cluster approximation substantially shift the metal-insulator phase boundary relative to the prediction of the single-site approximation and increase the predicted energy gap of the insulating phase by about 1 eV above the single-site result. Vertex corrections occurring in the four-site approximation act to dramatically increase the value of the optical conductivity near the gap edge, resulting in a better agreement with the data. The calculations reveal two distinct correlated insulating states: the `magnetically correlated insulator', in which nontrivial intersite correlations play an essential role in stabilizing the insulating state, and the strongly correlated insulator, in which local physics suffices. Comparison of the calculations to the data place the cuprates in the magnetically correlated Mott insulator regime...|$|R
40|$|International audiencePressure driven flows {{typically}} {{occur in}} hydraulic networks, e. g. oil ducts, water supply, biological flows, microfluidic channels etc. However, Stokes and Navier-Stokes problems {{are most often}} studied in a framework where Dirichlet type boundary conditions on the velocity field are imposed, thanks to the simpler settings from the theoretical and numerical points of view. In this work, we propose a novel formulation of the Stokes system with pressure boundary condition, together with no tangential flow, {{on a part of}} the boundary in a standard Stokes functional framework using Lagrange multipliers to enforce the latter constraint on velocity. More precisely, we carry out (i) a complete analysis of the formulation from the continuous to discrete level in two and three dimensions (ii) the description of our solution strategy, (iii) a <b>verification</b> of the <b>convergence</b> properties with an analytic solution and finally (iv) three-dimensional simulations of blood ow in the cerebral venous network that are in line with in-vivo measurements and the presentation of some performance metrics with respect to our solution strategy...|$|R
40|$|In {{this paper}} we {{investigate}} the numerical approximation of {{a variant of}} the mean curvature flow. We consider the evolution of hypersurfaces with normal speed given by H^k, k > 1, where H denotes the mean curvature. We use a level set formulation of this flow and discretize the regularized level set equation with finite elements. In a previous paper we proved an a priori estimate for the approximation error between the finite element solution and the solution of the original level set equation. We obtained an upper bound for this error which is polynomial in the discretization parameter and the reciprocal regularization parameter. The aim of the present paper is the numerical study of the behavior of the evolution and the numerical <b>verification</b> of certain <b>convergence</b> rates. We restrict the consideration to the case that the level set function depends on two variables, i. e. the moving hypersurfaces are curves. Furthermore, we confirm for specific initial curves and different values of k that the flow improves the isoperimetrical deficit...|$|R
40|$|In {{electrical}} impedance tomography (EIT) {{one wants to}} image the conductivity distribution of a body from current and voltage measurements carried out on its boundary. In this paper we consider the underlying mathematical model, the inverse conductivity problem, in two dimensions and under the realistic assumption that {{only a part of}} the boundary is accessible to measurements. In this framework our data are modeled as a partial Neumann-to-Dirichlet map (ND map). We compare this data to the full-boundary ND map and prove that the error depends linearly {{on the size of the}} missing part of the boundary. The same linear dependence is further proved for the difference of the reconstructed conductivities - from partial and full boundary data. The reconstruction is based on a truncated and linearized D-bar method. Auxiliary results include an extrapolation method to estimate the full-boundary data from the measured one, an approximation of the complex geometrical optics solutions computed directly from the ND map as well as an approximate scattering transform for reconstructing the conductivity. Numerical <b>verification</b> of the <b>convergence</b> results and reconstructions are presented for simulated test cases...|$|R
40|$|In this article, we {{consider}} the reconstruction of a bandlimited function from its finite localized sample data. Truncating the classical Shannon sampling series results in an unsatisfactory convergence rate due to the slow decay of the sinc function. To overcome this drawback, a simple and highly effective method, called the Gaussian regularization of the Shannon series, was proposed in the engineering and has received remarkable attention. It works by multiplying the sinc function in the Shannon series with a regularized Gaussian function. Recently, it was proved that the upper error bound of this method can achieve the convergence rate of the order O(1 /√(n) (-π-δ/ 2 n)), where 0 <δ<π is the bandwidth and n {{is the number of}} sample data. The convergence rate is by far the best convergence rate among all regularized methods for the Shannon sampling series. The main objective {{of this article is to}} present the theoretical justification and numerical <b>verification</b> that such <b>convergence</b> rate is optimal when 0 <δ<π/ 2 by estimating the lower error bound of the truncated Gaussian regularized Shannon sampling series. Comment: 14 pages, 1 figur...|$|R
40|$|Cyber-physical systems (CPSs) {{are often}} treated modularly to tackle both {{complexity}} and heterogeneity; and their validation may be done modularly by co-simulation: the coupling {{of the individual}} subsystem simulations. This modular approach underlies the FMI standard. This paper presents an approach to verify both healthiness and well-formedness of an architectural design, expressed using a profile of SysML, {{as a prelude to}} FMI co-simulation. This checks the conformity of component connectors and the absence of algebraic loops, necessary for co-simulation <b>convergence.</b> <b>Verification</b> of these properties involves theorem proving and model-checking using: Fragmenta, a formal theory for representing typed visual models, with its mechanisation in the Isabelle/HOL proof assistant, and the CSP process algebra and its FDR 3 model-checker. The paper’s contributions lie in: a SysML profile for architectural modelling supporting multi-modelling and co-simulation; our approach to check the adequacy of a SysML model for co-simulation using theorem proving and model-checking; our verification and transformation workbench for typed visual models based on Fragmenta and Isabelle; an approach to detect algebraic loops using CSP and FDR 3; and a comparison of approaches to the detection of algebraic loops...|$|R
40|$|Variational {{methods have}} become an {{important}} kind of methods in signal and image restoration - a typical inverse problem. One important minimization model consists of the squared ℓ_ 2 data fidelity (corresponding to Gaussian noise) and a regularization term constructed by a regularization function (potential function) composed of first order difference operators. As contrasts are important features in signals and images, we study, in this paper, the possibility of contrast-preserving restoration by variational methods. We present both the motivation and implementation of a general truncated regularization framework. In particular, we show that, in both 1 D and 2 D, any convex or smooth regularization based variational model is impossible or with low probabilities to preserve edge contrasts. It is better to use those nonsmooth potential functions flat on (τ,+∞) for some positive τ, which are nonconvex. These discussions naturally yield a general regularization framework based on truncation. Some analysis in 1 D theoretically demonstrate its good contrast-preserving ability. We also give optimization algorithms with <b>convergence</b> <b>verification</b> in 2 D, where global minimizers of each subproblem (either convex or nonconvenx) are calculated. Experiments numerically show {{the advantages of the}} framework...|$|R
40|$|The article {{analyses}} {{the process}} of nominal convergence of the new EU member states (NMS) with particular attention paid to some applied and theoretical aspects, which may have impact on {{the process of}} the euro adoption. Chapter two addresses selected theoretical and methodological issues connected with the International Comparison Project (ICP). It discusses determinants and influences affecting price level convergence and some issues that have set off new trends, such as the globalization or process of the on-going European integration. This chapter also presents a brief summary of the main trends of price convergence observed by focusing on changes of comparative price levels (CPL) for various disaggregated items of GDP. It also deals with potential issues and problems arising in this context. Chapter three is aimed at an empirical <b>verification</b> of price <b>convergence</b> and at a search for main driving factors using data for the NMS and the old EU member states over 11 years (1995 – 2006). There are some differences in results depending on the applied econometric method. The most important determinants of price level are GDP and population, the openness and public finance’s indicators are not significant. The last section summarises the main findings...|$|R
40|$|This {{research}} project presents {{an overview of}} the Lattice Boltzmann Method (LBM), an alternative numerical approach to conventional CFD. LBM has increased in popularity among the scientific community in recent years, due to its promising abilities. Namely, it claims to achieve the same level of accuracy as that of traditional CFD, while offering new benefits such as easy parallelization and the possibility of implementing complex and multiscale flows. Unlike conventional CFD which focuses on the numerical solution of the Navier Stokes Equations, the Lattice Boltzmann Method focuses on microscopic particle interactions to represent the macroscopic behaviour of the fluid. The aim of this project is to appraise the ability of the Lattice Boltzmann Method to accurately simulate incompressible flows and to analyse its accuracy performance and stability. This report presents the theoretical basis of this novel method, as well as a <b>verification</b> of its <b>convergence</b> results through some examples. These examples are implemented through an open-source code (Palabos). This project not only focuses on matching the LBM solutions with analytical or existing solutions, but it also focuses on studying the effect that the parameters of the model have on the results provided, on stability and on computational cost. The results and their analysis show that LBM is an accurate method for representing incompressible flows. The report also describes how to implement the Lattice Boltzmann Method and suggests some ways to continue the work further...|$|R
40|$|The LMS {{algorithm}} {{is one of}} the most successful adaptive filtering algorithms. It uses the instantaneous value of the square of the error signal as an estimate of the mean-square error (MSE). The LMS algorithm changes (adapts) the filter tap weights so that the error signal is minimized in the mean square sense. In Trigonometric LMS (TLMS) and Hyperbolic LMS (HLMS), two new versions of LMS algorithms, same formulations are performed as in the LMS algorithm with the exception that filter tap weights are now expressed using trigonometric and hyperbolic formulations, in cases for TLMS and HLMS respectively. Hence appears the CORDIC algorithm as it can efficiently perform trigonometric, hyperbolic, linear and logarithmic functions. While hardware-efficient algorithms often exist, the dominance of the software systems has kept those algorithms out of the spotlight. Among these hardware- efficient algorithms, CORDIC is an iterative solution for trigonometric and other transcendental functions. Former researches worked on CORDIC algorithm to observe the convergence behavior of Trigonometric LMS (TLMS) algorithm and obtained a satisfactory result in the context of convergence performance of TLMS algorithm. But revious researches directly used the CORDIC block output in their simulation ignoring the internal step-by-step rotations of the CORDIC processor. This gives rise to a need for <b>verification</b> of the <b>convergence</b> performance of the TLMS algorithm to investigate if it actually performs satisfactorily if implemented with step-by-step CORDIC rotation. This research work has done this job. It focuses on the internal operations of the CORDIC hardware, implements the Trigonometric LMS (TLMS) and Hyperbolic LMS (HLMS) algorithms using actual CORDIC rotations. The obtained simulation results are highly satisfactory and also it shows that convergence behavior of HLMS is much better than TLMS. Comment: 12 pages, 5 figures, 1 table. Published in IJCNC; [URL] [URL]...|$|R
40|$|The DOE/NNSA Advanced Simulation & Computing (ASC) Program {{directs the}} development, {{demonstration}} and deployment of physics simulation codes. The defensible utilization of these codes for high-consequence decisions requires rigorous verification and {{validation of the}} simulation software. The physics and engineering codes used at Los Alamos National Laboratory (LANL), Lawrence Livermore National Laboratory (LLNL), and Sandia National Laboratory (SNL) are arguably among the most complex utilized in computational science. Verification represents {{an important aspect of}} the development, assessment and application of simulation software for physics and engineering. The purpose of this note is to formally document the existing tri-laboratory suite of verification problems used by LANL, LLNL, and SNL, i. e., the Tri-Lab Verification Test Suite. Verification {{is often referred to as}} ensuring that ''the [discrete] equations are solved [numerically] correctly''. More precisely, verification develops evidence of mathematical consistency between continuum partial differential equations (PDEs) and their discrete analogues, and provides an approach by which to estimate discretization errors. There are two variants of verification: (1) code verification, which compares simulation results to known analytical solutions, and (2) calculation <b>verification,</b> which estimates <b>convergence</b> rates and discretization errors without knowledge of a known solution. Together, these verification analyses support defensible verification and validation (V&V) of physics and engineering codes that are used to simulate complex problems that do not possess analytical solutions. Discretization errors (e. g., spatial and temporal errors) are embedded in the numerical solutions of the PDEs that model the relevant governing equations. Quantifying discretization errors, which comprise only a portion of the total numerical simulation error, is possible through code and calculation verification. Code verification computes the absolute value of discretization errors relative to an exact solution of the governing equations. In contrast, calculation verification, which does not utilize a reference solution, combines an assessment of stable self-convergence and exact solution prediction to quantitatively estimate discretization errors. In FY 01, representatives of the V&V programs at LANL, LLNL, and SNL identified a set of verification test problems for the Accelerated Strategic Computing Initiative (ASCI) Program. Specifically, a set of code verification test problems that exercise relevant single- and multiple-physics packages was agreed upon. The verification test suite problems can be evaluated in multidimensional geometry and span both smooth and non-smooth behavior...|$|R
40|$|Continuous {{stochastic}} {{control theory}} has found many applications in optimal investment. However, it lacks some reality, {{as it is}} based on the assumption that interventions are costless, which yields optimal strategies where the controller has to intervene at every time instant. This thesis consists of the examination of two types of more realistic control methods with possible applications. In the first chapter, we study the stochastic impulse control of a diffusion process. We suppose that the controller minimizes expected discounted costs accumulating as running and controlling cost, respectively. Each control action causes costs which are bounded from below by some positive constant. This makes a continuous control impossible as it would lead to an immediate ruin of the controller. We give a rigorous development of the relevant theory, where our guideline is to establish <b>verification</b> and <b>convergence</b> results under minimal assumptions, without focusing on the existence of solutions to the corresponding (quasi-) variational inequalities. If the impulse control problem can be characterized or approximated by (quasi-) variational inequalities, it remains to solve these equations. In Section 1. 2, we solve the stochastic impulse control problem for a one-dimensional diffusion process with constant coefficients and convex running costs. Further, in Section 1. 3, we solve a particular multi-dimensional example, where the uncontrolled process is given by an at least two-dimensional Brownian motion and the cost functions are rotationally symmetric. By symmetry, this problem can be reduced to a one-dimensional problem. In the last section of the first chapter, we suggest a new impulse control problem, where the controller is in addition allowed to invest his initial capital into a market consisting of a money market account and a risky asset. The costs which arise upon controlling the diffusion process and upon trading in this market have to be paid out of the controller's bond holdings. The aim of the controller is to minimize the running costs, caused by the abstract diffusion process, without getting ruined. The second chapter is based on a paper which is joint work with Holger Kraft and Frank Seifried. We analyze the portfolio decision of an investor trading in a market where the economy switches randomly between two possible states, a normal state where trading takes place continuously, and an illiquidity state where trading is not allowed at all. We allow for jumps in the market prices at the beginning and at the end of a trading interruption. Section 2. 1 provides an explicit representation of the investor's portfolio dynamics in the illiquidity state in an abstract market consisting of two assets. In Section 2. 2 we specify this market model and assume that the investor maximizes expected utility from terminal wealth. We establish convergence results, if the maximal number of liquidity breakdowns goes to infinity. In the Markovian framework of Section 2. 3, we provide the corresponding Hamilton-Jacobi-Bellman equations and prove a verification result. We apply these results to study the portfolio problem for a logarithmic investor and an investor with a power utility function, respectively. Further, we extend this model to an economy with three regimes. For instance, the third state could model an additional financial crisis where trading is still possible, but the excess return is lower and the volatility is higher than in the normal state. Stochastische Impulssteuerung und Portfolio-Optimierung mit Illiquiditä...|$|R
40|$|Understanding {{the highly}} {{non-linear}} biomechanics {{of the complex}} structure of human skin would not only provide valuable information {{for the development of}} biological comparable products {{that could be used for}} the improvement, restoration or maintenance of the biological tissue or replacement of the whole organ, but would also support the development of an advanced computational model (e. g. finite element skin models) that do not differ (or do not differ very much!) from experimental data. This could be very useful for surgical training, planning and navigation. In particular, the major goal of this thesis was the development of robust and easy to use computational models of the cutting and tearing of soft materials, including large deformations, and the development of repeatable, reproducible and reliable physical skin models in comparison to in-ex vivo human skin samples. In combination with advanced computational/mechanical methods, these could offer many possibilities, such as optimised device design which would be used for effective and reproducible skin penetration in the clinical setting and for in vivo measurements. To be able to carry out experimental cutting tests, physical models of skin were manufactured in the laboratory using silicone rubber. The mechanical properties of the physical models were examined experimentally by applying tensile and indentation tests to the test models using the Zwick universal testing machine and the Digital Image Correlation (DIC) System. To estimate the mechanical properties of the physical models and calculate the quantities, Poisson’s ratio, Young’s modulus and shear modulus -which were used later in computational cutting models - and inverse analyses were performed for each example of manufactured silicone rubber in the laboratory using the analytical study on indentation method, the curve fitting technique and DIC measurements. Then, the results were compared to the mechanical properties of human skin experimentally obtained in vivo/ ex vivo (from published studies). A new large deformation cohesive zone formulation was implemented using contact mechanics, which allows easy definition of crack paths in conventional finite element models. This was implemented in the widely used open source FE package FEBio through modification of the classical contact model to provide a specific implementation of a mesh independent method for straightforward controlling of (non-linear) fracture mechanical processes using the Mixed Mode Cohesive-Zone method. Additionally, new models of friction and thermodynamically coupled friction were developed and implemented. The computational model for the simulation of the cutting process (the finite element (FE) model of cutting) was reduced to the simplified model for the sharp interaction (triangular prisms wedge cutting), where the Neo Hookean hyperelastic material model was chosen to represent the skin layers for the FEM analysis. Practical, analytical and experimental <b>verification</b> tests, alongside <b>convergence</b> analysis, were performed. Comparison of the computational results with the analytical and experimental results revealed that applying the modified contact algorithm to the fracture problem was effective in predicting and simulating the cutting processes...|$|R

