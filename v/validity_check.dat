122|533|Public
25|$|Credit cards have {{a printed}} or {{embossed}} bank card number {{complying with the}} ISO/IEC 7812 numbering standard. The card number's prefix, called the Bank Identification Number, is the sequence of digits {{at the beginning of}} the number that determine the bank to which a credit card number belongs. This is the first six digits for MasterCard and Visa cards. The next nine digits are the individual account number, and the final digit is a <b>validity</b> <b>check</b> code.|$|E
5000|$|It will {{be shown}} that the field {{equations}} are a generalization of Poisson's classical field equation. The reduction to the classical limit, besides being a <b>validity</b> <b>check</b> on the field equations, gives as a byproduct {{the value of the}} constant [...]|$|E
50|$|UOWHFs {{are thought}} to be less {{computationally}} expensive than CRHFs, and are most often used for efficiency purposes in schemes where the choice of the hash function happens at some stage of execution, rather than beforehand. For instance, the Cramer-Shoup cryptosystem uses a UOWHF as part of the <b>validity</b> <b>check</b> in its ciphertexts.|$|E
40|$|Abstract. We {{show that}} one can recover the PIN from a {{standardised}} RSA-based PIN encryption algorithm from {{a small number of}} queries to a ciphertext <b>validity</b> <b>checking</b> oracle. The <b>validity</b> <b>checking</b> oracle required is rather special and we discuss whether such oracles could be obtained in the real world. Our method works using a minor extension to the ideas of Bleichenbacher and Manger, in particular we obtain information from negative, as well as positive, responses from the <b>validity</b> <b>checking</b> oracle. ...|$|R
30|$|Software {{code for}} the data fusion methods is {{presented}} in Additional file 1 : Annex A and software code for implementing the <b>validity</b> <b>checks</b> is given in Additional file 1 : Annex B for the hot deck matching method only. <b>Validity</b> <b>checking</b> for the other methods would be implemented in the same way.|$|R
30|$|Simple face <b>validity</b> <b>checks</b> will be {{used for}} the {{innovation}} model.|$|R
50|$|Credit cards have {{a printed}} or {{embossed}} bank card number {{complying with the}} ISO/IEC 7812 numbering standard. The card number's prefix, called the Bank Identification Number, is the sequence of digits {{at the beginning of}} the number that determine the bank to which a credit card number belongs. This is the first six digits for MasterCard and Visa cards. The next nine digits are the individual account number, and the final digit is a <b>validity</b> <b>check</b> code.|$|E
5000|$|When {{performing}} a document <b>validity</b> <b>check</b> {{the wording of}} the guidance is [...] "If you are given a false document, you will only be liable for a civil penalty if it is reasonably apparent that it is false. This means that a person who is untrained in the identification of false documents, examining it carefully, but briefly, and without the use of technological aids could reasonably be expected to realise that the document in question is not genuine." ...|$|E
5000|$|Conversion from Julian or Gregorian AD {{years to}} the Human Era can be {{achieved}} by adding 10,000 to the AD year. The current year of AD [...] can be transformed into a Holocene year by adding the digit [...] "1" [...] before it, making it [...] HE. BC years are converted by subtracting the BC year from 10,001. A useful <b>validity</b> <b>check</b> is that the last single digits of BC and HE equivalent pairs must add up to 1 or 11.|$|E
40|$|Abstract. Duration Calculus (DC) is a {{real-time}} logic with {{measurement of}} duration of propositions in observation intervals. It {{is a highly}} expressive logic with continuous time behaviours (also called signals) as its models. <b>Validity</b> <b>checking</b> of DC is undecidable. We propose a method for <b>validity</b> <b>checking</b> of Duration Calculus by reduction to a sampled time version of this logic called well sampled Interval Duration Logic (WSIDL). This reduction relies on representing a continuous time behaviour by a well-sampled behaviour with 1 -oversampling. We provide weak and strong reductions (abstractions) of logic DC to logic WSIDL which respectively preserve the validity and the counter examples. By combining these reductions with previous work on deciding IDL, we implement a tool for <b>validity</b> <b>checking</b> of Duration Calculus. This provides a partial but practical method for <b>validity</b> <b>checking</b> of Duration Calculus. We present some preliminary experimental results to measure {{the success of this}} approach. ...|$|R
50|$|<b>Validity</b> <b>checks</b> - {{controls}} that ensure only valid data is input or processed.|$|R
30|$|Conduct three <b>validity</b> <b>checks</b> {{to ensure}} {{reasonable}} RT thresholds. (The following two steps are {{performed with the}} validated RT thresholds).|$|R
50|$|As such, {{the design}} goals for OpenSMTPD are: security, ease of use, and performance. Security in OpenSMTPD is {{achieved}} by robust <b>validity</b> <b>check</b> in the network input path, use of bounded buffer operations via strlcpy, and privilege separation to mitigate the effects of possible security bugs exploiting the daemon through privilege escalation. In order to simplify the use of SMTP, OpenSMTPD implements a smaller set of functionalities than those available in other SMTP daemons, {{the objective is to}} provide enough features to satisfy typical usage at the risk of unsuitability for esoteric or niche requirements.|$|E
50|$|It {{is clear}} from the context that Fernando must have been working from the Barcelona copy when he wrote the portion of the {{biography}} describing the first voyage, as many details in the biography agree precisely with the Diario. In that sense, the primary utility of the biography is as a <b>validity</b> <b>check</b> on the Diario, a test which vindicates Las Casas in many respects. However, there are a few descriptions in the biography that are not already in the Diario. Most importantly, the biography asserts that Guanahani was fifteen leagues (60 miles) long, which seems contrary to Columbus' implication in the Diario that he had seen the entire island on a single day's boat trip.|$|E
50|$|OpenNTPD is {{an attempt}} by the OpenBSD team to produce an NTP daemon {{implementation}} that is secure, simple to audit, trivial to set up and administer, reasonably accurate, and light on system resources. As such, the design goals for OpenNTPD are: security, ease of use, and performance. Security in OpenNTPD is achieved by robust <b>validity</b> <b>check</b> in the network input path, use of bounded buffer operations via strlcpy, and privilege separation to mitigate the effects of possible security bugs exploiting the daemon through privilege escalation. In order to simplify the use of NTP, OpenNTPD implements a smaller set of functionalities than those available in other NTP daemons, such as that provided by the Network Time Protocol Project. The objective is to provide enough features to satisfy typical usage at the risk of unsuitability for esoteric or niche requirements. OpenNTPD is configured through the configuration file, ntpd.conf. A minimal number of options are offered: IP address or hostname on which OpenNTPD should listen, a timedelta sensor device to be used, and the set of servers from which the time will be synchronized. The accuracy of OpenNTPD is best-effort; the daemon attempts to be as accurate as possible but no specific accuracy is guaranteed.|$|E
5000|$|Widespread use of {{electronic}} handheld devices (PDAs and smartphones) and software designed by LAPOP to allow multilingual interviews and extensive <b>validity</b> <b>checks</b> ...|$|R
40|$|Abstract In this paper, we {{introduce}} a new <b>validity</b> <b>checking</b> problem over linear arithmetic constraints and present a decision procedure for the problem. Instead of considering the validity of any particular linear arithmetic constraint, we consider the following problem: Given a finite automaton accepting linear arithmetic constraints, does the automaton produce any constraint that is a tautology? This problem arises {{in the context of}} static verification of meta-programs, i. e., programs dynamically generating other programs. This paper gives the first decision procedure to perform <b>validity</b> <b>checking</b> of finite automata over linear arithmetic constraints. Our algorithm will enable advanced verification of meta-programs. ...|$|R
50|$|It is {{necessary}} that data streams {{be subject to}} accurate <b>validity</b> <b>checks</b> {{before they can be}} considered as being correct or trustworthy. Such checks are of a temporal, formal, logic and forecasting kind.|$|R
3000|$|... {{describe}} regular packet processing in {{the absence}} of packet loss. If the key-chain <b>validity</b> <b>check</b> in step [...]...|$|E
40|$|Bid {{opening in}} {{e-auction}} is e#cient when a homomorphic secret sharing function is employed {{to seal the}} bids and homomorphic secret reconstruction is employed to open the bids. However, this high e#ciency {{is based on an}} assumption: the bids are valid (e. g. within a special range). An undetected invalid bid can compromise correctness and fairness of the auction. Unfortunately, validity verification of the bids is ignored in the auction schemes employing homomorphic secret sharing (called homomorphic auction in this paper). In this paper, an attack against the homomorphic auction in the absence of bid <b>validity</b> <b>check</b> is presented and a necessary bid <b>validity</b> <b>check</b> mechanism is proposed. Then a batch cryptographic technique is introduced and applied to improve the e#ciency of bid <b>validity</b> <b>check...</b>|$|E
3000|$|As a first <b>validity</b> <b>check,</b> a null model {{without any}} {{mechanism}} in the utility functions is run, that is, ∀k:β [...]...|$|E
40|$|This survey covers basic {{information}} about public key infrastructures and summarizes the predominant technology and standards. Special attention {{is given to}} mechanisms for certificate revocation. Methods for CRL distribution and <b>validity</b> <b>checking</b> are compared. 1 Supported by KDD R&D Laboratories, Inc...|$|R
30|$|Problematic handwritings {{are no more}} {{applicable}} in EHR systems, {{the data}} collected via these systems are not mainly gathered for analytical purposes and contain many issues—missing data, incorrectness, miscoding—due to clinicians’ workloads, not user friendly user interfaces, and no <b>validity</b> <b>checks</b> by humans [66].|$|R
50|$|The keys to {{the success}} of pcc were its {{portability}} and improved diagnostic capabilities. The compiler was designed so that only a few of its source files were machine-dependent. It was relatively robust to syntax errors and performed more thorough <b>validity</b> <b>checks</b> than its contemporaries.|$|R
40|$|The paper {{describes}} a {{development of a}} mathematical model for a lay-on instrument capacitor that takes into account an actual electrode thickness. <b>Validity</b> <b>check</b> of the mathematical model has been carried out in the paper. The paper contains quantitative evaluations and recommendations on design optimization of the lay-on instrument capacitor. </p...|$|E
3000|$|... [...]. As the {{differential}} equations themselves are unchanged {{from the above}} well-known problem, the validity of our study lies {{in the implementation of}} boundary conditions (32) to (35). In the next section, we compare our results to those of Dunham and Ogden (2012) for a <b>validity</b> <b>check,</b> although their study adopted a long-wave approximation.|$|E
3000|$|The loop {{that starts}} at line three {{contains}} another loop over dom(C) ∪{NULL} (starting at line 7), where | dom(C)| ∝ N. Within this second loop, when ϕ _k is a variable CFD, the algorithm does a <b>validity</b> <b>check</b> that requires {{as many as}} N times string comparisons. Thus, the computational cost of IncRepair is O(N^ 3).|$|E
40|$|A rich dense-time logic, called Interval Duration Logic (IDL), {{is useful}} for specifying {{quantitative}} properties of timed systems. The logic is undecidable in general. However, several approaches {{can be used for}} <b>checking</b> <b>validity</b> (and model <b>checking)</b> of IDL formulae in practice. In this paper, we propose bounded <b>validity</b> <b>checking</b> of IDL formulae by polynomially reducing this to checking unsatisfiability of lin-sat formulae. We implement this technique and give performance results obtained by checking the unsatisfiability of the resulting lin-sat formulae using the ICS solver. We also perform experimental comparisons of several approaches for <b>checking</b> <b>validity</b> of IDL formulae, including (a) digitization followed by automata-theoretic analysis, (b) digitization followed by pure propositional SAT solving, and (c) lin-sat solving as proposed in this paper. Our experiments use a rich set of examples drawn from the Duration Calculus literature...|$|R
40|$|QDDC is a logic for specifying {{quantitative}} timing {{properties of}} reactive systems. An automata theoretic decision procedure for QDDC reduces each formula to a finite state automaton accepting precisely {{the models of}} the formula. This construction has been implemented into a validity/model checking tool for QDDC called DCVALID. Unfortunately, {{the size of the}} final automaton as well as the intermediate automata which are encountered in the construction can some times be prohibitively large. In this paper, we present some validity preserving transformations to QDDC formulae which result into more efficient construction of the formula automaton and hence reduce the <b>validity</b> <b>checking</b> time. The transformations can be computed in linear time. We provide a theoretical as well as an experimental analysis of the improvements in the formula automaton size and <b>validity</b> <b>checking</b> time due to our transformations...|$|R
5000|$|Content {{validation}} (also called face <b>validity)</b> <b>checks</b> {{how well}} {{the content of the}} research are related to the variables to be studied; it seeks to answer whether the research questions are representative of the variables being researched. It is a demonstration that the items of a test are drawn from the domain being measured.|$|R
40|$|Relapse and {{exacerbation}} of psychotic {{symptoms were}} investigated in a prospective study of 88 patients with recent-onset schizophrenia and related disorders. Relapse definitions {{were derived from}} expressed emotion and family intervention studies {{and based on the}} Brief Psychiatric Rating Scale (BPRS), the Present State Examination, and clinical judgment. Results indicate that research and clinical criteria represent different perspectives on relapse. Clinical criteria provide a <b>validity</b> <b>check</b> that can verify BPRS-rated changes in partially remitted patient...|$|E
30|$|There {{is a huge}} {{gap between}} the number of papers with an assumption-restricted {{modeling}} approach and those based on the complex reality of warehouses. The lack of empirical research in this area can be considered as a warning to academia. More case and action research studies must be done, which can help the research community to better understand the reality. In addition, research results with a <b>validity</b> <b>check</b> on real-case settings will have direct benefits for practice.|$|E
3000|$|... ’s cache, {{which leads}} to the traffic {{offloading}} at MCS. Moreover, from CCN’s inherent security characteristics, the processing for location update can be done without additional security mechanisms. That is, in MIPv 6, binding update and binding acknowledgement messages must be protected by IP security using Encapsulating Security Payload (ESP) protection with a non-NULL payload authentication algorithm [14, 15] while the proposed scheme can do the <b>validity</b> <b>check</b> of the “PREG” message through the embedded signature.|$|E
40|$|ATLAS reconstructs primary {{vertices}} {{with high}} efficiency and resolution. These vertices serve as input to other mission critical analysis tools, and are relied on by many physics analyses. This presentation surveys the ATLAS primary vertex reconstruction algorithms, and describes <b>validity</b> <b>checks</b> done using real data. The complications introduced by pileup are discussed, along with refinements currently under study...|$|R
40|$|Attribute-based {{encryption}} (ABE) is {{a popular}} cryptographic technology to protect the security of users’ data in cloud computing. In order to reduce its decryption cost, outsourcing the decryption of ciphertexts is an available method, which enables users to outsource {{a large number of}} decryption operations to the cloud service provider. To guarantee the correctness of transformed ciphertexts computed by the cloud server via the outsourced decryption, it is necessary to check the correctness of the outsourced decryption to ensure security for the data of users. Recently, Li et al. proposed a full verifiability of the outsourced decryption of ABE scheme (ABE-VOD) for the authorized users and unauthorized users, which can simultaneously check the correctness of the transformed ciphertext for both them. However, in this paper we show that their ABE-VOD scheme cannot obtain the results which they had shown, such as finding out all invalid ciphertexts, and checking the correctness of the transformed ciphertext for the authorized user via checking it for the unauthorized user. We first construct some invalid ciphertexts which can pass the <b>validity</b> <b>checking</b> in the decryption algorithm. That means their “verify-then-decrypt” skill is unavailable. Next, we show that the method to <b>check</b> the <b>validity</b> of the outsourced decryption for the authorized users via checking it for the unauthorized users is not always correct. That is to say, there exist some invalid ciphertexts which can pass the <b>validity</b> <b>checking</b> for the unauthorized user, but cannot pass the <b>validity</b> <b>checking</b> for the authorized user...|$|R
40|$|The {{problem of}} {{checking}} or optimally simplifying bisimulation formulas {{is likely to}} be computationally very hard. We take a different view at the problem: we set out to define a very fast algorithm, and then see what we can obtain. Sometimes our algorithm can simplify a formula perfectly, sometimes it cannot. However, the algorithm is extremely fast and can, therefore, be added to formula-based bisimulation model checkers at practically no cost. When the formula can be simplified by our algorithm, this can have a dramatic positive effect on the better, but also more time consuming, theorem provers which will finish the job. 1 Introduction The need for <b>validity</b> <b>checking</b> or optimal simplification of first order bisimulation formulas has arisen from recent work on symbolic bisimulation checking of value-passing calculi [4, 9, 15]. The NP-completeness of checking satisfiability of propositional formulas [3] implies that <b>validity</b> <b>checking</b> of that class of formulas is co-NP complete. Addit [...] ...|$|R
