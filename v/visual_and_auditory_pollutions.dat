0|10000|Public
40|$|Groh. Eye-centered, head-centered, {{and complex}} coding of <b>visual</b> <b>and</b> <b>auditory</b> {{targets in the}} intraparietal sulcus. J Neurophysiol 94 : 2331 – 2352, 2005. First {{published}} April 20, 2005; doi: 10. 1152 /jn. 00021. 2005. The integration of <b>visual</b> <b>and</b> <b>auditory</b> events is thought to require a joint representation of <b>visual</b> <b>and</b> <b>auditory</b> space in a common reference frame. We investigated the coding of <b>visual</b> <b>and</b> <b>auditory</b> space in the lateral and medial intraparietal areas (LIP, MIP) {{as a candidate for}} such a representation. We recorded the activity of 275 neurons in LIP and MIP of two monkeys while they performed saccades to a row of <b>visual</b> <b>and</b> <b>auditory</b> targets from three different eye positions. We found 45 % of these neurons to be modulated by the locations of visual targets, 19 % by <b>auditory</b> targets, <b>and</b> 9 % by both <b>visual</b> <b>and</b> <b>auditory</b> targets. The reference frame for both <b>visual</b> <b>and</b> <b>auditory</b> receptive fields ranged along a continuum between eye- and head-centered reference frames with � 10 % of <b>auditory</b> <b>and</b> 33 % of visual neurons having receptive fields that were mor...|$|R
40|$|The {{equivalence}} of <b>visual</b> <b>and</b> <b>auditory</b> scatterplots {{was examined}} in two experi-ments. Experiment 1 {{examined the relationship}} between actual Pearson's rand <b>visual</b> <b>and</b> <b>auditory</b> judgments of direction and magnitude of correlation for 24 bivariate data samples. Experiment 2 directly evaluated <b>visual</b> <b>and</b> <b>auditory</b> percep-tual sensitivity to outliers by examining changes in perceived magnitude and direc-tion of correlation estimates for scatterplots from Experiment 1 that were altered by the addition of outlier points. Results suggest that the information conveyed by <b>visual</b> <b>and</b> <b>auditory</b> scatterplots is used very similarly by the two modalities. Both <b>visual</b> <b>and</b> <b>auditory</b> scatterplots are quite efficient in conveying sign and magnitude of correlation, and the effect of outliers on judged magnitude of correlation is simi-lar for the two types of data display...|$|R
40|$|A divided {{attention}} paradigm {{was used}} to investigate whether graphemes and phonemes can mutually activate or inhibit each other during bimodal processing. In 3 experiments, Dutch subjects reacted to <b>visual</b> <b>and</b> <b>auditory</b> targets in single-channel or bimodal stimuli. In some bimodal conditions, the <b>visual</b> <b>and</b> <b>auditory</b> targets were nominally identical or redundant (e. g., <b>visual</b> A <b>and</b> <b>auditory</b> /a/); in others they were not (e. g., <b>visual</b> U <b>and</b> <b>auditory</b> /a/). Temporal aspects of cross-modal activation were examined by varying the stimulus onset asynchrony of <b>visual</b> <b>and</b> <b>auditory</b> stimuli. Cross-modal facilitation [...] but not inhibition [...] occurred rapidly and automatically between phoneme and grapheme representations. Implications for current models of bimodal processing and word recognition are discussed...|$|R
40|$|Three {{studies are}} {{reported}} examining the grounding of abstract concepts across two modalities (<b>visual</b> <b>and</b> <b>auditory)</b> <b>and</b> their symbolic representation. A {{comparison of the}} outcomes across these studies reveals that the symbolic representation of political concepts <b>and</b> their <b>visual</b> <b>and</b> <b>auditory</b> modalities is convergent. In other words, the spatial relationships between specific instances of the political categories are highly overlapping across the symbolic, <b>visual</b> <b>and</b> <b>auditory</b> modalities. These findings suggest that abstract categories display redundancy across modal and amodal representations, and are multimodal...|$|R
40|$|The paper {{presents}} a general framework that facilitates {{the exploration of}} a single information-processing system in which <b>auditory</b> <b>and</b> <b>visual</b> information is integrated. The framework allows for learning, adaptation, knowledge discovery, and decision making. An application of the framework is a personidentification task in which face and voice recognition are combined in one system. Experiments are performed using <b>visual</b> <b>and</b> <b>auditory</b> dynamic features which synchronously extract <b>visual</b> <b>and</b> <b>auditory</b> information flows. The experimental results {{support the hypothesis that}} the recognition rate is considerably enhanced by combining <b>visual</b> <b>and</b> <b>auditory</b> dynamic features...|$|R
5000|$|Attention, {{reaction}} time <b>and</b> vigilance (<b>visual</b> <b>and</b> <b>auditory).</b>|$|R
40|$|Looming objects produce ecologically {{important}} {{signals that}} can be perceived in both the <b>visual</b> <b>and</b> <b>auditory</b> domains. Using a preferential looking technique with looming <b>and</b> receding <b>visual</b> <b>and</b> <b>auditory</b> stimuli, we examined the multisensory integration of looming stimuli by rhesus monkeys. We found a strong attentional preference for coincident <b>visual</b> <b>and</b> <b>auditory</b> looming but no analogous preference for coincident stimulus recession. Consistent with previous findings, the effect occurred only with tonal stimuli and not with broadband noise. The results suggest an evolved capacity to integrate multisensory looming objects...|$|R
50|$|Multimedia {{learning}} {{refers to}} the use of <b>visual</b> <b>and</b> <b>auditory</b> teaching materials that may include video, computer and other information technology. Multimedia learning theory focuses on the principles that determine the effective use of multimedia in learning, with emphasis on using both the <b>visual</b> <b>and</b> <b>auditory</b> channels for information processing.|$|R
5000|$|Low-level {{processing}} (<b>visual</b> <b>and</b> <b>auditory</b> acuity, gender, age, mood, …) ...|$|R
5000|$|... #Subtitle level 2: Computational {{theories}} of <b>visual</b> <b>and</b> <b>auditory</b> receptive fields ...|$|R
5000|$|The <b>visual</b> <b>and</b> <b>auditory</b> {{components}} are external stimuli that are attended to.|$|R
5000|$|Neurologists {{look for}} motor skills, brain {{functions}} which include <b>visual</b> <b>and</b> <b>auditory</b> perception.|$|R
5000|$|Sensory processing, {{primarily}} in the <b>visual</b> <b>and</b> <b>auditory</b> modalities, including specific temporal and parietal areas ...|$|R
5000|$|Women have an Islamic {{right to}} <b>visual</b> <b>and</b> <b>auditory</b> {{access to the}} musalla (main sanctuary).|$|R
40|$|In {{synchronized}} trampolining, two gymnasts {{perform the}} same routine {{at the same}} time. While trained gymnasts are thought to coordinate their own movements with the movements of another gymnast by detecting relevant movement information, the question arises how <b>visual</b> <b>and</b> <b>auditory</b> information contribute {{to the emergence of}} synchronicity between both gymnasts. Therefore the aim {{of this study was to}} examine the role of <b>visual</b> <b>and</b> <b>auditory</b> information in the emergence of coordinated behaviour in synchronized trampolining. Twenty female gymnasts were asked to synchronize their leaps with the leaps of a model gymnast, while <b>visual</b> <b>and</b> <b>auditory</b> information was manipulated. The results revealed that gymnasts needed more leaps to reach synchronicity when only either auditory (12. 9 leaps) or visual information (10. 8 leaps) was available, as compared to when both <b>auditory</b> <b>and</b> <b>visual</b> information was available (8. 1 leaps). It is concluded that <b>visual</b> <b>and</b> <b>auditory</b> information play significant roles in synchronized trampolining, whilst visual information seems to be the dominant source for emerging behavioural synchronization, <b>and</b> <b>auditory</b> information supports this emergence...|$|R
40|$|The {{work was}} written in C + + whit qt toolkit: a "program for the {{measurement}} of reaction time to <b>visual</b> <b>and</b> <b>auditory</b> stimulus, which was tested {{on a group of}} patients and statistically evaluated the reaction time to <b>visual</b> <b>and</b> <b>auditory</b> stimuli. Measurements were performed on the age group from 18 to 28 years, which was not in any way physically or mentally handicapped...|$|R
40|$|Abstract — To {{localize}} a seen object, {{the superior}} colliculus {{of the barn}} owl integrates the <b>visual</b> <b>and</b> <b>auditory</b> localization cues which are accessed from the sensory system of the brain. These cues are formed as <b>visual</b> <b>and</b> <b>auditory</b> maps, thus the alignment between <b>visual</b> <b>and</b> <b>auditory</b> maps {{is very important for}} accurate localization in prey behavior. Blindness or prism wearing may disturb this alignment. The juvenile barn owl could adapt its auditory map to this mismatch after several weeks training. Here we investigate this process by building a computational model of <b>auditory</b> <b>and</b> <b>visual</b> integration with map adjustment in the deep superior colliculus. The adaptation is based on activity dependent axon developing which is instructed by an inhibitory network. In the inhibitory network, the strength of the inhibition is adjusted by spike timing dependent plasticity(STDP). The simulation results are in line with the biological experiment and support the idea that the STDP is involved in the alignment of sensory maps. The system of the model provides a new mechanism capable of eliminating the disparity in <b>visual</b> <b>and</b> <b>auditory</b> map integration. I...|$|R
40|$|The {{present study}} {{investigated}} whether the <b>visual</b> <b>and</b> <b>auditory</b> Simon effects could {{be accounted for}} by the same mechanism. In a single experiment we performed a detailed comparison of the <b>visual</b> <b>and</b> the <b>auditory</b> Simon effects arising in behavioural responses and in pupil dilation, a psychophysiological measure considered as a marker of the cognitive effort induced by conflict processing. To address our question, we performed sequential and distributional analyses on both reaction times and pupil dilation. Results confirmed that the mechanisms underlying the <b>visual</b> <b>and</b> <b>auditory</b> Simon effects are functionally equivalent in terms of the interaction between unconditional and conditional response processes. The two modalities, however, differ with respect to the strength of their activation and inhibition. Importantly, pupillary data mirrored the pattern observed in behavioural data for both tasks, adding physiological evidence to the current literature on the processing of <b>visual</b> <b>and</b> <b>auditory</b> information in a conflict task...|$|R
5|$|Birds {{communicate}} using primarily <b>visual</b> <b>and</b> <b>auditory</b> signals. Signals can be interspecific (between species) and intraspecific (within species).|$|R
50|$|Mental imagery, {{especially}} <b>visual</b> <b>and</b> <b>auditory</b> imagery, can exacerbate and aggravate {{a number}} of mental and physical conditions.|$|R
30|$|The {{reaction}} {{time when a}} controller makes a mistake for both <b>visual</b> <b>and</b> <b>auditory</b> stimuli is of similar values. There is {{no significant difference between}} the <b>visual</b> <b>and</b> <b>auditory</b> cases using a one-tail student's t-test (ρ = 0.05). The {{reaction time}} when making a mistake is always faster than when not making a mistake. However, because of the variance in reaction time, the test of significance results is inconclusive.|$|R
40|$|Chronic {{alcoholism}} {{leads to}} impaired <b>visual</b> <b>and</b> <b>auditory</b> processing of emotions, but the cross-modal (auditory-visual) processing of emotional stimuli {{has not yet}} been explored. Our objectives were to describe the electrophysiological correlates of unimodal (<b>visual</b> <b>and</b> <b>auditory)</b> impairments in emotion processing in people suffering from alcoholism, to determine whether this deficit is general or emotion-specific, and to explore potential deterioration in the specific cross-modal integration processes in alcoholism. Journal Articleinfo:eu-repo/semantics/publishe...|$|R
50|$|Robocop carries guns {{designed}} {{for him and}} is equipped with enhanced reflexes, speed <b>and</b> strength, <b>visual</b> <b>and</b> <b>auditory</b> capabilities.|$|R
40|$|Closing {{the eyes}} helps memory. We {{investigated}} the mechanisms underlying the eyeclosure effect by exposing 80 eyewitnesses to {{different types of}} distraction during the witness interview: blank screen (control), eyes closed, <b>visual</b> distraction, <b>and</b> <b>auditory</b> distraction. We examined the cognitive load hypothesis by comparing any type of distraction (visual or auditory) with minimal distraction (blank screen or eyes closed). We found recall to be significantly better when distraction was minimal, providing evidence that eyeclosure reduces cognitive load. We examined the modality-specific interference hypothesis by comparing the effects of <b>visual</b> <b>and</b> <b>auditory</b> distraction on recall of <b>visual</b> <b>and</b> <b>auditory</b> information. <b>Visual</b> <b>and</b> <b>auditory</b> distraction selectively impaired memory for information presented in the same modality, supporting the role of visualisation in the eyeclosure effect. Analysis of recall in terms of grain size revealed that recall of basic information about the event was robust, whereas recall of specific details was prone to both general and modality-specific disruptions...|$|R
40|$|Unilateral spatial neglect due {{to right}} brain damage (RBD) {{can occur in}} several {{different}} sensory modalities in the same patient. Previous studies of the association between <b>auditory</b> <b>and</b> <b>visual</b> neglect have yielded conflicting outcomes. Most such studies have compared performance on relatively simple clinical measures of visual neglect, such as target cancellation, with that on more sophisticated measures of auditory perception. This is problematic because such tasks are typically not matched for the cognitive processes they exercise. We overcame this limitation by using equivalent <b>visual</b> <b>and</b> <b>auditory</b> versions of extinction and temporal-order judgment (TOJ) tasks. RBD patients demonstrated lateralized deficits on both <b>visual</b> <b>and</b> <b>auditory</b> tasks when compared with same-aged, healthy controls. Critically, a significant association between the severity of <b>visual</b> <b>and</b> <b>auditory</b> deficits was apparent on the TOJ task but not the extinction task, suggesting that even when task demands are matched across modalities, dissociations between <b>visual</b> <b>and</b> <b>auditory</b> neglect can be apparent. Across the auditory tasks, patients showed more pronounced deficits for verbal stimuli than for non-verbal stimuli. These findings have implications for recent models proposed to explain the role of spatial attention in multimodal perception. (c) 2007 Elsevier Ltd. All rights reserved...|$|R
40|$|Older {{adults are}} known to have reduced {{inhibitory}} control and therefore to be more distractible than young adults. Recently, we have proposed that sensory modality plays a crucial role in age-related distractibility. In this study, we examined age differences in vulnerability to unimodal <b>and</b> cross-modal <b>visual</b> <b>and</b> <b>auditory</b> distraction. A group of 24 younger (mean age = 21. 7 years) and 22 older adults (mean age = 65. 4 years) performed <b>visual</b> <b>and</b> <b>auditory</b> n-back tasks while ignoring <b>visual</b> <b>and</b> <b>auditory</b> distraction. Whereas reaction time data indicated that both young and older adults are particularly affected by unimodal distraction, accuracy data revealed that older adults, but not younger adults, are vulnerable to cross-modal visual distraction. These results support the notion that age-related distractibility is modality dependent. (C) 2012 Elsevier B. V. All rights reserved...|$|R
40|$|Abstract. What impact do <b>visual</b> <b>and</b> <b>auditory</b> sensory cues as in-store {{innovations}} exert on shopper’s {{approach and}} touch behaviour at point-of-purchase in a retail setting? The presented research depicts shopper’s behavioral response {{in relation to}} the influence of sensory cues for an appealing and attracting store atmosphere and design. The author presents a review of theoretically relevant work and a field study through experimental and observational methods in examining the impact of <b>visual</b> <b>and</b> <b>auditory</b> sensory cues as in-store innovations in a retail setting. In the reported study, the author finds significant behavioral impact of introducing <b>visual</b> <b>and</b> <b>auditory</b> sensory cues on shopper’s approach and touch behaviour. The findings offer guidelines for retail managers in applying sensory cues as retailing innovations {{in relation to the}} human senses in creating successful sensory experiences at point-of-purchase...|$|R
40|$|The aim of {{this study}} is to know how <b>visual</b> <b>and</b> <b>auditory</b> stimuli {{influence}} the visual attention of the electronic magazines and to study if this kind of stimuli influences the comprehension of the text. With the eye tracking technique, orthogonal design and regression model, this paper, analyzed the audience’s eye movement when reading Chinese e-magazine under <b>visual</b> <b>and</b> <b>auditory</b> stimulations. The findings obtained in this study reveal a better knowledge of the e-magazine reading behavior...|$|R
40|$|BACKGROUND: It is {{well-known}} {{that human beings}} are able to associate stimuli (novel or not) perceived in their environment. For example, this ability is used by children in reading acquisition when arbitrary associations between <b>visual</b> <b>and</b> <b>auditory</b> stimuli must be learned. The studies tend to consider it as an "implicit" process triggered by the learning of letter/sound correspondences. The study described in this paper examined whether the addition of the visuo-haptic exploration would help adults to learn more effectively the arbitrary association between <b>visual</b> <b>and</b> <b>auditory</b> novel stimuli. METHODOLOGY/PRINCIPAL FINDINGS: Adults were asked to learn 15 new arbitrary associations between <b>visual</b> stimuli <b>and</b> their corresponding sounds using two learning methods which differed according to the perceptual modalities involved in the exploration of the visual stimuli. Adults used their visual modality in the "classic" learning method <b>and</b> both their <b>visual</b> <b>and</b> haptic modalities in the "multisensory" learning one. After both learning methods, participants showed a similar above-chance ability to recognize the <b>visual</b> <b>and</b> <b>auditory</b> stimuli <b>and</b> the audio-visual associations. However, the ability to recognize the visual-auditory associations was better after the multisensory method than after the classic one. CONCLUSION/SIGNIFICANCE: This study revealed that adults learned more efficiently the arbitrary association between <b>visual</b> <b>and</b> <b>auditory</b> novel stimuli when the visual stimuli were explored with both vision and touch. The results are discussed from the perspective of how they relate to the functional differences of the manual haptic modality and the hypothesis of a "haptic bond" between <b>visual</b> <b>and</b> <b>auditory</b> stimuli...|$|R
30|$|Reaction time {{to visual}} targets was slower than to <b>auditory</b> ones <b>and</b> was {{positively}} correlated to performance on both <b>visual</b> <b>and</b> <b>auditory</b> targets.|$|R
50|$|Normative {{theories}} for <b>visual</b> <b>and</b> <b>auditory</b> receptive fields {{founded on}} the scale-space framework are described in the article on axiomatic theory of receptive fields.|$|R
40|$|Objectives : To {{investigate}} {{the relationship between}} amyotrophic lateral sclerosis (ALS) and cognitive function by means of oddball event-related potentials (ERPs) and to determine the usefulness of this methodology in the cognitive status assessment of physically disabled patients. Methods : <b>Visual</b> <b>and</b> <b>auditory</b> oddball ERPs were recorded in 16 consecutive sporadic ALS patients. A comprehensive battery of neuropsychological (NP) tests assessed intelligence, executive functions, attention, memory, word fluency, visuo-motor and visual-constructive skills. Results : All patients performed <b>visual</b> <b>and</b> <b>auditory</b> ERPs <b>and</b> 75...|$|R
40|$|Multimodal {{interfaces}} {{with both}} <b>visual</b> <b>and</b> <b>auditory</b> output are becoming important, especially for applications using small-screen displays and for user access under mo-bile conditions. The research presented here investigated {{the feasibility of}} simulta-neously presenting distinct textual information through both <b>visual</b> <b>and</b> <b>auditory</b> channels by examining two multimodal interfaces with irrelevant or relevant auditory information. These interfaces were intended to study two problems: (a) Can users at-tend to and process additional information delivered through the auditory channel during a typical Web-browsing process, and (b) what are the effects of information overlap between the <b>visual</b> <b>and</b> <b>auditory</b> channels? Controlled experiments were con-ducted to evaluate these two questions. The findings suggest that users can attend to auditory information while visually browsing textual information and that informa-tion overlap may reduce distraction. These findings have implications {{for the design of}} multimodal interfaces for small-screen mobile applications. 1...|$|R
40|$|Alertness is a nonselective {{attention}} {{component that}} refers {{to a state of}} general readiness that improves stimulus processing and response initiation. We used functional magnetic resonance imaging (fMRI) to identify neural correlates of <b>visual</b> <b>and</b> <b>auditory</b> alertness. A further aim was to investigate the modulatory effects of the cholinergic agonist nicotine. Nonsmoking participants were given either placebo or nicotine (NICORETTE gum, 2 mg) and performed a target-detection task with warned and unwarned trials in the <b>visual</b> <b>and</b> <b>auditory</b> modality. Our results provide evidence for modality-specific correlates of <b>visual</b> <b>and</b> <b>auditory</b> alertness in respective higher-level sensory cortices and in posterior parietal and frontal brain areas. The only region commonly involved in <b>visual</b> <b>and</b> <b>auditory</b> alertness was the right superior temporal gyrus. A connectivity analysis showed that this supramodal region exhibited modality-dependent coupling with respective higher sensory cortices. Nicotine was found to mainly decrease <b>visual</b> <b>and</b> <b>auditory</b> alertness-related activity in several brain regions, which was evident as a significant interaction of nicotine-induced decreases in BOLD signal in warned trials and increases in unwarned trials. The cholinergic drug also affected alerting-dependent activity in the supramodal right superior temporal gyrus; here the effect {{was the result of a}} significant increase of neural activity in unwarned trials. We conclude that the role of the right superior temporal gyrus is to induce an "alert" state in response to warning cues and thereby optimize stimulus processing and responding. We speculate that nicotine increases brain mechanisms of alertness specifically in conditions where no extrinsic warning is provided...|$|R
25|$|As {{little as}} 0.2 millijoules may present an {{ignition}} hazard; such low spark energy is often below {{the threshold of}} human <b>visual</b> <b>and</b> <b>auditory</b> perception.|$|R
50|$|Harrison and Mortensen (1962) trained {{subjects}} using <b>visual</b> <b>and</b> <b>auditory</b> EMG biofeedback {{to control}} individual motor {{units in the}} tibialis anterior muscle of the leg.|$|R
