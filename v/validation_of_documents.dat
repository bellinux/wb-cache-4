6|10000|Public
50|$|The titles, {{the flag}} and the coat of arms are Bilbao's {{traditional}} symbols and belong to its historic patrimony, being used in formal acts, for the identification and decoration of specific places or for the <b>validation</b> <b>of</b> <b>documents.</b>|$|E
50|$|However, such {{documents}} are still fully parsable in the non-standalone mode of validating parsers, which signals an error if it can't locate these external entities with their specified public identifier (FPI) or system identifier (a URI), or are inaccessible. (Notations declared in the DTD are also referencing external entities, but these unparsed entities are not {{needed for the}} <b>validation</b> <b>of</b> <b>documents</b> in the standalone mode of these parsers: the validation of all external entities referenced by notations is left to the application using the SGML or XML parser). Non-validating parsers may eventually attempt to locate these external entities in the non-standalone mode (by partially interpreting the DTD only to resolve their declared parsable entities), but do not validate the content model of these documents.|$|E
30|$|The main {{objective}} of this work {{is to develop a}} method that generalizes the treatment of part of the particularities of NCL, allowing the development of a validation process that deals with a reduced number of particular cases. Besides generalizing part of the process, we also look forward to giving support to the incremental <b>validation</b> <b>of</b> <b>documents,</b> which is normally a desirable feature for authoring tools.|$|E
40|$|Dried {{leaves of}} the {{widespread}} African ethnomedicinal herb Acmella caulirhiza (Asteraceae) have been phytochemically characterised. The hexane extract yielded two unsaturated alkylamides, spilanthol and a novel compound tentatively identified as N-isobutylnona- 2 E, 4 E-dien- 8 -ynamide based on 1 H, 13 C, COSY and HETCOR spectra. The occurrence of these amides supports the <b>validation</b> <b>of</b> <b>documented</b> traditional usage patterns. Copyright © NISC Pty Ltd...|$|R
30|$|Among other NCL features, this {{perspective}} notion {{is one of}} the cases that hinder the <b>validation</b> <b>of</b> NCL <b>documents</b> with basis on XML Schema only. Actually, XML Schema would only be enough for syntactic and structural <b>validation</b> <b>of</b> the <b>documents,</b> since there are several particularities in NCL that it cannot express. For example, when checking the correctness of a link, it is necessary to be sure whether the nodes that are being associated are valid identifiers and are present in the same composition (which cannot be verified by an XML Schema specification alone).|$|R
40|$|Document {{binarization}} is {{an active}} research area for many years. There are many difficulties associated with satisfactory binarization <b>of</b> <b>document</b> images and especially in cases <b>of</b> degraded historical <b>documents.</b> In this paper, we try {{to answer the question}} “how well an existing binarization algorithm can binarize a degraded document image? ” We propose a new technique for the <b>validation</b> <b>of</b> <b>document</b> binarization algorithms. Our method is simple in its implementation and can be performed on any binarization algorithm since it doesn’t require anything more than the binarization stage. Then we apply the proposed technique to 30 existing binarization algorithms. Experimental results and conclusions are presented. 1...|$|R
40|$|The {{main purpose}} of the paper is to present {{a case study of}} the use of {{semantic}} technologies within e-Government context. We aim to describe the semantic layer that was designed in the scope of the SPOCS project to enable the <b>validation</b> <b>of</b> <b>documents</b> in the scope of cross border procedures. The SPOCS project (Simple Procedures Online for Cross- Border Services) 1 focuses in particular on the transfer of documents to European administrations in the scope of procedures to deliver services across borders. One of the problems that arise is the heterogeneity of documents between countries even they have the same role. With the increase of cross border activities encouraged by the service directive in particular, electronic procedures are more important than ever. The project has therefore set up an infrastructure including the transfer of documents in a metadata container and the <b>validation</b> <b>of</b> <b>documents.</b> Cross-border service pilots have been implemented to show the added value of the SPOCS infrastructure. To overcome the problem of semantic equivalence of documents, the European Commission has encouraged the implementation of a semantic layer to support the interoperability between administrative systems and the reuse of resources across government bodies. In the scope of the SPOCS project, we have implemented a set of pilots to illustrate the added value of implementing a semantic interoperability layer and developing common models and vocabularies. Common ontologies have been designed to describe documents, equivalences, and procedures. Specific access mechanisms have been implemented to automatically verify the status of European companies. Finally, semantic rules have been created to determine the procedure that needs to be used in order to provide a cross border service. SPOCS resources are posted to the JoinUp portal...|$|E
40|$|Digital {{documents}} {{become increasingly}} {{present in the}} routine of people and Higher Education Institutions (HEI) increasing the concern with the integrity of data, assurance of identity and confidentiality of the parties involved. The universities must, by law, keep the custody of student’s documents, records of grades and absences, ratings and so on, for years even for perpetual guard. Therefore, when a HEI adopt a model of custody, treatment and <b>validation</b> <b>of</b> <b>documents</b> in electronic or digital media, also adopts new management processes of information that may affect the organizational culture. The objective {{of this study was}} identify e analyze the interfaces between innovation in electronic document management and organizational culture related to process changes in electronic document custody through a case study at a community HEI. The results shows that, though there are adaptation difficulties to the process because of the HEI culture, with the maturation of the process, learning expands the benefits of the new process through dissemination of innovation and their reflects in other areas. The study concludes that the scanning process also brings advantages where it was applied, the benefits extended to other areas of the HEI...|$|E
40|$|False-positives are {{a problem}} in anomaly-based {{intrusion}} detection systems. To counter this issue, we discuss anomaly detection for the eXtensible Markup Language (XML) in a language-theoretic view. We argue that many XML-based attacks target the syntactic level, i. e. the tree structure or element content, and syntax validation of XML documents reduces the attack surface. XML offers so-called schemas for validation, but in real world, schemas are often unavailable, ignored or too general. In this work-in-progress paper we describe a grammatical inference approach to learn an automaton from example XML documents for detecting documents with anomalous syntax. We discuss properties and expressiveness of XML to understand limits of learnability. Our contributions are an XML Schema compatible lexical datatype system to abstract content in XML and an algorithm to learn visibly pushdown automata (VPA) directly from a set of examples. The proposed algorithm {{does not require the}} tree representation of XML, so it can process large documents or streams. The resulting deterministic VPA then allows stream <b>validation</b> <b>of</b> <b>documents</b> to recognize deviations in the underlying tree structure or datatypes. Comment: Paper accepted at First Int. Workshop on Emerging Cyberthreats and Countermeasures ECTCM 201...|$|E
40|$|Abstract: Document {{binarization}} is {{an active}} research area for many years. The choice of the most appropriate binarization algorithm for each case {{proved to be a}} very difficult procedure itself. In this paper, we propose a new technique for the <b>validation</b> <b>of</b> <b>document</b> binarization algorithms. Our method is simple in its implementation and can be performed on any binarization algorithm since it doesn’t require anything more than the binarization stage. As a demonstration of the proposed technique, we use the case <b>of</b> degraded historical <b>documents.</b> Then we apply the proposed technique to 30 binarization algorithms. Experimental results and conclusions are presented...|$|R
50|$|There {{are several}} {{companies}} that offer <b>document</b> <b>validation</b> and/or authentication <b>of</b> <b>documents</b> ranging from mobile-phone software to specialised document scanners.|$|R
40|$|Abstract: The paper {{presents}} a toolkit {{for the design}} and the interactive <b>validation</b> <b>of</b> message-based <b>document</b> exchange {{within the context of}} multi-partner electronic business transactions. The business case used in this paper is a simplified version of the purchasing process used in the electronics industry, based on the EDIFICE standard...|$|R
50|$|The current {{specification}} defines 4 types <b>of</b> <b>validation</b> {{and certification}} services:- Certification of Possession of Data (cpd),- Certification of Claim of Possession <b>of</b> Data (ccpd),- <b>Validation</b> <b>of</b> Digitally Signed <b>Document</b> (vsd), and- <b>Validation</b> <b>of</b> Public Key Certificates (vpkc).|$|R
40|$|We {{show that}} an XML DTD (Document Type Definition) {{can be viewed}} as the fixed point of a {{parametric}} content model. We then use natural transformations from the source content model to the target content model to derive DTDaware and validated XML <b>document</b> transformations. Benefits <b>of</b> such transformations include static type-checking of XML transformational programs, automatic <b>validation</b> <b>of</b> target <b>documents,</b> and modular compositions <b>of</b> XML <b>document</b> transformers...|$|R
50|$|<b>Validation</b> <b>of</b> an {{instance}} <b>document</b> against a schema {{can be regarded}} as a conceptually separate operation from XML parsing. In practice, however, many schema validators are integrated with an XML parser.|$|R
40|$|International audienceA {{multimedia}} document {{authoring system}} should provide analysis and validation tools that help authors find and correct mistakes before document deployment. Although very useful, multimedia validation tools are not often provided. Spatial <b>validation</b> <b>of</b> multimedia <b>documents</b> may be performed over the initial position of media items before presentation starts. However, {{such an approach}} {{does not lead to}} ideal results when media item placement changes over time. Some document authoring languages allow the definition of spatio-temporal relationships among media items and they can be moved or resized during runtime. Current validation approaches do not verify dynamic spatio-temporal relationships. This paper presents a novel approach for spatio-temporal <b>validation</b> <b>of</b> multimedia <b>documents.</b> We model the document state, extending the Simple Hyperme-dia Model (SHM), comprising media item positioning during the whole document presentation. Mapping between document states represent time lapse or user interaction. We also define a set of atomic formulas upon which the author's expectations related to the spatio-temporal layout can be described and analyzed...|$|R
40|$|This paper {{presents}} a visual {{approach to the}} representation and <b>validation</b> <b>of</b> multimedia <b>document</b> structures specified in XML and transformation of one structure to another. The underlying theory of our approach is a context-sensitive graph grammar formalism. The paper demonstrates the conciseness and expressiveness of the graph grammar formalism. An example XML structure is provided and its graph grammar representation, validation and transformation to a multimedia representation are presented...|$|R
50|$|In September 2008 the Luxembourg’s {{financial}} {{regulatory authority}} CSSF issued a circular 08/371 {{resulting in the}} official adoption of the e-file platform for the mandatory transmission of UCI simplified and full prospectuses and annual and semi-annual reports both to the CSSF and to the electronic reference database of the financial centre’s investment funds set up by Finesti.The platform allows the final <b>validation</b> <b>of</b> the <b>documents</b> {{that are to be}} sent.|$|R
40|$|A markup {{language}} for business reporting must satisfy many demanding criteria: readable by novices, extendable by users, minimum payload overheads, and a uniform graph structure to enable <b>validation</b> <b>of</b> <b>document</b> instance with minimal programming effort. To be elegant and robust {{it must be}} based on a model that reflects the intricacies of business reporting, and to be efficient in terms of maintenance it must be modular in structure. We suggest the skeleton of a derivative of the XBRL that exhibits most of the criteria stated above which uses the basic semantic structure provided in its specification and the associated C&I taxonomy. Our proposal provides domain-specific tags so that even the source documents are very readable. We provide a proof-of-concept schema for the Balance Sheet (using the XBRL C&I taxonomy) as an instance of a canonical generic labeled graph model for any financial statement. We also provide an algorithm for the <b>validation</b> <b>of</b> such labeled directed graph representation of a financial statement and its implementation in the programming language Java...|$|R
40|$|We discuss goals, concepts, and {{limitations}} of utilizing hypermedia technology to support Computer Science lecture, and communication and presentation techniques for interactive learning material. The paper describes a practical example of an interactive hypertextbook, its integration with our learning and programming environment, and lists problems, questions and some hints for future Mosaic and HTML enhancements. Contents 1 Courseware Elements 1 2 <b>Validation</b> <b>of</b> <b>Document</b> Concepts 2 3 Orientation and Navigation Support 3 4 Active Hypertext Documents 3 5 Presentation Techniques 4 6 Summary 4 1 Courseware Elements We exploit HTML and Mosaic to construct a local and non-generic hypertext version of a lecture manuscript "Basics of Computer Science" at Technical University of Darmstadt. So far we {{do not intend to}} implement a virtual university like for instance GNA 1 or Paieida 2, which use Internet as a campus-like base for distant education, even though many topics and problems ar [...] ...|$|R
50|$|The {{production}} <b>of</b> a data <b>validation</b> certificate {{in response}} to a signed request for <b>validation</b> <b>of</b> a signed <b>document</b> or public key certificate also provides evidence that due diligence was performed by the requester in validating a digital signature or public key certificate.|$|R
50|$|The {{original}} {{purpose of the}} doctype was to enable parsing and <b>validation</b> <b>of</b> HTML <b>documents</b> by SGML tools based on the Document Type Definition (DTD). The DTD to which the DOCTYPE refers contains a machine-readable grammar specifying the permitted and prohibited content for a document conforming to such a DTD. Browsers, on the other hand, do not implement HTML as an application of SGML and by consequence do not read the DTD.|$|R
40|$|Abstract-This article reviews recent {{research}} into the use of hierarchic agglomerative clustering methods for document retrieval. After {{an introduction to the}} calculation of interdocument similarities and to clustering methods that are appropriate for document clustering, the article discusses algorithms {{that can be used to}} allow the implementation of these methods on databases of nontrivial size. The <b>validation</b> <b>of</b> <b>document</b> hierarchies is described using tests based on the theory of random graphs and on empirical char-acteristics <b>of</b> <b>document</b> collections that are to be clustered. A range of search strategies is available for retrieval from document hierarchies and the results are presented of a series of research projects that have used these strategies to search the clusters result-ing from several different types of hierarchic agglomerative clustering method. It is sug-gested that the complete linkage method is probably the most effective method in terms of retrieval performance; however, it is also difficult to implement in an efficient man-ner. Other applications <b>of</b> <b>document</b> clustering techniques are discussed briefly; exper-imental evidence suggests that nearest neighbor clusters, possibly represented as a network model, provide a reasonably efficient and effective means of including inter-document similarity information in document retrieval systems. 1...|$|R
30|$|Nested Context Language (NCL) is a modular {{declarative}} hypermedia language {{focused on}} the definition of synchronization relationships between media objects. It is the standard language of ISDB-Tb for development of interactive applications and is also an international ITU-T recommendation for IPTV [9]. The complete <b>validation</b> <b>of</b> NCL <b>documents</b> cannot be made through traditional XML validation techniques, since NCL has a set of particularities that are not supported by these techniques. Section 3 brings up some of these particularities.|$|R
40|$|Different {{models have}} been {{recently}} proposed for representing temporal data, tracking historical information, and recovering the state <b>of</b> the <b>document</b> as <b>of</b> any given time, in XML documents. After presenting an abstract model for temporal XML, we discuss the problem <b>of</b> the <b>validation</b> <b>of</b> the temporal constraints imposed by this model. We first review the problem of checking and fixing isolated temporal inconsistencies. Then, {{we move on to}} study <b>validation</b> <b>of</b> a <b>document</b> when many temporal inconsistencies of different kinds are present. We study the conditions that allow to treat each inconsistency isolated from the rest, and give the corresponding proofs. These properties are intended to be the basis of efficient algorithms for checking temporal consistency in XML...|$|R
30|$|This {{paper is}} {{organized}} as follows: Sect.  2 presents some related work. Section 3 discusses the particularities <b>of</b> the <b>validation</b> <b>of</b> NCL <b>documents</b> and presents the metalanguage {{used to support}} the incremental validation proposed herein. Section 4 illustrates an implementation of the proposed method as a proof of concept. Section 5 presents the integration between our implementation and the IDE Composer. Section 6 discusses a performance analysis, making a comparison between a non-incremental and an incremental approach. Finally, Sect.  7 brings the conclusions of this work.|$|R
30|$|This work {{presents}} a detailed proposal for incremental <b>validation</b> <b>of</b> NCL <b>documents,</b> which meets the non-functional requirements {{raised in the}} previous paragraphs and carries out a study in order to quantify the gain <b>of</b> the incremental <b>validation,</b> compared to the traditional method. This validation proposal is also generic, that is, it {{can be adapted to}} various authoring environments, both visual and textual, and has a component-based architecture, allowing components to be replaced by others with the same interface, but still keeping the whole functional process.|$|R
40|$|<b>Validation</b> <b>of</b> XML <b>documents</b> is a {{challenging}} task whose {{aim is to}} ensure interoperability <b>of</b> such <b>documents</b> in various environments. With massive use of different XML languages, automated validation is often {{the only way to}} ensure compliance with different standards and recommendations. That's why it is important to use expressive validation languages and powerful and convenient validation tools and techniques to discover maximum standard violations automatically. This is the aim of the Relaxed project which is introduced within this text. Different approaches to face today's pressing validation issues and challenges used in Relaxed are examined; e. g. maximizing validation results by using expressive validation languages, schema modularization and reusability, effective compound <b>document</b> <b>validation</b> and <b>validation</b> <b>of</b> non-XML languages. Advanced approaches to XML document validatio...|$|R
50|$|When {{applied to}} {{required}} changes, a refined set of examples is effectively a specification and a business-oriented test for acceptance of software functionality. After {{the change is}} implemented, specification with examples becomes a document explaining existing functionality. As the <b>validation</b> <b>of</b> such <b>documents</b> is automated, when they are validated frequently, such documents are a reliable source of information on business functionality of underlying software. To distinguish between such documents and typical printed documentation, which quickly gets outdated, {{a complete set of}} specifications with examples is called Living Documentation.|$|R
40|$|The {{dominance}} of digital objects in today’s information landscape {{has changed the}} way humankind creates and exchanges information. However, it has also brought an entirely new problem: the longevity of digital objects. Due to the fast changes in technologies, digital documents have a short lifespan before they become obsolete. Digital preservation, i. e. actions to ensure longevity of digital information, thus has become a pressing challenge. Different strategies such as migration and emulation have been proposed; however, the decision between available tools for format migration is very complex. Preservation planning supports decision makers in reaching accountable decisions by evaluating potential strategies against well-defined requirements. Especially the evaluation of different migration tools for digital preservation has to rely on validating the converted objects and thus on {{an analysis of the}} logical structure and the content <b>of</b> <b>documents.</b> This paper presents the eXtensible Characterisation Languages (XCL) that support the automatic <b>validation</b> <b>of</b> <b>document</b> conversions and the evaluation of migration quality by hierarchically decomposing a document and XML language. We present the context of the development of these languages and tools and describe the overall concept and features of the languages and how they {{can be applied to the}} evaluation of digital preservation solutions...|$|R
40|$|We {{investigate}} the incremental <b>validation</b> <b>of</b> XML <b>documents</b> {{with respect to}} DTDs and XML Schemas, under updates consisting of element tag renamings, insertions and deletions. DTDs are modeled as extended context-free grammars and XML Schemas are abstracted as "specialized DTDs", allowing to decouple element types from element tags. For DTDs, we exhibit an O(m log n) incremental validation algorithm using an auxiliary structure of size O(n), where n is the size <b>of</b> the <b>document</b> and m the number of updates. For specialized DTDs, we provide an O(m log² n) incremental algorithm, again using an auxiliary structure of size O(n). This is a significant improvement over brute-force re-validation from scratch...|$|R
40|$|AbstractThe <b>validation</b> <b>of</b> XML <b>documents</b> {{against a}} DTD is well {{understood}} and tools exist {{to accomplish this}} task. But the problem considered here is the <b>validation</b> <b>of</b> a generator <b>of</b> XML <b>documents.</b> The desired outcome is to establish for a particular generator that it is incapable of producing invalid output. Many (X) HTML web pages are generated from a document containing embedded scripts written in languages such as PHP. Existing tools can validate any particular instance of the XHTML generated from the document. Howevere there is no tool for validating the document itself, guaranteeing that all instances that might be generated are valid. A prototype validating tool for scripted-documents has been developed which uses a notation developed to capture the generalised output from the document and a systematically augmented DTD...|$|R
40|$|XSEM is a {{modeling}} language {{created for}} modeling XML schemas. Its implementation, called eXolutio, provides a user friendly interface for modeling schemas in XSEM. The {{aim of this}} work is to design an algorithm for <b>validation</b> <b>of</b> XML <b>documents</b> against XSEM PSM schemas and comparison of the expressive power of XSEM PSM with other XML schema languages. The thesis contains the description of fundamental algorithm and all the extensions required to fully support all the constructs and capabilities provided by XSEM. The practical part describes {{the implementation of the}} proposed algorithm as an extension of eXolutio...|$|R
40|$|We {{describe}} an e#cient method for the incremental <b>validation</b> <b>of</b> XML <b>documents</b> after composite updates. We introduce {{the class of}} Bounded-Edit (BE) DTDs and XML Schemas, and give a simple incremental revalidation algorithm that yields optimal performance for them, {{in the sense that}} its time complexity is linear in the number of operations in the update. We give extensive experimental results showing that our algorithm exhibits excellent scalability. Finally, we provide a statistical analysis of over 250 DTDs and XML Schema specifications found on the Web, showing that over 99 % of them are in fact in BE...|$|R
30|$|There {{are several}} works in {{literature}} that discuss XML incremental validation {{in many different}} scenarios. The fact that XML has become the de facto standard for storage and exchange of information in modern computer systems has increased {{the need for more}} efficient <b>validation</b> techniques <b>of</b> such <b>documents.</b> In this subsection is discussed some of these works in several contexts.|$|R
40|$|Abstract. We {{present an}} {{algorithm}} that generalizes HTML <b>validation</b> <b>of</b> individual <b>documents</b> {{to work on}} context-free sets <b>of</b> <b>documents.</b> To-gether with a program analysis that soundly approximates the output of Java Servlets and JSP web applications as context-free languages, we obtain a method for statically checking that such web applications never produce invalid HTML at runtime. Experiments with our prototype im-plementation demonstrate that the approach is useful: On 6 open source web applications consisting {{of a total of}} 104 pages, our tool nds 64 er-rors in less than a second per page, with 0 false positives. It produces detailed error messages that help the programmer locate the sources of the errors. After manually correcting the errors reported by the tool, the soundness of the analysis ensures that no more validity errors exist in the applications. ...|$|R
