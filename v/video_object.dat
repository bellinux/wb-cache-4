601|927|Public
50|$|MPEG-4 Part 2 is H.263 {{compatible}} in {{the sense}} that a basic H.263 bitstream is correctly decoded by an MPEG-4 Video decoder. (MPEG-4 Video decoder is natively capable of decoding a basic form of H.263.) In MPEG-4 Visual, {{there are two types of}} <b>video</b> <b>object</b> layers: the <b>video</b> <b>object</b> layer that provides full MPEG-4 functionality, and a reduced functionality <b>video</b> <b>object</b> layer, the <b>video</b> <b>object</b> layer with short headers (which provides bitstream compatibility with base-line H.263). MPEG-4 Part 2 is partially based on ITU-T H.263. The first MPEG-4 Video Verification Model (simulation and test model) used ITU-T H.263 coding tools together with shape coding.|$|E
50|$|Multiple kernel {{learning}} {{approaches have}} been used in many applications, such as event recognition in <b>video,</b> <b>object</b> recognition in images, and biomedical data fusion.|$|E
5000|$|VTS_01_0.VOB file: Video Title Set 01, <b>Video</b> <b>Object</b> 0, {{contains}} the menu for this title. This file {{is not required}} to be present on a DVD-compliant disc.|$|E
40|$|In this paper, {{we present}} an {{efficient}} video data model to represent moving trajectories of <b>video</b> <b>objects</b> and spatiotemporal {{relationships among the}} <b>video</b> <b>objects.</b> A <b>video</b> clips is segmented into a set of common appearance intervals (CAI) s. a CAI is a time interval that <b>video</b> <b>objects</b> appear together. Transitions among CAIs record the appearance/disappearance of <b>video</b> <b>objects.</b> Depending on the properties of <b>video</b> <b>objects,</b> they are classified as foreground and background <b>video</b> <b>objects.</b> Foreground <b>video</b> <b>objects</b> are further divided into moving <b>video</b> <b>objects</b> and static <b>video</b> <b>objects.</b> Different models are designed to capture these <b>video</b> <b>objects</b> and spatio-temporal relationships among foreground <b>video</b> <b>objects.</b> 1...|$|R
40|$|We {{present a}} novel scheme for object-based video {{sequence}} representation using appearance spaces. Our scheme enables fully automatic extraction of semantic <b>video</b> <b>objects</b> {{for a class}} of sequences, and their supervised organization in an object-class hierarchy. The hierarchy {{can be used for}} generic classification of query <b>video</b> <b>objects,</b> and transcoding using semantics of <b>video</b> <b>objects...</b>|$|R
40|$|To support {{heterogeneous}} application types a video {{digital library}} will contain {{a large number}} of <b>video</b> <b>objects</b> with various lengths and display requirements. Multi-user access to the same <b>video</b> <b>objects</b> is required in order to increase the availability of video information and to make full use of the limited computing and storage resources. The access frequency and delay sensitivity of <b>video</b> <b>objects</b> require special methods to guarantee smooth playback of <b>video</b> <b>objects</b> and to minimize average waiting time. We propose an integrated approach to bu#er and disk management for dynamic loading and simultaneous delivery of multiple <b>video</b> <b>objects</b> to multiple users. The allocation of bu#er and disk resources in this study is based on qualityof service variables suchasaverage waiting time, display continuity, and viewer enrollment. To Appear in the Special Issue of the Journal of Parallel and Distributed Computing on Distributed Multimedia Systems # This researchwas supported in part b [...] ...|$|R
50|$|VOB (<b>Video</b> <b>Object)</b> is the {{container}} format in DVD-Video media. VOB can contain digital video, digital audio, subtitles, DVD menus and navigation contents multiplexed {{together into a}} stream form. Files in VOB format may be encrypted.|$|E
5000|$|VIDEO_TS.VOB file: the first-play <b>Video</b> <b>Object</b> of the DVD-Video disc, {{usually a}} {{copyright}} notice or a menu. It {{is part of}} Video Manager (VMG). This file {{is not required to}} be present on a DVD-compliant disc.|$|E
5000|$|VTS_01_1.VOB file: Video Title Set 01, <b>Video</b> <b>Object</b> 1, {{contains}} the video for this title. At least one file [...] "VTS_zz_1.VOB" [...] {{is required in}} the VTS and each [...] "VTS_zz_x". DVD-Video can contain up to 99 (1-99) titles with max 10 (0-9) VOB files each. The last possible VOB file is VTS_99_9.VOB.|$|E
40|$|MPEG- 4 video {{consists}} of various <b>video</b> <b>objects,</b> rather than frames, allowing a true interactivity and manipulation of separate arbitrary shape object. Software-based encoding of MPEG- 4 <b>video</b> <b>objects</b> {{can be carried}} out by wing parallel processing with efficient scheduling scheme to speedup the computation. In this paper, we propose two dynamic scheduling algorithms which have different scheduling costs and performance levels. The algorithms assign the multiple <b>video</b> <b>objects</b> encoding tasks to the cluster of workstations with proper load balancing. The algorithms allow user on-line interactions and perform the concurrent encoding on the <b>video</b> <b>objects</b> to achieve real-time speed. The experimental results, while showing real-time encoding rates, exhibit trade-offs between load balancing, overhead scheduling cost and global performance...|$|R
40|$|This paper {{presents}} a n ew object-based video compression approach. It consists on pr edicting <b>video</b> <b>objects</b> motions throughout the scene. Neural networks {{are used to}} carry out the prediction step. A multi-step- ahead prediction is performed to predict the <b>video</b> <b>objects</b> trajectories over the sequence. In order to reduce video data, only the background of the video sequence is transmitted with the different detected <b>video</b> <b>objects</b> as well as their initial properties such as placement and dimensions. Experimental results show the effectiveness of the proposed approach in terms of the compression rates...|$|R
40|$|This paper {{describes}} an MPEG- 4 video compliant {{framework for the}} creation, encoding and decoding of video scenes composed of multiple <b>video</b> <b>objects.</b> The generated scenes can be compliantly encoded and the bitstreams can be decoded resulting in individual <b>video</b> <b>objects</b> that can be independently accessed in the decoded scene. 1...|$|R
50|$|Enhanced <b>Video</b> <b>Object,</b> {{also known}} as Enhanced VOB or EVO, is a {{container}} format for HD DVD video media. It contains the actual digital video, digital audio, subtitle and DVD menu contents in stream form. It is an extension to VOB. It can contain video encoded in H.264/MPEG-4 AVC, VC-1, or H.262/MPEG-2 Part 2 and audio encoded in AC-3, E-AC-3, Dolby TrueHD, DTS, DTS-HD, PCM, and MPEG-2 Part 3.|$|E
50|$|Video, audio, {{subtitle}} {{and navigation}} streams are multiplexed and stored on a DVD-Video disc in the VOB container format (<b>Video</b> <b>Object).</b> VOB {{is based on}} the MPEG program stream format, but with additional limitations and specifications in the private streams. The MPEG program stream has provisions for non-standard data (as AC-3, DTS, LPCM or subtitles used in VOB files) in the form of so-called private streams. VOB files are a very strict subset of the MPEG program stream standard. While all VOB files are MPEG program streams, not all MPEG program streams comply with the definition for a VOB file.|$|E
30|$|As the {{distance}} between <b>video</b> <b>object</b> and viewer/camera increases, interesting ratio of the <b>video</b> <b>object</b> decreases.|$|E
40|$|The {{increasing}} {{availability of}} object-based video content requires new technologies for automatically extracting and matching {{of the low}} level features of arbitrarily shaped video. In this paper, we propose a shape retrieval method for <b>video</b> <b>objects.</b> Our method takes into account not only the still shape features but also the shape deformations that may occur in the lifespan of <b>video</b> <b>objects.</b> We define a new shape similarity measure {{that is based on}} the shape similarity of the representative temporal instances of <b>video</b> <b>objects.</b> We also propose shape deformation features that are based on the variances of the still shape features. The proposed visual features can be derived directly from the MPEG- 4 compressed domain or computed from the shape masks of the <b>video</b> <b>objects</b> in the spatial domain. Our experiments show that the proposed method offers good retrieval performance results...|$|R
40|$|In this paper, {{we propose}} a new method for {{measuring}} the similarity between two arbitrarily shaped <b>video</b> <b>objects.</b> Our method is based on comparing the low-level still features of the representative planes of <b>video</b> <b>objects.</b> We demonstrate {{the performance of the}} proposed method in a shape retrieval system in which boundary- and region-based still shape features were employed to retrieve <b>video</b> <b>objects.</b> The experimental results show that i) the retrieval performance using the proposed similarity matching technique significantly outperforms the commonly used feature vector averaging technique and ii) the distance measure performs robustly when the content of the object changes in time. 1...|$|R
40|$|In {{this paper}} we present an {{approach}} for the generation and coding of 3 D <b>video</b> <b>objects</b> where the quality is scalable in a definable manner. At first a production chain for the generation and display of 3 D <b>video</b> <b>objects</b> based on image based rendering (IBR) methods is described. Starting with this specific generation chain, issues of applying a scalable coding framework for 3 D <b>video</b> <b>objects</b> are discussed. By developing a common model of generation a theoretical approach is introduced and basic experiments are presented. For the comparison and the {{validation of the}} proposed methodology a quality metric (3 DVQM) is utilized and explained further...|$|R
40|$|This paper {{presents}} a Learning Vector Quantization (LVQ) -based temporal tracking method for semi-automatic <b>video</b> <b>object</b> segmentation. A semantic <b>video</b> <b>object</b> is initialized using user assistance in a reference frame to give initial classification of <b>video</b> <b>object</b> and its background regions. The LVQ training approximates <b>video</b> <b>object</b> and background classification {{and use them}} for automatic segmentation of the <b>video</b> <b>object</b> on the following frames thus performing temporal tracking. For LVQ training input, we sampling each pixel of a video frame as a 5 -dimensional vector combining 2 -dimensional pixel position (X,Y) and 3 -dimensional HSV color space. This paper also demonstrates experiments using some MPEG- 4 standard test video sequences to evaluate {{the accuracy of the}} proposed method...|$|E
40|$|A robust <b>video</b> <b>object</b> based {{watermarking}} scheme, {{based on}} Zernike and Hu moments, is proposed in this paper. Firstly, a human <b>video</b> <b>object</b> detector {{is applied to}} the initial image. Zernike and the Hu moments of each human <b>video</b> <b>object</b> are estimated and an invariant function for watermarking is incorporated. Then, the watermark is generated modifying the moment values o...|$|E
40|$|Abstract—Object-based video representation, {{such as the}} one {{suggested}} by the MPEG- 4 standard, offers a framework that is better suited for object-based video indexing and retrieval. In such a framework, the concept of a “key frame ” is replaced by that of a “key <b>video</b> <b>object</b> plane. ” In this paper, we propose a method for key <b>video</b> <b>object</b> plane selection using the shape information in the MPEG- 4 compressed domain. The shape of the <b>video</b> <b>object</b> (VO) is approximated using the shape coding modes of I, P, and B <b>video</b> <b>object</b> planes (VOPs) without decoding the shape informa-tion in the MPEG- 4 bit stream. Two popular shape distance mea-sures, the Hamming and Hausdorff distance measures, are modi-fied to measure the similarities between the approximated shapes of the video objects. Although they feature different computational and implementation complexity tradeoffs, the corresponding algo-rithms achieve essentially the same performance levels in selecting key <b>video</b> <b>object</b> planes that represent efficiently the salient content of the video objects. Index Terms—Key frame selection, key <b>video</b> <b>object</b> plane selec-tion, object-based video retrieval, video databases, video summa-rization. I...|$|E
40|$|This paper {{presents}} a new method to estimate in natural video sequences {{the position of}} the projection of the light source in the image plane and a shadow segmentation of previously segmented <b>video</b> <b>objects.</b> The proposed method is based on simple geometric constraints between the object, its shadow and the light source location. It requires the segmentation of <b>video</b> <b>objects</b> during the sequence, but without any assumptions on its surface. Furthermore, this method is applied to video post-production applications such as the insertion and the suppression of <b>video</b> <b>objects</b> in original sequences. The method has been tested and validated on natural and synthetic scenes...|$|R
40|$|Abstract. In this paper, an {{original}} spatial scene level shape error concealment technique for segmented objectbased video scenes, {{to be used}} in error-prone environments such as mobile networks, is proposed. This technique is different from existing shape concealment techniques because it considers, not only the <b>video</b> <b>objects</b> that have to be concealed, but also the context in which these objects are inserted. The obtained results suggest that the use of this technique, instead of independently concealing <b>video</b> <b>objects,</b> significantly improves the subjective visual impact of scenes on the end-user, in addition to making the concealment operation itself easier since the spatially adjacent shape data from surrounding <b>video</b> <b>objects</b> can also be used. Index Terms—Spatial error concealment, binary shape data, segmented video scenes different approaches that can be adopted to create object-based video scenes. The <b>video</b> <b>objects</b> in a scene can be defined either by segmentation of an existing video sequence (segmented scene) or by composition of pre-existing <b>video</b> <b>objects</b> whose shapes do not necessarily have to fit perfectly together (composed scene). These two cases will imply different problems and solutions when it comes to error concealment. Additionally, it is also possible to define a scene using a combination of both approaches above...|$|R
40|$|In this thesis, the {{problems}} associated with the encoding and transcoding of multiple <b>video</b> <b>objects</b> are considered. In contrast to frame-based video processing, the handling of multiple <b>video</b> <b>objects</b> introduces new degrees of freedom that can be exploited in terms of compression capabilities and transmission through the network. Additionally, new sources of information must be considered, such as the boundary definition of an object. One of th...|$|R
40|$|A robust <b>video</b> <b>object</b> based {{watermarking}} scheme, {{based on}} Zernike and Hu moments, is proposed in this paper. Firstly, a human <b>video</b> <b>object</b> detector {{is applied to}} the initial image. Zernike and the Hu moments of each human <b>video</b> <b>object</b> are estimated and an invariant function for watermarking is incorporated. Then, the watermark is generated modifying the moment values of each human <b>video</b> <b>object.</b> In the detection scheme, a neural network classifier is initially used in order to extract possible watermarked human video objects from each received input image. Then, a watermark detection procedure is applied for <b>video</b> <b>object</b> authentication. A full experiment confirms the promising performance of the proposed scheme. Furthermore, the performances of the two types of moments are extensively investigated under several attacks, verifying the robustness of Zernike moments comparing to Hu moments. © Springer-Verlag Berlin Heidelberg 2006...|$|E
40|$|<b>Video</b> <b>object</b> {{tracking}} play {{an important}} role in security surveillance in current scenario. The explosion of successful digital device, the ease of use of high quality and economical video cameras, and the increasing need for computerized video analysis has generated a great deal of interest in video tracking methods. There are three techniques for video analysis: exposure of interesting moving target, tracking of such target from frame to frame, and analysis of target tracks to identify their activities. The successful <b>video</b> <b>object</b> tracking system faced a problem of false detection of moving <b>video</b> <b>object.</b> The false <b>video</b> <b>object</b> detection arises due to drastic change of background of moving video. For the maintenance of background updating various authors proposed a method for automatic background updating. In this paper we study of different <b>video</b> <b>object</b> tracking method using background updating factor...|$|E
40|$|This paper {{presents}} a 3 -D shape-adaptive directional wavelet coding technique for object-based scalable video coding. This technique includes 3 -D object-based directional threading and extensions of weighted adaptive lifting (WAL) scheme. The 3 -D object-based directional threading, which unifies {{the concept of}} temporal motion threading and 2 -D spatial directional threading, provides the opportunities to align a series of <b>video</b> <b>object</b> planes to form a 3 -D <b>video</b> <b>object</b> and exploit the spatio-temporal correlation inside the 3 -D <b>video</b> <b>object.</b> The WAL scheme, which is extended from 2 -D frame-based image coding to 3 -D object-based video coding, is employed to decompose the 3 -D <b>video</b> <b>object</b> into a 3 -D multispatio-temporal resolution <b>video</b> <b>object</b> pyramid for object-based scalable video coding. Experimental {{results show that the}} proposed 3 -D shape-adaptive directional wavelet coding technique consistently outperforms MPEG- 4 and other wavelet-based schemes for coding arbitrarily shaped video objects...|$|E
40|$|Global motion {{estimation}} {{is used for}} esploiting temporal redundancies in arbitrarily shaped <b>video</b> <b>objects,</b> which is coinpulatiorlally Uie most demanding part within the MPEG- 4 standard. In this paper, we propose a robust and fast method for the estimation of global motion of <b>video</b> <b>objects</b> used in the MPEG- 4. Predictors are widely used in block-based {{motion estimation}} and it is well proven {{that the use of}} good predictors can speed up the motion estimation process. The proposed algorithm makes use of the shape information (alpha-plane) of <b>video</b> <b>objects</b> to form a new predictor, From the experiniental results, we conclude< that the new predictor provides additional performance gains with reducing computational complesiry of global motion estimation. 1...|$|R
40|$|International audienceThis paper {{presents}} a new method to determine efficiently the relative depth of <b>video</b> <b>objects</b> {{and its use}} for region-based interpolation. The depth is extracted by an analyse of occlusion areas of <b>video</b> <b>objects</b> along {{the time and the}} image interpolation takes into account this occlusion phenomena to increase the interpolated image quality. This region-based interpolation using relative depth of objects is very important for object-based video compression and manipulation...|$|R
3000|$|QSWTs were {{selected}} for both host <b>video</b> <b>objects</b> to embed the signals. For simplicity, in the performed experiments, [...]...|$|R
40|$|A {{semi-automatic}} algorithm {{to extract}} the semantic <b>video</b> <b>object</b> in image sequences is proposed. Different schemes are used to get the initial <b>video</b> <b>object</b> in the first frame and other frames of a sequence. In the first frame, two polygons are input by the user to specify {{the area in which}} the object boundary is located. Then the <b>video</b> <b>object</b> is extracted automatically based on only the first frame. In the following frames, the image frame is segmented into intensity homogeneous regions. The moving regions are detected by a morphological filter, non-moving regions are selected by the object model from the previous frame. These regions form the initial <b>video</b> <b>object.</b> In each frame, after the initial object is available, the edges which belong to the <b>video</b> <b>object</b> of interest are selected by a local object contour model. Finally, an active contour model (snake) is applied {{to extract the}} final object contour. 1...|$|E
40|$|Abstract: Video {{surveillance}} {{process takes}} video as an input, processes the video frames and performs actions accordingly. The video surveillance process consists of many phases. <b>Video</b> <b>object</b> segmentation and tracking are two crucial {{building blocks of}} smart surveillance systems. Threshold decision is a complex problem for <b>video</b> <b>object</b> segmentation with a multibackground model. Some conditions like nonrigid object movement, target appearance variations due to changes in illumination, and background mess make robust <b>video</b> <b>object</b> tracking complex. Multiple thresholds might be needed in connection with more refined algorithms. To make it completely automatic for variant conditions, First an automatic threshold decision technique that can automatically and specifically determine the threshold values for dynamic backgrounds is proposed. Second, a <b>video</b> <b>object</b> tracking framework based on a particle filter is proposed with the probability function composed of diffusion distance for measuring color histogram similarity and motion clue from <b>video</b> <b>object</b> segmentation. The proposed framework will track multiple moving objects under drastic changes in illumination and background mess. ...|$|E
40|$|Embedding generic shape {{information}} into probabilistic spatio-temporal <b>video</b> <b>object</b> segmentation is of pivotal importance to achieving better segmentation, since it provides valuable perceptual clues for humans in both distinguishing and recognising objects. Recently a probabilistic spatio-temporal <b>video</b> <b>object</b> segmentation algorithm incorporating shape {{information has been}} proposed, though since it is restricted to only pixel features, {{the probability of a}} pixel belonging to a certain cluster is directly correlated with its spatial location, which theoretically limits the segmentation performance of the technique. To address this problem, this paper proposes a new probabilistic spatio-temporal <b>video</b> <b>object</b> segmentation algorithm that incorporates generic shape information based on its region. Experimental results reveal a significant performance improvement in arbitrary-shaped <b>video</b> <b>object</b> segmentation compared with other contemporary methods for a variety of standard video test sequence...|$|E
50|$|A new set {{of video}} {{digitisation}} and preservation standards, Guidelines on the Production and Preservation of Digital <b>Video</b> <b>Objects</b> (IASA-TC 06), is in preparation.|$|R
5000|$|Toy Story, Veeraraghavan’s {{most recent}} solo at GALLERYSKE, {{comprised}} prints, a <b>video,</b> <b>objects</b> {{and a little}} book and had critic and curator, Marta Jakimowicz write, ...|$|R
40|$|We {{present a}} novel video {{streaming}} technique called catching for on-demand delivery of "hot" (i. e., frequently accessed) <b>video</b> <b>objects</b> {{to a large}} number of clients. This technique not only significantly reduces the server and network resource requirements but also is capable of providing near-instantaneous service to {{a large number of}} clients. By combining this technique for delivery of "hot" <b>video</b> <b>objects</b> with controlled multicast [4] for delivery of "cold" <b>video</b> <b>objects,</b> we design an efficient video delivery scheme referred to as selective catching. Through empirical studies, we demonstrate the efficacy of the proposed video delivery schemes. Keywords Multimedia servers, Video-on-Demand, Multicast. 1 Introduction The past few years have seen the dramatic growth of multimedia applications which involve video streaming over the Internet. Server and network resources (in particular, server I/O bandwidth and network bandwidth) have proved to be a major limiting factor in the widespr [...] ...|$|R
