29|65|Public
5000|$|IP Central/ IP Pro - In June 2011, {{the company}} {{introduced}} its IP Central platform targeted {{to design and}} <b>verification</b> <b>teams</b> for IP reuse, including publishing, sharing and integration and revisions of internal and third party IP, and IP bug dependency tracing.|$|E
50|$|In {{order to}} help the AEC perform its task, the CPA obligates {{the parties to the}} CPA (the NCP and the SPLM) to work with the AEC with a view to {{improving}} the institutions and arrangements created under the CPA and making the unity of Sudan attractive to the people of South Sudan. Also, the AEC can present reports to the Presidency of the Republic of Sudan (and publish them with the consent of the Presidency), advise the Presidency on the arrangements and institutions created under the CPA, form <b>verification</b> <b>teams</b> on any questions connected with the CPA, and evaluate the fulfillment of international commitments and support for CPA implementation, under the AEC Rules of Procedure.|$|E
50|$|Macau's {{clothes and}} {{textiles}} continued {{to enter the}} United States under quotas separate from those of China. Under {{the terms of a}} September 2000 bilateral Memorandum of Understanding, Macau and the US government cooperate in enforcing textile quotas and preventing illegal trans-shipment. The US continued periodic visits by U.S. Customs Textile Production <b>Verification</b> <b>Teams</b> to ensure compliance with Macau bilateral textile commitments. The protection of intellectual property rights remains a priority issue on the U.S.-Macau bilateral economic agenda. Macau progress since 1999 in strengthening IPR laws, tightening controls over DVD and VCD manufacturing, and stepping up street-level IPR enforcement resulted in Macau being removed from USTR's Special 301 list in 2002. Macau's new customs service worked with U.S. industry associations and maintained high tempo operations to combat piracy https://www.state.gov/p/eap/rls/rpt/19555.htm.|$|E
5000|$|<b>Verification</b> <b>team</b> of the 13th annual ceremonies {{consists}} of individual who expert {{on television and}} entertainment. They are: ...|$|R
40|$|With {{increasing}} {{demands from}} customers, companies face {{the challenges of}} shortening the new product development (NPD) period, reducing development cost and increasing development efficiency. High quality and efficiency of NPD can increase the chance for companies to be competitive on the market. Forming up cross-functional teams becomes a popular way of maximizing the knowledge then increase the development quality and efficiency. However, {{it is challenging to}} integrate the resources from different functions and even more challenging when the resources are from different countries. To overcome the integration challenges can increase the efficiency of NPD projects therefore finally win global market shares. This study’s purpose is to identify and analyze integration challenges experienced by a <b>verification</b> <b>team</b> involved in global NPD and to suggest how those challenges can be managed by answering two research questions: 1) What are the integration challenges with the current work process encountered by the global <b>verification</b> <b>team?</b> 2) How can the integration challenges be met in the improved work process? The case study is used as the research method in order to get deep insight of the challenges that a global <b>verification</b> <b>team</b> faces. VSM Group AB, an international leading sewing company is selected as the case company.  The case <b>team,</b> a global <b>verification</b> <b>team</b> locates both in Sweden and China, plays the role of verification work within NPD process. The case team verifies the design and new products in the process.  Through interviews, observation and literature reviews, the challenges in this global <b>verification</b> <b>team</b> are identified. It is found that the case team needs to overcome language barriers, culture difference, task planning and formalization and standardization on work performance during integration. Based on the investigations, a set of solutions are proposed in the end to meet the challenges. These solutions are an improved work process, work performance system, training program and uniformed documentation. These proposals are inspired by the integration mechanisms such as formalization and standardization, special reports etc and then fit them into the case team context. By simulation and evaluation the solutions within the global <b>verification</b> <b>team,</b> the feedback on the proposals helped for improvement. This case study at VSM is an empirical example of implementation of integration mechanisms into a real life context...|$|R
50|$|Once all {{details are}} completed, all listings are {{passed to the}} <b>verification</b> <b>team</b> based in Lisbon before the listing goes live. This team will verify photo quality, descriptions, perform fraud {{detection}} checks and liaise with the host in cases where information is lacking.|$|R
40|$|Given the {{increasing}} adoption and maturity of SystemVerilog {{as a viable}} HVL (High-level Verification Language), Cadence has released two flavors of the Incisive Plan-to-Closure methodology: 1) SystemVerilog Module-Based Universal Reuse Methodology, and, 2) SystemVerilog Class-Based Reuse Methodology. Both flavors draw heavily from the proven e Reuse Methodology (eRM). According to the documentation, the module-based approach is targeted towards design teams who quickly want to get started {{with the creation of}} block- and chip-level environments. On the other hand, the class-based approach is geared towards <b>verification</b> <b>teams</b> who can take advantage of advanced object-oriented software techniques to implement constrained-random, plan-driven verification environments. From a user perspective, this begs the question of whether to use a class-based approach or a module-based approach. Both design and <b>verification</b> <b>teams</b> are intimately involved in a verification project. So, should one recommend creating two separate environments, one for the design team to do module level testing, and leave the heavy-duty verification effort to the verification team using a class-based approach? Alternatively, does it make sense to do it all in one approach? Even better, should the design team create a module-based environment, which is then reused by the class-based approach? T...|$|E
40|$|The {{challenge}} of verifying a modern microprocessor design is an overwhelming one: Increasingly complex micro-architectures combined with heavy time-to-market pressure have forced microprocessor vendors to employ immense <b>verification</b> <b>teams</b> {{in the hope}} of finding the most critical bugs in a timely manner. Unfortunately, too often size doesn’t seem to matter for <b>verification</b> <b>teams,</b> as design schedules continue to slip and microprocessors find their way to the marketplace with design errors. In this paper, we describe a simulationbased random test generation tool, called StressTest, that provides assistance in locating hard-to-find corner-case design bugs and performance problems. StressTest is based on a Markov-model-driven random instruction generator with activity monitors. The model is generated from the userspecified template programs and is used to generate the instructions sent to the design under test (DUT). In addition, the user specifies key activity points within the design that should be stressed and monitored throughout the simulation. The StressTest engine then uses closed-loop feedback techniques to transform the Markov model into one that effectively stresses the points of interest. In parallel, StressTest monitors the correctness of the DUT response to the supplied stimuli, and if the design behaves unexpectedly, a bug and a trace that leads to it are reported. Using two micro-architectures as example testbeds, we demonstrate that StressTest finds more bugs with less effort than open-loop random instruction test generation techniques...|$|E
40|$|Many {{industrial}} <b>verification</b> <b>teams</b> {{are developing}} suitable event sequence languages for hardware verification. Such languages must be expressive, designer friendly, and hardware specific, {{as well as}} efficient to verify. While the formal verification community has formal models for assessing the efficiency of an event sequence language, none of these models also accounts for designer friendliness. We propose an intermediate language for event sequences that addresses both concerns. The language achieves usability through a correlation to timing diagrams; its efficiency arises from its mapping into deterministic weak automata. We present the language, relate it to existing event sequence languages, and prove its relationship to deterministic weak automata. These results indicate that timing diagrams can become more expressive while remaining more efficient for symbolic model checking than LTI...|$|E
50|$|A joint <b>verification</b> <b>team</b> by Kenyan and Ugandan {{officials}} {{established that}} in the two calendar years 2014 and 2015, total production averaged 398,408 metric tonnes annually. Annual sugar consumption averaged 336,111 metric tonnes annually, leaving an average of 62,297 metric tonnes available for export.|$|R
50|$|The nominees and award {{winners are}} {{selected}} based {{selection of the}} most popular viewers via SMS. While the election has its own special award from the organizers. All awards through the <b>verification</b> <b>team</b> Indosiar and music observers. For a special award will be chosen by the judges.|$|R
50|$|The <b>verification</b> <b>team</b> of 12th annual ceremonies {{consists}} of individual who expert on television program, are Wishnutama (Trans TV), Yeni P. Ashar and Rosiana Silalahi (SCTV, Nana Putra (TPI), Manoj Punjabi (MD Entertainment), Irfan Ramli (Association of Indonesian advertising), Idi Subandi (Students of the Graduate Faculty of Communication UI and cultural observer) and Anjasmara (actor).|$|R
40|$|Simulation {{times of}} complex System-on-Chips (SoC) have grown {{exponentially}} as designs reach the multi-million ASIC gate range. <b>Verification</b> <b>teams</b> have adopted emulation as a prominent methodology, incorporating high-level testbenches and FPGA/ASIC hardware for system-level testing (SLT). In addition to SLT, emulation enables software teams to incorporate software applications with cycle-accurate hardware {{early on in}} the design cycle. The Standard for Co-Emulation Modeling Interface (SCE-MI) developed by the Accelera Initiative, is a widely used communication protocol for emulation which has been accepted by major electronic design automation (EDA) companies. Scan-chain is a design-for-test (DFT) methodology used for testing digital circuits. To allow more controllability and observability of the system, design registers are transformed into scan registers, allowing <b>verification</b> <b>teams</b> to shift in test vectors and observe the behavior of combinatorial logic. As SoC complexity increases, thousands of registers {{can be used in a}} design, which makes it difficult to implement full-scan testing. More so, as the complexity of the scan algorithm is dependent on the number of design registers, large SoC scan designs can no longer be verified in RTL simulation unless portioned into smaller sub-blocks. To complete a full scan cycle in RTL simulation for large system-level designs, it may take hours, days, or even weeks depending on the complexity of the circuit. This thesis proposes a methodology to decrease scan-chain verification time utilizing SCE-MI protocol and an FPGA-based emulation platform. A high-level (SystemC) testbench and FPGA synthesizable hardware transactor models are developed for the ISCAS 89 S 400 benchmark circuit for high-speed communication between the CPU workstation and FPGA emulator. The emulation results are compared to other verification methodologies, and found to be 82 % faster than regular RTL simulation. In addition, the emulation runs in the MHz speed range, allowing the incorporation of software applications, drivers, and operating systems, as opposed to the Hz range in RTL simulation...|$|E
40|$|Proper {{verification}} of FPGA-code requires knowledge, skills, tools and resources. ABB Robotics uses FPGA technology in their robot control system. The increasing complexity of their FPGA designs requires increasingly more advanced verification methods. This provides an appropriate research on methodologies, languages and tools for more detailed evaluation based on ABB Robotics requirements and possibilities. The thesis demonstrates the chosen methods, languages and tools for {{verification of}} a FPGA-design by verification methods that are {{state of the}} art. The verification environment such as functional verification, open source VHDL verification methodology (OSVVM), and universal verification methodology (UVM) were investigated in practical tests followed by an evaluation of {{advantages and disadvantages of}} the tests according the company requirements. This provides the <b>verification</b> <b>teams</b> with different test environments and presents available options for verification development and future work...|$|E
40|$|Over {{the past}} few years, the {{discussion}} of hardware verification languages (HVLs) has come full circle. At first, <b>verification</b> <b>teams</b> tried to assess {{the strengths and weaknesses}} of individual language features with the goal of creating their own verification libraries and environments but generally without the context of a reuse methodology. As these groups became more sophisticated and sought to exchange and reuse verification IP (VIP), they coalesced on the two IEEE standardized verification languages – 1800 SystemVerilog and 1647 e and moved toward the industry supported methodologies and libraries built with these languages. With the advent of a single methodology implemented in both languages – OVM multi-language – the discussion has returned to HVL features but now that the reuse methodology known, a clear apples versus apples comparison i...|$|E
50|$|In the {{automated}} design of integrated circuits, signoff (also written as sign-off) checks is the collective name {{given to a}} series of verification steps that the design must pass before it can be taped out. This implies an iterative process involving incremental fixes across the board using one or more check types, and then retesting the design. There are two types of sign-off's: front-end sign-off and back-end sign-off. After back-end sign-off the chip goes to fabrication. After listing out all the features in the specification, the verification engineer will write coverage for those features to identify bugs, and send back the RTL design to the designer. Bugs, or defects, can include issues like missing features (comparing the layout to the specification), errors in design (typo and functional errors), etc. When the coverage reaches a maximum% then the <b>verification</b> <b>team</b> will sign it off. By using a methodology like UVM, OVM, or VMM, the <b>verification</b> <b>team</b> develops a reusable environment. Nowadays, UVM is more popular than others.|$|R
50|$|Selection of award {{categories}} {{is based}} on research conducted by AGB Nielsen Media Research. Each nomination is verified by the <b>verification</b> <b>team</b> composed of practitioners {{and people who are}} experts in the field of television and entertainment. Initial screening program of 12th annual ceremonies were did by AGB Media Research. Methods poll conducted by telephone on 1300 respondents were spread across ten cities in Indonesia.|$|R
40|$|A {{blessing and}} a curse – Flexibile and widely {{applicable}} to many different SoC hardware applications – Many implementation variants present {{a challenge for}} the <b>verification</b> <b>team</b> • Formal <b>verification</b> of protocols is a superior approach •- 2 - © 2009 Jasper Design Automation because formal is comprehensive; exhaustively removing all doubt of incorrect interface behavior – Jasper configurable OCP formal verification IP generator automatically creates appropriate OCP properties for a specific design – Silicon-tested method of developing a verification plan for OCP design...|$|R
40|$|Abstract—Functional {{coverage}} plays {{a pivotal}} role in assuring the quality of input stimuli used in the verification of modern digital designs. For an out-of-order multi-processor design, simulation of a detailed model of the design is often required to observe relevant design behaviors for functional coverage. However, since such a model is not available during the early phases of test development, <b>verification</b> <b>teams</b> are forced to wait until much later in the verification process to evaluate the quality of their test cases. Even then, the quality of the tests can be assured only on one specific design implementation- an undesirable characteristic for test and regression suites that are meant to be used across multiple generations and/or implementations of an architecture. This work addresses this issue by presentinga novel, implementation-independent,execution tracebased,coverage collection solution. Our solutionenables theearly evaluation of multi-processor tests using a high-level model of a design. In addition, it can be deployed with detailed design models, if desired, for further analysis alongside implementationspecific coverage models. I...|$|E
40|$|Abstract—Hybrid {{systems with}} both {{discrete}} and continuous dynamics {{are an important}} model for real-world physical systems. The key challenge is how to ensure their correct functioning w. r. t. safety requirements. Promising techniques to ensure safety seem to be model-driven engineering to develop hybrid systems in a well-defined and traceable manner and formal verification to prove their correctness, forming the vision of verification-driven engineering. Despite the remarkable progress in automating formal verification of hybrid systems, the construction of proofs of complex systems often requires significant human guidance, since hybrid systems verification tools work over an undecidable theory. It is thus not uncommon for <b>verification</b> <b>teams</b> to consist of many players with diverse expertise. This paper introduces a verification-driven engineering toolset that extends our previous work on hybrid and arithmetic verification with tools for (i) modeling hybrid systems, (ii) exchanging and comparing models and proofs, and (iii) managing verification tasks. This toolset {{makes it easier to}} tackle large-scale verification tasks. I...|$|E
40|$|In this paper, {{we present}} Generic System Verilog Universal Verification Methodology based ReusableVerification Environment for {{efficient}} verification of Image Signal Processing IP’s/SoC’s. With the tightschedules on all projects {{it is important}} to have a strong verification methodology which contributes toFirst Silicon Success. Deploy methodologies which enforce full functional coverage and verification ofcorner cases through pseudo random test scenarios is required. Also, standardization of verification flow isneeded. Previously, inside imaging group of ST, Specman (e) /Verilog based Verification Environment forIP/Subsystem level verification and C/C++/Verilog based Directed Verification Environment for SoC LevelVerification was used for Functional Verification. Different Verification Environments were used at IPlevel and SoC level. Different Verification/Validation Methodologies were used for SoC Verification acrossmultiple sites. <b>Verification</b> <b>teams</b> were also looking for the ways how to catch bugs early in the designcycle? Thus, Generic System Verilog Universal Verification Methodology (UVM) based ReusableVerification Environment is required to avoid the problem of having so many methodologies and provides astandard unified solution which compiles on all tools...|$|E
40|$|Analyses {{presented}} made in {{the frame}} of SCIAMACHY Level 2 product verification activities under leadership of ESA. The initiated <b>verification</b> <b>team</b> consists of members of several european research institutions. The presented work is part of the SUPPRO project funded by ESA (ESAESTEC Contract 13594 / 99 /NL/PR). The goal of the <b>verification</b> <b>team</b> is to analyse Level- 2 products with respect to expected precision and accuracy. This is done by comparing Level- 2 products with results from independent retrievals applied on calibrated Level- 1 C spectra, which bases on similar algorithms than the operational processor using similar set-ups. Selected sets of SCIAMACHY data specifically prepared by ESA for this task are used. As a first step it has been analysed if the data considered is consistent and physically meaningful. This paper focusses on BrO and SO 2 products, for which data has been made available, but of only very limited extend. Both Level- 2 and independently retrieved slant columns obtained are much too large compared to climatology mean values. Large residuals have been observed after the retrieval pointing on severe Level- 1 B calibration inaccuracies. All SCIAMACHY results presented in this paper are intermediate results from running analyses and are therefore preliminary. ...|$|R
40|$|A typical {{hardware}} development flow {{starts the}} verification process concurrently with RTL, {{but the overall}} schedule becomes limited by the effort required to complete all the necessary verification tasks. Being the limiting factor, verification schedules become unpredictable, often resulting in slippage of the tapeout dates. This paper looks at ways to restructure the flow to complete {{a significant part of}} this effort during the architectural phase of the project, {{prior to the start of}} RTL. This front-loading of the schedule allows a smaller <b>verification</b> <b>team</b> to complete the process with a tighter schedule...|$|R
40|$|Conduct {{verification}} {{surveys of}} grids at the DWI 1630 Site in Knoxville, Tennessee. The independent <b>verification</b> <b>team</b> (IVT) from ORISE, conducted verification activities in whole and partial grids, as completed by BJC. ORISE site activities included gamma surface scans and soil sampling within 33 grids; G 11 through G 14; H 11 through H 15; X 14, X 15, X 19, and X 21; J 13 through J 15 and J 17 through J 21; K 7 through K 9 and K 13 through K 15; L 13 through L 15; and M 14 through M 1...|$|R
40|$|Abstract. Hybrid {{systems with}} both {{discrete}} and continuous dynamics {{are an important}} model for real-world cyber-physical systems. The key challenge is to ensure their correct functioning w. r. t. safety requirements. Promising techniques to ensure safety seem to be model-driven engi-neering to develop hybrid systems in a well-defined and traceable manner, and formal verification to prove their correctness. Their combination forms the vision of verification-driven engineering. Often, hybrid systems are rather complex in that they require expertise from many domains (e. g., robotics, control systems, computer science, software engineering, and mechanical engineering). Moreover, despite the remarkable progress in automating formal verification of hybrid systems, the construction of proofs of complex systems often requires nontrivial human guidance, since hybrid systems verification tools solve undecidable problems. It is, thus, not uncommon for development and <b>verification</b> <b>teams</b> to consist of many players with diverse expertise. This paper introduces a verification-driven engineering toolset that extends our previous work on hybrid and arithmetic verification with tools for (i) graphical (UML) and textual modeling of hybrid systems, (ii) ex-changing and comparing models and proofs, and (iii) managing verification tasks. This toolset {{makes it easier to}} tackle large-scale verification tasks...|$|E
40|$|Implementation and {{verification}} of algorithms such as image processing algorithms via deploying into field programmable gate arrays can be time consuming and involves {{a lot of}} technical complexities. Modern digital systems are expanding in terms of size and design complexity which becomes even more complicated due to task division between hardware and software as well as design and <b>verification</b> <b>teams.</b> Therefore this project proposes to model image processing algorithm such as human skin detection algorithm for hardware-software co-simulation. The skin detection algorithm was first designed as pure software followed by software profiling process to identify the compute-intensive modules. This {{was followed by the}} design of hardware accelerators for the compute-intensive modules and hardware-software co-simulation of the whole system. The hardware which is designed using SystemVerilog and the software which is in c programming language communicate through direct programming interface(DPI-C). MATLAB is used as the golden reference model to verify the hardware-software co-simulation. The co-simulation and verification process is automated with the aid of the MATLAB engine. When the hardwares-oftware co-simulation was implemented a speed improvement of up to 2. 5 times was obtained as compared to pure software implementatio...|$|E
40|$|SystemVerilog Assertions (SVA) {{are getting}} lots of {{attention}} in the verification community, and rightfully so. Assertions Based Verification Methodology is a critical improvement for verifying large, complex designs. But, we design engineers want to play too! Verification engineers add assertions to a design after the HDL models have been written. This means placing the assertions on module boundaries, binding external assertions to signals within the model, or modifying the design models to insert assertions within the code. Design engineers can, and should, add assertions within a design while the HDL models are being coded. There are challenges with doing this, however. A decision {{needs to be made}} before design work begins as to what types of scenarios the design engineer should provide assertions for. The design and <b>verification</b> <b>teams</b> need to coordinate efforts to avoid writing redundant assertions, or worse, to miss adding assertions on key functionality. Design engineers have to learn the SVA (or PSL) assertions language, which is a complex languages in and of itself. The design process will probably take longer, because adding (and debugging) assertions as the design is being coded may not be trivial...|$|E
50|$|The Unité permanente anticorruption (English: Permanent Anticorruption unit) is a Quebec {{government}} agency whose {{aim is to}} fight corruption, collusion and other economic crimes. Founded in February 2011, the unit served to coordinate the efforts of Opération Marteau, the contractual <b>verification</b> <b>team</b> of the Ministry of Municipal Affairs, the anti-fraud squad of Revenu Québec, the anti-collusion unit of Transports Québec, Régie du Bâtiment investigators as well as Commission de la construction du Québec inspectors.In late 2016 UPAC began investigating two Montreal School Boards, the English Montreal School Board and the Lester B. Pearson School Board, over allegations of irregularities.|$|R
5000|$|The 14th Annual Panasonic Gobel Awards was a {{ceremony}} held to honoring the favorite in Indonesian television programming/individual/production works, as {{chosen by the}} <b>verification</b> <b>team</b> of this ceremony awards. It was held on March 25, 2011, at the Djakarta Theater XXI in Jalan M.H. Thamrin, Menteng, Central Jakarta. This year's edition of the ceremony awards was themed [...] "Bersama Untuk Bumi Indonesia" [...] (en: Together for Indonesian Earth). The ceremony was hosted by presenter Indra Bekti and Sari Nila {{on the red carpet}} and by Fanny Febriana for the live event and also introduced as Miss Green.|$|R
40|$|The primary {{objective}} of the independent verification {{was to determine if}} BJC performed the appropriate actions to meet the specified “hot spot” cleanup criteria of 500 picocuries per gram (pCi/g) uranium- 238 (U- 238) in surface soil. Specific tasks performed by the independent <b>verification</b> <b>team</b> (IVT) to satisfy this objective included: 1) performing radiological walkover surveys, and 2) collecting soil samples for independent analyses. The independent verification (IV) efforts were designed to evaluate radioactive contaminants (specifically U- 238) in the exposed surfaces below one foot of the original site grade, given that the top one foot layer of soil on the site was removed in its entirety...|$|R
40|$|In this paper, {{the authors}} discuss options for {{developing}} institutions for joint implementation (JI) projects. They {{focus on the}} tasks which are unique to JI projects or require additional institutional needs [...] accepting the project by the host and investor countries and assessing the project`s greenhouse gas (GHG) emission reduction or sequestration [...] and they suggest the types of institutions that would enhance their performance. The evaluation is based on four sets of governmental and international criteria for JI projects, the experiences of ten pilot JI projects, and the perspectives of seven collaborating authors from China, Egypt, India, Mexico, and Thailand, who interviewed relevant government and non-government staff involved in JI issue assessment in their countries. After examining the roles for potential JI institutions, they present early findings arguing for a decentralized national JI structure, which includes: (1) national governmental panels providing host country acceptance of proposed JI projects; (2) project parties providing the assessment data on the GHG reduction or sequestration for the projects; (3) technical experts calculating these GHG flows; (4) certified <b>verification</b> <b>teams</b> checking the GHG calculations; and (5) members of an international JI Secretariat training and certifying the assessors, as well as resolving challenges to the verifications. 86 refs...|$|E
40|$|Verification {{of complex}} {{hardware}} designs remains a very labor-intensive process often involving multiple designer and <b>verification</b> <b>teams.</b> Unfortunately, often the designs under verification are poorly documented and invariants of circuit operations are ill-defined increasing {{the burden on}} testers and verification engineers, who must understand internal aspects of the design written by other people. Since forcing the design team to write complete and exact specifications for each submodule is extremely costly and counterproductive, {{it is much more}} effective to help engineers to deduce internal design invariants from interface level behavior. This paper presents Vesper (VErilog Signal ProfilER) -a tool that identifies internal design signals causing interface level behavior through signal value profiling. Vesper engine automatically extracts internal design signal from registertransition level Verilog and creates monitoring environment based on user-described interface events. After running several testbenches the analytical engine of Vesper reasons on which signals could have caused the events to happen based on the frequency of the observed signal values. Vesper is very flexible and can be parameterized through several preand post-simulation filters to effectively limit the number of signals reported and thus help verification engineers in understanding the internal structure of the design. Our tests show that Vesper is impervious to false-negatives and is quite precise even with a small number of testbenches. We also postulate how Vesper can be used in verification {{as a part of a}} directed random simulation engine, making much of this work automatic and increasing productivity of the verification process. 1...|$|E
40|$|Abstract—The {{challenge}} of verifying a modern microprocessor design is an overwhelming one: Increasingly complex microarchitectures combined with heavy time-to-market pressure have forced microprocessor vendors to employ immense <b>verification</b> <b>teams</b> {{in the hope}} of finding the most critical bugs in a timely manner. Unfortunately, too often, size does not seem to matter in verification, as design schedules continue to slip and microprocessors find their way to the marketplace with design errors. In this paper, we describe a novel closed-loop simulation-based approach to hardware verification and present a tool called StressTest that uses our methods to locate hard-to-find corner-case design bugs and performance problems. StressTest is based on a Markov-model-driven random instruction generator with activity monitors. The model is generated from the user-specified template files and is used to generate the instructions sent to the design under test (DUT). In addition, the user specifies key activity nodes within the design that should be stressed and monitored throughout the simulation. The StressTest engine then uses closed-loop feedback techniques to transform the Markov model into one that effectively stresses the user-selected points of interest. In parallel, StressTest monitors the correctness of the DUT response and, if the design behaves against expectation, it reports a bug and a trace leading to it. Using two microarchitectures as example testbeds, we demonstrate that StressTest finds more bugs with less effort than open-loop random instruction test generation techniques. Index Terms—Architectural simulation, directed-random simulation, high-performance simulation. I...|$|E
40|$|The {{previous}} verification environment used directed tests {{written in}} Verilog to verify a memory based design. This paper discusses how SystemVerilog {{was used to}} make the verification environment more efficient. Constrained randomization was used to generate stimulus, SystemVerilog assertions (SVA) were used to verify protocol and contribute to coverage, and covergroups were used to measure the functional coverage. Author(s) Biography Tim Pylant is a Sr. Technical Leader in the Verification Division of Cadence Design Systems, Inc. He has worked in the design and verification field for 22 years, of which the last four have been worked with assertions, SystemC and SystemVerilog Guy Horowitz is a Design and <b>Verification</b> <b>Team</b> Leader at Saifun Semiconductors Ltd...|$|R
40|$|Parallelism in {{processor}} {{architecture and}} design imposes a verification challenge as the exponential {{growth in the}} number of execution combinations becomes unwieldy. In this paper we report on the verification of a Very Large Instruction Word processor. The <b>verification</b> <b>team</b> used a sophisticated test program generator that modeled the parallel aspects as sequential constraints, and augmented the tool with manually written test templates. The system created large numbers of legal stimuli, however the quality of the tests was proved insufficient by several post silicon bugs. We analyze this experience and suggest an alternative, parallel generation technique. We show through experiments the feasibility of the new technique and its superior quality along several dimensions. We claim that the results apply to other parallel architectures and verification environments...|$|R
40|$|Assessment of PNPM-MPd’s {{proposal}} not run {{quickly and}} objectively. The {{purpose of this}} research is to create software to help the <b>verification</b> <b>team</b> in conducting an objective assessment of the proposals using the profile matching method and the analytic hierarchy process (AHP). Profile matching method is used to provide an assessment, determination of the gap, and the weighting criteria. While the AHP method is used to calculate the pairwise comparison matrices, eigenvalues​​, priorities, maximum eigenvalues​​, consistency index (CI) and consistency ratio (CR). CR values ​​are used to determine the order in which proposals will be funded by BLM. The final result {{of this research is}} decision support system software for assessment of PNPM-MPd’s proposal that can determine the rank of the highest value to lowest. Keywords : Assessment; Profile matching; Analytic Hierarchy Process (AHP) </p...|$|R
