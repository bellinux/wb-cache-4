17|21|Public
50|$|<b>Variable</b> <b>initialization</b> can now {{operate on}} arrays.|$|E
5000|$|Classes {{checking}} (e.g. unused functions, <b>variable</b> <b>initialization</b> {{and memory}} duplication) ...|$|E
50|$|Extended Pascal {{addresses}} many {{of these}} early criticisms. It supports variable-length strings, <b>variable</b> <b>initialization,</b> separate compilation, short-circuit boolean operators, and default (OTHERWISE) clauses for case statements.|$|E
50|$|The cipher maps an 80-bit key and a <b>variable</b> length <b>initialization</b> vector (0 to 80 bits) to a {{keystream}} with {{a maximum}} length of 240 bits.|$|R
5000|$|Optional concise <b>variable</b> {{declaration}} and <b>initialization</b> through type inference ( [...] not [...] or [...] ).|$|R
50|$|Here is {{an example}} in Smalltalk, of a typical {{accessor}} method to return {{the value of a}} <b>variable</b> using lazy <b>initialization.</b>|$|R
50|$|JavaScript has simple scoping rules, but <b>variable</b> <b>initialization</b> {{and name}} {{resolution}} rules can cause problems, and {{the widespread use}} of closures for callbacks means the lexical environment of a function when defined (which is used for name resolution) can be very different from the lexical environment when it is called (which is irrelevant for name resolution). JavaScript objects have name resolution for properties, but this is a separate topic.|$|E
5000|$|This {{is known}} as {{variable}} hoisting - the declaration, but not the initialization, is hoisted {{to the top of}} the function. Thirdly, accessing variables before initialization yields , rather than a syntax error. Fourthly, for function declarations, the declaration and the initialization are both hoisted {{to the top of the}} function, unlike for <b>variable</b> <b>initialization.</b> For example, the following code produces a dialog with output undefined, as the local variable declaration is hoisted, shadowing the global variable, but the initialization is not, so the variable is undefined when used:a = 1;function f (...) { alert(a); var a = 2;}f (...) ...|$|E
5000|$|For variables, Python has {{function}} scope, module scope, {{and global}} scope. Names enter scope {{at the start}} of a context (function, module, or globally), and exit scope when a non-nested function is called or the context ends. If a name is used prior to <b>variable</b> <b>initialization,</b> this raises a runtime exception. If a variable is simply accessed (not assigned to) in a context, name resolution follows the LEGB rule (Local, Enclosing, Global, Built-in). However, if a variable is assigned to, it defaults to creating a local variable, which is in scope for the entire context. Both these rules can be overridden with a [...] or [...] (in Python 3) declaration prior to use, which allows accessing global variables even if there is an intervening nonlocal variable, and assigning to global or nonlocal variables.|$|E
25|$|The {{initialization}} declares (and perhaps assigns to) any variables required. The type of {{a variable}} should be same {{if you are using}} multiple <b>variables</b> in <b>initialization</b> part. The condition checks a condition, and quits the loop if false. The afterthought is performed exactly once every time the loop ends and then repeats.|$|R
40|$|Fault management, {{including}} {{fault detection}} and location, {{is an important}} task in management of Web Services. Fault detection can be performed through testing, which can be active or passive. Based on passive observation of interactions between a Web Service and its client, a passive tester tries to detect possible misbehaviors in requests and/or responses. Passive observation is performed in two steps: passive homing and fault detection. In FSM-based observers, the homing consists of state recognition. However, it consists of state recognition and <b>variables</b> <b>initialization</b> in EFSM-based observers. In this paper, we present a novel approach to speed up homing of EFSM-based observers designed for observation of Web Services. Our approach is based on combining observed events and backward walks in the EFSM model to recognize states and appropriately initialize variables. We present different algorithms and illustrate the procedure through an example where faults would not be detected unless backward walks are considered...|$|R
40|$|The topics {{discussed}} {{include the}} following: multiscale {{application of the}} 5 th-generation PSU/NCAR mesoscale model, the coupling of nonhydrostatic atmospheric and hydrostatic ocean models for air-sea interaction studies; a numerical simulation of cloud formation over complex topography; adaptive grid simulations of convection; an unstructured grid, nonhydrostatic meso/cloud scale model; efficient mesoscale modeling for multiple scales using <b>variable</b> resolution; <b>initialization</b> of cloud-scale models with Doppler radar data; and making effective use of future computing architectures, networks, and visualization software...|$|R
30|$|The {{first step}} is the <b>variable</b> <b>initialization.</b>|$|E
40|$|Introduction to CCL CCL Code CCL Environment Model of the Environment The Environment The Environment Theorem Prover Theorem Prover System System A CCL {{specification}} P = (I,C) {{consists of}} two parts: initial conditions I and clauses C. Clauses are guarded commands of the form g:r, where g is a predicate on states and r is a relation between states. When a clause is executed, if the guard evaluates to true a new state is generated that satisfies the relation. program prog 1 (param_ 1, [...] ., param_k) := { assignments [...] . guard : { rules } guard : { rules } [...] . }; <b>Variable</b> <b>initialization.</b> <b>Variable</b> <b>initialization.</b> Any number of clauses. Guards are boolean expressions and rules are assignments to variables or control commands. program prog 3 ( [...] .) := prog 1 ( [...] .) + prog 2 ( [...] .) sharing x, y, z, [...] .; exec prog (1. 1, 2. 0); This makes a new program with conjoined initial section and includes all clauses from prog 1 and prog 2. x,y,and z are shared and other variables are local Start...|$|E
40|$|Initialization, {{an outcome}} of {{language}} contact common to signed languages, has become a global phenomenon. I define initialization as the incorporation of the orthography of a word of a dominant spoken language via the cultural construct of a manual orthography into signs with a semantic correspondence to that word. Despite its being very common within (relatively) well-documented sign languages such as American Sign Language (Padden & Brentari, 2001), Australian Sign Language (Schembri & Johnston, 2007) and Québec Sign Language (Machabée, 1995), {{the literature on the}} subject is very small. To assist in expanding the nascent fields of sociolinguistics and anthropology of Deaf communities, ethnographic research involving primarily corpus building, interviews and participant observation was performed within the Deaf community of central Honduras to offer preliminary insights into how the personal and group identities of the Honduran Deaf are negotiated through linguistic interactions. <b>Variable</b> <b>initialization</b> is quite a salient marker because of its use in the diverging sociolects of Deaf Honduras. This poses the questions: what instances of <b>variable</b> <b>initialization</b> exist in the community; how are these variable forms manipulated to construct identities; how does variant initialization mark social differentiation in the community? How are linguistic variation and social differentiation intertwined? Social relationships and individual identity are studied by means of this linguistic marker as language is used to build social meaning. In particular, I argue that linguistic variation is polarizing as variant initialization is used to both reflect and justify the social division of the community into central and peripheral...|$|E
40|$|It {{can take}} {{weeks or months}} to {{incorporate}} a new aerodynamic model into a vehicle simulation and validate {{the performance of the}} model. The Dynamic Aerospace Vehicle Exchange Markup Language (DAVE-ML) has been proposed as a means to reduce the time required to accomplish this task by defining a standard format for typical components of a flight dynamic model. The {{purpose of this paper is}} to describe an object-oriented C++ implementation of a class that interfaces a vehicle subsystem model specified in DAVE-ML and a vehicle simulation. Using the DaveMLTranslator class, aerodynamic or other subsystem models can be automatically imported and verified at run-time, significantly reducing the elapsed time between receipt of a DAVE-ML model and its integration into a simulation environment. The translator performs <b>variable</b> <b>initializations,</b> data table lookups, and mathematical calculations for the aerodynamic build-up, and executes any embedded static check-cases for verification. The implementation is efficient, enabling real-time execution. Simple interface code for the model inputs and outputs is the only requirement to integrate the DaveMLTranslator as a vehicle aerodynamic model. The translator makes use of existing table-lookup utilities from the Langley Standard Real-Time Simulation in C++ (LaSRS++). The design and operation of the translator class is described and comparisons with existing, conventional, C++ aerodynamic models of the same vehicle are given...|$|R
5000|$|... {{where the}} {{optional}} [...] parameter specifies a non-default kind, and the [...] notation delimits {{the type and}} attributes from variable name(s) and their optional initial values, allowing full <b>variable</b> specification and <b>initialization</b> to be typed in one statement (in previous standards, attributes and initializers had to be declared in several statements). While it is not required in above examples (as there are no additional attributes and initialization), most Fortran-90 programmers acquire the habit to use it everywhere.|$|R
40|$|Previous {{studies have}} {{demonstrated}} the use of Earth Observation satellite data to improve the initialization and the calibration of hydrological models. Indeed, they allow the assess-ment of several surface properties (surface temperature, soil water content, soil roughness, type of soil, dynamics of the vegetation [...] .). Studies have recently shown that Soil Vegetation Atmosphere Transfers (SVAT) models can be corrected thanks to the brightness tempera-ture estimated by thermal infrared satellite data. However the thermal infrared instruments resolution is rather coarse if a good repetitivity is required. Consequently, {{there is a need}} to explore the potentialities of low spatial resolution captors to inform higher resolution mod-elisations (Boudevillain et al., 2004). The purpose of this paper is to show the contribution and limits of thermal infrared brightness temperature to calibrate the surface parameters and <b>initialization</b> <b>variables</b> of a SVAT model at field scale. The Alpilles-ReSeDA database (Baret, 2002; Olioso and co-authors, 2002 a,b) has been used for this work. The two layers and two sources SVAT model used in this study and developed at CETP/IPSL calculates the surface energy and water transfers and include several soil and vegeta-tion parameters and <b>initialization</b> <b>variables</b> (especially soil water content in the surface and roo...|$|R
40|$|Abstract — Code to {{estimate}} position and attitude of a spacecraft or aircraft {{belongs to the}} most safety-critical parts of ¤ight software. The complex underlying mathematics and abundance of design details make it error-prone and reliable implementations costly. AutoFilter is a program synthesis tool for the automatic generation of state estimation code from compact speci£cations. It can automatically produce additional safety certi£cates which formally guarantee that each generated program individually satis£es a set of important safety policies. These safety policies (e. g., array-bounds, <b>variable</b> <b>initialization)</b> form a core of properties which are essential for high-assurance software. Here we describe the AutoFilter system and its certi£cate generator and compare our approach to the static analysis tool PolySpace. I...|$|E
40|$|Phase {{equilibrium}} calculations {{in systems}} subject to chemical reactions {{are involved in}} the design, synthesis and optimization of reactive separation processes. Until now, several methods have been developed to perform simultaneously physical and chemical equilibrium calculations. However, published methods may face numerical difficulties such as <b>variable</b> <b>initialization</b> dependence, divergence and convergence to trivial solutions or unstable equilibrium states. Besides, these methods generally use conventional composition variables and reactions extents as unknowns which directly affect the numerical implementation, reliability and efficiency of solving strategies. The objective of this work is to introduce and test an alternative approach to perform Gibbs energy minimization in phase equilibrium problems for reactive systems. Specifically, we have employed the transformed composition variables of Ung and Doherty and the stochastic optimization method Simulated Annealing for two-phase equilibrium calculations in reacting systems. Performance of this strategy has been tested using several benchmark problems and results show that proposed approach is generally suitable for the global minimization of transformed Gibbs energy in reactive systems with two-phase equilibrium. Key words: Global optimization, Gibbs energy minimization, simulated annealing, chemical equilibrium, phase equilibriu...|$|E
40|$|Version 0. 5. 1 (2017 - 02 - 14) The CHANGELOG for {{the current}} {{development}} version is available at [URL] Downloads Source code (zip) Source code (tar. gz) New Features The EnsembleVoteClassifier has a new refit attribute that prevents refitting classifiers if refit=False to save computational time. Added a new lift_score function in evaluate to compute lift score (via Batuhan Bardak). StackingClassifier and StackingRegressor support multivariate targets if the underlying models do (via kernc). StackingClassifier has a new use_features_in_secondary attribute like StackingCVClassifier. Changes Changed default verbosity level in SequentialFeatureSelector to 0 The EnsembleVoteClassifier now raises a NotFittedError if the estimator wasn't fit before calling predict. (via Anton Loss) Added new TensorFlow <b>variable</b> <b>initialization</b> syntax to guarantee compatibility with TensorFlow 1. 0 Bug Fixes Fixed wrong default value for k_features in SequentialFeatureSelector Cast selected feature subsets in the SequentialFeautureSelector as sets to prevent the iterator from getting stuck if the k_idx are different permutations of the same combination (via Zac Wellmer). Fixed an issue with learning curves that caused the performance metrics to be reversed (via ipashchenko) Fixed a bug that could occur in the SequentialFeatureSelector if there are similarly-well performing subsets in the floating variants (via Zac Wellmer) ...|$|E
40|$|The present {{dissertation}} {{investigates the}} airflow pattern in naturally ventilated buildings using advanced computer simulation {{techniques such as}} field or Computational Fluid Dynamics (CFD) models. In particular, two-equation turbulence models, such as the Standard k-ε, the RNG k-ε, the “Realizable” k-ε, {{as well as the}} Reynolds Stress Model (RSM) are used to predict the flow field in and around the buildings considered. In the case of the Standard k-ε model the potential of improving the prediction accuracy is investigated by means of the following ways: (a) Application of a Two-layer Standard k-ε model, and (b) Modification according to the inlet flow variables profiles of the Atmospheric Boundary Layer (ABL). The aforementioned turbulence models and the proposed modifications are passive ventilation of a pilot building and of a real-scale building subject to actual passive ventilation (for which an experimental procedure is conducted). The CFD model is then used for the prediction of thermal comfort in naturally ventilated buildings by implementing the most appropriate thermal comfort indices (TCI), such as the Predicted Mean Vote (PMV), which, according to literature information, is modified for pure natural ventilation as follows: (a) Extension to account for the metabolic-rate reduction expected in warm environments and for the expectancy factor due to occupant’s habitat, and (b) Correction of the PMV for humidity effects, based on the Standard Effective Temperature (SET*). The present thermal comfort model is completed with the implementation of the Percentage Dissatisfied (PD) index to account for discomfort due to air draughts as well. Additionally, Indoor Air Quality (IAQ) indices are also implemented in the CFD model, i. e. the ventilation effectiveness related to the removal of common pollutants. Following the aforementioned coupled model (CFD-TC/IAQ) a database of input-output pairs is formed, which is used to train and validate radial basis functions (RBF) artificial neural networks (ANN) according to the fuzzy means method. The ANN distributions are then utilized to formulate a multiobjective optimization problem, which accounts for the available thermal comfort and indoor air quality indices, occupant activity level and special constraints provided by design guidelines. The problem is confronted using a gradient-based algorithm associated with a special handling of constraints and of the initialization space of the design <b>variables</b> (<b>Initialization</b> Grid Concept, IGC). Additionally, a technique to account for the non-dominated solutions of the objective functions according to the Pareto criteria is also introduced. Finally, for each case studied the optimal designs are presented by means of nomographs, which may serve as look-up tables for either decision-making strategies or automated systems to create “Intelligent Buildings”. ...|$|R
40|$|Model-based {{design and}} {{automated}} code generation {{are being used}} increasingly at NASA. Many NASA projects now use MathWorks Simulink and Real-Time Workshop {{for at least some}} of their modeling and code development. However, there are substantial obstacles to more widespread adoption of code generators in safety-critical domains. Since code generators are typically not qualified, {{there is no guarantee that}} their output is correct, and consequently the generated code still needs to be fully tested and certified. Moreover, the regeneration of code can require complete recertification, which offsets many of the advantages of using a generator. Indeed, manual review of autocode can be more challenging than for hand-written code. Since the direct V&V of code generators is too laborious and complicated due to their complex (and often proprietary) nature, we have developed a generator plug-in to support the certification of the auto-generated code. Specifically, the AutoCert tool supports certification by formally verifying that the generated code is free of different safety violations, by constructing an independently verifiable certificate, and by explaining its analysis in a textual form suitable for code reviews. The generated documentation also contains substantial tracing information, allowing users to trace between model, code, documentation, and V&V artifacts. This enables missions to obtain assurance about the safety and reliability of the code without excessive manual V&V effort and, as a consequence, eases the acceptance of code generators in safety-critical contexts. The generation of explicit certificates and textual reports is particularly well-suited to supporting independent V&V. The primary contribution of this approach is the combination of human-friendly documentation with formal analysis. The key technical idea is to exploit the idiomatic nature of auto-generated code in order to automatically infer logical annotations. The annotation inference algorithm itself is generic, and parametrized with respect to a library of coding patterns that depend on the safety policies and the code generator. The patterns characterize the notions of definitions and uses that are specific to the given safety property. For example, for initialization safety, definitions correspond to <b>variable</b> <b>initializations</b> while uses are statements which read a variable, whereas for array bounds safety, definitions are the array declarations, while uses are statements which access an array variable. The inferred annotations are thus highly dependent on the actual program and the properties being proven. The annotations, themselves, need not be trusted, but are crucial to obtain the automatic formal verification of the safety properties without requiring access to the internals of the code generator. The approach has been applied to both in-house and commercial code generators, but is independent of the particular generator used. It is currently being adapted to flight code generated using MathWorks Real-Time Workshop, an automatic code generator that translates from Simulink/Stateflow models into embedded C code...|$|R
5000|$|Developers also {{refer to}} fail-fast code to a code {{that tries to}} fail as soon as {{possible}} at <b>variable</b> or object <b>initialization.</b> In OOP, a fail-fast designed object initializes the internal state of the object in the constructor, launching an exception if something is wrong (vs allowing non-initialized or partially initialized objects that will fail later due to a wrong [...] "setter"). The object can then be made immutable if no more changes to the internal state are expected. In functions, fail-fast code will check input parameters in the precondition. In client-server architectures, fail-fast will check the client request just upon arrival, before processing or redirecting it to other internal components, returning an error if the request fails (incorrect parameters, ...). Fail-fast designed code decreases the internal software entropy, and reduces debugging effort.|$|R
40|$|In {{its most}} general form, an attack {{signature}} {{is a program}} that can correctly determine if an input network packet sequence can successfully attack a protected network application. Filter rules used in firewall and network intrusion prevention systems (NIPS) are an abstract form of attack signature. This paper presents the design, implementation, and evaluation of an automated attack signature generation system called Trag, that automatically generates an executable attack signature program from a victim program’s source and a given attack input. Trag leverages dynamic data and control dependencies to extract relevant code in the victim program, accurately identifies <b>variable</b> <b>initialization</b> statements that are not executed in the given attack, is able to generate attack signatures for multi-process network applications, and reduces the size of attack signatures by exploiting responses from victim programs. Experiments with a fully working Trag prototype show that Trag’s signatures can indeed prevent attacks against multiple production-grade vulnerable server/web applications, such as apache, wu-ftpd and MyBullentinBoard, with up to 65 % reduction in size {{when compared with the}} victim program. In terms of performance overhead, the additional latency as observed from the client-side is no more than 25 µsec for multi-process web applications, while the overall throughput remains unaffected. 1...|$|E
40|$|The {{ability to}} obtain {{appropriate}} parameters for an advanced {{pressurized water reactor}} (PWR) unit model is of great significance for power system analysis. The attributes of that ability include the following: nonlinear relationships, long transition time, intercoupled parameters and difficult obtainment from practical test, posed complexity and difficult parameter identification. In this paper, a model and a parameter identification method for the PWR primary loop system were investigated. A parameter identification process was proposed, using a particle swarm optimization (PSO) algorithm {{that is based on}} random perturbation (RP-PSO). The identification process included model <b>variable</b> <b>initialization</b> based on the differential equations of each sub-module and program setting method, parameter obtainment through sub-module identification in the Matlab/Simulink Software (Math Works Inc., Natick, MA, USA) as well as adaptation analysis for an integrated model. A lot of parameter identification work was carried out, the results of which verified the effectiveness of the method. It was found that the change of some parameters, like the fuel temperature and coolant temperature feedback coefficients, changed the model gain, of which the trajectory sensitivities were not zero. Thus, obtaining their appropriate values had significant effects on the simulation results. The trajectory sensitivities of some parameters in the core neutron dynamic module were interrelated, causing the parameters to be difficult to identify. The model parameter sensitivity could be different, which would be influenced by the model input conditions, reflecting the parameter identifiability difficulty degree for various input conditions...|$|E
40|$|This paper details {{a set of}} {{optimizations}} {{that were}} made to the Icon compiler. Several optimizations are implemented to the type inferencing system and the intermediate code generation with the goals of improving execution time of the generated executable and lower memory requirements. The University of Texas at San Antonio Division of Computer Science San Antonio, TX 78249, USA This page intentionally left blank. Contents 1 Introduction 1 1. 1 The Icon Compiler : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 1 1. 2 Type Inference : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2 1. 3 Redundant Function Calls : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2 1. 4 Constant Propagation : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2 1. 5 <b>Variable</b> <b>Initialization</b> : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 3 1. 6 Changes to the Compiler Source : : : : : : : : : : : [...] ...|$|E
40|$|Critical to {{structural}} modelling of the intracellular thiamine kinetics in the intestine {{tissue is}} the incomplete {{knowledge of the}} intrinsic nonlinearity of the absorption process from plasma to cells {{as well as the}} paucity of the available experimental data. The latter cause may be also responsible for the failure of traditional black-box modelling approaches. This paper deals with a novel hybrid modelling framework which builds on the qualitative structural knowledge a good initialization of an approximation scheme of the nonlinear functional relationship between the input-output <b>variables.</b> Such an <b>initialization</b> allows us to efficiently cope with the poor data set and to derive an input-output model of the intracellular thiamine kinetics in the intestine tissue. Although the structural assumptions are relaxed and only a descriptive quantitative knowledge may be derived, the model obtained is robust enough {{to be used as a}} simulator. The interpretative potential of the model derived has been tested on diabetic subjects...|$|R
40|$|An {{evolutionary}} algorithm typically initializes its population randomly, although domain specific knowledge {{can also be}} used to bias the search. Evaluation measures the fitness of each individual according to its worth in some environment. Evaluation may be as simple as computing a fitness function or as complex as running an elaborate simulation. Selection is often performed in two steps, parent selection and survival. Parent selection decides who becomes parents and how many children the parents have. Children are created via recombination, which exchanges information between parents, and mutation, which further perturbs the children. The children are then evaluated. Finally, the survival step decides who survives in the population. Let us illustrate an {{evolutionary algorithm}} with a simple example. Suppose an automotive manufacturer wishes to design a new engine and fuel system in order to maximize performance, reliability, and gas-mileage, while minimizing emissions. Let us further suppose that an engine simulation unit can test various engines and return a single value indicating the fitness score of the engine. However, the number of possible engines is large and there is insufficient time to test them all. How would one attack such a problem with an evolutionary algorithm? First, we define each individual to represent a specific engine. For example, suppose the cubic inch displacement (CID), fuel system, number of valves, cylinders, and presence of turbo-charging are all engine <b>variables.</b> The <b>initialization</b> step would create an initial population of possible engines. For the sake of simplicity, let us assume a (very small) population of size four. Here is an example initial population...|$|R
40|$|MiniMarch is a {{modified}} version of MiniSat 2. 0. The design goal was to make a version of MiniSat that is less sensitive to shuffling of the input formula. MiniMarch extends MiniSat with the following techniques: ˆ Clause sorting By sorting the clauses we not only hope to find conflict clauses imposing strong constraints faster but also facilitate resistance against the shuffling of clauses in the input formula. ˆ Symmetric simplifier The simplifying pre-processor introduced in MiniSat 2. 0 is rather sensitive to the swapping of signs in the input formula. Our modifications make it symmetric. ˆ Branching heuristics MiniMarch combines dynamic lookahead branching heuristics for balanced variables with static occurrence based heuristics for unbalanced <b>variables.</b> ˆ Activity <b>initialization</b> The activities are initialized, to make the initial variable decisions taken less sensitive to variable index shuffling. 1. Clause sorting Clauses are sorted, the first sorting criterion is the shortest size first. Amongst clauses of the same length the clauses with the highest weight wclause are ordered first. wclause(c) = ∑ wliteral(l) = l∈c wliteral(l) c∈Clauses ∧ l∈...|$|R
40|$|Abstract:- Often {{implementation}} of the program will change. Implementations are changed to reduce running time and/or to reduce memory consumption (space complexity) of the program. Often there is need to test the two version of the software, one current and another newer version. Newer version will be having some extra methods/functions, but the remaining methods/functions will be {{same as that of}} current version. We {{need to make sure that}} these methods of current version have not been affected by the changes done in new version. (Regression Testing), and also often the methods will be refactored to different prototypes/signatures to offer abstraction. These new prototyped methods (signature changed methods) will intern invoke the previous method (before to new prototyping), i. e. newer prototyped methods are just wrappers around the previous methods. For instance APIs are wrapped around by corresponding methods. In this situation, it becomes important to test the newly prototyp ed methods that are wrappers around the old methods/APIs, as we need to verify the correct bindings/mappings of the older and newly prototyped methods. Usually developers write the unit tests to test their logic. But the testers cannot write them as tester lacks the knowledge of logic implemented, and tester may not have any knowledge of coding, but tester knows what each method does and what is it’s expected behavior/return type. Thus we need to offer new way to test each method. We propose a novel framework, which addresses these important issues. Framework takes three input parameters namely, class to be tested, <b>variable</b> <b>initialization</b> values (test data), and expected results. From this information, framework automatically builds test driver class at runtime; on the fly on running the framework. Test driver class is used to test the class under test. This test driver class is compiled and executed to get the actual results for class under test. These generated actual results are compared with expected result to find methods different behaviors. Methods whose actual result is not matching with the expected result, then this implies that methods have different behavior, thus the test is failure, an...|$|E
30|$|Process {{models are}} {{relatively}} robust and applicable across ecological boundaries (Dickinson 2010). However, process models may require inputs {{that are not}} widely available to land managers. For example, Campbell’s soil heating model (Campbell et al. 1994, 1995) requires initialization data (e.g., soil moisture and texture, soil bulk density, particle density, initial temperature, and thermal conductivity) and boundary conditions (e.g., heat release rate and duration of heating at the soil surface) {{in the form of}} an input file (see also Massman et al. 2010). For most management applications, it is simply not feasible to acquire these data. FOFEM, which houses this model, attempts to support the model’s use by providing default values for many of these inputs. Soil surface heating inputs (boundary conditions) are provided by other models in FOFEM (e.g., the burnout model for large woody debris [Burnup, Albini et al. 1995], Table 1). Similar initialization and fire-behavior-related boundary conditions are required for other process models that would predict tree injury and mortality (Butler and Dickinson 2010) and effects on shrub and herbaceous species (Stephan et al. 2010). To apply existing and future fire effects models, considerable effort will be required in developing datasets and methods for estimating <b>initialization</b> <b>variables</b> and predicting boundary conditions from fire behavior models and measurements (Kremens et al. 2010). In fact, a primary challenge in implementing WFDSS has been obtaining required locally-developed data sources to supplement available LANDFIRE data (T. Zimmerman, personal communication).|$|R
40|$|The macro {{facility}} {{allows us}} to write macro code that instructs SAS ® how to generate the more familiar open code found in DATA steps and PROCs. With such a tool, we can consolidate families of related programs, execute the same piece of code repetitively, and develop applications for others to use. This replaces error-prone, highmaintenance jobs such as saving multiple versions of essentially the same program (with slight differences depending on the task) or excessive typing or copying and pasting. The best part {{is that most of}} the code is similar to what we find in open code. The key to understanding how the macro processor works is in understanding the differences – both in the code itself, and in the way the two types of code are processed. This theme will be emphasized throughout the paper. Discussion begins with macro <b>variables,</b> beginning with <b>initialization</b> and referencing, followed by other subjects such as functions, quoting, and combinations. Macros are then presented, beginning with creation and referencing. Examples begin with “open-code ” macros followed by a discussion of macro logic and examples of its use with open code in “mixed-code ” macros. Finally, I will discuss different ways macros are written and used, depending on who they are written for. This paper is perfect for programmers who are comfortable writing in BASE SAS and are ready to take the next step toward automating their programs. It is applicable in any operating system...|$|R
40|$|Purpose: The {{purpose of}} this paper is to {{investigate}} the effectiveness of implementing unmanned aerial delivery vehicles in delivery networks. We investigate the notion of the reduced overall delivery time and energy for a truck-drone network by comparing the in-tandem system with a stand-alone delivery effort. The objectives are (1) to investigate the time and energy associated to a truck-drone delivery network compared to standalone truck or drone, (2) to propose an optimization algorithm that determines the optimal number of launch sites and locations given delivery requirements, and drones per truck, (3) to develop mathematical formulations for closed form estimations for the optimal number of launch locations and the optimal total time of delivery. Design/methodology/approach: The design of the algorithm herein computes the minimal time of delivery utilizing K-means clustering to find launch locations, as well as a genetic algorithm to solve the truck route as a traveling salesmen problem (TSP). The optimal solution is determined by finding the minimum cost associated to the parabolic convex cost function. The optimal min-cost is determined by finding the most efficient launch locations using K-means algorithms to determine launch locations and a genetic algorithm to determine truck route between those launch locations. Findings: Results show improvements with in-tandem delivery efforts as opposed to standalone systems. Further, multiple drones per truck are more optimal and contribute to savings in both energy and time. For this, we sampled various <b>initialization</b> <b>variables</b> to derive closed form mathematical solutions for the problem. Originality/value: Ultimately, this provides the necessary analysis of an integrated truck-drone delivery system which could be implemented by a company in order to maximize deliveries while minimizing time and energy. Closed-form mathematical solutions can be used as close estimators for the optimal number of launch locations and the optimal delivery time. Peer Reviewe...|$|R
40|$|A neural network-based method (CANYON: CArbonate {{system and}} Nutrients {{concentration}} from hYdrological properties and Oxygen using a Neural-network) {{was developed to}} estimate water-column (i. e., from surface to 8, 000 m depth) biogeochemically relevant variables in the Global Ocean. These are the concentrations of three nutrients [nitrate (NO 3 −), phosphate (PO 43 −), and silicate (Si(OH) 4) ] and four carbonate system parameters [total alkalinity (AT), dissolved inorganic carbon (CT), pH (pHT), and partial pressure of CO 2 (pCO 2) ], which are estimated from concurrent in situ measurements of temperature, salinity, hydrostatic pressure, and oxygen (O 2) together with sampling latitude, longitude, and date. Seven neural-networks were developed using the GLODAPv 2 database, which is largely representative {{of the diversity of}} open-ocean conditions, hence making CANYON potentially applicable to most oceanic environments. For each variable, CANYON was trained using 80 % randomly chosen data from the whole database (after eight 10 ° × 10 ° zones removed providing an “independent data-set” for additional validation), the remaining 20 % data were used for the neural-network test of validation. Overall, CANYON retrieved the variables with high accuracies (RMSE) : 1. 04 μmol kg− 1 (NO 3 −), 0. 074 μmol kg− 1 (PO 43 −), 3. 2 μmol kg− 1 (Si(OH) 4), 0. 020 (pHT), 9 μmol kg− 1 (AT), 11 μmol kg− 1 (CT) and 7. 6 % (pCO 2) (30 μatm at 400 μatm). This was confirmed for the eight independent zones not included in the training process. CANYON was also applied to the Hawaiian Time Series site to produce a 22 years long simulated time series for the above seven variables. Comparison of modeled and measured data was also very satisfactory (RMSE in the order of magnitude of RMSE from validation test). CANYON is thus a promising method to derive distributions of key biogeochemical variables. It could be used for a variety of global and regional applications ranging from data quality control to the production of datasets of <b>variables</b> required for <b>initialization</b> and validation of biogeochemical models that are difficult to obtain. In particular, combining the increased coverage of the global Biogeochemical-Argo program, where O 2 is one of the core variables now very accurately measured, with the CANYON approach offers the fascinating perspective of obtaining large-scale estimates of key biogeochemical variables with unprecedented spatial and temporal resolutions. The Matlab and R codes of the proposed algorithms are provided as Supplementary Material...|$|R
40|$|Integrated land surface-groundwater {{models are}} {{valuable}} tools in simulating the terrestrial hydrologic cycle {{as a continuous}} system and exploring the extent of land surface-subsurface interactions from catchment to regional scales. However, the fidelity of model simulations is impacted {{not only by the}} vegetation and subsurface parameterizations, but also by the antecedent condition of model state variables, such as the initial soil moisture, depth to groundwater, and ground temperature. In land surface modeling, a given model is often run repeatedly over a single year of forcing data until it reaches an equilibrium state: the point at which there is minimal artificial drift in the model state or prognostic variables (most often the soil moisture). For more complex coupled and integrated systems, where there is an increased computational cost of simulation and the number of <b>variables</b> sensitive to <b>initialization</b> is greater than in traditional uncoupled land surface modeling schemes, the challenge is to minimize the impact of initialization while using the smallest spin-up time possible. In this study, multicriteria analysis was performed to assess the spin-up behavior of the ParFlow. CLM integrated groundwater-surface water-land surface model over a 208 km 2 subcatchment of the Ringkobing Fjord catchment in Denmark. Various measures of spin-up performance were computed for model state variables such as the soil moisture and groundwater storage, as well as for diagnostic variables such as the latent and sensible heat fluxes. The impacts of initial conditions on surface water-groundwater interactions were then explored. Our analysis illustrates that the determination of an equilibrium state depends strongly on the variable and performance measure used. Choosing an improper initialization of the model can generate simulations that lead to a misinterpretation of land surface-subsurface feedback processes and result in large biases in simulated discharge. Estimated spin-up time from a series of spin-up functions revealed that 20 (or 21) years of simulation were sufficient for the catchment to equilibrate according to at least one criterion at the 0. 1 % (0. 01 %) threshold level. Amongst a range of convergence metrics examined, percentage changes in monthly values of groundwater and unsaturated zone storages produced a slow system convergence to equilibrium, whereas criteria based on ground temperature allowed a more rapid spin-up. Slow convergence of unsaturated and saturated zone storages {{is a result of the}} dynamic adjustment of the water table in response to a physically arbitrary or inconsistent initialization of a spatially uniform water table. Achieving equilibrium in subsurface storage ensured equilibrium across a spectrum of other variables, hence providing a good measure of system-wide equilibrium. Overall, results highlight the importance of correctly identifying the key variable affecting model equilibrium and also the need to use a multicriteria approach to achieve a rapid and stable model spin-up...|$|R
