20|185|Public
5000|$|Damodar Mall is an Indian {{retail sector}} professional. He is Chief Executive Officer of Reliance Retail Ltd. (<b>Value</b> <b>Format)</b> ...|$|E
50|$|On Linux, {{there is}} the top command which is good for CPU and {{processes}} but does not cover disks and networks. For disk I/O, the iostat command can give you the details. But neither of these commands allow saving data in a format suitable for a spreadsheet or simple further processing. Linux utility dstat {{can be used to}} produce text data, even in comma separated <b>value</b> <b>format,</b> which is quite suitable for spreadsheet programs.|$|E
50|$|AAF is {{designed}} to be a data representation of works in progress, as compared to MXF (Material Exchange Format), which is for exchanging finished media products. While MXF uses a KLV (Key Length <b>Value)</b> <b>format</b> for storage, AAF uses the Microsoft Structured Storage system. MXF was developed to be essentially a subset of the AAF data model, under the Zero Divergence Directive (ZDD) policy. This allows for workflows that involve the mixing of AAF and MXF.|$|E
50|$|Import (CSV): Specifies {{whether the}} product {{supports}} import {{data from a}} comma-separated <b>values</b> <b>formatted</b> file.|$|R
50|$|Socrata {{offers a}} free account for users to upload {{datasets}} in comma-separated <b>values</b> <b>format.</b> Users can download a dataset or access data via API.|$|R
50|$|Any {{programming}} language that has input/output and string processing functionality {{is able to}} read and write CSV files. Here is a list of individual {{programming language}} support for the comma-separated <b>values</b> <b>format.</b>|$|R
50|$|In November 2004 Mike Cormack (then CEO Archipelago Holding) {{spoke at}} the FPL (FIX Protocol Ltd) conference in New York {{regarding}} a call for action {{to meet the challenges}} of the increased market data volumes. The increasing volumes of market data were causing delays, preventing market data from reaching traders in a timely fashion, thus disrupting their ability to trade. The classic FIX tag <b>value</b> <b>format</b> was considered to be too verbose and had a high processing overhead. A working group was formed within FPL shortly after the conference.|$|E
50|$|GDELT {{includes}} {{data from}} 1979 to the present. The data is available as zip files in tab-separated <b>value</b> <b>format</b> using a CSV extension for easy import into Microsoft Excel or similar spreadsheet software. Data from 1979 to 2005 {{is available in}} the form of one zip file per year, with the file size gradually increased infrom 14.3 MB in 1979 to 125.9 MB in 2005, reflecting {{the increase in the number}} of news media and the frequency and comprehensiveness of event recording. Data files from January 2006 to March 2013 are available at monthly granularity, with the zipped file size rising from 11 MB in January 2006 to 103.2 MB in March 2013. Data files from April 1, 2013 onward are available at a daily granularity. The data file for each date is made available by 6 AM Eastern Standard Time the next day. As of June 2014, the size of the daily zipped file is about 5-12 MB. The data files use Conflict and Mediation Event Observations (CAMEO) coding for recording events.|$|E
3000|$|Upon {{acquisition}} of data, we exported data for each walk to comma-separated <b>value</b> <b>format</b> and imported {{this information to}} a mathematical computation program. The walk data contained a matrix with time, [...]...|$|E
5000|$|WMIDiag.vbs: The WMI Diagnosis Tool is a VBScript {{downloadable}} from Microsoft {{here and}} is a tool for testing and validating WMI on Windows 2000 and greater. The download includes pretty thorough documentation and the tool supports numerous switches. When run, it will generate up to four text files which: list the steps taken (the LOG file), {{an overview of the}} results (REPORT file), a statistics file (in comma separated <b>values</b> <b>format),</b> and optionally a file listing of the providers registered on the machine (PROVIDERS, also in comma separated <b>values</b> <b>format).</b> The report file that is generated includes a list of the issues identified and potential ways to fix them.|$|R
50|$|Export (CSV): Specifies {{whether the}} product support {{exporting}} selected rows to a comma-separated <b>values</b> <b>formatted</b> file. Usually also implies capability to the clipboard (in CSV format) for pasting into applications supporting pasting from CSV files such as Excel.|$|R
5000|$|Breaking the encoded <b>value</b> <b>formatted</b> as [...] into 4 bits of {{mantissa}} , 3 bits of exponent [...] and 1 {{sign bit}} , the decoded linear value [...] {{is given by}} formulawhich is a 14-bit signed integer in the range ±0 to ±8031.|$|R
40|$|This paper {{describes}} a research toward {{the accuracy of}} floating-point values, and effort to reveal the real accuracy. The methods used in this research paper are assignment of values, assignment of value of arithmetic expressions, and output the values using floating-point <b>value</b> <b>format</b> that helps reveal the accuracy. The programming-tool used are Visual C# 9, Visual C++ 9, Java 5, and Visual BASIC 9. These tools run on top of Intel 80 x 86 hardware. The results show that 1 * 10 -x cannot be accurately represented, and the approximate accuracy ranges only from 7 to 16 decimal digits...|$|E
40|$|We model {{a scheme}} for the {{coherent}} control of light waves and currents in metallic nanospheres which applies {{independently of the}} nonlinear multiphoton processes at the origin of waves and currents. Using exact mathematical formulae, we calculate numerically with a custom fortran code the effect of an external control field which enable us to change the radiation pattern and suppress radiative losses or to reduce absorption, enabling the particle to behave as a perfect scatterer or as a perfect absorber. Data are provided in tabular, comma delimited <b>value</b> <b>format</b> and illustrate narrow features in {{the response of the}} particles that result in high sensitivity to small variations in the local environment, including subwavelength spatial shifts...|$|E
40|$|Boolean Search {{logic is}} incorporated. Use {{operators}} such as AND, OR, NOT to expand or narrow the search results. Access the “Help Center ” for full Boolean Search overview. Company Record Tools: Tools allow end users to download Hoover’s company data into a spreadsheet or report format, generate company lists or add a specific company to a Watch List (Access {{varies depending on}} subscription level) Download Company Data › Subscribers can download a specific company’s data into an Excel Workbook or CSV format. › Note: When downloading into an Excel Workbook, change file type from “Web Page ” to “Excel Workbook” › CSV: Use Comma Separated <b>Value</b> <b>format</b> when importing data into a Customer Relationship Management too...|$|E
5000|$|... iRows {{supported}} conventional spreadsheet features functions, <b>value</b> <b>formatting</b> {{and charts}} and added web oriented spreadsheet capabilities like collaboration (multiple people using a shared spreadsheet, sending a spreadsheet as a link {{instead of an}} attachment and ability to publish spreadsheets on other web pages (e.g. blogs).|$|R
40|$|The {{intersection}} of science, knowledge, informatics and technocratic specialization with power is discussed. The personal, social, {{political and economic}} <b>values</b> <b>formatting</b> the behavior of social members as citizens is discussed {{as the result of}} bureaucratic structures as a controlling system cannot be analyzed without social connections. The overstepping of mass communication is suggested as the {{solution to the problem of}} equal social system...|$|R
50|$|The comma-separated <b>values</b> (CSV) <b>format</b> is very {{commonly}} used in exchanging text data between database and spreadsheet formats.|$|R
40|$|A {{macroscopic}} {{approach to}} departmental cost finding {{is combined with}} a microscopic approach to the weighting of laboratory tests in a mathematical model which, when incorporated into a relative unit <b>value</b> <b>format,</b> yields unit costs for such tests under {{a wide variety of}} operational conditions. The task of updating such costs to reflect changing conditions can be facilitated by a computer program incorporating the capability of pricing the various tests to achieve any desired profit or loss or to break even. Among other potential uses of such a technique, the effects on unit cost per test caused by increasing or decreasing the number of technicians or the volume of tests can be systematically examined, and pricing can be updated each year as hospital costs change...|$|E
3000|$|... [*]GHz and {{consisting}} of a low-noise amplifier (LNA). We use an in-house software tool called SQUIRREL (Spectrum Query Utility Interface for Real-time Radio Electromagnetics) to communicate remotely with the spectrum analyzer via commands issued through a simple graphical user interface on a laptop. The GUI accepts details such as the center frequency, the span around the center frequency, and the resolution bandwidth. SQUIRREL communicates with the spectrum analyzer using TCL (Tool Command Language) over TCP/IP. After the sweep action is performed by the spectrum analyzer, the data points are returned to the GUI in a comma-spaced <b>value</b> <b>format.</b> In its current format, the GUI and the server are written in JAVA and can be deployed {{on a variety of}} operating systems and computers.|$|E
40|$|Life cycle costs (LCC) {{concepts}} are merged with installation and operating practices for a pumping system {{to form a}} reliability model. Reliability models show how inherent component life is reduced by various practices. As failed components are replaced, changes occur in the LCC values. The outcome of several installation/use alternatives and several grades of pumps are described in net present <b>value</b> <b>format.</b> Life cycle costs are total costs from inception to disposal for both equipment and projects (Barringer 1996). Analytical studies and estimates of total costs are methods for finding life cycle costs. The objective of LCC analysis is to choose the most cost-effective approach {{from a series of}} alternatives so the least long-term cost of ownership is achieved. LCC is strongly influenced by equipment grade...|$|E
5000|$|Data Model: the {{description}} of the data types bound with elements of the interface. At runtime, modifying the state of an interactor will change also the value of the bound data element and vice versa, in order to describe dynamic UI changes (correlation between UI elements, conditional layout, conditional connections between presentations, input <b>values</b> <b>format</b> etc.). The data model is defined using the standard XML Schema Definition constructs.|$|R
50|$|In some delimiter-separated <b>values</b> file <b>formats,</b> the {{semicolon}} {{is used as}} the separator character, as {{an alternative}} to comma-separated values.|$|R
50|$|A tab-separated values (TSV) file is {{a simple}} text format for storing data in a tabular structure, e.g., {{database}} table or spreadsheet data, {{and a way of}} exchanging information between databases. Each record in the table is one line of the text file. Each field value of a record is separated from the next by a tab character. The TSV format is thus a type of the more general delimiter-separated <b>values</b> <b>format.</b>|$|R
40|$|A {{distinction}} can {{be drawn}} between extensive and intensive quantities. Extensive quantities (e. g., volume, distance), which have {{been the focus of}} developmental research, depend upon additive combination. Intensive quantities (e. g., density, speed), which have been relatively neglected, derive from proportional relations between variables. Thus, while proportional relations can be expressed with extensive quantities, these relations are constitutive with intensive quantities. One consequence is that factors, which are theorized as marginal with extensive quantities, are conceptually central in intensive contexts, and may need to be recognized in developmental models. Two such factors, termed variable salience and relational focus, are examined here, via a study where 963 Scottish children aged 7 - 12 years were asked to solve 42 intensive quantity problems in comparison and missing <b>value</b> <b>format.</b> Reasoning improved with age, but at all ages it was strongly influenced by variable salience and relational focus. Moreover, the manner in which these two factors interacted with other factors differed from what might be expected from models of proportional reasoning with extensive quantities. Based upon these results, it is argued that the distinction between extensive and intensive quantities is theoretically significant, and intensive quantities need to be granted more attention in the future...|$|E
30|$|The {{series of}} actions of the “Interaction” such as “require <inputOfMemberForAsking>” {{represent}} the system process for accepting or outputting the input/output data. The “System” partition represents the fundamental process of business logic so as to clarify the important aspects for interactive software such as the data flow of entities and the process for the entities. The {{series of actions}} of the “System” such as “retrieve the borrowers of the selected book” represent business logic briefly. The series of object nodes of the “System” such as “borrowersOfSelectedBook” represent entities handled by the business logic. In the left of Figure 2, the classes are assigned into the object nodes by the analysts so as to clarify the relation between a structure and objects. There {{are two types of}} objects in the activity diagrams. One is “entity” class whose objects appear at the partition of “System.” The “entity” objects are corresponding with the conceptual model represented as a class diagram. The other is “boundary” class whose objects appear at the partition of “User” or “Interaction.” The “boundary” objects are derived from the “entity” objects because the “boundary” objects exist for inputting or displaying the “entity” objects. In the right of Figure 2, the analysts define the object diagram for each object node so that the users and analysts can understand the <b>value</b> <b>format</b> and range of each structure enough.|$|E
40|$|Recent {{research}} suggests that when we face a choice between several options described {{with a large number}} of attributes, we make better choices if we do not consciously ponder over the alternatives but instead engage in a mindless task while our unconscious mind deliberates before finally pointing out the optimal option for us. Subsequent research attempting to replicate this deliberation-without-attention effect, however, provided mitigated support for its existence. The present research had two objectives. First, it aimed to improve the methodology used to test for this effect by using a choice task where attributes values were experimentally controlled. Secondly, it aimed to examine the effect of the format of presentation of the attribute values on choice quality. Participants were randomly assigned to one cell of a 2 (attribute <b>value</b> <b>format)</b> x 2 (deliberation type) design. Half of the participants received attribute values in a numerical format whereas the remaining half received them in a visual format. Within each of these conditions, half of the participants engaged in conscious deliberation before making their choice; the other half engaged unconscious deliberation. Results showed that conscious deliberation led to better decisions when the attribute values were presented in a numerical format whereas unconscious thought led to a better discrimination between products when cue values were presented in a visual format. Methodological and theoretical implications for testing the propositions of Unconscious Thought Theory will be discussed...|$|E
5000|$|The [...] {{command-line}} program generates {{lists of}} installed devices and drivers, {{similar to the}} Device Manager's output, which the user may view on-screen or redirect to a file. This is useful for note-taking and for reporting problems to remote third parties such as technical support personnel. The program has switches to control the output detail and format, including an [...] switch with [...] parameter to generate output in comma-separated <b>values</b> <b>format,</b> suitable for importing into a spreadsheet application such as Microsoft Excel.|$|R
50|$|The {{web service}} {{provides}} means for visualizing data with pie charts, bar charts, lineplots, scatterplots, timelines, and geographical maps. Data are exported in a comma-separated <b>values</b> file <b>format.</b>|$|R
50|$|Web format: <b>Value</b> in {{appropriate}} <b>format</b> for web browsers.|$|R
40|$|Abstract: Protein Data Bank (PDB) is {{a public}} web {{database}} with more than 100, 000 biological macromolecular structures. With this large amount of protein structures available on PDB the use of tools for acquisition and analysis of specific sets of biological macromolecules is a necessity. Hence, in this work we propose {{the development of a}} tool for acquiring, storing and analyzing specific sets of proteins from the PDB database. The proposed tool runs on desktop environment allowing the user to acquire the structures from the RESTful web-service provided by PDB server. After the acquisition of a set of interesting PDBs the user can manipulate these data in an off-line environment through a local database that stores the information about the characteristics of the structures, for example, ligands, mutations, residues, sequences and docking results. The protein files are locally stored in the users’ computer and can be used, for instance, for molecular docking simulations and alignment of sequences and structures. Having a set of proteins of interest available locally and using our proposed tool the user can perform analysis related to alignments and visualize important proteins characteristics improving the knowledge about specific target. Besides, the user can select PDB files to be visualized on a graphical environment that is integrated in our tool. Other features are related to the exporting of sequence alignments results in csv (comma separated <b>value)</b> <b>format</b> or exporting sequences that have...|$|E
40|$|The {{purpose of}} this study was to assess the {{feasibility}} of 3 D intraoral scanning for documentation of palatal soft tissue by evaluating the accuracy of shape, color, and curvature. Intraoral scans of ten participants' upper dentition and palate were acquired with the TRIOS® 3 D intraoral scanner by two observers. Conventional impressions were taken and digitized as a gold standard. The resulting surface models were aligned using an Iterative Closest Point approach. The absolute distance measurements between the intraoral models and the digitized impression were used to quantify the trueness and precision of intraoral scanning. The mean color of the palatal soft tissue was extracted in HSV (hue, saturation, <b>value)</b> <b>format</b> to establish the color precision. Finally, the mean curvature of the surface models was calculated and used for surface irregularity. Mean average distance error between the conventional impression models and the intraoral models was 0. 02 ± 0. 07 mm (p = 0. 30). Mean interobserver color difference was - 0. 08 ± 1. 49 ° (p = 0. 864), 0. 28 ± 0. 78 % (p = 0. 286), and 0. 30 ± 1. 14 % (p = 0. 426) for respectively hue, saturation, and value. The interobserver differences for overall and maximum surface irregularity were 0. 01 ± 0. 03 and 0. 00 ± 0. 05 mm. This study supports the hypothesis that the intraoral scan can perform a 3 D documentation of palatal soft tissue in terms of shape, color, and curvature. An intraoral scanner can be an objective tool, adjunctive to the clinical examination of the palatal tissu...|$|E
40|$|Background: The {{molecular}} diagnostics laboratory {{faces the}} challenge of improving test turnaround time (TAT). Low and consistent TATs are of great clinical and regulatory importance, especially for molecular virology tests. Laboratory information systems (LISs) contain all the data elements necessary to do accurate quality assurance (QA) reporting of TAT and other measures, but these reports are in most cases still performed manually: a time-consuming and error-prone task. The {{aim of this study}} was to develop a web-based real-time QA platform that would automate QA reporting in the molecular diagnostics laboratory at our institution, and minimize the time expended in preparing these reports. Methods: Using a standard Linux, Nginx, MariaDB, PHP stack virtual machine running atop a Dell Precision 5810, we designed and built a web-based QA platform, code-named Alchemy. Data files pulled periodically from the LIS in comma-separated <b>value</b> <b>format</b> were used to autogenerate QA reports for the human immunodeficiency virus (HIV) quantitation, hepatitis C virus (HCV) quantitation, and BK virus (BKV) quantitation. Alchemy allowed the user to select a specific timeframe to be analyzed and calculated key QA statistics in real-time, including the average TAT in days, tests falling outside the expected TAT ranges, and test result ranges. Results: Before implementing Alchemy, reporting QA for the HIV, HCV, and BKV quantitation assays took 45 – 60 min of personnel time per test every month. With Alchemy, that time has decreased to 15 min total per month. Alchemy allowed the user to select specific periods of time and analyzed the TAT data in-depth without the need of extensive manual calculations. Conclusions: Alchemy has significantly decreased the time and the human error associated with QA report generation in our molecular diagnostics laboratory. Other tests will be added to this web-based platform in future updates. This effort shows the utility of informatician-supervised resident/fellow programming projects as learning opportunities and workflow improvements in the molecular laboratory...|$|E
50|$|Sensage holds U.S. patent #7,024,414 for parsing table {{data into}} columns of <b>values,</b> <b>formatting</b> each column into a data stream, and {{transferring}} each data stream to a storage device {{in a continuous}} strip of data. In this architecture, the data is stored in columns instead of rows, which {{eliminates the need for}} indices when storing event data to increase data compression and retrieval speeds. The event data warehouse software uses an extraction, transformation and loading tool to pull records into the data warehouse, where it is compressed and spread across server nodes. Data queries are distributed across data warehouse nodes as well.|$|R
5000|$|... cprintf - <b>Formats</b> <b>values</b> {{and writes}} them {{directly}} to the console.|$|R
5000|$|Transformation of data: {{converting}} <b>values</b> {{to other}} <b>formats,</b> normalizing and denormalizing.|$|R
