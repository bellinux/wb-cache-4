2946|10000|Public
5|$|Moreover, the <b>volume</b> <b>of</b> <b>data</b> is {{increasing}} exponentially with a doubling time of approximately 10 months.|$|E
25|$|The {{rapid growth}} is {{predominantly}} due to new data sources, and increased <b>volume</b> <b>of</b> <b>data</b> from existing sources. New data sources include 22,288 samples from The Cancer Genome Atlas, and 920,441 {{samples from the}} Catalogue of Somatic Mutation in Cancer (COSMIC).|$|E
25|$|There {{are many}} {{observable}} trends in mathematics, {{the most notable}} being that the subject is growing ever larger, computers are ever more important and powerful, the application of mathematics to bioinformatics is rapidly expanding, the <b>volume</b> <b>of</b> <b>data</b> to be analyzed being produced by science and industry, facilitated by computers, is explosively expanding.|$|E
5000|$|Exchange large <b>volumes</b> <b>of</b> <b>data</b> using Electronic Data Interchange (EDI) ...|$|R
40|$|Desktop Grid is {{increasing}} in popularity because of relatively very low cost and good performance in institutions. Data-intensive applications require data management in scientific experiments conducted by researchers and scientists in Desktop Grid-based Distributed Computing Infrastructure (DCI). Some of these data-intensive applications deal with large <b>volumes</b> <b>of</b> <b>data.</b> Several solutions for data-intensive applications {{have been proposed}} for Desktop Grid (DG) {{but they are not}} efficient in handling large <b>volumes</b> <b>of</b> <b>data.</b> Data management in this environment deals with data access and integration, maintaining basic properties of databases, architecture for querying data, etc. Data in data-intensive applications has to be replicated in multiple nodes for improving data availability and reducing response time. Peer-to-Peer (P 2 P) is a well established technique for handling large <b>volumes</b> <b>of</b> <b>data</b> and is widely used on the internet. Its environment is similar to the environment of DG. The performance of existing P 2 P-based solution dealing with generic architecture for replicating large <b>volumes</b> <b>of</b> <b>data</b> is not efficient in DG-based DCI. Therefore, {{there is a need for}} a generic architecture for replicating large <b>volumes</b> <b>of</b> <b>data</b> efficiently by using P 2 P in BOINC based Desktop Grid. Present solutions for data-intensive applications mainly deal with read only <b>data.</b> New type <b>of</b> applications are emerging which deal large <b>volumes</b> <b>of</b> <b>data</b> and Read/Write <b>of</b> <b>data.</b> In emerging scientific experiments, some nodes of DG generate new snapshot <b>of</b> scientific <b>data</b> after regular intervals. This new snapshot <b>of</b> <b>data</b> is generated by updating some of the values <b>of</b> existing <b>data</b> fields. This updated data has to be synchronised in all DG nodes for maintaining data consistency. The performance <b>of</b> <b>data</b> management in DG can be improved by addressing efficient data replication and consistency. Therefore, there is need for algorithms which deal with data Read/Write consistency along with replication for large <b>volumes</b> <b>of</b> <b>data</b> in BOINC based Desktop Grid. The research is to identify efficient solutions for data replication in handling large <b>volumes</b> <b>of</b> <b>data</b> and maintaining Read/Write data consistency using Peer-to-Peer techniques in BOINC based Desktop Grid. This thesis presents the solutions that have been carried out to complete the research...|$|R
30|$|Using large <b>volumes</b> <b>of</b> <b>data</b> {{but with}} low variety for {{training}} {{the classification models}} [12].|$|R
25|$|Optimizing {{the speed}} and {{accuracy}} of these steps for use in large-scale automated structure prediction is {{a key component of}} structural genomics initiatives, partly because the resulting <b>volume</b> <b>of</b> <b>data</b> will be too large to process manually and partly because the goal of structural genomics requires providing models of reasonable quality to researchers who are not themselves structure prediction experts.|$|E
25|$|Microarray {{data was}} found to be more useful when {{compared}} to other similar datasets. The sheer <b>volume</b> <b>of</b> <b>data,</b> specialized formats (such as MIAME), and curation efforts associated with the datasets require specialized databases to store the data. A number of open-source data warehousing solutions, such as InterMine and , have been created for the specific purpose of integrating diverse biological datasets, and also support analysis.|$|E
25|$|The Mars 2 and 3 orbiters {{sent back}} a {{relatively}} large <b>volume</b> <b>of</b> <b>data</b> covering the period from December 1971 to March 1972, although transmissions continued through to August. By 22 August 1972, after sending back data {{and a total of}} 60 pictures, Mars 2 and 3 concluded their missions. The images and data enabled creation of surface relief maps, and gave information on the Martian gravity and magnetic fields.|$|E
40|$|Today, the {{new trend}} that {{companies}} are following is to digitalize all data {{they have in}} order to reduce costs with physical space and better handle data volume. The new era, the era <b>of</b> bundling <b>data</b> sources and huge <b>volumes</b> <b>of</b> <b>data</b> continues this decade also. In all industries, companies are starting to understand and appreciate the value <b>of</b> <b>data,</b> the value that these <b>volumes</b> <b>of</b> <b>data</b> can produce. Companies are collecting huge <b>volumes</b> <b>of</b> <b>data</b> but not always the actual solution for business intelligence (BI) can handle these volumes. To obtain information, those <b>volumes</b> <b>of</b> <b>data</b> are analyzed with extract-transform-load (ETL) software solutions. Companies, in the current economic context, are finding hard to invest in improvements of BI process including in ETL process. In this paper I will demonstrate why this kind of investment is necessary and also I will demonstrate that ETL process must be included in BI and big data architectures. In the following pages I will refer to business architectures as BI and big data architectures...|$|R
50|$|Data-intensive {{computing}} is a {{class of}} parallel computing applications which use a data parallel approach to process large <b>volumes</b> <b>of</b> <b>data</b> typically terabytes or petabytes in size and typically referred to as big data. Computing applications which devote most of their execution time to computational requirements are deemed compute-intensive, whereas computing applications which require large <b>volumes</b> <b>of</b> <b>data</b> and devote most of their processing time to I/O and manipulation <b>of</b> <b>data</b> are deemed data-intensive.|$|R
50|$|ADABAS is {{typically}} used in applications that require high <b>volumes</b> <b>of</b> <b>data</b> processing or in high transaction online analytical processing environments.|$|R
25|$|A {{medical imaging}} method {{employing}} tomography where digital geometry processing {{is used to}} generate a three-dimensional image of the internal structures of an object from a large series of two-dimensional X-ray images taken around a single axis of rotation. CT produces a <b>volume</b> <b>of</b> <b>data,</b> which can be manipulated, through {{a process known as}} windowing, in order to demonstrate various structures based on their ability to attenuate and prevent transmission of the incident X-ray beam.|$|E
25|$|The mainshock was {{recorded}} by few seismometers in the region. In contrast, the 1986 event {{occurred at a}} time when far more instruments were operating in southern California, and the additional <b>volume</b> <b>of</b> <b>data</b> that was generated provided a means for relocating and confirming the mechanism of faulting for the older event. The amplitude of the waveforms shown on the 1948 seismograms were 20–30% larger than those of the later shock. Like the 1986 event, the 1948 event was presumed to have occurred near the northwest-striking (and steeply-dipping) Banning Fault, which was relatively unknown at the time.|$|E
25|$|The LHC Computing Grid was {{constructed}} {{as part of}} the LHC design, to handle the massive amounts of data expected for its collisions. It is an international collaborative project that consists of a grid-based computer network infrastructure initially connecting 140 computing centres in 35 countries (over 170 in 36 countries as of 2012). It was designed by CERN to handle the significant <b>volume</b> <b>of</b> <b>data</b> produced by LHC experiments, incorporating both private fibre optic cable links and existing high-speed portions of the public Internet to enable data transfer from CERN to academic institutions around the world. The Open Science Grid is used as the primary infrastructure in the United States, and also as part of an interoperable federation with the LHC Computing Grid.|$|E
5000|$|Data Management: As BANs {{generate}} large <b>volumes</b> <b>of</b> <b>data,</b> {{the need}} to manage and maintain these datasets is of utmost importance.|$|R
30|$|Enterprise Control Language was {{specifically}} designed for manipulation of large amounts <b>of</b> <b>data.</b> It enables implementation <b>of</b> <b>data</b> intensive applications with complex data-flows and huge <b>volumes</b> <b>of</b> <b>data.</b>|$|R
50|$|Despite {{the wide}} {{utilization}} of cloud computing, efficient sharing <b>of</b> large <b>volumes</b> <b>of</b> <b>data</b> in an untrusted cloud {{is still a}} challenge.|$|R
25|$|Despite the {{compelling}} seismogram from the 1940 event in El Centro, strong-motion seismology was not explicitly sought until later events occurred—the San Fernando earthquake made evident {{the need for}} more data for earthquake engineering applications. The California Strong Motion Instrumentation Program was initiated in 1971 with the goal of maximizing the <b>volume</b> <b>of</b> <b>data</b> by furnishing and maintaining instruments at selected lifeline structures, buildings, and ground response stations. By the late 1980s, the program had instrumented more than 450 structures, bridges, dams, and power plants. The 1979 Imperial Valley and 1987 Whittier Narrows earthquakes were presented as gainful events that were recorded during that period, because both produced valuable data that increased knowledge of how moderate events affect buildings. The success of the Imperial Valley event was especially pronounced because of a recently constructed and fully instrumented government building that was shaken to the point of failure.|$|E
25|$|Eye {{movement}} in reading involves the visual processing of written text. This {{was described by}} the French ophthalmologist Louis Émile Javal in the late 19th century. He reported that eyes do not move continuously along a line of text, but make short, rapid movements (saccades) intermingled with short stops (fixations). Javal's observations were characterised by a reliance on naked-eye observation of eye {{movement in}} the absence of technology. From the late 19th to the mid-20th century, investigators used early tracking technologies to assist their observation, in a research climate that emphasised the measurement of human behaviour and skill for educational ends. Most basic knowledge about eye movement was obtained during this period. Since the mid-20th century, there have been three major changes: the development of non-invasive eye-movement tracking equipment; the introduction of computer technology to enhance the power of this equipment to pick up, record and process the huge <b>volume</b> <b>of</b> <b>data</b> that eye movement generates; and the emergence of cognitive psychology as a theoretical and methodological framework within which reading processes are examined. Sereno & Rayner (2003) believed that the best current approach to discover immediate signs of word recognition is through the recordings of eye movements and event-related potential.|$|E
2500|$|... {{the sheer}} <b>volume</b> <b>of</b> <b>data</b> {{and the ability}} to share it (Data warehousing) ...|$|E
5000|$|Hidalgo has co-authored {{a number}} <b>of</b> popular <b>data</b> {{visualization}} engines. These are tools that make available vast <b>volumes</b> <b>of</b> <b>data</b> through visualizations. These visualization engines include: ...|$|R
40|$|A quick scan of {{the daily}} {{newspaper}} shows just how much data-driven information is being produced these days and how everyone, from decision-makers in business and government to scientists and researchers, is drawing on ever-increasing <b>volumes</b> <b>of</b> <b>data</b> {{to try to solve}} problems. However, good decision-making requires more than just great <b>volumes</b> <b>of</b> <b>data,</b> no matter how accurate and up-to-date it is. Data has to be carefully mined for the right information, for the gold to be sifted from the gravel...|$|R
40|$|International audienceData {{aggregation}} {{is one of}} the {{key features}} used in databases, especially for Business Intelligence (e. g., ETL, OLAP) and analytics/data mining. When considering SQL databases, aggregation is used to prepare and visualize data for deeper analyses. However, these operations are often impossible on very large <b>volumes</b> <b>of</b> <b>data</b> regarding memory-and-timeconsumption. In this paper, we show how NoSQL databases such as MongoDB and its key-value stores, thanks to the native MapReduce algorithm, can provide an efficient framework to aggregate large <b>volumes</b> <b>of</b> <b>data.</b> We provide basic material about the MapReduce algorithm, the different NoSQL databases (read intensive vs. write intensive). We investigate how to efficiently modelize the data framework for BI and analytics. For this purpose, we focus on read intensive NoSQL databases using MongoDB and we show how NoSQL and MapReduce can help handling large <b>volumes</b> <b>of</b> <b>data...</b>|$|R
2500|$|Despite this {{attention}} to detail, and the {{improvements in the}} <b>volume</b> <b>of</b> <b>data</b> with each reprinting, {{a lot of the}} data was out of date or incomplete. [...] For example, the Galway–Clifden railway is described as being operational, but it had closed in 1935. Ireland was also described as perfectly suited to military operations because of its [...] "excellent network of roads", and details on population centres such as Derry and Belfast were accurate but lacked information on British troop concentrations based in these cities. On the other hand, the Ardnacrusha power station on the lower Shannon was entirely detailed in the plan, thanks to the help of the German firm Siemens, which had built it prior to the war.|$|E
2500|$|Studied the {{population}} genetics and physical anthropology of Brazilian populations, especially the Amerindians and the Blacks. On {{the basis of}} DNA markers linked to the sickle cell anemia gene, he demonstrated that the Brazilian black population is predominantly of Bantu origin, with a lesser contribution from the Benin and very little contribution from the Senegambia. These data demonstrated that this Black population is significantly different from its U.S. or Caribbean counterparts, with implications for the medical genetic studies that compare these populations and for {{the understanding of the}} hereditary diseases. Furthermore, his studies on markers linked to the mitochondrial DNA, the Y chromosome, several nuclear genes and VNTRs have contributed {{to the understanding of the}} genetic diversity of the Amerindian populations, the relationship with the founder populations, and the black Brazilian populations. His laboratory and his collaborators have contributed the largest <b>volume</b> <b>of</b> <b>data</b> on DNA markers in the Brazilian populations thus far.|$|E
2500|$|Facebook may be {{accessed}} {{by a large}} range of desktops, laptops, tablet computers, and smartphones over the Internet and mobile networks. After registering to use the site, users can create a user profile indicating their name, occupation, schools attended and so on. Users can add other users as [...] "friends", exchange messages, post status updates and digital photos, share digital videos and links, use various software applications ("apps"), and receive notifications when others update their profiles or make posts. Additionally, users may join common-interest user groups organized by workplace, school, hobbies or other topics, and categorize their friends into lists such as [...] "People From Work" [...] or [...] "Close Friends". In groups, editors can pin posts to top. Additionally, users can complain about or block unpleasant people. Because of the large <b>volume</b> <b>of</b> <b>data</b> that users submit to the service, Facebook has come under scrutiny for its privacy policies. Facebook makes most of its revenue from advertisements which appear onscreen, marketing access for its customers to its users and offering highly selective advertising opportunities.|$|E
30|$|Data storage: The {{architecture}} {{should be}} able to store the gigantic <b>volumes</b> <b>of</b> <b>data</b> produced by the IoE devices without compromising on the read/write efficiencies.|$|R
50|$|Previously {{these systems}} {{would have been}} {{impracticable}} due to the sheer <b>volumes</b> <b>of</b> <b>data</b> processing and the operator time needed to apply constant manual updates.|$|R
50|$|Advantages {{of managed}} storage are that more space {{can be ordered}} as required. Depending upon your SSP, backups may also be managed.Faster data access can be ordered as required. Also, {{maintenance}} costs may be reduced, particularly for larger organizations who store a large or increasing <b>volumes</b> <b>of</b> <b>data.</b> Another advantage is that best practices {{are likely to be}} followed. Disadvantages are that the cost may be prohibitive, for small organizations or individuals who deal with smaller amounts or static <b>volumes</b> <b>of</b> <b>data</b> and that there's less control <b>of</b> <b>data</b> systems.|$|R
5000|$|... {{the sheer}} <b>volume</b> <b>of</b> <b>data</b> {{and the ability}} to share it (Data warehousing) ...|$|E
50|$|It is also {{hoped that}} the vast <b>volume</b> <b>of</b> <b>data</b> {{produced}} will lead to additional serendipitous discoveries.|$|E
50|$|Machine {{learning}} {{is the study}} of systems that can identify trends in data. Tasks in machine learning frequently involve manipulating and classifying a large <b>volume</b> <b>of</b> <b>data</b> in high-dimensional vector spaces. The runtime of classical machine learning algorithms is limited by a polynomial dependence on both the <b>volume</b> <b>of</b> <b>data</b> and the dimensions of the space. Quantum computers are capable of manipulating high-dimensional vectors using tensor product spaces are thus the perfect platform for machine learning algorithms.|$|E
30|$|To {{designate}} a {{data protection officer}} (DPO) for larger institutions or for institutions processing particular types or <b>volumes</b> <b>of</b> <b>data.</b> These are defined in the legislation.|$|R
40|$|High <b>volumes</b> <b>of</b> <b>data</b> can be {{summarized}} most efficiently using OLAP technology. As <b>volumes</b> <b>of</b> <b>data</b> approach the millions and even billions <b>of</b> rows <b>of</b> <b>data,</b> loading an OLAP cube can become extremely challenging due to system constraints. This paper presents a technique that uses SAS ® macro programming to divide and load data incrementally into an OLAP cube. Using this technique ensures the cube will build successfully but also provides users access to the cube while it is being loaded. This paper also highlights OLAP programming techniques {{that allow you to}} streamline the cube building process and minimize the impact of errors. CONCEPT The amount <b>of</b> <b>data</b> that organizations must be able to analyze in an ever growing competitive market continues to grow larger and larger. This change in volume creates constraints for loading data into an OLAP cube as the size and structure <b>of</b> source <b>data</b> grows. Build times run longer as the data volume grows. As more dimensions and measures are defined in the OLAP cube, the build time grows even longer because more data has to be analyzed to build the NWAY aggregation. This is because the NWAY aggregation is the summarization of all measures by the grouping of each level defined in the OLAP cube. One way to circumvent this lengthy build process with high <b>volumes</b> <b>of</b> <b>data</b> is to avoid building the NWAY aggregation and define a relational OLAP cube, also known as ROLAP technology. When large <b>volumes</b> <b>of</b> <b>data</b> are queried, the end user’s wait time increases as the data is summarized at run time. This paper presents a technique that loads large <b>volumes</b> <b>of</b> <b>data</b> into an OLAP cube by dividing the source data into segments and incrementally loading the OLAP cube in place. This technique uses the default NWAY aggregatio...|$|R
30|$|Such ONDMs however {{come at a}} cost of {{increased}} performance overhead, which may have a significant economic impact, especially in large distributed setups involving massive <b>volumes</b> <b>of</b> <b>data.</b>|$|R
