3|10000|Public
50|$|In September 2008 the Luxembourg’s {{financial}} {{regulatory authority}} CSSF issued a circular 08/371 {{resulting in the}} official adoption of the e-file platform for the mandatory transmission of UCI simplified and full prospectuses and annual and semi-annual reports both to the CSSF and to the electronic reference database of the financial centre’s investment funds set up by Finesti.The platform allows the final <b>validation</b> <b>of</b> <b>the</b> <b>documents</b> {{that are to be}} sent.|$|E
30|$|Among other NCL features, this {{perspective}} notion {{is one of}} the cases that hinder the validation of NCL documents with basis on XML Schema only. Actually, XML Schema would only be enough for syntactic and structural <b>validation</b> <b>of</b> <b>the</b> <b>documents,</b> since there are several particularities in NCL that it cannot express. For example, when checking the correctness of a link, it is necessary to be sure whether the nodes that are being associated are valid identifiers and are present in the same composition (which cannot be verified by an XML Schema specification alone).|$|E
40|$|Complex {{languages}} {{built on}} XML require support for validation that {{goes far beyond}} what can be provided using traditional schema languages like XML Schema. In this paper {{we look at the}} experience we made as part of the FpML Validation Working Group, which is specifying standard validation rules for the Financial Products Markup Language (FpML), a cross-industry standard for financial derivatives trades. The issues surfaced in here are likely to be of interest to any organisation or standards body dealing with data that exhibits complex structure or semantics. We discuss the process for validation rule gathering that the working group followed, and the issues it has been facing. We then look at the two reference implementation languages, Schematron and CLIX, that we used to formally express the validation rules, and some of the requirements we found: <b>validation</b> <b>of</b> <b>the</b> <b>documents</b> against external data sources like relational databases, expressiveness requirements, and complex data type support...|$|E
40|$|XML is the {{lingua franca}} for data {{exchange}} on the Internet. Within applications or communities, XML data {{is usually not}} arbitrary but adheres to some structure possibly imposed by a schema. The advantages offered by <b>the</b> presence <b>of</b> such a schema are numerous. The most direct application is <b>of</b> course automatic <b>validation</b> <b>of</b> <b>the</b> <b>document</b> structure. Input <b>validation,</b> for instance, not only facilitates automatic processing but also ensures soundness <b>of</b> <b>the</b> input data. Indeed, unvalidated input from web requests is considered as the number one vulnerability for web applications. <b>The</b> presence <b>of</b> a schema allows for automation and optimization of search, integration, and processing of XML data. Further, <b>the</b> existence <b>of</b> schemas is imperative when integrating (meta) data through schema matching and in <b>the</b> area <b>of</b> generic model management. A final advantage of a schema is that it assigns meaning to the data. That is, it provides a user with a concrete semantics <b>of</b> <b>the</b> <b>document</b> and aids in <b>the</b> specification <b>of</b> meaningful queries over XML data. Although the examples mentioned here just scrape <b>the</b> surface <b>of</b> current applications, they already underscore <b>the</b> importance <b>of</b> schemas accompanying XML data. References are provided in the original paper. 2 Problem setting Given a collection of XML documents, a schema should be inferred without user intervention an...|$|R
40|$|Motivation: Biomedical {{literature}} is <b>the</b> principal repository <b>of</b> bio-medical knowledge, with PubMed {{being the most}} complete data-base collecting, organising, and analysing such textual knowledge. There are numerous efforts that attempt to exploit this information by using text mining and machine learning techniques. We developed a novel approach, called PuReD-MCL (Pubmed Related Documents-MCL), {{which is based on}} the graph clustering algorithm MCL and relevant resources from PubMed. Methods: PuReD-MCL avoids using natural language processing (NLP) techniques directly; instead, it takes advantage of existing resources, available from PubMed. PuReD-MCL then clusters <b>documents</b> efficiently using <b>the</b> MCL graph clustering algorithm, which is based on graph flow simulation. This process allows users to analyse the results by highlighting important clues, and finally to visualise the clusters and all relevant information using an interac-tive graph layout algorithm, for instance BioLayout Express 3 D. Results: The methodology was applied to two different datasets, previously used for <b>the</b> <b>validation</b> <b>of</b> <b>the</b> <b>document</b> clustering tool TextQuest. The first dataset involves the organisms E. coli and yeast, whereas the second is related to Drosophila development. PuReD-MCL successfully reproduces the annotated results obtained from TextQuest, {{while at the same time}} provides additional insights into the clusters and <b>the</b> corresponding <b>documents.</b> Availability: Source code in perl and R are available fro...|$|R
40|$|Abstract Background The need {{to build}} a tool to {{facilitate}} the quick creation and editing of models encoded in the Systems Biology Markup language (SBML) has been growing with <b>the</b> number <b>of</b> users and <b>the</b> increased complexity <b>of</b> <b>the</b> language. SBMLeditor tries to answer this need by providing a very simple, low level editor of SBML files. Users can create and remove all the necessary bits and pieces of SBML in a controlled way, that maintains <b>the</b> validity <b>of</b> <b>the</b> final SBML file. Results SBMLeditor is written in JAVA using JCompneur, a library providing interfaces to easily display an XML document as a tree. This decreases dramatically the development time for a new XML editor. The possibility to include custom dialogs for different tags allows a lot <b>of</b> freedom for <b>the</b> editing and <b>validation</b> <b>of</b> <b>the</b> <b>document.</b> In addition to Xerces, SBMLeditor uses libSBML to check the validity and consistency of SBML files. A graphical equation editor allows an easy manipulation of MathML. SBMLeditor {{can be used as a}} module <b>of</b> <b>the</b> Systems Biology Workbench. Conclusion SBMLeditor contains many improvements compared to a generic XML editor, and allow users to create an SBML model quickly and without syntactic errors. </p...|$|R
40|$|<b>The</b> {{financing}} <b>of</b> <b>the</b> {{health system}} is a crucial factor that determines <b>the</b> state <b>of</b> health and well-being <b>of</b> <b>the</b> population. In Niger, as in many poor countries, spending is still too weak to ensure equitable access to services and essential health interventions. The reduction or absence of public resources subsidy of health services and health care pricing have created a dysfunctional public health care institutions with {{a negative impact on}} <b>the</b> ability <b>of</b> households to cope with <b>the</b> burdens <b>of</b> <b>the</b> disease. To identify the health funding problem, Niger through <b>the</b> Ministry <b>of</b> Health developed this <b>document.</b> <b>The</b> development <b>of</b> <b>the</b> funding strategy was done in three phases. A phase of collecting information in the field; A Development <b>of</b> <b>the</b> first draft phase; And a <b>validation</b> phase <b>of</b> <b>the</b> <b>document</b> in national workshop...|$|R
40|$|We {{show that}} an XML DTD (Document Type Definition) {{can be viewed}} as <b>the</b> fixed point <b>of</b> a {{parametric}} content model. Based on the parametric content model, we develop a model of modular transformation of XML documents. A fold operator is used to capture a class of functions that consume valid XML document trees in a bottomup matter. Similarly, an unfold operator is used to generate valid XML document trees in a top-down fashion. We then show that DTD-aware XML document transformation, which consumes a document of one DTD and generates a document of another DTD, can be thought as both a fold operation and an unfold operation. This leads us to model certain DTD-aware document transformations by mappings from the source content models to the target content models. From these mappings, we derive DTD-aware XML document transformational programs. Benefits of such derived programs include automatic <b>validation</b> <b>of</b> <b>the</b> target <b>documents</b> (no invalid document will be generated) and modular property in <b>the</b> composition <b>of</b> these programs (intermediate results from successive transformations can be eliminated). Categories and Subject Descriptor...|$|R
40|$|It {{is widely}} {{recognized}} that information systems constitute a key {{tool for the}} overall performance improvement of administrative tasks in academic institutions. However at their genesis lies a latent promise of a paper-less environment that stays most <b>of</b> <b>the</b> time unfulfilled due to <b>the</b> lack <b>of</b> appropriate digital document integrity and accountability mechanisms. Academic institutions are thus most <b>of</b> <b>the</b> time still relying on traditional security trust methods based on paper documents for signing and archiving critical documents. While this method delivers an inefficient, inconvenient and costly workflow, {{it is still a}} common method to provide some sort of workable verifiable integrity and accountability that is still considered to be appropriate for the digital data that is being managed by the institutional information systems. Paper based documents have been relying on physical signatures and stamping policies and <b>the</b> physical properties <b>of</b> paper and ink for their integrity and authenticity for a long time. However, <b>the</b> evaluation <b>of</b> a paper document signature or stamp is not a straight forward process. It requires the recipient to have a notarized copy <b>of</b> <b>the</b> signer's signature or stamp for comparison and requires handwritten signature evaluation training that is often beyond <b>the</b> scope <b>of</b> many office employee training. This can lead to situations where <b>the</b> level <b>of</b> credibility and integrity of paper based document is not adequate and makes the verification process entirely dependent on the administrative staff capacity of recognizing hand written signatures and puts too much trust on physical stamps, some of which are non-locally issued and thus very difficult to authenticate. In critical contexts this clearly is not enough to provide appropriate levels of non-repudiation and integrity for critical documents issued by institutions. Digitally signed structured XML documents provide an interesting solution to this problem. Not only can <b>the</b> <b>validation</b> <b>of</b> <b>the</b> <b>document</b> be fully automatized and its integrity verifiable in real time by the information system, {{but it can also be}} implemented in such way that the information contained in such structured documents can be safely and more easily integrated into different information systems without human intervention, thus allowing for substantial cost reduction and leading to faster process work-flows with increased security and data quality. In this paper we propose a PDF based document framework where any signed XML (PDF) <b>document,</b> produced by <b>the</b> institution can be at a later stage directly dematerialized and integrated into any compliant information system in a secure way while maintaining the information integrity and the ability to be self-verifiable. This framework involves <b>the</b> embedding <b>of</b> an encapsulated XAdES signed XML <b>document</b> with <b>the</b> information used on its production as an attachment to a PDF document with an institutional rendering visualization <b>of</b> <b>the</b> signed XML data. <b>The</b> attached XML <b>document</b> and <b>the</b> PDF are both time stamped by an external entity and signed by employees and the issuing institution...|$|R
30|$|As for the resources’ {{discovery}} and selection operation, {{it involves the}} following steps: 1) at the VIP’s side - loading and <b>validation</b> <b>of</b> <b>the</b> XML <b>documents</b> containing <b>the</b> description <b>of</b> <b>the</b> resources requested, as well as creation and sending of a GET REST request with the loaded resource discovery <b>document</b> to <b>the</b> broker; and 2) at the Broker’s side - extraction <b>of</b> <b>the</b> resource discovery <b>document</b> from <b>the</b> received discovery request and its processing/validation, execution <b>of</b> <b>the</b> resource selection algorithm to select the resources that comply with the request, and sending <b>of</b> <b>the</b> list <b>of</b> matched resources to the VIP Node. As shown in Figure  16, the resources’ {{discovery and}} selection operation shows a polynomial (quadratic) growth pattern {{in terms of both}} response time and generated network load. The quadratic trend line for the response time gives insights about <b>the</b> performance <b>of</b> <b>the</b> broker’s selection algorithm. As shown in the figure, the response time shows a faster increase as <b>the</b> number <b>of</b> resources processed during selection increases – with values ranging from 181 ms for 2 discovered resources/ 50 processed resources during selection, to 17 seconds for a 1000 discovered resources/ 5000 processed resources during selection. As for the network load’s quadratic trend line, it is associated with <b>the</b> number <b>of</b> resources discovered, with values ranging from 23 KB (for 2 discovered resources) to 1.37 MB (for a 1000 discovered resources). As <b>the</b> number <b>of</b> discovered resources increases, so does <b>the</b> size <b>of</b> <b>the</b> XML payload carried by the response message sent back by the broker.|$|R
40|$|<b>The</b> purpose <b>of</b> <b>the</b> “Thermal Transport Evaluations Related to Waste Package Design” Task # 19 of Cooperative Agreement Number DE-FC 28 - 98 NV 12081 was {{to develop}} a new CFDHT model for heat {{transfer}} and fluid flow in the potential rsepository at the Yucca Mountain, Nevada and to study <b>the</b> effects <b>of</b> forced convection during the pre-closure period and natural convection during the postclosure period. The analysis was performed for the drift dimensions shown in Figure 4 - 1 below. <b>The</b> intended use <b>of</b> <b>the</b> model is to estimate the velocity and temperature distribution as well as the highest temperature in the drift during the pre-closure and post-closure periods. <b>The</b> <b>validation</b> <b>of</b> <b>the</b> model is <b>documented</b> in section 6 <b>of</b> this report. <b>The</b> analysis was performed using both STAR-CD v. 3. 150 and CFDHT v. 1. 0, which are qualified software. The final result is the maximum temperature value in the drift during the pre-closure and post-closure period and the velocity and temperature distribution around the canisters...|$|R
40|$|<b>The</b> purpose <b>of</b> this Model Report (REV 02) is to <b>document</b> <b>the</b> {{unsaturated}} zone (UZ) models {{used to evaluate}} <b>the</b> potential effects <b>of</b> coupled thermal-hydrological-chemical (THC) processes on UZ flow and transport. This Model Report has been developed {{in accordance with the}} ''Technical Work Plan for: Performance Assessment Unsaturated Zone'' (Bechtel SAIC Company, LLC (BSC) 2002 [160819]). The technical work plan (TWP) describes planning information pertaining to the technical scope, content, and management of this Model Report in Section 1. 12, Work Package AUZM 08, ''Coupled Effects on Flow and Seepage''. <b>The</b> plan for <b>validation</b> <b>of</b> <b>the</b> models <b>documented</b> in this Model Report is given in Attachment I, Model Validation Plans, Section I- 3 - 4, <b>of</b> <b>the</b> TWP. Except for variations in acceptance criteria (Section 4. 2), there were no deviations from this TWP. This report was developed in accordance with AP-SIII. 10 Q, ''Models''. This Model Report <b>documents</b> <b>the</b> THC Seepage Model and the Drift Scale Test (DST) THC Model. The THC Seepage Model is a drift-scale process model for predicting <b>the</b> composition <b>of</b> gas and water that could enter waste emplacement drifts and <b>the</b> effects <b>of</b> mineral alteration on flow in rocks surrounding drifts. The DST THC model is a drift-scale process model relying on the same conceptual model and much <b>of</b> <b>the</b> same input data (i. e., physical, hydrological, thermodynamic, and kinetic) as the THC Seepage Model. The DST THC Model is the primary method for validating the THC Seepage Model. The DST THC Model compares predicted water and gas compositions, as well as mineral alteration patterns, with observed data from the DST. These models provide the framework to evaluate THC coupled processes at the drift scale, predict flow and transport behavior for specified thermal-loading conditions, and predict <b>the</b> evolution <b>of</b> mineral alteration and fluid chemistry around potential waste emplacement drifts. The DST THC Model is used solely for <b>the</b> <b>validation</b> <b>of</b> <b>the</b> THC Seepage Model and is not used for calibration to measured data...|$|R
40|$|In an ever-developing society, a strong, viable {{economy is}} vital for any country that seeks to survive on the global market and to provide {{upwardly}} decent living standards for its citizens. Recognizing the above mentioned points as mandatory, but also prompted in its actions by <b>the</b> European community <b>of</b> which it recently became a member, Romania is taking steps to develop its electronic commerce to meet 21 st century global standards. Some <b>of</b> <b>the</b> more important legal measures that have aided <b>the</b> development <b>of</b> e-commerce in Romania include <b>the</b> liberalization <b>of</b> telecommunications, <b>the</b> <b>validation</b> <b>of</b> electronic <b>documents,</b> <b>the</b> creation <b>of</b> customer protection services and regulations and <b>the</b> facilitation <b>of</b> electronic fund transfer through debit/credit cards. The obstacles encountered in <b>the</b> implementation <b>of</b> e-commercial transactions are manifold. <b>The</b> small number <b>of</b> users that can access the Internet from home and the people’s mistrust and lack of familiarity with e commercial transactions {{are only a few}} <b>of</b> <b>the</b> hindrances setting back their development in Romania. Emanating from the present study are numerous solutions for the improvement and popularization of Romanian e-commerce which would raise awareness about <b>the</b> advantages <b>of</b> electronic commerce on the Romanian business scene. ...|$|R
40|$|The human {{muscular system}} {{represents}} nearly 75 % <b>of</b> <b>the</b> body mass and encompasses two major muscle forms- striated and smooth. Striated muscle, composed broadly of myofibers, accompanying membrane systems, cytoskeletal networks {{together with the}} metabolic and regulatory machinery, have revealed complexities in composition, structure and function. A disruption to any component within this complex system of interactions lead to disorders <b>of</b> <b>the</b> muscle, typically characterized by muscle fiber loss, reduced motor output {{and in some cases}} death. Advent of high-throughput technologies coupled with elegant approaches to deciphering data using bioinformatics and systems biology, are providing new venues for detailed exploration of mammalian muscle. This dissertation describes <b>the</b> use <b>of</b> publicly available high-throughput data, in conjunction with co-expression network methodologies developed for a comprehensive, interpretable systems-level perspective on mechanisms underlying associated muscle pathologies. This study begins with <b>the</b> exploration <b>of</b> <b>the</b> temporal transcriptional response of skeletal muscle to Botulinum Neurotoxin-A (Botox ®) over a 1 -year period, in <b>the</b> framework <b>of</b> muscle physiology. Next, utilizing co-expression network analysis, putative markers associated with recovery of muscle trophicity are identified, furthermore providing an unbiased <b>validation</b> <b>of</b> <b>the</b> response <b>documented</b> earlier. These studies represent the first attempt at categorically assessing the whole-transcriptomic changes associated with BoNT-A treatment in muscle. <b>The</b> latter half <b>of</b> this research focuses on discerning patho-mechanisms of human diseases affecting muscle. Particularly, co-expression network statistics are leveraged to identify dysregulated pathways and biomarkers of disease progression, underlying duchenne muscular dystrophy. Next, a quantitative framework integrating transcriptional, protein interaction, and drug-target data is developed to extract functional similarities and mechanisms amongst 20 diseases affecting the muscle. Lastly, an approach to differential co-expression analysis using signed and weighted co-expression networks is described. This approach is subsequently utilized to assess and identify differential mechanisms underlying ischemic and idiopathic dilated cardiomyopathy. The analysis and results from the aforementioned studies have enabled a deeper understanding <b>of</b> <b>the</b> complex interactions underlying muscle pathologies; providing opportunities for drug development and personalized medicine...|$|R
40|$|In {{this thesis}} the {{analysis}} on double charmonium production at <b>the</b> energy <b>of</b> <b>the</b> Υ(4 S), with <b>the</b> data is <b>documented.</b> <b>The</b> aim <b>of</b> {{this analysis is}} to understand <b>the</b> mechanism <b>of</b> production of double charmonium states fro annihilation, in particular after the discrepancies which at <b>the</b> beginning <b>of</b> these studies appeared. With successive studies, these discrepancies have been almost solved. This analysis was already performed by collaboration shuwei, and in this thesis we want to update that work, with a luminosity early four times higher (468 fb^ 1. In the mean time, also Belle collaboration published on this analysis bellecccc, obtaining results compatible with and finding out a new charmonium state, named X(3940). We aim here also to confirm this state. This thesis is composed by five chapters. Chapter I is an introduction to charmonium spectroscopy, with a description <b>of</b> <b>the</b> NRQCD, which <b>the</b> theorethical framework <b>of</b> this analysi, then the potential models {{that have been developed}} to describe the mass spectrum. The theories related to the double charmonium production mechanisms are presented in Chapter II: in particular <b>the</b> calculation <b>of</b> <b>the</b> cross section and the dicrepancies between theory and experiment. Then in Chapter III the detector is described. In Chapter IV there is a description on how has been performed the analysis: the analysis strategy, selection and cut optimization, and <b>validation</b> <b>of</b> <b>the</b> fit are <b>documented,</b> before <b>the</b> unblind <b>of</b> <b>the</b> interested region. In chapter V we give the final results, after unblind...|$|R
40|$|AbstractThe {{transport}} {{process is}} a summary of time and substantive processes {{that make up the}} transport chain. <b>The</b> requirement <b>of</b> relocation exists at <b>the</b> beginning <b>of</b> each transport process in passenger transport. Finding the appropriate connection, familiarity with the transport conditions, <b>the</b> purchase <b>of</b> travel <b>documents</b> and their subsequent validation are other activities involving the transport process. One form <b>of</b> <b>validation</b> can be ensured by <b>the</b> responsible employees <b>of</b> <b>the</b> carrier. The second form is a creation of self-service system on the track section. Economic matters of carrier points to disadvantages <b>of</b> <b>validation</b> <b>the</b> travel <b>documents</b> by train crew in comparison to the tracks where is implemented the self-service system. Paper analyzes <b>the</b> possibilities <b>of</b> <b>validation</b> machine placement for <b>the</b> <b>validation</b> <b>of</b> travel <b>documents,</b> which can be placed at railway stations and stops or in vehicles in low demand areas <b>of</b> railway transport. <b>The</b> paper analyzes the factors that affect <b>the</b> required quantity <b>of</b> <b>validation</b> machines at <b>the</b> railway stations and stops or in railway vehicles, depending on <b>the</b> parameters <b>of</b> railway infrastructure (number of tracks, maximum track speed, number of railway stations and stops on the track and their average mutual distance), railway vehicle fleet (construction <b>of</b> <b>the</b> vehicles, number <b>of</b> doors, transport capacity) and transport technology (time of train sets circulation, transport distance, travel speed, creating timetable). Based on <b>the</b> analysis <b>of</b> these factors paper provides the methodology for calculating <b>the</b> quantity <b>of</b> <b>validation</b> machines in railway transport. The methodology is applied to the model...|$|R
40|$|Small {{reservoirs}} represent one <b>of</b> <b>the</b> {{most important}} sources {{of water for}} livelihoods in the Mzingwane catchment, which constitutes <b>the</b> entirety <b>of</b> <b>the</b> Limpopo basin in Zimbabwe, because of semi-arid conditions that prevail in the area. Despite the water reforms that, among other things, were meant to improve water-based livelihoods, {{little attention has been}} paid to systematic management of small reservoirs as illustrated by disparate and in some cases nonexisting information. Recent developments in Information and Communication Technologies (ICTs) provide opportunities for identification and characterization of small reservoirs, which is critical to effective local water resource management. This is because non-ICT methods are worthwhile, but they are time and cost ineffective. The study therefore sought to develop and test an ICT-based tool for effective management <b>of</b> small reservoirs. <b>The</b> study built upon previous work that demonstrated <b>the</b> possibility <b>of</b> identifying small reservoirs and estimating their capacities using satellite images and GIS. <b>The</b> objective <b>of</b> this study was to integrate the output information â€“ small reservoir location, capacity, river or subcatchment, reliability, and status â€“ in decision making processes by using Gwanda district located in southwest Zimbabwe as a case study. The study focused on identifying and characterising small reservoirs, and building up a database of little physical requirements. Identification was done through GIS processing of Landsat TM 4 - 5 images of February-March and April-May 2009. Finding recent images <b>of</b> <b>the</b> period around September for the dry season period was a major limitation. Fields surveys were done for ground truthing and for further identification and characterisation (name, place, capacity, turbidity and chlorophyll-a <b>validation)</b> <b>of</b> <b>the</b> reservoirs. <b>Documents</b> reviews were carried out to explore <b>the</b> contents <b>of</b> <b>the</b> existing databases. A total of 256 small reservoirs were identified and their status characterised in the district. <b>The</b> distribution <b>of</b> <b>the</b> capacities among the three subcatchments (Shashe, Lower and Upper Mzingwane) was found to be proportional to their surface area coverage over the district. Capacities were found to vary widely from around 4, 000 m 3 to over 650, 000 m 3; with <b>the</b> majority <b>of</b> them being around 30, 000 m 3. The total capacity in the district was estimated to 17 million m 3 from which ward 23, one <b>of</b> <b>the</b> 24 rural wards in the district, accounted for more than 40...|$|R
40|$|<b>The</b> purpose <b>of</b> {{this report}} (REV 04) is to <b>document</b> <b>the</b> thermal-hydrologic-chemical (THC) seepage model, which simulates <b>the</b> {{composition}} <b>of</b> waters {{that could potentially}} seep into emplacement drifts, and <b>the</b> composition <b>of</b> <b>the</b> gas phase. The THC seepage model is processed and abstracted {{for use in the}} total system performance assessment (TSPA) for the license application (LA). This report has been developed in accordance with ''Technical Work Plan for: Near-Field Environment and Transport: Coupled Processes (Mountain-Scale TH/THC/THM, Drift-Scale THC Seepage, and Post-Processing Analysis for THC Seepage) Report Integration'' (BSC 2005 [DIRS 172761]). The technical work plan (TWP) describes planning information pertaining to the technical scope, content, and management <b>of</b> this report. <b>The</b> plan for <b>validation</b> <b>of</b> <b>the</b> models <b>documented</b> in this report is given in Section 2. 2. 2, ''Model Validation for the DS THC Seepage Model,'' <b>of</b> <b>the</b> TWP. The TWP (Section 3. 2. 2) identifies Acceptance Criteria 1 to 4 for ''Quantity and Chemistry of Water Contacting Engineered Barriers and Waste Forms'' (NRC 2003 [DIRS 163274]) as being applicable to this report; however, in variance to the TWP, Acceptance Criterion 5 has also been determined to be applicable, and is addressed, along with the other Acceptance Criteria, in Section 4. 2 of this report. Also, three FEPS not listed in the TWP (2. 2. 10. 01. 0 A, 2. 2. 10. 06. 0 A, and 2. 2. 11. 02. 0 A) are partially addressed in this report, and have been added to <b>the</b> list <b>of</b> excluded FEPS in Table 6. 1 - 2. This report has been developed in accordance with LP-SIII. 10 Q-BSC, ''Models''. This report <b>documents</b> <b>the</b> THC seepage model and a derivative used for validation, the Drift Scale Test (DST) THC submodel. The THC seepage model is a drift-scale process model for predicting <b>the</b> composition <b>of</b> gas and water that could enter waste emplacement drifts and <b>the</b> effects <b>of</b> mineral alteration on flow in rocks surrounding drifts. The DST THC submodel uses a drift-scale process model relying on the same conceptual model and many <b>of</b> <b>the</b> same input data (i. e., physical, hydrologic, thermodynamic, and kinetic) as the THC seepage model. The DST THC submodel is the primary means for validating the THC seepage model. The DST THC submodel compares predicted water and gas compositions, and mineral alteration patterns, with observed data from the DST. These models provide the framework to evaluate THC coupled processes at the drift scale, predict flow and transport behavior for specified thermal-loading conditions, and predict <b>the</b> evolution <b>of</b> mineral alteration and fluid chemistry around potential waste emplacement drifts. The DST THC submodel is used solely for <b>the</b> <b>validation</b> <b>of</b> <b>the</b> THC seepage model and is not used for calibration to measured data...|$|R
5000|$|<b>The</b> Test Report <b>documents</b> <b>the</b> <b>validation</b> <b>of</b> <b>the</b> repair process.|$|R
40|$|Different {{models have}} been {{recently}} proposed for representing temporal data, tracking historical information, and recovering <b>the</b> state <b>of</b> <b>the</b> <b>document</b> as <b>of</b> any given time, in XML documents. After presenting an abstract model for temporal XML, we discuss <b>the</b> problem <b>of</b> <b>the</b> <b>validation</b> <b>of</b> <b>the</b> temporal constraints imposed by this model. We first review <b>the</b> problem <b>of</b> checking and fixing isolated temporal inconsistencies. Then, {{we move on to}} study <b>validation</b> <b>of</b> a document when many temporal inconsistencies of different kinds are present. We study the conditions that allow to treat each inconsistency isolated from the rest, and give the corresponding proofs. These properties are intended to be <b>the</b> basis <b>of</b> efficient algorithms for checking temporal consistency in XML...|$|R
30|$|<b>The</b> {{calibration}} and <b>validation</b> <b>of</b> <b>the</b> overall optimization methodology {{consists in}} <b>the</b> calibration and <b>validation</b> <b>of</b> <b>the</b> cost function parameters and <b>of</b> <b>the</b> optimization methods (SA and parallel optimization).|$|R
40|$|ABSTRACT Objective: {{present the}} {{cross-cultural}} adaptation and content and semantic <b>validation</b> <b>of</b> <b>the</b> Difficult Intravenous Access Score for current use in Brazil. Method: cross-cultural adaptation and validation study, structured in six phases: initial translation, synthesis of translations, back-translation, assessment of documents by expert committee of specialized judges, pretest and presentation <b>of</b> <b>the</b> <b>documents</b> to <b>the</b> expert judges and to <b>the</b> author <b>of</b> <b>the</b> original instrument. Twenty health professionals were randomly recruited {{from a public}} hospital in <b>the</b> South <b>of</b> Brazil, working in pediatrics, {{in order to assess}} the agreement level with the variables in the instrument. In addition, a convenience sample of 30 pediatric patients was selected for <b>the</b> face <b>validation</b> <b>of</b> <b>the</b> same instrument. Cronbach’s alpha coefficient, simple and percentage frequencies, the Shapiro-Wilk and Fisher’s exact tests were used for the data analysis and reliability measures. Results: the cross-cultural adaptation phases were executed with totally clear translated variables, demonstrating satisfactory results in the content and semantic validation process. Conclusions: the Difficult Intravenous Access Score was adapted and its content and semantics were validated. External clinical validity, measuring equivalence and reproducibility analyses are needed...|$|R
40|$|This <b>document</b> {{presents}} <b>the</b> following {{reports on}} 5 in-beam tests {{to validate the}} proposed design concepts and simulation tools for experimental programmes at the EURISOL facility: In-beam <b>Validation</b> <b>of</b> <b>the</b> Recoil Separator Design Concept for EURISOL In-beam <b>Validation</b> <b>of</b> <b>the</b> FAZIA Design Concept for EURISOL Direct Reactions at EURISOL: In-beam tests to validate <b>the</b> design <b>of</b> an array for light charged-particle and gamma-ray measurements In-beam <b>validation</b> <b>of</b> <b>the</b> Paul trapping <b>of</b> low energy radioactive ions and direct detection <b>of</b> <b>the</b> beta-decay Development and <b>validation</b> <b>of</b> neutron detection simulations for EURISO...|$|R
50|$|Planning <b>the</b> <b>Validation</b> <b>of</b> <b>the</b> Guidelines.|$|R
50|$|<b>Validation</b> <b>of</b> <b>the</b> {{contract}} was ordered.|$|R
5000|$|<b>The</b> use <b>of</b> {{a mobile}} phone SIM as a Digital {{identity}} in Australia provides some level <b>of</b> <b>validation</b> <b>of</b> <b>the</b> digital identity <b>of</b> <b>the</b> holder. <b>Validation</b> <b>of</b> <b>the</b> holder {{can be done}} by sending them an SMS to their phone number. <b>The</b> advantage <b>of</b> this mechanism is: ...|$|R
40|$|Worldwide {{practice}} recommends <b>validation</b> <b>of</b> <b>the</b> HDM {{models with}} some other software {{that can be used}} for comparison <b>of</b> <b>the</b> forecasting results. The program package MATLAB is used in this case, as it enables for modelling <b>of</b> all <b>the</b> HDM models. A statistic <b>validation</b> <b>of</b> <b>the</b> results <b>of</b> <b>the</b> forecasts concerning <b>the</b> condition <b>of</b> <b>the</b> pavements in HDM with the on-field measuring results was also performed. This paper shall present <b>the</b> results <b>of</b> <b>the</b> <b>validation</b> <b>of</b> <b>the</b> coefficients <b>of</b> calibration <b>of</b> <b>the</b> deterioration models in HDM 4 on the Macedonian highways...|$|R
50|$|Some {{information}} on <b>validation</b> <b>of</b> <b>the</b> method is available.|$|R
5000|$|... #Subtitle level 3: <b>Validation</b> <b>of</b> <b>the</b> {{accuracy}} <b>of</b> <b>the</b> algorithm ...|$|R
5000|$|The Exegesis Programme - {{a literal}} <b>validation</b> <b>of</b> <b>the</b> {{programme}} ...|$|R
50|$|As a result, NZI sought <b>validation</b> <b>of</b> <b>the</b> {{transaction}} in question.|$|R
50|$|<b>The</b> grade <b>of</b> 10/20 is {{sufficient}} for <b>validation</b> <b>of</b> <b>the</b> courses.|$|R
5000|$|The monitor {{can report}} {{violation}} or <b>validation</b> <b>of</b> <b>the</b> desired specification.|$|R
5000|$|It should {{encourage}} community contribution, peer review, and <b>validation</b> <b>of</b> <b>the</b> software.|$|R
40|$|This paper {{reports on}} a {{collaborative}} project to pilot <b>the</b> use <b>of</b> formal methods in <b>the</b> development <b>of</b> safety-related somare. Using the SVRC’s Cogito methodology, stafs from CSC Australia undertook: formal specijication; <b>validation</b> <b>of</b> <b>the</b> specijication by mathematical consistency checks; hazard analysis; and <b>validation</b> <b>of</b> <b>the</b> speciJication against the safety requirements. Part <b>of</b> <b>the</b> design was modelled formally and verijied...|$|R
50|$|<b>The</b> {{court ordered}} <b>validation</b> <b>of</b> <b>the</b> mortgage, {{but only to}} <b>the</b> extent <b>of</b> $75,000.|$|R
