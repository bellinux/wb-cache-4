344|1447|Public
50|$|L. Lebart, <b>Validation</b> <b>technique</b> in Text Mining. In : Spiros Sirmakessis (ed.), Text Mining and its Application, Springer Verlag, Berlin - Heidelberg, p 169-178, 2004.|$|E
50|$|Patterson {{is known}} for her work in {{computational}} linguistics and its applications to computer security. In 2005, she presented the first parse tree <b>validation</b> <b>technique</b> for stopping SQL injection attacks at the Black Hat conference in Las Vegas.|$|E
50|$|Peer {{review is}} the primary <b>validation</b> <b>technique</b> {{employed}} by scientific publications. However, a prominent medical journal tested the system and found major failings. It supplied research with induced errors and found that most reviewers failed to spot the mistakes, even after being told of the tests.|$|E
40|$|This paper {{discusses}} <b>validation</b> <b>techniques</b> {{for communication}} protocols and analyzes the practical use of selected <b>validation</b> <b>techniques</b> in an automated manner. A {{case study on}} validating the ATM (Asynchronous Transfer Mode) Signalling Protocol as specified by ITU-T in Q. 2931 is used for this analysis. Different error classes are identified and validated. An assessment of the different <b>validation</b> <b>techniques</b> in terms of effort and quality of results is given. As {{a result of the}} case study, an evaluation of the Q. 2931 SDL specification completes the paper...|$|R
40|$|Modeling TCP {{performance}} {{is an important}} issue that attracted research attention over the past decade. In this paper, we present an overview of models used to capture the TCP behavior. We compare several existing analytical models with respect to modeled attributes, modeling assumptions, and <b>validation</b> <b>techniques.</b> We also identify features that new TCP models should possess. Finally, we address the importance of devising common <b>validation</b> <b>techniques</b> and performance evaluation metrics for TCP models...|$|R
5000|$|Advanced data {{validation}} and reconciliation (DVR) is an integrated approach of combining data reconciliation and data <b>validation</b> <b>techniques,</b> which {{is characterized by}} ...|$|R
50|$|Our {{choice of}} the cost {{function}} {{is based on the}} supposed optimization mechanisms carried on by our CNS. It needs to be clinically validated, especially in unhealthy patients. In 2007 a list of possible cost functions with a brief rationale and the suggested model <b>validation</b> <b>technique</b> is available.|$|E
50|$|By this method, {{using the}} theory and formula of cosmological {{redshift}} as the <b>validation</b> <b>technique</b> in determining distances to high-redshift supernovae, the results of 43 high-redshift (Z = 0.172 - 1.755) type Ia supernovae show that the redshift distances determined by the cosmological redshift formula and the Luminosity distance have a good match within the measurement uncertainties.|$|E
5000|$|MPI {{developed}} the first visual manager search application, Enterprise Prospector, {{which can be}} used to create custom statistics, performance measures, peer groups and investment product rating systems. Four years later, MPI introduced the first Dynamic Style Analysis (DSA) model, an advanced methodology for returns-based analysis of hedge funds. DSA featured a proprietary cross <b>validation</b> <b>technique,</b> Predicted R-Squared. In 2008, MPI released its patent-pending Calibrated Frontiers methodology, an innovative and robust approach to portfolio resampling. Two years later, MPI released the new Stylus Web with “plan level” reporting functionality.|$|E
5000|$|While {{antibodies}} are commercially available, <b>validation</b> <b>techniques</b> may not {{be consistent}} and poorly characterized antibodies may yield nonspecific results with low reproducibility.|$|R
40|$|This paper proposes an {{approach}} for Web requirements validation {{by applying the}} model-driven paradigm in classical requirements <b>validation</b> <b>techniques.</b> In particular, we present how the Navigational Development Techniques (NDT) approach exploits the model-driven paradigm to improve its requirements validation task by exploring tool cases that systematize or even automate the application of requirements <b>validation</b> <b>techniques.</b> Our solution is validated by applying it in a real industrial environment. The results and the learned lessons are presented accordingly...|$|R
40|$|The {{complexity}} of developing today's hybrid powertrain control systems is increasingly being {{dealt with by}} the use of model based design methods. This introduces new opportunities, and challenges, in identifying and using advanced <b>validation</b> <b>techniques</b> to best utilise model based design artifacts. This paper discusses some of the advanced <b>validation</b> <b>techniques</b> used within the premium automotive research and development (PARD) programme and identifies further on-going work within a new project called evolutionary validation of complex systems (EvoCS) ...|$|R
5000|$|Cross-validation, {{sometimes}} called rotation estimation, {{is a model}} <b>validation</b> <b>technique</b> for assessing how {{the results of a}} statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (testing dataset). The goal of cross validation is to define a dataset to [...] "test" [...] the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting , give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem), etc.|$|E
3000|$|For {{covariate}} shift methods, how {{to determine}} appropriate parameters {{is an important}} issue. Cross <b>validation</b> <b>technique</b> is used broadly for the problem. However, cross <b>validation</b> <b>technique</b> needs some labelled test data to be as validation dataset. When the prediction model is used in changed situation where test data are completely not labelled, people cannot apply cross validation. Here, we give an empirical parameter estimation way to modify the training-test k-NN reweighting method. We call it adaptive k-NN reweighting method, which includes how to determine k and how to determine [...] γ.|$|E
40|$|A new turbine model <b>validation</b> <b>technique</b> that {{is based}} on {{adaptation}} of look-up tables is described in this article. Simulation results from the VIDYN turbine simulation program and measurements from Big Glenn wind turbine, located outside Gothenburg, Sweden, are used as an input to this new model <b>validation</b> <b>technique.</b> The models of the flapwise bending moment and power coefficient are validated for Big Glenn turbine. Measurement data are acquired during normal turbine operation. Verification results show good agreement between model outputs and measured data. The method allows prediction {{in a wide range of}} turbine operating variables, using only few measured points...|$|E
30|$|The same <b>validation</b> <b>techniques</b> {{can be used}} {{as shown}} for Scenario 0. The transit times are checked and the {{activities}} are checked for blockages.|$|R
2500|$|Smoothed cross {{validation}} (SCV) is {{a subset of}} a larger class of cross <b>validation</b> <b>techniques.</b> The SCV estimator differs from the plug-in estimator in the second term ...|$|R
50|$|It is {{difficult}} to support certain <b>validation</b> <b>techniques,</b> employed by DTD and XML Schema (e.g., default attributes and elements), that require modifications to the XML instances being parsed.|$|R
40|$|Over the years, various {{diffusion}} based {{cortical surface}} data smoothing techniques [1, 2] {{have been proposed}} but without numerical validation. We present a novel <b>validation</b> <b>technique</b> that uses the analytical solution of a diffusion equation as the ground truth. The proposed framework is used in validating and comparing th...|$|E
40|$|Papers are {{presented}} {{on the following}} topics: measurement of software technology, recent studies of the Software Engineering Lab, software management tools, expert systems, error seeding as a program <b>validation</b> <b>technique,</b> software quality assurance, software engineering environments (including knowledge-based environments), the Distributed Computing Design System, and various Ada experiments...|$|E
40|$|General {{validation}} {{principles for}} quantitative structure-activity re-lationship (qSAR) {{models in the}} context of chemical regulation were developed [1] due to the importance and implication in of these methods in drug design. A brief analysis of different techniques used in valida-tion of multiple linear regression models [2] is reviewed. The hierarchi-cal steps in models validation are highlighted and a <b>validation</b> <b>technique</b> is proposed. The following statistical approaches are considered: cor-relation analysis (Pearson, Spearman, Kendall and Gamma coefficients as parameters and associated significance levels [3]), regression analy-sis (leave-one-out cross-validation and determination coefficients), and other inferential statistics (cross correlation coefficients, training vs. test experiment, correlated correlations analysis). The proposed sta-tistical <b>validation</b> <b>technique</b> is exemplified on a qSAR model obtained by applying the molecular descriptors family on the structure-activity relationship approach [4]. Acknowledgments: UEFISCSU Romani...|$|E
40|$|Most of the {{existing}} model verification and <b>validation</b> <b>techniques</b> are largely used in the industrial and system engineering fields. The agent-based modeling approach is different from traditional equation-based modeling approach in many aspects. As the agent-based modeling approach has recently become an attractive and efficient way for modeling large-scale complex systems, there are few formalized validation methodologies existing for model validation. In our proposed work, we design, develop, adapt, and apply various verification and <b>validation</b> <b>techniques</b> to an agent-based scientific model and investigate the sufficiency of these <b>techniques</b> for the <b>validation</b> of agent-based models. ...|$|R
50|$|Stat-Ease {{was used}} by Los Alamos National Laboratory {{researchers}} in designing a set of experiments designed to demonstrate the application of model <b>validation</b> <b>techniques</b> to a structural dynamics problem.|$|R
50|$|L. Lebart, Chapter 7 : <b>Validation</b> <b>Techniques</b> in Multiple Correspondence Analysis. In : M. Greenacre et J. Blasius, eds., Multiple Correspondence Analysis {{and related}} techniques, Chapman and Hall/CRC, p 179-196, 2006.|$|R
30|$|Conclusion: This {{independent}} <b>validation</b> <b>technique</b> proves Poisson resampling software correctly generates expected noise characteristics even down to 1 count/pixel. This is reassuring {{given the}} increasing applications of such software, and {{indicates that the}} Poisson software can reliably be used in studies {{to examine the effects}} of reduced time or activity.|$|E
40|$|Rangarajan J. R., Yin T., Gilbert J., Atre A., Ribeiro R., Eriksson A., Dresselaers T., Maes F., Ahlgren U., Himmelreich U., ''Optical Projection Tomography as a {{quantitative}} 3 D <b>validation</b> <b>technique</b> for MRI of labeled pancreatic islets'', European molecular imaging meeting - EMIM 2014, June 4 - 6, 2014, Antwerp, Belgium. status: publishe...|$|E
40|$|To {{provide a}} {{computing}} {{system to be}} dependable fault tolerance mechanisms have to be included. Especially massive parallelism represents a new challenge for fault tolerance. In this paper we discuss basic hardware fault tolerance measures for massively parallel multiprocessors and solutions realized for and integrated into different multiprocessor architectures. Further we present our <b>validation</b> <b>technique</b> for dependability based on simulation-based fault injection...|$|E
40|$|Abstract: This paper applies {{different}} methods of map comparison {{to quantify the}} characteristics of three different land change models. The land change models used for simulation are termed as ―Stochastic Markov (St_Markov) ‖, ―Cellular Automata Markov (CA_Markov) ‖ and ―Multi Layer Perceptron Markov (MLP_Markov) ‖ models. Various model <b>validation</b> <b>techniques</b> such as per category method, kappa statistics, components of agreement and disagreement, three map comparison and fuzzy methods have then been applied. A comparative analysis of the <b>validation</b> <b>techniques</b> has also been discussed. In all cases, {{it is found that}} ―MLP_Markov ‖ gives the best results among the three modeling techniques. Fuzzy set theory is the method that seems best able to distinguish areas of minor spatial errors from major spatial errors. Based on the outcome of this paper, it is recommended that scientists should try to use the Kappa, three map comparison and fuzzy methods for model validation. This paper facilitates communication among land change modelers, because it illustrates the range of results for a variety of model <b>validation</b> <b>techniques</b> and articulates priorities for future research...|$|R
40|$|As {{modeling}} techniques {{become increasingly}} popular and effective means for simulating real-world phenomena, it becomes increasingly important to enhance or verify our confidence in them. Verification and <b>validation</b> <b>techniques</b> are neither as widely used nor as formalized {{as one would}} expect when applied to simulation models. In this paper, we present our methods and results through two very different case studies: a scientific model and an economic model. We show {{that we were able to}} successively verify the simulations and, in turn, identify general guidelines on the best approach to a new simulation experiment. We also draw conclusions on effective verification and <b>validation</b> <b>techniques</b> and note their applicability...|$|R
40|$|Abstract: We describe, {{with respect}} to {{high-level}} survivability requirements, the validation of a survivable publish-subscribe system that is under development. We use a top-down approach that methodically breaks the task of validation into manageable tasks, and for each task, applies techniques best suited to its accomplishment. These efforts can be largely independent and {{use a variety of}} <b>validation</b> <b>techniques,</b> and the results, which complement and supplement each other, are seamlessly integrated to provide a convincing assurance argument. We also demonstrate the use of model-based <b>validation</b> <b>techniques,</b> {{as a part of the}} overall validation procedure, to guide the system’s design by exploring different configurations and evaluating trade-offs...|$|R
40|$|ABSTRACT: The DNA {{microarray}} is {{an important}} technique that allows researchers to analyze many gene expression data in parallel. Although the data can be more significant if {{they come out of}} separate experiments, one of the most challenging phases in the microarray context is the integration of separate expression level datasets that have gathered through different techniques. In this paper, we present a general novel method for the integration of any collected data whose distributions have been linearly transformed. The new method is based on the information theory concepts. More than that, this article presents a new approach for checking of the linearity between two distributions as a <b>validation</b> <b>technique.</b> The <b>validation</b> <b>technique</b> assists in taking the feature reduction process in effect prior to the integration phase. The time complexity of the proposed algorithm is low and the new presented methods show good functionality. The experimental results are presented {{at the end of the}} paper...|$|E
40|$|To {{understand}} complex biological systems, {{the research}} community has produced huge corpus of gene expression data. A {{large number of}} clustering approaches have been proposed {{for the analysis of}} gene expression data. However, extracting important biological knowledge is still harder. To address this task, clustering techniques are used. In this paper, hybrid Hierarchical k-Means algorithm is used for clustering and biclustering gene expression data is used. To discover both local and global clustering structure biclustering and clustering algorithms are utilized. A <b>validation</b> <b>technique,</b> Figure of Merit is used to determine the quality of clustering results. Appropriate knowledge is mined from the clusters by embedding a BLAST similarity search program into the clustering and biclustering process. To discover both local and global clustering structure biclustering and clustering algorithms are utilized. To determine the quality of clustering results, a <b>validation</b> <b>technique,</b> Figure of Merit is used. Appropriate knowledge is mined from the clusters by embedding a BLAST similarity search program into the clustering and biclustering process...|$|E
40|$|AbstractQSAR {{studies on}} a series of inhibitors for {{anti-apoptotic}} Bcl- 2 proteins based on BHI- 1 were performed the activities are found to correlate well with some topological parameters and steric parameters. The results are critically discussed on the basis of regression data, Pogliani factor Q and cross <b>validation</b> <b>technique.</b> The results are found to be useful in discussing the mechanism of drug–receptor interaction...|$|E
50|$|Assuring {{an email}} address is of a good quality {{requires}} a combination of various <b>validation</b> <b>techniques.</b> Large websites, bulk mailers and spammers require fast algorithms that predict validity of email address. Such methods depend heavily on heuristic algorithms and statistical models.|$|R
40|$|This {{paper is}} closed access until 31 st August 2018. This paper {{implements}} and compares {{the performance of}} a number of techniques proposed for improving the accuracy of Automatic Speech Recognition (ASR) systems. As ASR that uses only speech can be contaminated by environmental noise, in some applications it may improve performance to employ Audio-Visual Speech Recognition (AVSR), in which recognition uses both audio information and mouth movements obtained from a video recording of the speaker’s face region. In this paper, model <b>validation</b> <b>techniques,</b> namely the holdout method, leave-one-out cross validation and bootstrap validation, are implemented to validate {{the performance of a}}n AVSR system as well as to provide a comparison of the performance of the <b>validation</b> <b>techniques</b> themselves. A new speech data corpus is used, namely the Loughborough University Audio-Visual (LUNA-V) dataset that contains 10 speakers with five sets of samples uttered by each speaker. The database is divided into training and testing sets and processed in manners suitable for the <b>validation</b> <b>techniques</b> under investigation. The performance is evaluated using a range of different signal-to-noise ratio values using a variety of noise types obtained from the NOISEX- 92 dataset...|$|R
40|$|Requirements {{validation}} is {{a critical}} phase of requirements engineering processes, which makes sure that requirements are correct, consistent, complete and accurate. Requirements validation is used in determining the right requirements, while verification determines that implementation is correct with respect to its requirements. The main objective of validation is to certify that requirement specification document is the acceptable description of the system, {{which is going to}} be implemented. Requirements <b>validation</b> <b>techniques</b> (RVTs) play pivotal role to detect possible defects in the requirements. RVTs can help in the completion of projects, within given schedule, budget and according to the desired functionality. The studies of six companies regarding requirements validation, is presented in this thesis. This study explores the requirements <b>validation</b> <b>techniques</b> that are presented in academia and practiced in industry as well. Interview studies are conducted in two countries, which is an attempt to find the usage of requirements <b>validation</b> <b>techniques</b> in both of the countries. The pros and cons of identified RVTs are discussed, along with it; the comparison of different RVTs with respect to the satisfaction level of specific RVT in terms of catching defects, time/schedule and cost is presented as well...|$|R
