2469|3119|Public
500|$|Only one {{repetitive}} dive {{is allowed}} {{as there is}} no <b>validation</b> <b>data</b> for multiple repetitive dives ...|$|E
5000|$|Corresponding {{electronic}} signature <b>validation</b> <b>data</b> and {{electronic signature}} creation data ...|$|E
5000|$|Only one {{repetitive}} dive {{is allowed}} {{as there is}} no <b>validation</b> <b>data</b> for multiple repetitive dives ...|$|E
40|$|Abstract. There {{are many}} tools {{suitable}} to model systems and to generate software code from system models, but these tools {{do not support}} <b>data</b> <b>validation.</b> Available <b>data</b> <b>validation</b> tools are domain specific and require manual definition of <b>data</b> <b>validation</b> rules. Thus, the lack of the tool supporting both system modelling and automated generation of <b>data</b> <b>validation</b> rules from system models is obvious. The paper discusses the use of system models represented by UML in automation of <b>data</b> <b>validation.</b> The method to derive rules from UML models is presented. A fragment of UML model and derivation of rules represented in the model is demonstrated. The tool supporting proposed method is introduced {{and the process of}} automated <b>data</b> <b>validation</b> rules generation from UML models is presented. 1...|$|R
5000|$|The OSCAR Near-realtime global {{ocean surface}} {{currents}} website describes {{the project and}} links to <b>data</b> <b>validation</b> and <b>data</b> downloads.|$|R
50|$|Failures or {{omissions}} in <b>data</b> <b>validation</b> {{can lead}} to data corruption or a security vulnerability. <b>Data</b> <b>validation</b> checks that <b>data</b> are valid, sensible, reasonable, and secure before they are processed.|$|R
50|$|Flowmaster {{software}} {{itself was}} based on extensive experimental <b>validation</b> <b>data</b> from D. S. Miller’s internationally respected scientific textbook, ‘Internal Flow Systems’, first published in 1978.|$|E
50|$|In k-fold cross-validation, the {{original}} sample is randomly partitioned into k equal sized subsamples.Of the k subsamples, a single subsample is retained as the <b>validation</b> <b>data</b> for testing the model, {{and the remaining}} k − 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), {{with each of the}} k subsamples used exactly once as the <b>validation</b> <b>data.</b> The k results from the folds can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used, but in general k remains an unfixed parameter.|$|E
50|$|This method, {{also known}} as Monte Carlo cross-validation, {{randomly}} splits the dataset into training and <b>validation</b> <b>data.</b> For each such split, the model is fit to the training data, and predictive accuracy is assessed using the <b>validation</b> <b>data.</b> The results are then averaged over the splits. The advantage of this method (over k-fold cross validation) is {{that the proportion of}} the training/validation split is not dependent on the number of iterations (folds). The disadvantage of this method is that some observations may never be selected in the validation subsample, whereas others may be selected more than once. In other words, validation subsets may overlap. This method also exhibits Monte Carlo variation, meaning that the results will vary if the analysis is repeated with different random splits.|$|E
5000|$|... 1. A <b>Data</b> <b>Validation</b> Certificate (DVC), {{delivering}} {{the results of}} <b>data</b> <b>validation</b> operations, performed by the DVCS.|$|R
50|$|In such CDMS the {{investigators}} directly uploads {{the data on}} CDMS and the data can then be viewed by the <b>data</b> <b>validation</b> staff. Once the data are uploaded by site, <b>data</b> <b>validation</b> team can send the electronic alerts to sites {{if there are any}} problems. Such systems eliminate paper usage in clinical trial <b>validation</b> of <b>data.</b>|$|R
5000|$|Advanced <b>data</b> <b>validation</b> and {{reconciliation}} (DVR) is an integrated approach of combining data reconciliation and <b>data</b> <b>validation</b> techniques, which {{is characterized by}} ...|$|R
50|$|Below is the C++ {{implementation}} of the US Standard Atmosphere version 1976. The calculations below 51 km are adopted from equations in Practical Meteorology and the calculations above 51 km are adopted from Atmospheric Models at Rocket and Space Technology. Some <b>validation</b> <b>data</b> is available at avs.org.|$|E
50|$|Regexes {{are useful}} {{in a wide variety}} of text {{processing}} tasks, and more generally string processing, where the data need not be textual. Common applications include data <b>validation,</b> <b>data</b> scraping (especially web scraping), data wrangling, simple parsing, the production of syntax highlighting systems, and many other tasks.|$|E
50|$|Suppose {{we have a}} {{model with}} one or more unknown parameters, and a data set to which the model can be fit (the {{training}} data set). The fitting process optimizes the model parameters to make the model fit the training data as well as possible. If we then take an independent sample of <b>validation</b> <b>data</b> from the same population as the training data, it will generally turn out that the model does not fit the <b>validation</b> <b>data</b> as well as it fits the training data. This is called overfitting , and is particularly likely to happen when the size of the training data set is small, or when the number of parameters in the model is large. Cross-validation is a way to predict the fit of a model to a hypothetical validation set when an explicit validation set is not available.|$|E
5000|$|... 6. {{check the}} {{validity}} of its own signing key and certificate before delivering <b>data</b> <b>validation</b> certificates and MUST not deliver <b>data</b> <b>validation</b> certificate in case of failure.|$|R
5000|$|Form: form <b>validation,</b> form <b>data</b> is {{retrieved}} {{and saved}} back to database automatically ...|$|R
40|$|A PIV <b>data</b> <b>validation</b> and {{post-processing}} {{software package}} {{was developed to}} provide semi-automated <b>data</b> <b>validation</b> and <b>data</b> reduction capabilities for Particle Image Velocimetry data sets. The software provides three primary capabilities including (1) removal of spurious vector data, (2) filtering, smoothing, and interpolating of PIV data, and (3) calculations of out-of-plane vorticity, ensemble statistics, and turbulence statistics information. The software runs on an IBM PC/AT host computer working either under Microsoft Windows 3. 1 or Windows 95 operating systems...|$|R
50|$|Probes are {{published}} on the Portal along with a highlight of the probe’s <b>validation</b> <b>data,</b> {{and it takes a}} few weeks for the SAB ratings (denoted with 0 to 4 stars) to emerge. Probe pages also contain SAB comments and links to related databases or sources (e.g., journals) where additional information can be found. Probes must receive an average of 3 stars to earn an endorsement from the Portal.|$|E
50|$|One natural regularization {{parameter}} is {{the number}} of gradient boosting iterations M (i.e. the number of trees in the model when the base learner is a decision tree). Increasing M reduces the error on training set, but setting it too high may lead to overfitting. An optimal value of M is often selected by monitoring prediction error on a separate <b>validation</b> <b>data</b> set. Besides controlling M, several other regularization techniques are used.|$|E
50|$|In {{this stage}} {{data from the}} {{development}} phase are gathered and analyzed to define the commercial manufacturing process. By understanding the commercial process a framework for quality specifications can be established and used {{as the foundation of}} a control strategy. Process design is the first of three stages of process <b>validation.</b> <b>Data</b> from the development phase is gathered and analyzed to understand end-to-end system processes. These data are used to establish benchmarks for quality and production control.|$|E
40|$|AbstractIn {{order to}} solve {{problems}} existing in household travel survey, such as tedious designing work of survey forms, single survey method, single function, poor pertinence of existing data statistical analysis software, and the difficulty to examine <b>data</b> <b>validation,</b> a survey <b>data</b> statistical analysis software named OD Star had been developed for engineering. The OD Star sets a series functions of independent designing and generating of the questionnaire, <b>data</b> entry, <b>data</b> <b>validation,</b> statistical analysis, graphic charts generation and engineering management. The main improved points of the software lie in highly integrated function, key technologies of statistical analysis, examination of <b>data</b> <b>validation</b> and so on...|$|R
50|$|Some notable {{features}} of JOSM are importing GPX files (GPS tracks), working with aerial imagery (including WMS, TMS and WMTS protocols), support for multiple cartographic projections, layers, relations editing, <b>data</b> <b>validation</b> tools, <b>data</b> filtering, offline work, presets and rendering styles. JOSM provides more than 200 keyboard shortcuts for the core functions.|$|R
50|$|A <b>Data</b> <b>Validation</b> and Certification Server (DVCS) is a Trusted Third Party (TTP) {{providing}} <b>data</b> <b>validation</b> services, asserting correctness of digitally signed documents, {{validity of}} public key certificates, and possession or existence of data.|$|R
50|$|Many {{programs}} {{attempt to}} verify or validate licensing keys over the Internet {{by establishing a}} session with a licensing application of the software publisher. Advanced keygens bypass this mechanism, and include additional features for key verification, for example by generating the <b>validation</b> <b>data</b> which would otherwise be returned by an activation server. If the software offers phone activation then the keygen could generate the correct activation code to finish activation. Another method {{that has been used}} is activation server emulation, this patches the program memory to use the keygen as activation server.|$|E
50|$|The {{quality of}} the {{training}} data {{is essential for the}} evolution of good solutions. A good training set should be representative of the problem at hand and also well-balanced, otherwise the algorithm might get stuck at some local optimum. In addition, {{it is also important to}} avoid using unnecessarily large datasets for training as this will slow things down unnecessarily. A good rule of thumb is to choose enough records for training to enable a good generalization in the <b>validation</b> <b>data</b> and leave the remaining records for validation and testing.|$|E
5000|$|Where ki is {{the number}} of neighbors of gene i, zi is the {{normalized}} z-score and σi is a binary variable ( [...] i.e 1 means upregulated upon activation and -1 means downregulated). This step is to estimate the activation level, in which sw AV is the activity score. A linear regression model was then applied to estimate the pathway activation levels. Thus, tij and pij denote the t-statistics and p-value associated with, whereas p<0.05 indicates a significance.To assess the consistency in a <b>validation</b> <b>data</b> set D, the performance measure Vij is denoted: ...|$|E
50|$|<b>Data</b> <b>Validation</b> and Certification Server (DVCS) is {{a public}} key {{infrastructure}} or PKI service providing <b>data</b> <b>validation</b> services, asserting correctness of digitally signed documents, validity of public key certificates and possession or existence of data.|$|R
50|$|The data in CDMS {{are then}} {{transferred}} for the <b>data</b> <b>validation.</b> Also, in these systems during <b>validation</b> the <b>data</b> clarification from sites are done through paper forms, which are printed {{with the problem}} description {{and sent to the}} investigator site and the site responds by answering on forms and mailing them back.|$|R
40|$|This paper {{addresses}} {{two important}} components of our knowledge-based system, VIE-VENT, a monitoring and therapy planning system for artificially ventilated newborn infants: <b>data</b> <b>validation</b> and <b>data</b> abstraction. VIE-VENT is {{specifically designed for}} practical use under real-time constraints in Neonatal Intensive Care Units (NICUs). Monitoring includes observing and guiding {{the behavior of a}} system. We concentrate on the ini [...] ...|$|R
50|$|Account Validation Levels - The firm may {{establish}} different {{criteria for}} exporting <b>validation</b> <b>data,</b> importing cost transactions, and modifying or correcting client account numbers. Allowing different criteria {{at different points}} in the processing and exchange of data provides a greater degree of flexibility. For example, new or pending accounts may be extracted from the billing system and sent to external cost recovery system(s) so that costs incurred for those accounts may be pre-identified. However, cost transactions for the new or pending accounts {{may not be able to}} get loaded into accounts receivable until they are formally added to the accounting database (i.e. after a contract is signed).|$|E
50|$|Since {{experts are}} invoked when {{quantities}} of interest are uncertain, {{the goal of}} structured expert judgment is a defensible quantification of uncertainty. Confronted with uncertainty, society at large will always harken to prophets, oracles, pundits, blue ribbon panels, crowd wisdom reputed to have performed well in the past. Scientists and engineers, in contrast, are typically averse to any methodology which eschews empirical validation. Most invocations of expert judgment do not attempt any form of validation, as if the predicate “expert” were validation enough. The classical model’s emphasis on validation is its distinguishing feature. Virtually all <b>validation</b> <b>data</b> with real experts and real applications (as opposed to academic exercises) has been generated by practitioners with the classical model.|$|E
50|$|Advanced {{electronic}} signature - an {{electronic signature}} is considered advanced if it meets certain requirements. It provides unique identifying information that links {{it to its}} signatory. The signatory has sole control of the data used to create the electronic signature. It must be capable of identifying if the data accompanying the message has been tampered with after being signed. If the signed data has changed, the signature is marked invalid. Certificate for electronic signature - electronic proof that confirms {{the identity of the}} signatory and links the electronic signature <b>validation</b> <b>data</b> to that person. Advanced electronic signatures can be technically implemented, following the XAdES, PAdES, CAdES or ASiC Baseline Profile (Associated Signature Containers) standard for digital signatures, specified by the ETSI.|$|E
40|$|In {{the present}} {{information}} age, with the invent of new packages and programming languages, there is need of migrating {{the data from}} one platform to another as versions are changing, to perform quality assurance of any migrated data is tedious task which require more work force, time and quality checks. To perform efficient data migration process, legacy data should be mapped to new system by considering extraction and loading entities. The new system should handle all the data formats, the good design of data migration process should take minimal time for extraction and more time for loading. Post loading into new system results in verification and <b>validation</b> of <b>data</b> accurately, by comparing with the benchmark values and derive accuracy of data migration process. The manual <b>data</b> <b>validation</b> and verification process is time consuming and not accurate so automated <b>data</b> <b>validation</b> improve <b>data</b> quality in less time, cost and attaining good data quality, Author‟s emphasis on Automation of Data migration process for quality and security across industries...|$|R
50|$|If {{the request}} is valid, the DVCS {{performs}} all necessary verification steps, and generates a <b>Data</b> <b>Validation</b> Certificate(DVC), and sends a response message containing the DVC {{back to the}} requestor. The <b>Data</b> <b>Validation</b> Certificate {{is formed as a}} signed document (CMS Signed Data).|$|R
40|$|Abstract—To {{reduce the}} {{deficiency}} {{of the traditional}} methods of <b>data</b> <b>validation</b> in web applications, research and analysis of Struts framework and AJAX technology are carried out, and a dynamic, real time and interactive web <b>data</b> <b>validation</b> model based on asynchronous communication is proposed. This model combines {{the functions of the}} flexible and dynamic presentation in AJAX and the controller in Struts. The architecture and implementation method of this model are elaborated in detail. Index Terms—Struts, AJAX, asynchronous communication, Web <b>data</b> <b>validation</b> mode...|$|R
